title,date,keywords,abstract,multilingual_abstract
ResNet과 SIFT를 이용한 특허 도면의 유사도 평가 및  검색 연구,2024,"['특허 도면', '이진 이미지', '분류 및 검색', 'ResNet-50', 'SIFT', '유사도 비교', 'Patent drawings', 'binary images', 'classification and retrieval', 'ResNet-50', 'SIFT', 'similarity Comparison']","특허 문헌의 유사성 평가 및 검색 연구는 특허 문헌의 효율적인 관리뿐 아니라 산업 및 기술 분야에서효율적이고 빠른 정보 수집을 위해 중요한 주제로 다뤄지고 있다. 특히 특허 도면은 산업 기술의 발전과 혁신의 결과물을 시각적으로 표현해왔으나 지금까지 텍스트에 비해 중요하게 다뤄지지 못한 측면이 있다.본 연구는 효과적인 특허 도면의 검색을 위해 딥러닝의 대표 모델 ResNet-50과 전통적인 컴퓨터비전알고리즘 SIFT를 이용하여 유사성을 평가하는 연구이다. 먼저 시각적 유형의 유사성을 평가하기 위해총 10,827개의 특허도면을 이용한 유형 분류 실험을 진행했으며 분류성능은 95%가 넘는 Accuracy를나타냈다. 두 번째로 기술도면 5,000개를 사용하여 ResNet과 SIFT를 이용한 검색 실험을 진행하여 각모델의 유사성을 평가하는 특징을 살펴보았다. 마지막으로 원본 데이터 50개와 원본 데이터를 다양한형태로 증강한 데이터 4,800개를 이용하여 편집 유형별로 검색 및 매칭한 결과, ResNet은 72.54%, SIFT는 86.71%의 평균적인 매칭 결과를 나타냈다.연구 수행 결과, 이미지 전체의 정보를 이용하여 유사도를 비교하는 ResNet-50과 달리 SIFT는 이미지내 특징점 등 속성 정보를 이용하여 유사도를 판단하므로 시각적으로 유사한 이미지를 찾는 일에는ResNet이, 같은 이미지를 찾는 일에는 SIFT가 더 강점이 있는 것으로 평가할 수 있다.","The study of the similarity evaluation and retrieval of patent documents is critical not only for the efficient management of patent literature, but also for the rapid and effective collection of information in industrial and technological fields. Patent drawings visually represent the outcomes of technological advancements and innovations, but have not been given as much importance as texts in the past.This study evaluated the similarity of patent drawings for effective retrieval using the representative deep-learning model, ResNet-50, and the traditional computer vision algorithm, scale-invariant feature transform (SIFT). First, a classification experiment using 10,827 patent drawings was conducted to evaluate the similarity of the visual types, achieving a classification performance with an accuracy exceeding 95%. Second, a retrieval experiment using 5,000 technical drawings was conducted to compare the features of ResNet and SIFT based on their similarity. Finally, the retrieval and matching performances of ResNet and SIFT were evaluated using 50 original data samples and 4,800 augmented data samples created by various forms of editing. ResNet demonstrated an average matching performance of 72.54%, whereas SIFT achieved an average matching performance of 86.71%.The findings reveal that, unlike ResNet-50, which compares similarity using the entire image information, SIFT evaluates similarity based on attribute information, such as key points within the image. Consequently, ResNet is advantageous for identifying visually similar images, whereas SIFT excels in identifying identical images."
ResNet50 알고리즘을 활용한 백혈구 이미지 분석 연구,2024,"['백혈구', '인공지능', '딥러닝', 'ResNet-50', '이미지 분석', 'White blood cells', 'Artificial Intelligence', 'Deep Learning', 'ResNet-50', 'Image analysis']","백혈구 연구는 면역학, 세포생물학, 유전학 분야에서 수 세기에 걸쳐 이뤄졌으며, 백혈구의 기능, 발달, 면역 반응, 건강 영향을 이해하는데 기여하였다. 이를 토대로 감염병, 암, 면역 질환의 치료와 예방 방법이 발전하였다. 특히 림프구와 T세포 연구는 백신 개발과 암 치료에 혁명적인 영향을 미쳤다. 백혈구 연구는 혈액 관련 질병 예방과 치료, 호중구, 단핵구, 호산구 등의 세포 연구로도 자가면역 질환, 알레르기, 염증성 질환의 치료를 개선하였다. 그러나 백혈구의 정확한 기능, 상호작용, 면역 조절 연구는 아직 미흡하다. 최근 디지털 기술 발전으로 대규모 이미지 데이터 생성이 가능해졌으며, AI 알고리즘을 훈련하는 데 활용되고 있다. 특히 딥러닝 기술은 이미지 분석 분야에서 중요한 역할을 하며, 백혈구 연구에도 응용되고 있다. 이 연구는 ResNet-50 모델의 실제 응용 가능성을 확인하고자 하였으며, 모델의 성능을 확인한 결과 ResNet-50모델에 Epoch를 30으로 설정했을 때 성능이 균형적이고 안정적으로 나타나는 것을 확인하였다.","White blood cell research spans centuries in the fields of immunology, cell biology, and genetics and has contributed to our understanding of white blood cell function, development, immune responses, and health effects. Based on this, treatment and prevention methods for infectious diseases, cancer, and immune diseases have been developed. In particular, lymphocyte and T cell research has had a revolutionary impact on vaccine development and cancer treatment. White blood cell research has been used to prevent and treat blood-related diseases, and research on cells such as neutrophils, monocytes, and eosinophils has also improved the treatment of autoimmune diseases, allergies, and inflammatory diseases. However, research on the exact function, interaction, and immune regulation of white blood cells is still insufficient. Recent advances in digital technology have made it possible to generate large-scale image data, which is being used to train AI algorithms. In particular, deep learning technology plays an important role in the field of image analysis and is also applied to white blood cell research. This study sought to confirm the actual applicability of the ResNet-50 model, and as a result of checking the model's performance, it was confirmed that when Epoch was set to 30 in the ResNet-50 model, the performance was balanced and stable."
웹사이트 게시글 및 상품 리뷰 검색 기능 향상: ResNet-Transformer 모델을 이용한 BM25 랭킹 알고리즘 성능 개선,2024,"['레즈넷', '트랜스포머', '레즈넷-트랜스포머', '웹사이트 검색', '랭킹 알고리즘', 'BM25', 'ResNet', 'Transformer', 'ResNet-Transformer', 'Website search', 'BM25', 'Rangking algorithm']",,"This paper proposes a method to improve the search functionality for website posts and product reviews by using a ResNet-Transformer model in conjunction with the BM25 ranking algorithm. BM25 is a widely used algorithm in text-based search that ranks documents by evaluating their relevance to user queries. However, it has limitations in capturing local features of words and understanding the context of a sentences. To address these issues, this study applies a classification approach that combines the ResNet model, which excels at extracting local features, with the Transformer model, known for its strong contextual understanding, as weights for BM25. Experimental results demonstrate that the proposed method improves the nDCG metric by 9.38% and the aP@5 metric by 11.82% compared to BM25 alone. This suggests that implementing this method in search engines across various websites can provide more accurate results for post and review searches."
가상공간에서 ResNet을 활용한 공간감 증대를 통한 시각 및 음향 환경 향상에 관한 연구,2024,"['Virtual Space', 'YOLOv5', 'ResNet', 'Indoor and Outdoor', 'Accuracy', 'Immersive', '가상 공간', 'YOLOv5', 'ResNet', '실내와 실외', '정확도', '몰입감']","가상공간 환경에서 사용자의 몰입과 경험을 극대화하기 위해서는 시각적, 음향적 효과가 핵심적인 역할을 해야 한다. 하지만 온라인상에서 현장감이 부족하거나, 음질이 떨어져 시각 및 음향의 질적 수준이 오프라인에 비해 떨어진다는 의견이 존재한다. 가상 공간 내에서 실내외를 정확히 식별하여 사용자의 움직임에 따라 상호작용을 통해 현실감을 높이기 위해 가상환경 시스템에서 현실적인 효과를 제공할 필요가 있다. 이를 위해서는 가상공간에서의 실내외 환경 식별 정확도를 높이기 위한 연구가 진행되어야 한다. COCO DataSet 내 객체 수에 따른 실내외 식별 방법과 기존 알고리즘을 수정한 실내외 식별 방법에 따른 실내외 식별 방법을 통해 결과값에 대한 정확도를 향상할 수 있다. 이에 본 연구에서는 객체 수 및 기존 알고리즘 수정을 통한 가상공간 특성의 정확도 문제를 해결한다. 이는 향후 객체의 특징을 학습하여 데이터화 시키고 사용 DataSet 이외의 객체들을 식별할 수 있게 하여 새로운 공간 구축을 위한 변화의 방향을 제시하고자 한다.","To maximize the user immersion and experience in a virtual space environment, visual and acoustic effects must play a pivotal role. However, there are opinions that the quality level of visuals and sound is lower than offline due to a lack of realism online or poor sound quality. It is necessary to provide realistic effects in a virtual environment system to accurately identify the indoor and outdoor space within the virtual space and increase the sense of reality through interaction according to the user movement. To this end, research should be conducted to increase the accuracy of identifying the indoor and outdoor environment in the virtual space. The accuracy of the obtained results can be enhanced by utilizing indoor and outdoor identification methods based on the object count within the Common Objects in Context (COCO) DataSet and the existing algorithm. Therefore, this study solves the problem of the accuracy of virtual space characteristics through the number of objects and the revision of the existing algorithm. This aims to suggest the direction of change for the construction of a new space by learning the characteristics of objects in the future, making them data, and allowing the identification of objects other than the DataSet used."
음성 감정인식의 정확도 향상을 위한 DA-S기법을 활용한 ResNet 모델,2024,"['ResNet', 'Speech Emotion Recognition', 'Deep Learning', 'Data Augmentation']",,"In recent years, there has been active research on emotion recognition based on speech data that can be utilized in various platforms. Despite the significant progress in emotion recognition research based on the Korean language within the country, the main issue remains the lack of Korean language databases. Due to the absence of such data, there are cases where overfitting issues arise in models proposed in previous studies. Therefore, this study proposes a ResNet model using the data augmentation with saturation (DA-S) method to improve the performance of speech emotion recognition using the existing model. In this study, the number of data was increased from 5,596 to 11,192 by applying DA-S with the AI-HUB database. Consequently, the proposed model successfully addressed the overfitting issue, resulting in a 31.76% improvement in the accuracy of speech emotion recognition.Furthermore, experiments were conducted using a total of 11,192 data samples, including both the original data and the data with DA-S applied to demonstrate the effects of data augmentation techniques in transforming and expanding data, as well as performance improvements due to the increase in data volume. The result showed that there was a 23.4% improvement when DA-S was applied."
수면 소리 데이터를 활용한 1D ResNet 딥러닝 모델 기반 수면무호흡증 탐지 연구,2024,"['Sleep apnea', 'Polysomnography', 'Sleep sound', 'Deep learning', 'MFCC']","수면무호흡증(Sleep Apnea)은 전 세계적으로 심각한 건강 문제를 일으키는 질환으로, 주로 수면 중 상부 기도의 폐쇄로 인해 발생한다. 현재 수면무호흡증의 표준 진단 방법인 수면다원검사(Polysomnography, PSG)는 높은 비용과 복잡성 등의 한계가 있다. 본 연구에서는 수면 중 발생하는 소리 데이터를 활용하여 수면무호흡증을 탐지하는 비침습적 방법을 제안하였다. 소리 데이터는 MFCC(Mel-Frequency Cepstral Coefficients)로 특징을 추출하였으며, 1D CNN 기반의 ResNet(Residual Network) 모델을 통해 분류를 수행하였다. 실험 결과, 제안된 모델은 5겹 교차 검증을 통해 평균 정확도(Accuracy) 97.8%, 재현율(Recall) 97.7%, 민간도(Precision) 97.9%, AUC 0.978의 성능을 달성하였다. 향후 연구에서는 데이터셋을 확장하고 다양한 딥러닝 모델을 실험하여 모델의 성능을 더욱 향상시킬 계획이다. 본 연구는 수면무호흡증 탐지의 정확성을 높이고, 효율적인 건강관리 시스템 구축에 기여할 수 있을 것으로 기대한다.","Sleep Apnea is a serious global health issue caused primarily by the obstruction of the upper airway during sleep. The current standard diagnostic method for sleep apnea, Polysomnography (PSG), has limitations such as high cost and complexity. In this study, we propose a non-invasive method to detect sleep apnea by utilizing sound data recorded during sleep. The sound data features were extracted using Mel-Frequency Cepstral Coefficients (MFCC), and classification was performed using a 1D CNN-based ResNet (Residual Network) model. The experimental results show that the proposed model achieved an average accuracy of 97.8%, a recall of 97.7%, a precision of 97.9%, and an AUC of 0.978 through 5-fold cross-validation. Future research will focus on expanding the dataset and experimenting with various deep learning models to further improve the model's performance. This study is expected to contribute to improving the accuracy of sleep apnea detection and the development of an efficient healthcare management system."
ONNX 기반 런타임 성능 분석: YOLO와 ResNet,2024,"['ONNX', '딥 모델', 'Inference Runtime', '객체 감지', '이미지 분류', 'Deep Model', 'Object Detection', 'Image Classification']",,"In the field of computer vision, models such as You Look Only Once (YOLO) and ResNet are widely used due to their real-time performance and high accuracy. However, to apply these models in real-world environments, factors such as runtime compatibility, memory usage, computing resources, and real-time conditions must be considered. This study compares the characteristics of three deep model runtimes: ONNX Runtime, TensorRT, and OpenCV DNN, and analyzes their performance on two models. The aim of this paper is to provide criteria for runtime selection for practical applications. The experiments compare runtimes based on the evaluation metrics of time, memory usage, and accuracy for vehicle license plate recognition and classification tasks. The experimental results show that ONNX Runtime excels in complex object detection performance, OpenCV DNN is suitable for environments with limited memory, and TensorRT offers superior execution speed for complex models."
잔차블록을 적용한 향상된 ResNet VAE-GAN  3D 객체 생성 시스템,2024,"['Voxel', 'Mesh', '3D object generation', 'VAE', 'GAN', 'Residual block']","3D 모델링은 게임, AR, VR, 메타버스 등 다양한 분야에서 활용되고 있다. 최근 컴퓨터 하드웨어의 성능 향상으로 3D 공간에서의 시각화와 연산이 가속화되고 있으며, GAN 기술의 진보로 3D 객체를 생성하는 방법이 연구되고 있다. GAN 기반 네트워크는 이미지를 입력으로 받아 복셀(Voxel)을 생성하고, Wasserstein 손실 함수 도입 및 그래디언트 패널티 적용을 통해 학습하는 3D-VAE-IWGAN 방식을 제안하였다. GAN은 훈련에 포함되지 않은 여러 모델을 생성할 수 있지만, 아티팩트가 생기는 문제가 있다. 또 다른 방식으로는 2D에서 지도 학습하고 3D에서는 비지도 학습을 통해 3D 레이블 생성 비용을 줄인 DIB-R과 같은 네트워크가 제안되었다. DIB-R은 아티팩트를 줄일 수 있지만, 오토인코더 기반 네트워크로는 다양한 모델을 생성하기 어렵다. 본 논문은 3D-VAE-IWGAN에서 성능을 높인 Variational Autoencoder(VAE)와 Generative Adversarial Network(GAN)을 결합한 VAE-GAN에 잔차블록(Residual block)을 적용하는 방법을 제안하며 이미지 생성자와 판별자에 더 많은 특징을 추출하여 고품질 이미지 생성 및 잠재 공간 보간 성능이 향상된 시스템을 제안한다. 기존 네트워크와 비교한 결과는 의자 클래스에서 137.15로 116.33% 더 나은 결과를 보였고 침대에서도 137.24로 130.4%로 향상된 결과를 보였다.","3D modeling is used in various fields such as games, AR, VR, and metaverse. Recently, visualization and computation in 3D space are accelerating due to improvements in the performance of computer hardware, and methods for generating 3D objects are being researched due to advances in GAN technology. The GAN-based network proposed a 3D-VAE-IWGAN method that receives images as input, generates voxels, and learns by introducing a Wasserstein loss function and applying a gradient penalty. GAN can generate multiple models that are not included in training, but there is a problem with artifacts occurring. As another method, a network such as DIB-R, which reduces the cost of 3D label generation through supervised learning in 2D and unsupervised learning in 3D, has been proposed. DIB-R can reduce artifacts, but it is difficult to generate diverse models with an autoencoder-based network. This paper proposes a method of applying residual blocks to VAE-GAN, which combines Variational Autoencoder (VAE) and Generative Adversarial Network (GAN), which improves performance in 3D-VAE-IWGAN, and further improves the image generator and discriminator. We propose a system that extracts many features to generate high-quality images and improve latent space interpolation performance.  The results of index compared to the existing network showed a better result of 116.33% at 137.15 in the chair class, and an improvement of 130.4% at 137.24 in the bed class."
"고추 작물의 정밀 질병 진단을 위한 딥러닝 모델 통합 연구: YOLOv8, ResNet50, Faster R-CNN의 성능 분석",2024,"['스마트 농업', '작물 질병 진단', '욜로v8', '레스넷50', '딥러닝 모델 비교', 'Smart Agriculture', 'Crop Disease Diagnosis', 'YOLOv8', 'ResNet50', 'Deep Learning Model Comparison']","본 연구의 목적은 YOLOv8, ResNet50, Faster R-CNN 모델을 활용하여 고추 작물의 질병을 진단하고, 각 모델의 성능을 비교하는 것이다. 첫 번째 모델은 YOLOv8을 사용하여 질병을 진단하였고, 두 번째 모델은 ResNet50을 단독으로 사용하였다. 세 번째 모델은 YOLOv8과 ResNet50을 결합하여 질병을 진단하였으며, 네 번째 모델은 Faster R-CNN을 사용하여 질병을 진단하였다. 각 모델의 성능은 정확도, 정밀도, 재현율, F1-Score 지표로 평가된다. 연구 결과, YOLOv8과 ResNet50을 결합한 모델이 가장 높은 성능을 보였으며, YOLOv8 단독모델도 높은 성능을 나타냈다.","The purpose of this study is to diagnose diseases in pepper crops using YOLOv8, ResNet50, and Faster R-CNN models and compare their performance. The first model utilizes YOLOv8 for disease diagnosis, the second model uses ResNet50 alone, the third model combines YOLOv8 and ResNet50, and the fourth model uses Faster R-CNN. The performance of each model was evaluated using metrics such as accuracy, precision, recall, and F1-Score. The results show that the combined YOLOv8 and ResNet50 model achieved the highest performance, while the YOLOv8 standalone model also demonstrated high performance."
핵의학 갑상샘 팬텀 영상에서 보간법이 초고분해능 ResNet 모델성능에 미치는 영향에 관한 연구,2024,"['핵의학', '보간법', 'ResNet모델', '정량분석', 'Nuclear medicine', 'Interpolation techniques', 'ResNet model', 'Quantitative analysis']","핵의학 영상은 다양한 보간법을 적용하여 획득 영상의 축소 및 확대를 통하여 영상의 질을 개선한다. 또한, 입력 데이터셋과 정답 데이터셋 간의 특징을 추출하는 딥러닝 알고리즘 기반의 영상처리 기술도 영상의 질 향상에 크게 작용하고 있다. 이에 본 연구의 목적은 갑상샘 팬텀 영상을 이용하여 다양한 보간 방법을 적용하여 획득한 입력 데이터에 따른 딥러닝 알고리즘의 성능을 정량적 분석 인자를 이용하여 평가하고자 한다. 256 × 256 크기의 매트릭스로 갑상샘 팬텀 영상을 99mTcO₄- 37M㏃로 총 200장을 각각 1분씩 획득하여 정답 영상을 생성하였다. 최근접 이웃, 선형, 이차, 삼차, 오차 보간 방법을 다운샘플링과 업샘플링에서 적용하여 입력 데이터셋을 구축하였다. 학습률 0.0001, 학습횟수 300회로 설정된 초고분해능 잔차학습네트워크 (super resolution residual network, SRResNet) 구조를 구현하였으며, 데이터셋은 8:1:1 비율로 훈련, 검증 및 테스트 세트로 분할하여 학습하였다. 생성된 출력 영상은 변동계수 (coefficient of variationi, COV) 및 신호대잡음비 (contrast to noise ratio, CNR)를 사용하여 분석하였다. 그 결과 SRResNet 네트워크는 저분해능 영상을 생성하는데 삼차 보간법을 적용했을 때 가장 우수한 COV 및 CNR 값을 보였다. 그러므로 본 연구는 보간법 기반 딥러닝 알고리즘을 적용한 핵의학 갑상샘 영상의 질화질 개선 측면에서 입력 데이터로써의 적절한 보간법 적용이 생성 영상에 미치는 영상이 크다는 것을 확인하였으며, 영상의 질 향상을 위한 검사에 따른 적절한 보간법이 설정되어야 함을 증명하였다.","The nuclear medicine imaging can improve the image quality through the application of various interpolation techniques. Additionally, deep learning algorithm, which perform feature extraction between input and label datasets, is widely utilized to improve image quality in nuclear medicine. Thus, the purpose of this study was to confirm the performance of deep learning algorithm according to applied various interpolation methods as input data using the thyroid phantom images. A total of 200 thyroid phantom images, each sized 256 × 256, were acquired at an activity of 37 M㏃ for 1 minute to generate the label images. Interpolation methods including nearest neighbor, bilinear, biquadratic, bicubic, biquartic, and biquintic were applied during both downsampling and upsampling processes. The super-resolution residual network (SRResNet) architecture was implemented with a learning rate of 0.0001 and 300 epochs, using an 8:1:1 ratio for train, validation, and test sets, respectively. The generated output images analyzed using coefficient of variation (COV) and contrast to noise ratio (CNR). Consequently, the SRResNet algorithm, which used low-resolution images generated with the bicubic interpolation method, showed the highest performance. This study demonstrates the importance of selecting appropriate interpolation methods for generating input images to improve the accuracy of the SRResNet algorithm in nuclear medicine thyroid imaging and even in other medical fields for diagnosis."
Similarity Analysis Model with 6CH ResNet Structure,2024,"['Convolutional Neural Network (CNN)', 'Image Similarity', 'Large Waste']",,"Largescale waste similarity analysis is crucial for automating waste management on a large scale. It involvesconfirming the match between waste discharged from homes and that collected by agencies, which is essentialfor a stable automated system. This paper compares feature extraction methods for similarity measurement,including the scaleinvariant feature transform (SIFT) algorithm with added HSV color features, convolutionalneural networkbased encoders, and a modified 6channel (6CH) ResNet for endtoend learning. The resultsdemonstrate that the 6CH ResNet achieves up to 4.9% higher accuracy than both the basic SIFT method andencoders, as well as the SIFT algorithm with HSV color features. Implementing the 6CH ResNet in automatedwaste management systems can enhance object similarity measurement while using fewer computing resources."
Online to Offline 상점을 위한 한글 메뉴판 인식 : 어텐션 메커니즘을 적용한 VGG-ResNet 융합 모델,2024,"['Online to Offline(O2O)', 'OCR(Optical Character Recognition)', 'Text Recognition', 'Attention Mechanism', 'Korean Menu']",,
GAN기반의 Semi Supervised Learning을 활용한 이미지 생성 및 분류,2024,"['인공지능', 'GAN', 'ResNet50', 'Image Classification', 'Neural Network', 'Artificial intelligence', 'Generative Adversarial Network', 'ResNet50', 'Image Classification', 'Neural Network']",,"This study deals with a method of combining image generation using Semi Supervised Learning based on GAN (Generative Adversarial Network) and image classification using ResNet50. Through this, a new approach was proposed to obtain more accurate and diverse results by integrating image generation and classification.The generator and discriminator are trained to distinguish generated images from actual images, and image classification is performed using ResNet50. In the experimental results, it was confirmed that the quality of the generated images changes depending on the epoch, and through this, we aim to improve the accuracy of industrial accident prediction. In addition, we would like to present an efficient method to improve the quality of image generation and increase the accuracy of image classification through the combination of GAN and ResNet50."
CNN 기반의 전이학습과 데이터 증강을 통한  화재 영상 분류 개선,2024,"['산불 감지', 'CNN', '전이학습', '데이터 증강', 'ResNet50', 'Wildfire Detection', 'Convolutional Neural Networks', 'Transfer Learning', 'Data Augmentation', 'ResNet50']","전 세계는 기상이변의 영향으로 산불 등 자연 재해가 끊이지 않고 있으며 이로 인한 사회 안전에 심각한 위협이 되고 있다. 특히 대한민국 동해안 지역은 매년 산불 피해로 인한 막대한 재산 피해가 발생하고 있다. 초기 화재 감지 모델의 필수적인 개발은 훈련을 위한 제한된 이미지 데이터와 관련된 도전을 극복해야 하며, 이는 과적합의 위험을 증가시킨다. 이를 해결하기 위해, 랜덤하게 50% 범위까지 회전, 랜덤하게 20% 범위까지 축소 및 확대, 랜덤하게 50%까지 가로 및 세로 뒤집기를 적용하였다. 성능 평가에서 6층 신경망이 7층 신경망보다 더 우수한 성능을 보였으며, 이는 제한된 데이터셋을 가지고 계층 수를 늘리는 것이 바람직하지 않음을 의미한다. 또한, 화재 이미지 분류를 위한 딥러닝 기반 CNN 모델과 ResNet50 전이 학습 모델의 평가는 전이 학습의 우수한 효과를 확인하였다. 이러한 발견은 초기 화재 감지 모델 개발에 도움이 될 것으로 기대하며, 미래 시스템을 위한 귀중한 통찰력을 제공할 것이다.","Natural disasters, such as wildfires, due to climate change are a constant and serious threat to the world and societal safety. Every year, the eastern coastal region of South Korea experiences significant property damage due to wildfires. The imperative development of early fire detection models necessitates overcoming challenges associated with limited image data for training, which elevates the risk of overfitting. To address this, data augmentation techniques, including random rotation (up to 50%), random scaling (up to 20%), and random horizontal and vertical flipping (up to 50%), were employed to augment the training dataset. Performance evaluation indicated that the 6-layer neural network outperformed its 7-layer counterpart, highlighting the impracticality of increasing layer count with a limited dataset. Furthermore, an assessment of deep learning-based CNN models and ResNet50 transfer learning models for fire image classification underscored the superior efficacy of transfer learning. These findings hold promise for advancing early fire detection model development, offering valuable insights for future systems."
EEG 스펙트로그램 이미지를 기반으로 한 행동 분류 모델의 2D-CNN,2024,"['합성곱 신경망', '뇌파', '스펙트로그램', 'BCI', '딥러닝', 'ResNet', 'Convolutional Neural Network', 'EEG', 'Spectrogram', 'BCI', 'Deep Learning', 'ResNet']","본 논문은 뇌의 전기적 활동을 기록하는 뇌파 데이터를 이용한 행동 분류 모델을 개발하는 데 초점을 맞춰 진행했다. 뇌파 신호의 잡음을 해결하기 위해 Notch filter(NF)와 Band-pass filter(BF)의 전처리 기법을 사용했다. 이후 신호를 단시간 Fourier 변환(STFT)을 이용하여 스펙트로그램으로 변환하여 시간과 주파수 활성화를 시각화했다. 스펙트로그램 이미지를 10초, 15초, 30초 간격으로 나누어 저장하여 시간적 유의성을 판단했다. 이 스펙트로그램을 기반으로 2D-CNN의 일종인 사전 학습된 ResNet18을 이용하여 모델을 구성했다. 실험 결과, 30초 간격으로 전처리를 적용한 데이터에서 NF와 Band-pass filter 모두 82.89%의 정확도를 보였으며, 이는 시간적 유의성을 가지는 것으로 나타났다. 이 연구는 뇌파 데이터의 효과적인 처리와 분류 성능 향상을 위한 전처리 방법의 중요성을 강조한다.","This paper focused on developing a behavior classification model using EEG data that records the electrical activity of the brain. To solve the noise of EEG signals, preprocessing techniques such as Notch filter(NF) and Band-pass filter(BF) were used. After that, the signal was converted into a spectrogram using Short Time Fourier Transform(STFT) to visualize time and frequency activation. The temporal significance was determined by dividing the spectrogram image into 10-second, 15-second, and 30-second intervals and storing it. Based on this spectrogram, a model was constructed using pre-trained ResNet18, a type of 2D-CNN. As a result of the experiment, both the NF and the BF showed accuracy of 82.89% in the data to which the preprocessing was applied at 30-second intervals, which was found to have temporal significance. This study emphasizes the importance of preprocessing methods for improving the effective processing and classification performance of EEG data."
주거 및 공공장소 이상행동탐지를 위한 서비스 설계,2024,"['Anomaly Detection', 'AI Learning Data', 'ResNet50', '3D-CNN', 'GridCV', 'GRU', '이상행동 탐지', 'AI 학습 데이터', 'ResNet50', '3D-CNN', 'GridCV', 'GRU']","본 연구에서는 보안과 범죄 예방 강화의 하나로 공공 CCTV와 보안 카메라의 영상 데이터를 사용하여주거 및 공용 공간에서 이상행동을 탐지하기 위한 AI 학습 데이터 세트의 구축과 이를 활용한 모델을 시범 개발하였다. AI 학습 데이터 세트와 모델은 민간 기업의 AI 기술 발전과 AI 프로젝트 개발을 촉진하기 위해 설계되었다.데이터 세트 구축 시 비디오 프레임에서 특징을 추출하기 위하여 ResNet50을, JSON 파일에서 스켈레톤 포인트를처리하기 위하여 3D-CNN을 사용하여 모듈화 하였다. 이 데이터를 사전에 정의된 이상행동에 따라 Labeling 하였다. 또한 GridCV를 사용하여 SVM 분류기와 비디오 시퀀스 처리를 위한 GRU를 활용하였다. 모델의 학습 성능평가에서는 주요 정확도(main accuracy)가 지속해서 향상되었으며, 상세 손실(detailed loss) 또한 감소하는 추세를 보였다. 이를 바탕으로 학습된 모델은 주어진 비디오 시퀀스에서 나타나는 행동의 범주를 예측할 수 있다. 본연구에서 구축된 AI 학습 데이터 세트와 모델 시범 개발로 즉각적인 이상행동 감지를 통한 범죄예방 및 범인 검거를 위해 인공지능 학습에 필요한 데이터 확보, 구축 및 배포하여 민간기업의 AI 기술 발전 및 인공지능 사업의발전을 도모하고자 이상행동 탐지 기능 개발의 실용성에 대한 귀중한 인사이트를 제공하여 공공 안전 분야에서AI 애플리케이션의 발전에 기여할 것으로 기대된다.","This study presents the construction of an AI learning dataset and the prototypical development of a model for detecting anomalous behaviors in residential and public spaces, as part of an effort to enhance security and crime prevention. The AI learning dataset and model were designed to stimulate the advancement of AI technology and the development of AI projects in private companies. During the dataset construction, ResNet50 was modularized to extract features from video frames, and 3D-CNN was used to process skeleton points from JSON files. This data was then labeled according to predefined anomalous behaviors.Furthermore, GridCV was employed to utilize the SVM classifier and GRUs for processing video sequences. The learning performance evaluation of the model demonstrated a continuous improvement in main accuracy and a decreasing trend in detailed loss.. The trained model can predict the category of behavior appearing in a given video sequence. The AI learning dataset and model prototyped in this study provide valuable insights into the practicality of developing anomaly detection functions. It is expected to contribute to the advancement of AI applications in the field of public safety by securing, constructing, and distributing data necessary for AI learning for immediate anomaly detection, crime prevention, and offender apprehension."
Implant Thread Shape Classification by Placement Site from Dental Panoramic Images Using Deep Neural Networks,2024,"['Artificial intelligence', 'Convolutional neural networks', 'Classification', 'Deep learning', 'Implant system']",,"Purpose: In this study, we aimed to classify an implant system by comparing the types of implant thread shapes shown on radiographs using various Convolutional Neural Networks (CNNs), particularly Xception, InceptionV3, ResNet50V2, and ResNet101V2. The accuracy of the CNN based on the implant site was compared.Materials and Methods: A total of 1000 radiographic images, consisting of eight types of implants, were preprocessed by resizing and CLAHE filtering, and then augmented. CNNs were trained and validated for implant thread shape prediction. Grad-CAM was used to visualize class activation maps (CAM) on the implant threads shown within the radiographic image.Results: Averaged over 10 validation folds, each model achieved an AUC of over 0.96: AUC of 0.961 (95% CI 0.952–0.970) with Xception, 0.973 (95% CI 0.966-0.980) with InceptionV3, 0.980 (95% CI 0.974-0.988) with ResNet50V2, and 0.983 (95% CI 0.975-0.992) with ResNet101V2. Accuracy was higher in the posterior region than in the anterior area in all four models. Most CAMs highlighted the implant surface where the threads were present; however, some showed responses in other areas.Conclusion: The CNN models accurately classified implants in all areas of the oral cavity according to the thread shape, using radiographic images."
콘크리트 구조물 균열 파악을 위한 분류 모델 분석,2024,"['Concrete Crack', 'Convolution Neural Network', 'Classification']",,"Purpose: The current study was used to propose a model for identifying cracks in concrete structures by comparing and analyzing various models using Convolutional Neural Networks (CNNs).Methods: Two CNN-based classification models, VGG-16 and ResNet-50, were developed, compared, and analyzed. A confusion matrix was employed as a performance indicator to evaluate their performance in individual instances.Results: The comparative analysis indicated that ResNet-50 outperformed VGG-16 in performance metrics. Additionally, the inference speed based on test data revealed a significant difference, with ResNet-50 requiring 35 seconds compared to VGG-16's 77 seconds.Conclusion: The ResNet-50 showed excellent performance in confusion matrix-based performance indicators and inference speed. It shows strong potential for practical applications in identifying concrete crack structures in real-world scenarios."
자생 매미 음성 분류를 위한 딥러닝 접근 : 주파수 변화 분석과 모델 최적화,2024,"['Artificial Intelligence', 'Deep-learning', 'CNN', 'Classification', 'Spectogram']","본 논문은 한국에서 서식하는 매미과 속인 말매미 등 12종의 국내 자생종 매미의 음성 데이터를 활용하여 딥러닝기법을 통해 매미 종을 분류하는 새로운 접근 방식을 제시하였다. 표준화된 데이터 전처리 및 시간에 따른 주파수 변화를 시각적으로 나타내는 그래픽 표현방식인 스펙트로그램(Spectrogram)의 적용을 통해 주파수 변화와 시간적 변화를동시에 시각화하여 데이터의 특성을 파악하고 활용하며 딥러닝 모델인 ResNet34, ResNet50, AlexNet 모델을 적용하였다. 드롭아웃 기법을 적용하여 과적합(Overfitting)을 방지하며, 다양한 학습률(Learning Rates)을 적용하여 모델의학습 및 검증 과정을 최적화하였다. 이러한 접근을 통해 98% 이상의 높은 정확도로 매미 종을 식별을 검증하였다. 본연구는 인공지능 기술인 CNN(Convolutional Neural Network)를 활용하여 생물 다양성 보존과 종 식별의 정확성을높이기 위해 수행하였으며, 음성 데이터 기반의 딥러닝 시스템이 생태학적 연구와 환경 모니터링에 크게 기여할 수 있음을 시사한다. 나아가 본 연구는 생태계 보존 및 관리에 중요한 도구로 활용될 수 있음을 보여주며, 인공지능 기술과 생물분류학을 결합하여 향후 생물 다양성 연구와 환경 보호를 위한 새로운 방법을 제시할 수 있다","This paper presents a novel approach to classifying cicada species by using deep learning techniques that utilize acoustic data of 12 cicada species found in Korea, including Meimuna opalifera.Standardized data preprocessing and the application of spectrograms, which visually represent frequency changes over time, were used to simultaneously visualize both frequency and temporal changes, allowing for species identification from data characteristics. Deep learning models such as ResNet34, ResNet50, and AlexNet were applied. Dropout techniques were employed to prevent overfitting, and various learning rates were applied to optimize the training and validation processes of the models. The approach successfully identified cicada species with an accuracy of over 98%. This study enhances the accuracy of species identification and conservation of biodiversity by using the artificial intelligence technology of a convolutional neural network. It suggests that deep learning systems based on acoustic data can significantly contribute to ecological research and environmental monitoring. Furthermore, this study has the potential for use as an essential tool in ecosystem conservation and management, combining AI and taxonomy to propose new methods for future biodiversity research and environmental protection."
Violent crowd flow detection from surveillance cameras using deep transfer learning-gated recurrent unit,2024,"['deep learning', 'deep transfer learning', 'video processing', 'violence detection']",,"Violence can be committed anywhere, even in crowded places. It is hence necessary to monitor human activities for public safety. Surveillance cameras can monitor surrounding activities but require human assistance to continuously monitor every incident. Automatic violence detection is needed for early warning and fast response. However, such automation is still challenging because of low video resolution and blind spots. This paper uses ResNet50v2 and the gated recurrent unit (GRU) algorithm to detect violence in the Movies, Hockey, and Crowd video datasets. Spatial features were extracted from each frame sequence of the video using a pretrained model from ResNet50V2, which was then classified using the optimal trained model on the GRU architecture. The experimental results were then compared with wavelet feature extraction methods and classification models, such as the convolutional neural network and long short-term memory. The results show that the proposed combination of ResNet50V2 and GRU is robust and delivers the best performance in terms of accuracy, recall, precision, and F1-score. The use of ResNet50V2 for feature extraction can improve model performance."
임베디드 시스템에서의 객체 탐지 네트워크의 가속을 위한  중요도 탐색 필터 가지치기 기법 연구,2024,"['Object detection', 'Network compression', 'Pruning', 'Inference time']","최근, 컴퓨터 기술이 발달하면서 CNN 기반의 객체 탐지 네트워크와 관련된 연구가 활발히 진행되고 있다. 하지만 많은 수의 CNN은 제한된 메모리와 연산량을 가지는 임베디드 환경에서의 추론을 어렵게 하는 원인이 된다. 이 문제의 대표적인 해결 방법으로 네트워크 가지치기 기법이 있다. 네트워크 가지치기 기법은 중복된 역할을 하는 파라미터를 제거하여 추론 시 요구되는 메모리와 연산량을 감소시켜 임베디드 보드에서의 추론을 용이하게 할 수 있다. 하지만 대부분의 가지치기 기법은 두 단계의 학습을 요구하여 많은 시간과 자원이 소모되고 가지치기에 따른 채널의 관계 변화를 반영하지 못해 최적의 경량 네트워크를 보장할 수 없다. 따라서 본 논문에서는 최적의 경량 네트워크를 얻기 위한 중요도 탐색 기법을 제안하고 가지치기의 과정을 단순화하여 한 단계의 훈련만으로 가지치기가 가능한 중요도 탐색 필터 가지치기 기법을 제안한다. 본 논문에서는 VGG-16과 ResNet-50을 백본 네트워크로 가지는 SSD 네트워크에 가지치기를 적용하고 Jetson Xavier NX에서 추론 속도를 측정하였다. ResNet-50을 이용하는 네트워크에서 실험 결과 mAP(0.5)는 가지치기 비율에 따라 0.5 %, 0.7 %, 1.0 % 감소하였지만, 추론 시간은 12.75 %, 16.03 %, 21.66 % 향상되었다. 또한 학습 시간은 다른 기법보다 최대 43.85 % 빠르며 유사한 가지치기 비율의 네트워크와 비교할 때 높은 성능을 가진다.","In recent years, with the development of computer technology, research on CNN-based object detection networks has been actively conducted. However, a large number of CNNs can make inference difficult in embedded environments with limited memory and computation. A typical solution to this problem is network pruning. Network pruning can facilitate inference on embedded boards by reducing the amount of memory and computation required by removing redundant parameters. However, most pruning methods require two stages of training, which consumes a lot of time and resources, and cannot guarantee an optimal lightweight network because they cannot reflect the changes in channel relationships due to pruning. Therefore, this paper proposes an importance search method to obtain an optimal lightweight network, and simplifies the pruning process to propose an importance search filter pruning method that can be pruned with only one stage of training. In this paper, we apply pruning to the SSD network with VGG-16 and ResNet-50 as the backbone network, and measure the inference speed on Jetson Xavier NX. In the network using ResNet-50, the experimental results showed that mAP(0.5) decreased by 0.5%, 0.7%, and 1.0% depending on the pruning ratio, but inference time improved by 12.75%, 16.03%, and 21.66%. In addition, the learning time is up to 43.85% faster than other methods and has high performance when compared to networks with similar pruning ratios."
Prediction Model of Spinal Osteoporosis Using Lumbar Spine X-Ray from Transfer Learning Deep Convolutional Neural Networks,2024,"['Deep learning', 'Lumbar vertebrae', 'Osteoporosis', 'Spine', 'X-rays']",,"Objective: Osteoporosis is highly prevalent among older adults and women. This condition leads to a deterioration in bone mineral density and microarchitecture, significantly increasing the risk of fractures. Additionally, osteoporosis commonly results in complications such as screw loosening and non-union during spinal surgery. Deep-learning algorithms have now achieved an accuracy comparable to the current human margin of error. Therefore, this study explored the potential of using transfer learning in deep learning algorithms to predict, diagnose, and screen for osteoporosis using commonly obtained sagittal spine X-rays from patients with spinal conditions.Methods: We retrospectively evaluated 2,300 consecutive patients who underwent dual energy X-ray absorptiometry (DXA) and lumbar sagittal plain X-ray exams between 2013 and 2021. The exclusion criteria included: (1) a gap of more than 1 year between the DXA and X-ray exams; (2) vertebrae that had undergone vertebroplasty; (3) lack of spine anterior-posterior DXA; and (4) images that were unassessable. Ultimately, 256 patients (images) were included in the study. Transfer learning was applied using convolutional neural network (CNN) techniques, specifically visual geometry group (VGG) 16, VGG 19, ResNet50, and Xception.Results: The most accurate CNN model in the training group was ResNet50, with an accuracy of 0.95. ResNet50 showed the best performance, with an accuracy of 0.82, precision of 0.80, recall of 0.86, and F1-score of 0.83. Additionally, its area under the curve (0.76) was higher than that other CNN models. The confusion matrix for ResNet50’s performance displayed the outcomes for images predicted as osteoporosis (n=12) among the test data osteoporosis images (n=14)Conclusion: Artificial intelligence (AI) technology employing deep learning techniques is significantly nearing human capabilities in the role of diagnostic assistance. The diagnosis of osteoporosis using bone mineral density is expected to evolve into a comprehensive diagnostic aid or decision-making tool with the integration of AI in the future."
Cox Model Improvement Using Residual Blocks in Neural Networks: A Study on the Predictive Model of Cervical Cancer Mortality,2024,"['자궁경부암', '생존 예측 모델', '콕스 비례 위험', '기계 학습', '심층 신경망', 'ResNet', 'Cervical Cancer', 'Survival Prediction Model', 'Cox Proportional Hazards', 'Machine Learning', 'Deep Neural Networks', 'ResNet']",,"Cervical cancer is the fourth most common cancer in women worldwide, and more than 604,000 new cases were reported in 2020alone, resulting in approximately 341,831 deaths. The Cox regression model is a major model widely adopted in cancer research, butconsidering the existence of nonlinear associations, it faces limitations due to linear assumptions. To address this problem, this paperproposes ResSurvNet, a new model that improves the accuracy of cervical cancer mortality prediction using ResNet's residual learningframework. This model showed accuracy that outperforms the DNN, CPH, CoxLasso, Cox Gradient Boost, and RSF models comparedin this study. As this model showed accuracy that outperformed the DNN, CPH, CoxLasso, Cox Gradient Boost, and RSF models comparedin this study, this excellent predictive performance demonstrates great value in early diagnosis and treatment strategy establishment inthe management of cervical cancer patients and represents significant progress in the field of survival analysis."
Research on a Lightweight Deep Learning Model Suitable for Face Recognition for Mobile Devices,2024,"['embedding environment', 'lightweight deep learning', 'face recognition', 'MobileFaceNet', 'ResNet']",,"Recently, research on lightweight deep learning has been applied to various fields due to issues such as cost reduction, security, and power consumption due to decentralization. The lightweight deep learning model provides distributed processing of data and various services through a mobile environment. In this study, we compare two lightweight facial recognition deep learning models suitable for the mobile environment and propose a more suitable model. MobileFaceNet is a model optimized for deployment in an embedding environment, and we sought to find a more suitable model by comparing it with the ResNet model that has been recently studied. WebFace42M was used as the dataset, and landmarks were extracted using RetinaFace as a face alignment technique, and faces were aligned using opencv's affine transformation. As a result of applying the two models, ResNet-100 showed better performance in the same embedding environment."
딥러닝을 활용한 전시 정원 디자인 유사성 인지 모형 연구,2024,,,"The purpose of this study is to propose a method for evaluating the similarity of Show gardens using Deep Learning models, specifically VGG-16 and ResNet50. A model for judging the similarity of show gardens based on VGG-16 and ResNet50 models was developed, and was referred to as DRG (Deep Recognition of similarity in show Garden design). An algorithm utilizing GAP and Pearson correlation coefficient was employed to construct the model, and the accuracy of similarity was analyzed by comparing the total number of similar images derived at 1st (Top1), 3rd (Top3), and 5th (Top5) ranks with the original images. The image data used for the DRG model consisted of a total of 278 works from the Le Festival International des Jardins de Chaumont-sur-Loire, 27 works from the Seoul International Garden Show, and 17 works from the Korea Garden Show. Image analysis was conducted using the DRG model for both the same group and different groups, resulting in the establishment of guidelines for assessing show garden similarity. First, overall image similarity analysis was best suited for applying data augmentation techniques based on the ResNet50 model. Second, for image analysis focusing on internal structure and outer form, it was effective to apply a certain size filter (16cm × 16cm) to generate images emphasizing form and then compare similarity using the VGG-16 model. It was suggested that an image size of 448 × 448 pixels and the original image in full color are the optimal settings. Based on these research findings, a quantitative method for assessing show gardens is proposed and it is expected to contribute to the continuous development of garden culture through interdisciplinary research moving forward."
Identification of Atrial Fibrillation With Single-Lead Mobile ECG During Normal Sinus Rhythm Using Deep Learning,2024,"['Artificial Intelligence', 'Atrial Fibrillation', 'Electrocardiography', 'Mobile Applications', 'Probability Learning']",,"Background: The acquisition of single-lead electrocardiogram (ECG) from mobile devices offers a more practical approach to arrhythmia detection. Using artificial intelligence for atrial fibrillation (AF) identification enhances screening efficiency. However, the potential of singlelead ECG for AF identification during normal sinus rhythm (NSR) remains under-explored.This study introduces a method to identify AF using single-lead mobile ECG during NSR.Methods: We employed three deep learning models: recurrent neural network (RNN), long short-term memory (LSTM), and residual neural networks (ResNet50). From a dataset comprising 13,509 ECGs from 6,719 patients, 10,287 NSR ECGs from 5,170 patients were selected. Single-lead mobile ECGs underwent noise filtering and segmentation into 10-second intervals. A random under-sampling was applied to reduce bias from data imbalance. The final analysis involved 31,767 ECG segments, including 15,157 labeled as masked AF and 16,610 as Healthy.Results: ResNet50 outperformed the other models, achieving a recall of 79.3%, precision of 65.8%, F1-score of 71.9%, accuracy of 70.5%, and an area under the receiver operating characteristic curve (AUC) of 0.79 in identifying AF from NSR ECGs. Comparative performance scores for RNN and LSTM were 0.75 and 0.74, respectively. In an external validation set, ResNet50 attained an F1-score of 64.1%, recall of 68.9%, precision of 60.0%, accuracy of 63.4%, and AUC of 0.68.Conclusion: The deep learning model using single-lead mobile ECG during NSR effectively identified AF at risk in future. However, further research is needed to enhance the performance of deep learning models for clinical application."
드론 식별을 위한 CNN 기반 이미지 분류 모델 성능 비교,2024,"['Drones', 'Transfer learning', 'Image classification', 'Convolutional Neural Networks', 'Aerial targets', '드론', '전이 학습', '이미지 분류', '합성곱 신경망', '공중표적']","최근 전장에서의 드론 활용이 정찰뿐만 아니라 화력 지원까지 확장됨에 따라, 드론을 조기에 자동으로 식별하는 기술의 중요성이 더욱 증가하고 있다. 본 연구에서는 드론과 크기 및 외형이 유사한 다른 공중 표적들인 새와 풍선을 구분할 수 있는 효과적인 이미지 분류 모델을 확인하기 위해, 인터넷에서 수집한 3,600장의 이미지 데이터셋을 사용하고, 세 가지 사전 학습된 합성곱 신경망 모델(VGG16, ResNet50, InceptionV3)의 특징 추출기능과 추가 분류기를 결합한 전이 학습 접근 방식을 채택하였다. 즉, 가장 우수한 모델을 확인하기 위해 세 가지 사전 학습된 모델(VGG16, ResNet50, InceptionV3)의 성능을 비교 분석하였으며, 실험 결과 InceptionV3 모델이 99.66%의 최고 정확도를 나타냄을 확인하였다. 본 연구는 기존의 합성곱 신경망 모델과 전이 학습을 활용하여 드론을 식별하는 새로운 시도로써, 드론 식별 기술의 발전에 크게 기여 할 것으로 기대된다.","Recent developments in the use of drones on battlefields, extending beyond reconnaissance to firepower support, have greatly increased the importance of technologies for early automatic drone identification. In this study, to identify an effective image classification model that can distinguish drones from other aerial targets of similar size and appearance, such as birds and balloons, we utilized a dataset of 3,600 images collected from the internet. We adopted a transfer learning approach that combines the feature extraction capabilities of three pre-trained convolutional neural network models (VGG16, ResNet50, InceptionV3) with an additional classifier. Specifically, we conducted a comparative analysis of the performance of these three pre-trained models to determine the most effective one. The results showed that the InceptionV3 model achieved the highest accuracy at 99.66%. This research represents a new endeavor in utilizing existing convolutional neural network models and transfer learning for drone identification, which is expected to make a significant contribution to the advancement of drone identification technologies."
Improving Chest X-ray Image Classification via Integration of Self-Supervised Learning and Machine Learning Algorithms,2024,"['Chest X-ray image', 'Contrastive learning', 'Image classification', 'Self-supervised learning']",,"In this study, we present a novel approach for enhancing chest X-ray image classification (normal, Covid-19, edema, massnodules, and pneumothorax) by combining contrastive learning and machine learning algorithms. A vast amount of unlabeleddata was leveraged to learn representations so that data efficiency is improved as a means of addressing the limited availabilityof labeled data in X-ray images. Our approach involves training classification algorithms using the extracted features from alinear fine-tuned Momentum Contrast (MoCo) model. The MoCo architecture with a Resnet34, Resnet50, or Resnet101backbone is trained to learn features from unlabeled data. Instead of only fine-tuning the linear classifier layer on the MoCopretrainedmodel, we propose training nonlinear classifiers as substitutes for softmax in deep networks. The empirical resultsshow that while the linear fine-tuned ImageNet-pretrained models achieved the highest accuracy of only 82.9% and the linearfine-tuned MoCo-pretrained models an increased highest accuracy of 84.8%, our proposed method offered a significantimprovement and achieved the highest accuracy of 87.9%."
Classification of mandibular molar furcation involvement in periapical radiographs by deep learning,2024,"['Mandible', 'Molar', 'Periodontitis', 'Radiography', 'Deep Learning']",,"Purpose: The purpose of this study was to classify mandibular molar furcation involvement (FI) in periapical radiographs using a deep learning algorithm.Materials and Methods: Full mouth series taken at East Carolina University School of Dental Medicine from 2011-2023 were screened. Diagnostic-quality mandibular premolar and molar periapical radiographs with healthy or FI mandibular molars were included. The radiographs were cropped into individual molar images, annotated as “healthy” or “FI,” and divided into training, validation, and testing datasets. The images were preprocessed by PyTorch transformations. ResNet-18, a convolutional neural network model, was refined using the PyTorch deep learning framework for the specific imaging classification task. CrossEntropyLoss and the AdamW optimizer were employed for loss function training and optimizing the learning rate, respectively. The images were loaded by PyTorch DataLoader for efficiency. The performance of ResNet-18 algorithm was evaluated with multiple metrics, including training and validation losses, confusion matrix, accuracy, sensitivity, specificity, the receiver operating characteristic (ROC) curve, and the area under the ROC curve.Results: After adequate training, ResNet-18 classified healthy vs. FI molars in the testing set with an accuracy of 96.47%, indicating its suitability for image classification.Conclusion: The deep learning algorithm developed in this study was shown to be promising for classifying mandibular molar FI. It could serve as a valuable supplemental tool for detecting and managing periodontal diseases."
불법 주정차 단속을 위한 딥러닝 기반 이미지 인식 모델,2024,"['Deep learning', 'Illegal parking', 'ResNet18', 'YOLOv8']",,"Recently, research on the convergence of drones and artificial intelligence technologies have been conducted in various industrial fields. In this paper, we propose an illegal parking vehicle recognition model using deep learning-based object recognition and classification algorithms. The model of object recognition and classification consist of YOLOv8 and ResNet18, respectively. The proposed model was trained using image data collected in general road environment, and the trained model showed high accuracy in determining illegal parking. From simulation results, it was confirmed that the proposed model has generalization performance to identify illegal parking vehicles from various images."
LLVM IR 대상 악성코드 탐지를 위한 이미지 기반 머신러닝 모델,2024,"['LLVM IR', 'Image based', 'Malware Detection', 'ResNet50V2']","최근 정적분석 기반의 시그니처 및 패턴 탐지 기술은 고도화되는 IT 기술에 따라 한계점이 드러나고 있다. 이는 여러 아키텍처에 대한 호환 문제와 시그니처 및 패턴 탐지의 본질적인 문제이다. 악성코드는 자신의 정체를 숨기기 위하여 난독화, 패킹 기법 등을 사용하고 있으며 또한, 코드 재정렬, 레지스터 변경, 분기문 추가 등 기존 정적분석 기반의 시그니처 및 패턴 탐지 기법을 회피하고 있다. 이에 본 논문에서는 이러한 문제를 해결할 수 있는 머신러닝을 통한 LLVM IR 코드 이미지 기반 악성코드 정적분석 자동화 기술을 제안한다. 바이너리가 난독화되거나 패킹된 사실에 불구하고 정적 분석 및 최적화를 위한 중간언어인 LLVM IR로 디컴파일한다. 이후 LLVM IR 코드를 이미지로 변환하여 CNN을 이용한 알고리즘 중 전이 학습 및 Keras에서 지원하는 ResNet50v2으로 학습하여 악성코드를 탐지하는 모델을 제시한다.",
A Cost-Effective Blind Spot Detection System with High Recall Rate using Deep Learning and a Comparative Analysis of Implementation Hardware Platforms,2024,"['Blind Spot Detection', 'Advanced Driver Assistance Systems', 'Deep Learning Accelerator', 'FPGA', 'ResNet-18', 'Energy Efficiency', 'Automotive Safety', 'Vehicle Detection', 'Model Parameter Quantization']",,"Blind Spot Detection (BSD) is critical for enhancing vehicle safety and is an integral component of many Advanced Driver Assistance Systems (ADAS). In this work, we propose a novel, cost-effective BSD solution utilizing a deep learning approach with the vehicle’s existing rearview camera. While previous works such as Histogram of Oriented Gradients (HOG) combined with Support Vector Machine (SVM) have been used for BSD, these approaches often struggle to maintain high accuracy, particularly in diverse real-world environments. To address these limitations, we employed ResNet-18, a deep learning network, to improve both recall and precision in detecting vehicles in the blind spot. Our approach was evaluated across multiple hardware platforms, including CPU, GPU, and Field Programmable Gate Arrays (FPGA). We conducted comparative analyses in terms of detection performance, processing speed, energy consumption, and cost efficiency. The results showed that our deep learning-based BSD system achieved a 100% recall rate on all hardware platforms, ensuring no critical events were missed, thereby greatly enhancing safety. Among the platforms, FPGA demonstrated superior energy efficiency. Over time, FPGA emerged as the most cost-effective platform due to its low operational costs. This work contributes to the development of more reliable and efficient BSD systems by leveraging deep learning and identifying the optimal hardware platforms for real-time vehicle detection in ADAS applications."
Deep Learning-Based Defect Detection in Cu-Cu Bonding Processes,2024,"['Cu-Cu Bonding', 'Deep Learning', 'Defect Detection', 'Defect Map', 'ResNet', 'CNN']",,"Cu-Cu bonding, one of the key technologies in advanced packaging, enhances semiconductor chip performance, miniaturization, and energy efficiency by facilitating rapid data transfer and low power consumption. However, the quality of the interface bonding can significantly impact overall bond quality, necessitating strategies to quickly detect and classify in-process defects. This study presents a methodology for detecting defects in wafer junction areas from Scanning Acoustic Microscopy images using a ResNet-50 based deep learning model. Additionally, the use of the defect map is proposed to rapidly inspect and categorize defects occurring during the Cu-Cu bonding process, thereby improving yield and productivity in semiconductor manufacturing."
Deep Learning-Based Defect Detection in Cu-Cu Bonding Processes,2024,"['Cu-Cu Bonding', 'Deep Learning', 'Defect Detection', 'Defect Map', 'ResNet', 'CNN']",,"Cu-Cu bonding, one of the key technologies in advanced packaging, enhances semiconductor chip performance, miniaturization, and energy efficiency by facilitating rapid data transfer and low power consumption. However, the quality of the interface bonding can significantly impact overall bond quality, necessitating strategies to quickly detect and classify in-process defects. This study presents a methodology for detecting defects in wafer junction areas from Scanning Acoustic Microscopy images using a ResNet-50 based deep learning model. Additionally, the use of the defect map is proposed to rapidly inspect and categorize defects occurring during the Cu-Cu bonding process, thereby improving yield and productivity in semiconductor manufacturing."
한정된 레이블 데이터를 이용한 효율적인 철도 표면 결함 감지 방법,2024,"['Rail surface', 'Semi-supervised', 'thresholding', 'sorting', 'selection.']",,"In this research, we propose a Semi-Supervised learning based railroad surface defect detection method. The Resnet50 model, pretrained on ImageNet, was employed for the training. Data without labels are randomly selected, and then labeled to train the ResNet50 model. The trained model is used to predict the results of the remaining unlabeled training data. The predicted values exceeding a certain threshold are selected, sorted in descending order, and added to the training data. Pseudo-labeling is performed based on the class with the highest probability during this process. An experiment was conducted to assess the overall class classification performance based on the initial number of labeled data. The results showed an accuracy of 98% at best with less than 10% labeled training data compared to the overall training data."
반려동물 안구 질환을 위한 딥러닝 모델 기반 진단 시스템,2024,"['artificial intelligence', 'deep learning', 'ocular diseases', 'system', 'convolutional neural network', '인공지능', '딥러닝', '안구 질환', '시스템', '합성곱 신경망']","반려동물의 안구 질환은 늦은 진단과 치료로 인해 실명 등의 중대한 결과를 초래할 수 있다. 이는 반려동물 양육 가정의 증가에 따라 점점 더 중요한 문제로 부상하고 있다. 본 논문은 이 문제에 대응하기 위해 인공지능 기반의 조기 진단과 치료 방향을 제시하는 시스템을 설계하고 구현한다. 제안하는 시스템은 AIHUB의 라벨링된 데이터셋을 활용하여 ResNet과 EfficientNet 모델을 최적화한다. 또한 고정밀 질병 분류가 가능하도록 하여 전문가의 진단을 보조하고, 진료가 어려운 지역의 사용성을 높인다. 결과적으로 제안하는 시스템은 반려동물의 안구 건강을 효과적으로 관리하고 보호자의 부담을 경감할 수 있다. 성능평가를 통해, 제안하는 모델은 반려동물의 안구 질환 분류에서 90% 이상의 높은 정확도를 나타냄을 보인다.","Pet eye diseases can have serious consequences, including blindness, if not diagnosed and treated promptly. This issue is becoming increasingly important as more households own pets. In this paper, we present a system that uses artificial intelligence to provide early diagnosis and treatment recommendations for pet eye diseases. We use labeled data sets from AIHUB to optimize ResNet and EfficientNet models for diagnosing these diseases. The proposed system helps experts classify diseases with high precision and makes it more accessible in areas with limited medical services. As a result, the system effectively manages and protects the ocular health of cats and dogs, reducing the burden on their caregivers. Performance evaluations demonstrate that the proposed model achieves over 90% accuracy in classifying eye diseases."
엣지 딥 러닝 가속기의 추론 성능 분석,2024,"['Deep learning', 'Deep learning accelerator', 'Inference', 'Edge device', 'Performance analysis']","엣지 장치에서 딥 러닝 기반 추론을 위해 추론 가속기가 탑재되고 있다. 딥 러닝 추론 가속기를 통해 연산 성능과 에너지 효율을 증가시킬 수 있다. 하지만 가속기에 최적화되지 않은 모델 구조와 설정을 사용하면 메모리 접근 등의 오버헤드로 인해 최적 성능을 낼 수 없다. 본 논문에서는 사전 학습된 MobileNet v2, ResNet50 v1 모델을 사용해 NVIDIA Jetson에서 Graphic Processing Unit (GPU)와 Deep Learning Accelerator (DLA)의 추론 성능을 분석하였다. 실험을 통해 DLA에 최적화되지 않은 모델을 실행하면 GPU보다 최대 5.1배 추론 시간이 증가함을 보였다. 특히, 프로파일링을 통해 DLA에서 지원하지 않는 연산을 GPU로 폴백 (fallback)하는 과정의 오버헤드로 추론 시간이 증가함을 보였다.","Inference accelerators are currently being utilized for deep learning inference on edge devices. Deep learning inference accelerators can enhance computational performance and energy efficiency. However, it is important to note that optimal performance cannot be achieved if the model structure and settings (e.g. hyperparameters) are not optimized for the accelerator, which can result in overheads, such as frequent memory access. This study analyses the inference performance of the Graphic Processing Unit (GPU) and the Deep Learning Accelerator (DLA) on NVIDIA Jetson with pre-trained MobileNet v2 and ResNet50 v1 models. The results of our experiments show that running non-optimized models on the DLA results in up to 5.1 times longer inference time compared to the GPU. This paper showed through profiling that the increase in inference time is due to the overhead of GPU fallback to perform operations not supported by DLA."
어깨 초음파 영상에서 딥러닝 알고리즘을 이용한 컴퓨터 자동진단의 응용,2024,"['컴퓨터 자동 진단', '딥러닝 알고리즘', '초음파 영상', '영상 구분', '영상 병변 탐지', 'Computer Automated Diagnostics', 'Deep learning algorithm', 'Ultrasound  image', 'Image classification', 'Image Lesion Detection']","본 연구는 딥러닝 알고리즘을 이용해 어깨 초음파 영상에서 이두근 건의 정상 및 병변을 구분하고, 영상 속 삼출액 병변을 탐지하는 컴퓨터 자동 진단 성능을 평가하고자 한다. D 병원에서 진료받은 어깨 통증 환자들의 초음 파 영상 증례 260건을 사용하였다. 딥러닝으로서 구분 알고리즘에는 ResNet-50, 탐지 알고리즘에는 DeepLabV3+ 를 적용하였으며 성능 평가 지표로 ROC 곡선, AUC, F1-Score 등을 사용하였다. 결과로 구분 알고리즘에서 정확 도 95%, 정밀도 100%, 재현율 91%, AUC 94%를 탐지 알고리즘에서 전역 정확도 97%, 평균 IOU 85%, F1-Score 66% 등을 나타냈다. 본 논문의 제시 모델을 바탕으로 추가 데이터 획득 및 여러 알고리즘을 적용한다면 임상에서 초음파 자동 진단 시스템으로의 응용이 가능하다고 판단된다.","This study aims to evaluate the computer's automatic diagnosis performance to distinguish normal and lesions of biceps from shoulder ultrasound images using a deep learning algorithm and to detect exudate lesions in the images. 260 cases of ultrasound imaging of shoulder pain patients treated at D hospital were used. As deep learning, ResNet-50 was applied to the classification algorithm and DeepLabV3+ was applied to the detection algorithm, and ROC curves, AUC, and F1-Score were used as performance evaluation indicators. As a result, 95% accuracy, 100% precision, 91% reproduction rate, and 94% AUC in the classification algorithm showed 97% global accuracy, 85% average IOU, and 66% F1-Score in the detection algorithm. Based on the model presented in this paper, it is judged that the automatic ultrasound diagnosis system can be applied in clinical practice if additional data is acquired and several algorithms are applied."
비휘발성 메모리 기반 IMC을 활용한단일 코어/다중 레이어 CNN 가속기 최적화,2024,"['Design space exploration', 'IMC', 'CNN', 'accelerator optimization', 'Non-volatile memory']","이 논문에서는 비휘발성 메모리 기반 In-Memory Computing(IMC)를 활용한 단일 코어/다중 레이어 CNN 가속기의 설계 초기단계에서 가속기의 성능과 면적을 예측하여 최적 메모리 데이터 플로우 및 인터페이스를 탐색할 수 있는 새로운 설계 공간 탐색기를제안한다. 이를 위해 다양한 메모리 레이아웃, 인터페이스, 매핑 방법을 탐색 공간에 포함하였다. 설계 옵션들을 완전 탐색 방식으로탐색하고, 성능과 면적을 예측하여 최적 메모리 데이터 플로우 및 인터페이스를 탐색했다. ResNet-18를 목표 네트워크로 설정하고,제안하는 설계 공간 탐색기를 통해 찾아낸 최적 메모리 데이터 플로우 및 인터페이스는 baseline 대비 면적 효율 측면에서 약 132배 향상이 가능함을 확인했다.","This paper presents a novel Design Space Explorer(DSE) that can predict the performance and area of asingle-core/multi-layer CNN accelerator using non-volatile memory-based In-Memory Computing(IMC) at an earlystage of design to explore the optimal memory data flow and interface. To achieve this, we include variousmemory layouts, interfaces, and mapping methods in the exploration space. Design options were explored in anexhaustive search manner, and the optimal memory data flow and interfaces were explored by predictingperformance and area. Using ResNet-18 as the target network, we found that the optimal memory data flow andinterface found by the proposed DSE can improve the area efficiency by about 132 times compared to thebaseline."
합성곱 신경망을 이용한 상지 엑스선 영상 분류 모델 유용성 평가,2024,"['합성곱 신경망', '딥러닝', '의료 영상', '영상 분류', '검사 오류', 'Convolutional Neural Network', 'Deep Learning', 'Medical Image', 'Image Classification', 'Examination Error']","본 연구는 엑스선 검사 과정에서 환자와 코드를 정확히 확인하지 않아 발생할 수 있는 실수나 오류를 예방하는 것을 목표로 하고 있다. 이를 통해 방사선사의 작업 효율성을 높이고 의료 사고를 방지하며, 합성곱 신경망 기반 이미지 분류 기술을 활용한 실질적인 임상 적용 방안을 제안하고자 한다. 연구는 19,381개의 상지 근골격계 엑스선 이미지를 7개의 영역, 19개의 class로 분류하였으며, 학습, 검증, 평가 세트 비율을 8:1:1로 분할하였다. 딥러닝 모델은 VGG-16, DenseNet-121, ResNet-152v2와 같은 심층 학습 모델을 사용하여 정확도, 정밀도, 재현율, F1 스코어 및 혼동 행렬을 기반으로 모델 성능을 평가하였다. 학습결과 DenseNet-121의 전체 정확도에서 87.77%, 평균 class 정확도에서 98.71%, 정밀도에서 91.78%, 재현율에서 86.93%, F1스코어에서 86.71%를 보였다. 모든지표에서 DenseNet-121이 가장 높은 성능을 보였다. 본 연구는 상지 X선 이미지를 활용한 다양한 심층 학습 모델의 성능을 평가하였으며, 충분한 성능을 보여주었다. 이를 통해 작업 효율성을 높이고 의료 사고 방지가능성을 확인하였다.","This study aims to prevent errors that may occur during the radiography examination process, such as misinterpretation of images, by utilizing artificial intelligence, a core technology of the Fourth Industrial Revolution. Through this, we sought to enhance the work efficiency of radiologic technologists, prevent medical accidents. We labeled 19,381 upper ex- tremity musculo-skeletal X-ray images into 7 regions and 19 classes, and divided them into training, validation, and test sets at a ratio of 8:1:1. We used deep learning models such as VGG-16, DenseNet-121, and ResNet-152v2 to evaluate model performance based on accuracy, precision, recall, F1-score, and confusion matrix. The results showed that DenseNet-121 achieved an overall accuracy of 87.77%, an average class accuracy of 98.71%, a precision of 91.78%, a recall of 86.93%, and an F1 score of 86.71%. DenseNet-121 demonstrated the highest performance across all metrics. This study evaluated the performance of various deep learning models using upper extremity radiographic image and demonstrated sufficient performance. Through this, the potential to improve work efficiency and prevent medical accidents was confirmed."
영상에서 효율적인 잡음제거를 위한 dRED-TL-GAN 모델,2024,"['딥러닝', '잡음제거', 'deformable 컨볼루션', 'GAN', 'Deep learning', 'deformable convolution', 'dRED-TL-GAN', 'image denoising']","영상에서 잡음은 시각적인 왜곡이나 불편을 주는 것 외에도 영상 시스템에서 성능 저하를 가져옴으로써 영상에서 잡음제거는 영상처리에서 중요한 전처리 과정이다. 본 논문에서는 영상에서 잡음제거를 위해 GAN 모델에서 파생된 DCGAN 기반의 deformable RED and transfer learning based generative adversarial networks (dRED-TL-GAN)모델을 제안하고자 한다. 제안된 dRED-TL-GAN 모델에서 생성자는 인코더-디코더 구조로 이루어진 deformable RED 구조이고, 판별자는 전이학습 기반 구조이다. 여기서 deformable RED 구조는 인코더의 컨볼루션 층에서 표준 컨볼루션 대신 deformable 컨볼루션을 사용하여 영상의 특징을 고려하였고, 판별자는 ResNet-18 모델을 사용하여 학습 속도가 분류 정확도를 높혔다. 본 논문에서 제안된 dRED-TL-GAN 모델의 성능 평가를 위해 전통적인 Mean 필터, Median 필터와 BM3D 필터, 그리고 기존 딥러닝의 DnCNN 모델, RED-CNN 모델 그리고 DCGAN 모델을 고려하였으며, 다양한 잡음, 즉, 가우시안 잡음 (Caussian noise), 포아송 잡음 (Poisson noise) 그리고 스팩클 잡음 (Speckle noise)으로 훼손된 얼굴 영상을 대상으로 실험하였다. 성능 실험은 정성적인 평가와 정량적인 평가로 구성되며, 정성적인 평가 결과, Mean 필터, Median 필터, 그리고 BM3D 필터를 포함한 공간 필터들은 대체로 잡음이 남아있고, 또한 호릿한 영상을 얻었고, 제안된 dRED-TL-GAN 모델은 다른 딥러닝 모델보다 에지있는 선명한 영상을 얻었다. 또한, 정량적인 평가 척도인 Peak signal-to-noise ratio (PSNR), Mean squared error (MSE) 그리고 Structural similarity index measure (SSIM) 면에서 dRED-TL-GAN 모델은 모든 잡음과 모든 평가 척도에서 가장 좋은 성능 수치를 얻었다.","Noise in images not only causes visual distortion or inconvenience, but also reduces performance in the imaging system, so image denoising is an important preprocessing process in image processing. In this paper, we propose a dRED-TL-GAN model based on DCGAN, derived from GAN, to remove noise from images. The generator of the dRED-TL-GAN model is a deformable RED structure consisting of an encoder-decoder structure, and the discriminator is a transfer learning-based structure. Here, the deformable RED structure used deformable convolutin in the encoder’s convolution layer to remove noise by considering the characteristics of the image, and used the ResNet-18 model in the discriminator to increase learning speed and classification accuracy. To evaluate the performance of the proposed dRED-TL-GAN model, traditional filters including Mean filter, Median filter, and BM3D filter, and existing deep learning models including DnCNN model, RED-CNN model, and DCGAN model were considered. An performance experiment was conducted on face images damaged by various noises, namely Gaussian noise, Poisson noise, and Speckle noise. The performance experiment consists of qualitative and quantitative evaluations. First, in the qualitative evaluation, spatial filters including the Mean filter, Median filter, and BM3D filter generally remained noisy and resulted in blurry results, and the propose dRED-TL-GAN model obtained clearer images with edges than other deep learning models. Additionally, in a quantitative evaluation using PSNR, MSE, and SSIM metrics, the dRED-TL-GAN model performs well under all noises considered and on all evaluation metrics."
EfficientNet-B0 outperforms other CNNs in image-based five-class embryo grading: a comparative analysis,2024,"['blastocyst', 'convolutional neural networks', 'deep learning', 'embryo', 'in vitro fertilization']",,"Background: Evaluating embryo quality is crucial for the success of in vitro fertilization procedures. Traditional methods, such as the Gardner grading system, rely on subjective human assessment of morphological features, leading to potential inconsistencies and errors. Artificial intelligence-powered grading systems offer a more objective and consistent approach by reducing human biases and enhancing accuracy and reliability.Methods: We evaluated the performance of five convolutional neural network architectures—EfficientNet-B0, InceptionV3, ResNet18, ResNet50, and VGG16— in grading blastocysts into five quality classes using only embryo images, without incorporating clinical or patient data. Transfer learning was applied to adapt pretrained models to our dataset, and data augmentation techniques were employed to improve model generalizability and address class imbalance.Results: EfficientNet-B0 outperformed the other architectures, achieving the highest accuracy, area under the receiver operating characteristic curve, and F1-score across all evaluation metrics. Gradient-weighted Class Activation Mapping was used to interpret the models’ decision-making processes, revealing that the most successful models predominantly focused on the inner cell mass, a critical determinant of embryo quality.Conclusions: Convolutional neural networks, particularly EfficientNet-B0, can significantly enhance the reliability and consistency of embryo grading in in vitro fertilization procedures by providing objective assessments based solely on embryo images. This approach offers a promising alternative to traditional subjective morphological evaluations."
CNN 기반 인코더와 Transformer 기반 인코더의 이미지 캡셔닝 성능 비교 분석,2024,"['image captioning', 'residual network 50', 'visual geometry group-16', 'vision transformers', 'shifted window transformer', '.']","이미지 캡셔닝은 이미지의 특징을 추출하여 이미지를 인식하고 자연어 처리와 결합하여 이미지에 대한 설명을 생성하는 작업이다. 이미지 캡셔닝 결과는 때때로 부자연스러운 텍스트를 생성한다. 이러한 문제의 원인을 정확하게 파악하기 위해 인코더들의 성능을 비교 실험한다. 이미지 캡션 생성 과정은 인코더, 디코더 구조를 가진다. 인코더에서 얻어지는 이미지 특징 추출 결과에 따라 디코더에서 생성되는 텍스트에 많은 영향을 미친다. 그에 따라 CNN 계열의 Resnet50, VGG-16과 트랜스포머 계열의 비전 트랜스포머, 스윈 트랜스포머 인코더의 성능을 비교하여 캡션 생성에 있어서 결정적인 영향을 주는지를 분석한다. 정성 및 정량 평가한 결과를 수치화하고 그래프 및 표로 제시하여 CNN 계열과 트랜스포머 계열의 인코딩 결과를 비교 분석하였다.","Image captioning involves extracting features from an image to recognize its content and combining them with natural language processing to generate a description of the image. However, the results of image captioning sometimes generate unnatural text. To accurately identify the cause of this issue, a comparative experiment of various encoders’ performance is conducted. The image caption generation process employs an encoder-decoder architecture. Since the text generated by the decoder is heavily influenced by the results of image feature extraction obtained from the encoder. This study compares the performance of CNN-based encoders, such as ResNet50 and VGG-16, with Transformer-based encoders, like Vision Transformer and Swin Transformer, to analyze whether they have a decisive impact on caption generation. This study quantified the results of the qualitative and quantitative evaluation and presented them in graphs and tables to compare and analyze the encoding performance between CNN-based and Transformer-based models."
교통사고 영상 분석을 통한 과실 판단을 위한 딥러닝 기반 방법 연구,2024,"['Action recognition', 'Computer vision', 'Deep learning', 'Image classification', 'Video analysis']","자율주행 차량에 대한 연구가 활발하게 이뤄지고 있다. 자율주행 차량이 등장함에 따라 기존의 차량과 자율주행 차량이 공존하는 과도기가 올 것이며, 이러한 과도기에는 사고율이 더욱 높아질 것이라 예상된다. 현재 교통사고 발생 시 손해보험협회의 ‘자동차 사고 과실 비율 인정기준’에 따라서 과실 비율을 측정한다. 그러나, 발생한 사고가 어떠한 유형의 사고인지 조사하는 데 소모되는 비용이 매우 크다. 또한 이미 과실 비율 책정이 완료된 사례에 대해서도 재심의를 요구하는 과실 비율 분쟁도 늘어나는 추세이다. 이러한 시간적, 물적 비용을 줄이기 위해 자동으로 과실 비율을 판단하는 딥러닝 모델을 제안하고자 한다. 본 논문에서는  ResNet-18 이미지 분류 모델과 TSN을 통한 비디오 행동 인식을 통해 사고 영상을 바탕으로 과실 비율을 판단하고자 한다. 모델이 상용화된다면, 과실 비율을 측정하는데 소요되는 시간을 획기적으로 단축할 수 있다. 또한 피의자에게 제공할 수 있는 과실 비율에 대한 객관적인 지표가 생기므로 과실 비율 분쟁도 완화될 것으로 기대된다.","Research on autonomous vehicles is being actively conducted. As autonomous vehicles emerge, there will be a transitional period in which traditional and autonomous vehicles coexist, potentially leading to a higher accident rate. Currently, when a traffic accident occurs, the fault ratio is determined according to the criteria set by the General Insurance Association of Korea. However, the time required to investigate the type of accident is substantial. Additionally, there is an increasing trend in fault ratio disputes, with requests for reconsideration even after the fault ratio has been determined. To reduce these temporal and material costs, we propose a deep learning model that automatically determines fault ratios. In this study, we aimed to determine fault ratios based on accident video through a image classification model based on ResNet-18 and video action recognition using TSN. If this model commercialized, could significantly reduce the time required to measure fault ratios. Moreover, it provides an objective metric for fault ratios that can be offered to the parties involved, potentially alleviating fault ratio disputes."
정보보안을 위한 생체 인식 모델에 관한 연구,2024,"['생체 인식', '멀티모달', '특징 융합', '딥러닝', 'Biometrics', 'Multimodal', 'Feature Fusion', 'Deep Learning']","생체 인식은 사람의 생체적, 행동적 특징 정보를 특정 장치로 추출하여 본인 여부를 판별하는 기술이다. 생체 인식 분야에서 생체 특성 위조, 복제, 해킹 등 사이버 위협이 증가하고 있다. 이에 대응하여 보안 시스템이 강화되고 복잡해지며, 개인이 사용하기 어려워지고 있다. 이를 위해 다중 생체 인식 모델이 연구되고 있다. 기존 연구들은 특징 융합 방법을 제시하고 있으나, 특징 융합 방법 간의 비교는 부족하다. 이에 본 논문에서는 지문, 얼굴, 홍채 영상을 이용한 다중 생체 인식 모델의 융합 방법을 비교평가했다. 특징 추출을 위해 VGG-16, ResNet-50, EfficientNet-B1, EfficientNet-B4, EfficientNet-B7, Inception-v3를 사용했으며, 특성 융합을 위해 ‘Sensor-Level’, ‘Feature-Level’, ‘Score-Level’, ‘Rank-Level’ 융합 방법을 비교 평가했다. 비교 평가 결과 ‘Feature-Level’ 융합 방법에서 EfficientNet-B7 모델이 98.51%의 정확도를 보이며 높은 안정성을 보였다. 그러나 EfficietnNet-B7모델의 크기가 크기 때문에 생체 특성 융합을 위한 모델 경량화 연구가 필요하다.","Biometric recognition is a technology that determines whether a person is identified by extracting information on a person's biometric and behavioral characteristics with a specific device. Cyber threats such as forgery, duplication, and hacking of biometric characteristics are increasing in the field of biometrics. In response, the security system is strengthened and complex, and it is becoming difficult for individuals to use. To this end, multiple biometric models are being studied. Existing studies have suggested feature fusion methods, but comparisons between feature fusion methods are insufficient. Therefore, in this paper, we compared and evaluated the fusion method of multiple biometric models using fingerprint, face, and iris images. VGG-16, ResNet-50, EfficientNet-B1, EfficientNet-B4, EfficientNet-B7, and Inception-v3 were used for feature extraction, and the fusion methods of 'Sensor-Level', 'Feature-Level', 'Score-Level', and 'Rank-Level' were compared and evaluated for feature fusion. As a result of the comparative evaluation, the EfficientNet-B7 model showed 98.51% accuracy and high stability in the 'Feature-Level' fusion method. However, because the EfficietnNet-B7 model is large in size, model lightweight studies are needed for biocharacteristic fusion."
3D CNN-LSTM 알고리즘을 이용한 손동작 비디오 영상 분류,2024,"['Artificial Intelligence', 'Deep Learning', 'CNN', 'LSTM', 'Video Classification', 'Hand Gesture Classification']","손동작 인식은 이미지나 비디오 데이터로부터 인간의 동작 및 제스처를 식별하는 행동인식기술의 한 형태이다. 디지털 기술의 발전으로 제품에 스마트 기능이 추가되는 사례가 많아지면서 동작인식의 편리성과 효율성도 강조 되고 있다. 본 연구는 손동작 인식을 시도하기 위한 과정으로, 손동작을 기반으로 클래스를 나누어 각각의 클래스를 분류해내는 비디오 분류 연구를 진행한다. 비디오 영상 자체로 딥러닝 분류를 하게 되면 정확도도 높으며, 이미지를 통한 비디오 분류보다 다양한 분야에서 활용이 가능하다는 장점이 있다. 제시된 알고리즘은 3D CNN(Convolutional neural network)과 LSTM(Long Short-Term Memory)이 결합된 형태로 이루어져 있다. 개발한 3D CNN은 이미지나 비디오의 특징 추출에 주로 사용하는 2D CNN 중 ResNet-18의 구조에서 고안하였다. LSTM은 순차 데이터를 학습, 처리, 분류하는 데 주로 사용되고 있는 RNN(Recurrent Neural Network)중의 한 종류이다. 3D CNN을 통해 비디오의 특징을 추출하고, LSTM을 통해 추출된 특징의 시퀀스를 학습 후 각 비디오 시퀀스를 손동작의 변화를 기준으로 하는 다섯 가지 클래스로 분류하였으며 비디오 분류 결과 정확도 평균 87%를 보여 주었다.","Hand gesture recognition is a subset of motion recognition technology that identifies human actions from image or video data. With the advancement of digital technology and the increasing integration of smart functions into products, the convenience and efficiency of motion recognition have become more prominent. This research aims to explore the process of hand gesture recognition by classifying video sequences based on hand gestures. The research focuses on video classification using deep learning techniques, which offer higher accuracy and broader applicability compared to image-based video classification. The proposed algorithm combines a 3D Convolutional Neural Network (3D CNN) with a Long Short-Term Memory network (LSTM). The developed 3D CNN is based on the ResNet-18 architecture, which is commonly used for feature extraction in images and videos. The LSTM, an extension of the Recurrent Neural Network (RNN), is employed to learn, process, and classify sequential data. The 3D CNN extracts features from video sequences, and the LSTM learns these feature sequences to classify each video sequence into one of five classes based on variations in hand gestures. The combined network, utilizing 3D CNN for feature extraction and LSTM for sequence learning, provides a robust approach to classify hand gestures in video sequences, demonstrating potential for diverse applications in various fields. The video classification accuracy reached approximately 87%."
Deep Learning Model and its Application for the Diagnosis of Exudative Pharyngitis,2024,"['Artificial Intelligence', 'Deep Learning', 'Diagnosis', 'Pharyngitis', 'Telemedicine']",,"Objectives: Telemedicine is firmly established in the healthcare landscape of many countries. Acute respiratory infections arethe most common reason for telemedicine consultations. A throat examination is important for diagnosing bacterial pharyngitis,but this is challenging for doctors during a telemedicine consultation. A solution could be for patients to upload imagesof their throat to a web application. This study aimed to develop a deep learning model for the automated diagnosis ofexudative pharyngitis. Thereafter, the model will be deployed online. Methods: We used 343 throat images (139 with exudativepharyngitis and 204 without pharyngitis) in the study. ImageDataGenerator was used to augment the training data. Theconvolutional neural network models of MobileNetV3, ResNet50, and EfficientNetB0 were implemented to train the dataset,with hyperparameter tuning. Results: All three models were trained successfully; with successive epochs, the loss and trainingloss decreased, and accuracy and training accuracy increased. The EfficientNetB0 model achieved the highest accuracy(95.5%), compared to MobileNetV3 (82.1%) and ResNet50 (88.1%). The EfficientNetB0 model also achieved high precision(1.00), recall (0.89) and F1-score (0.94). Conclusions: We trained a deep learning model based on EfficientNetB0 that candiagnose exudative pharyngitis. Our model was able to achieve the highest accuracy, at 95.5%, out of all previous studies thatused machine learning for the diagnosis of exudative pharyngitis. We have deployed the model on a web application that canbe used to augment the doctor’s diagnosis of exudative pharyngitis."
Oriented object detection in satellite images using convolutional neural network based on ResNeXt,2024,"['box-boundary-aware vector', 'convolutional neural network', 'oriented object detection', 'ResNeXt101', 'satellite imagery']",,"Most object detection methods use a horizontal bounding box that causes problems between adjacent objects with arbitrary directions, resulting in misaligned detection. Hence, the horizontal anchor should be replaced by a rotating anchor to determine oriented bounding boxes. A two-stage process of delineating a horizontal bounding box and then converting it into an oriented bounding box is inefficient. To improve detection, a box-boundary-aware vector can be estimated based on a convolutional neural network. Specifically, we propose a ResNeXt101 encoder to overcome the weaknesses of the conven-tional ResNet, which is less effective as the network depth and complexity increase. Owing to the cardinality of using a homogeneous design and multi-branch architecture with few hyperparameters, ResNeXt captures better information than ResNet. Experimental results demonstrate more accurate and faster oriented object detection of our proposal compared with a baseline, achieving a mean average precision of 89.41% and inference rate of 23.67 fps."
가상현실 기반의 고정밀 얼굴 통증 인식을 위한 빠른 안구 움직임 검출 알고리즘,2024,"['Virtual Reality', 'Pain Recognition', 'Eye Movement Detection', 'Pediatric Medical Treatment', 'Deep Learning']","본 논문에서는 고정밀 얼굴 통증 인식을 위한 새로운 가상현실(VR) 기반 급속 안구 운동 검출 알고리즘을 제안한다. 어린이는 종종 예방 접종 및 치과 시술에 대한 두려움과 거부감을 나타내어 치료행위를 어렵게 한다. 그러므로 제안 방법은 VR을 통해 만화를 제공하여 어린이의 주의를 딴 데로 돌리는 한편, 무의식적인 눈의 움직임을 통해 통증 수준을 즉시 평가할 수 있다. 본 시스템은 VR 기술을 진보된 안구 운동 탐지 및 얼굴 표정 인식 알고리즘과 통합하여, 객관적인 통증 평가를 제공한다. 정밀한 안구 추적은 CamShift와 AdaBoost 알고리즘을 통해 구현되며, 통증 분류의 정확도는 ResNet18 및 Swin Transformer 아키텍처를 통합한 얼굴 인식 시스템을 통해 향상된다. 공개적으로 사용이 가능한 데이터 세트을 이용한 실험 결과는 제안된 방법이 통증 인식에서 높은 정확도를 달성하는 데 효과적임을 보여준다. 향후, 다양한 분야에서 VR 기반의 통증 평가 시스템을 적용하여 그 정확도를 높일 것으로 기대된다.","This paper proposes a novel VR-based rapid eye movement detection algorithm for high-precision facial pain recognition. Children often show fear and resistance to vaccinations and dental procedures, making treatment difficult. Therefore, the proposed method provides cartoons through VR to divert children's attention, while pain levels can be immediately assessed through eye movements. By integrating VR technology with advanced eye movement detection and facial expression recognition algorithms, the system provides objective pain assessment. Precise eye tracking is achieved using CamShift and AdaBoost algorithms, while pain classification accuracy is enhanced through a facial recognition system integrating ResNet18 and Swin Transformer architectures. Experimental results using publicly available datasets demonstrate the effectiveness of the proposed method in achieving high accuracy in pain recognition. In the future, it is expected that the accuracy of the VR-based pain assessment system will be improved by applying it in various fields."
배 병해충 이미지 분류를 위한 딥러닝 최적 모델 선택에 관한 연구,2024,"['배 병해충', '딥러닝 모델', '이미지 분류', '검역', '데이터 증강', 'Pear Pests and Diseases', 'Deep Learning Models', 'Image Classification', 'Quarantine', 'Data Augmentation']",,"With the increase in agricultural exports, pest and disease quarantine measures have been strengthened globally. Upon detection of pests or diseases in agricultural products, the entire shipment must be recalled or discarded. Therefore, detecting pests during the post-harvest sorting process is critical. This study aims to identify the optimal deep-learning model for classifying healthy and pest-infested pears during sorting. To achieve this, a dataset was created by collecting images of pest-infested pears under conditions similar to publicly available healthy pear images. The study compares CNN-based models (ResNet, MobileNet, EfficientNet, ConvNext) and a transformer-based model (ViT) using the dataset. Standard learning parameters and data augmentation techniques were also evaluated. Accuracy and Grad-CAM were used to analyze model performance. The results indicate that ResNet101 achieved the best performance based on accuracy and Grad-CAM."
DNN을 위한 비트 단위 파라미터 조작 프레임워크 및 파라미터와 정확도 간의 상호 연관성 분석,2024,"['딥 뉴럴 네트워크', '파라미터', '비트 연산', '정확도', '강인성', 'Deep Neural Network', 'parameters', 'bit-wise operations', 'accuracy', 'robustness']","최근 DNN이 다양한 산업에 확산되면서 IoT 기기 및 엣지 컴퓨팅에 적합한 경량 모델에 관한 연구가 급증하고 있 다. 본 논문에서는 기존에 없던 딥러닝 모델의 파라미터를 1 비트 단위로 조작할 수 있는 자동화 프레임워크를 개발 하며 파라미터 비트와 모델 정확도 사이의 관계를 실험 및 연구한다. 본 연구는 제안된 프레임워크를 사용하여 ImageNet 데이터셋으로 사전 학습된 DNN 모델 중 CNN 모델들의 파라미터를 하위 n-bit를 0, 1 또는 랜덤한 값으로 치환하는 3가지 방법을 통해 각각 정보 손실 발생시키면서 파라미터와 정확도 간의 강인성을 비트 단위로 실험하였다. 주요 모델로는 InceptionV3, InceptionResnetV2, ResNet50, Xception, DenseNet121, Mobile NetV1, MobileNetV2 을 사용하였다. 실험 결과, 성능이 낮은 모델일수록 하위 비트의 정보 손실에 민감하여 성 능이 좋은 모델보다 정확도를 유지하는 비트 수가 적다는 것을 실험적으로 확인했고, 파라미터와 정확도 간의 강인 성이 높다는 것을 확인하였다. 이러한 실험을 바탕으로 모델별 유효 파라미터 비트를 설정하여 파라미터를 줄이며 정확도를 유지할 수 있다.","Recently, with the proliferation of DNNs in various industries, there has been a surge in research on lightweight models suitable for IoT devices and edge computing. In this paper, we propose an automated framework that enables manipulation of deep learning model parameters at a 1-bit level, a capability not previously available. We investigate the relationship between parameter bits and model accuracy. Using the developed framework, we systematically experimented with the parameters of CNN models pre-trained on the ImageNet dataset by setting the lower n-bit to 0, 1, or a random value while each method inducing distinct information loss. The primary models evaluated include InceptionV3, InceptionResnetV2, ResNet50, Xception, DenseNet121, MobileNetV1, and MobileNetV2. Experimental results show that models with lower performance are more sensitive to information loss in the lower bits, requiring fewer bits to maintain accuracy compared to high-performing models. This concludes a high robustness between parameters and accuracy."
생체정보 보호를 위한 CNN 기반의 홍채 지문 영역 분할,2024,"['Segmentation', 'Detection', 'Artificial Intelligence Learning', 'Biometric Information', 'CNN', '영역 분할', '감지', '인공지능 학습', '생체 정보', '합성곱 신경망 모델']","스마트 기기의 발달과 고해상도 이미징 기술의 대중화로 인해, 지문이 노출되거나, 화상 회의, 화상 통화 등 고화질 얼굴 사진에서 홍채 정보가 노출되고 있다. 스마트기기의 대중화는 일상적인 디지털 활동에서 무분별한 사진 공유로 인해, 개인의 생체정보인 지문이나 홍채가 노출되어 위조 및 악용될 가능성이 높아지고 있다. 이러한 문제를 해결하기 위해서 본 논문에서는 원본 이미지로부터 생체정보를 보호하고 보안을 강화할 목적으로 CNN의 영역 분할기법을 활용하여 이미지 내 지문과 홍채를 식별 보호하는 방안을 제안한다. 제안된 모델은 입력 이미지에서 지문 및홍채를 식별한 후 해당 영역에 블러 처리를 적용하며, 이를 원본 이미지와 결합하여 보안성을 강화한다. U-Net 및ResNet-34를 백본으로 사용한 모델 구조를 통해 학습 시간 및 성능을 비교한다.","With the development of smart devices and the popularization of high-resolution imaging technology, fingerprints arebeing exposed, and iris information is being exposed in high-definition facial photos such as video conferences and videocalls. The popularization of smart devices has increased the possibility of personal biometric information such asfingerprints and irises being exposed and forged and misused due to indiscriminate photo sharing in everyday digitalactivities. To solve this problem, this paper proposes a method to identify and protect fingerprints and irises in imagesby utilizing CNN's region segmentation technique for the purpose of protecting biometric information from the originalimage and enhancing security. The proposed model identifies fingerprints and irises in the input image, applies blurringto the corresponding regions, and combines them with the original image to enhance security. The learning time andperformance are compared through model structures using U-Net and ResNet-34 as backbones."
볼륨-플로우 그래프 기반 폐질환 분류를 위한앙상블 딥러닝 모델,2024,"['합성곱 신경망', '앙상블 딥러닝 모델', '폐질환', '볼륨-플로우 그래프', 'Convolutional Neural Network', 'Ensemble Deep Learning Model', 'Pulmonary Disease', 'Flow Volume Loops']","만성 폐쇄성 폐질환은 만성적인 기도 폐쇄를 특징으로 하는 호흡기 질환이다. 만성 폐쇄성폐질환은 초기에 자각 증상이 거의 없어, 대부분 중증 상태로 악화된다. 또한, 인종, 성별, 키,몸무게 등 다양한 요인을 포함한 폐 질환 분류 회귀식은 복잡하고, 정확한 판별을 위해서는지속적인 갱신을 필요로 한다. 따라서 폐질환의 초기 진단이 용이하도록 간편한 휴대형 페기능 검사기를 통해 산출 가능한 볼륨-플로우 그래프 이미지 기반 분류 모델이 요구된다.본 논문에서는 폐질환 조기 진단을 위해 볼륨-플로우 그래프 이미지의 전처리 및 합성곱 신경망 기반 앙상블 딥러닝 모델을 구현하였고, 이를 검증했다. 합성곱 신경망 기반 앙상블 딥러닝 모델은 VGG16, VGG19, ResNet50, 그리고 MobileNet 구조 기반 4개의 모델로 구성되며, 전부 전이학습 및 미세조정하여 사용하였다. 세부적으로는 부족한 수의 학습 데이터를볼륨-플로우 그래프 이미지의 특성을 고려하여 적합한 데이터 증강기법을 적용하였고, 4개의 모델들은 가중치 기반의 간접투표 방식을 사용했다. 최종 앙상블 모델은 단순히 폐질환유무를 판별하는 것이 아닌 정상, 제한성 폐질환, 폐쇄성 폐질환, 그리고 혼합성 폐질환과 같이 총 4개의 클래스로 분류하는 모델임에도 불구하고, 테스트 데이터를 통한 성능은 정확도90.91%, 가중치 평균 정밀도 91.11%, 가중치 평균 재현율 90.91%로 높은 수치를 보였다.","Chronic Obstructive Pulmonary Disease (COPD) is a respiratory disease characterized bychronic airway obstruction. COPD often progresses to a severe stage, since thereare few noticeable symptoms in the early stages. Also regression equations involingvarious factors such as race, gender, height, and weight to determine whether ornot there is pulmonary disease is complex and needs to be updated periodically.Therefore, there is a demand for a system that can easily analze the presence orabsence of the pulmonary disease, even for non-experts. In this paper, aCNN-based flow volume loops classification model using ensemble learning andappropriate data pre-processing algorithms was proposed and validated to diagnosepulmonary disease in the early stages. The ensemble model was organized by fourCNN models based on VGG16, VGG19, Resnet50, and MobileNet and used transferlearning and fine-tuning for each pre-trained model. Specifically, to overcome asmall amount of data, several data augmentation techniques that took into accountthe characteristics of flow volume loops were used, and soft voting was employedfor the ensemble model. The proposed ensemble model not only could diagnose thepresence or absence of pulmonary disease but could also classify into a total of fourcategories: normal, restrictive, obstructive, and combined pulmonary diseases. As aresult of the experiment, the performance of the proposed ensemble model showedan accuracy of 90.91%, precision of 91.11%, and recall of 90.91%."
흉부 X-선 영상을 이용한 Vision transformer 기반 폐렴 진단 모델의 성능 평가,2024,"['딥러닝', '폐렴 진단', '흉부 X-선 영상', 'Vision transformer', 'Deep learning', 'Pneumonia detection', 'Chest X-ray image']",,"The various structures of artificial neural networks, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have been extensively studied and served as the backbone of numerous models. Among these, a transformer architecture has demonstrated its potential for natural language processing and become a subject of in-depth research. Currently, the techniques can be adapted for image processing through the modifications of its internal structure, leading to the development of Vision transformer (ViT) models. The ViTs have shown high accuracy and performance with large data-sets. This study aims to develop a ViT-based model for detecting pneumonia using chest X-ray images and quantitatively evaluate its performance. The various architectures of the ViT-based model were constructed by varying the number of encoder blocks, and different patch sizes were applied for network training. Also, the performance of the ViT-based model was compared to the CNN-based models, such as VGGNet, GoogLeNet, and ResNet. The results showed that the traninig efficiency and accuracy of the ViT-based model depended on the number of encoder blocks and the patch size, and the F1 scores of the ViT-based model ranged from 0.875 to 0.919. The training effeciency of the ViT-based model with a large patch size was superior to the CNN-based models, and the pneumonia detection accuracy of the ViT-based model was higher than that of the VGGNet. In conclusion, the ViT-based model can be potentially used for pneumonia detection using chest X-ray images, and the clinical availability of the ViT-based model would be improved by this study."
BCED-Net: Breast Cancer Ensemble Diagnosis Network using transfer learning and the XGBoost classifier with mammography images,2024,"['Breast cancer classification', 'Feature extraction and concatenation', 'Performance evaluation', 'XGBoost classifier']",,"Objectives Breast cancer poses a significant global health challenge, characterized by complex origins and the potential for life-threatening metastasis. The critical need for early and accurate detection is underscored by the 685,000 lives claimed by the disease worldwide in 2020. Deep learning has made strides in advancing the prompt diagnosis of breast cancer. However, obstacles persist, such as dealing with high-dimensional data and the risk of overfitting, necessitating fresh approaches to improve accuracy and real-world applicability.Methods In response to these challenges, we propose BCED-Net, which stands for Breast Cancer Ensemble Diagnosis Network. This innovative framework leverages transfer learning and the extreme gradient boosting (XGBoost) classifier on the Breast Cancer RSNA dataset. Our methodology involved feature extraction using pre-trained models—namely, Resnet50, EfficientnetB3, VGG19, Densenet121, and ConvNeXtTiny—followed by the concatenation of the extracted features. Our most promising configuration combined features extracted from deep convolutional neural networks—namely Resnet50, EfficientnetB3, and ConvNeXtTiny—that were classified using the XGBoost classifier.Results The ensemble approach demonstrated strong overall performance with an accuracy of 0.89. The precision, recall, and F1-score values, which were all at 0.86, highlight a balanced trade-off between correctly identified positive instances and the ability to capture all actual positive samples.Conclusion BCED-Net represents a significant leap forward in addressing persistent issues such as the high dimensionality of features and the risk of overfitting."
Vision Transformer를 활용한 운전자 이상행동 분류 딥러닝 시스템,2024,"['도로 교통', '운전자 이상행동', '딥러닝', 'Vision Transformer', 'Road traffic', 'Driver abnormal behavior', 'Deep learning', 'Vision Transformer']","도로 교통 사고와 교통 위반 행동은 현대 사회에서 급증하는 문제로, 이에 대한 효과적인 대응이 필요하다. 이러한 사고와 위반 행동은 세계적으로 증가하는 추세를 보이며, 그로 인한 사회 및 경제적 영향은 상당히 심각하다. 주로 운전자의 부주의로 발생하는 도로 교통 사고를 예방하기 위해, 딥러닝과 머신러닝을 활용한 시스템이 구축되고 있다. 이전의 연구 들은 주로 운전자의 이미지를 기반으로 한 모델을 사용하여 운전자의 이상행동을 감지하는 데 초점을 맞추었다. 그러나 이러한 기존 연구들은 대부분 컨볼루션 기반의 모델을 사용하여 운전자의 이상행동을 감지하고 분류하는 데 중점을 두고 있다. 컨볼루션 기반 모델은 초기 학습 단계에서 이미지에서 특정 패턴 및 특징을 학습하고, 이를 고정된 크기의 필터로 추출하는 특징이 있다. 이는 다양한 운전 상황에 대한 적응성이 제한된다는 한계가 있다. 따라서 본 논문은 컨볼루션 기반 모델의 한계를 극복하고자, Vision Transformer 모델을 활용한 운전자 이상행동 분류 모델을 구축하였다. 해당 모델의 우수성을 확인하기 위해 기존 연구에서 사용된 ResNet-101, VGG19, Xception, ConvNeXt 등의 모델과 분류 성능 평가 지표를 기반으로 비교 분석을 실시하였다. 비교 분석 결과, Vision Transformer 모델이 기존의 컨볼루션 기반 모델들보다 탁월한 성능을 보여주었다. 이러한 결과는 Vision Transformer의 학습 방식이 다양한 특징 및 패턴을 효과적으로 학습하고 이를 활용할 수 있음을 시사한다. 본 연구는 도로 교통 안전성 향상을 위한 혁신적인 모델의 가능성을 제시하며, 더 나아가 안전 운전 문화의 정착과 사회적 이익을 증진시킬 수 있다.","The surge in road traffic accidents and traffic violations is a pressing issue in modern society, demanding effective responses. These incidents display a global upward trend, with significant societal and economic repercussions. To mitigate road accidents, primarily caused by driver negligence, systems leveraging deep learning and machine learning are being developed. Previous research has predominantly focused on models based on driver images for detecting abnormal driving behavior, with a predominant emphasis on convolutional models. Convolutional models learn specific patterns and features from images during the initial stages of training, extracting them using fixed-size filters, thereby limiting adaptability to diverse driving scenarios. This paper addresses the limitations of convolutional models by introducing a driver abnormal behavior classification model using the Vision Transformer. To validate the superiority of this model, a comparative analysis was conducted with well-established models such as ResNet-101, VGG19, Xception, and ConvNeXt, employing classification performance metrics from previous studies. The results of the comparative analysis demonstrate that the Vision Transformer model outperforms traditional convolutional models. This outcome indicates the effectiveness of Vision Transformer’s learning approach in efficiently capturing and utilizing various features and patterns. This research not only presents the potential for an innovative model to enhance road traffic safety but also pledges to contribute to the establishment of a safety-oriented driving culture and the enhancement of societal benefits."
A study of Strawberry Maturity Classification Using Improved Faster R-CNN,2024,"['Convolutional Neural Network (CNN)', 'Faster R-CNN', 'Image Classification', 'RoI Align', 'Strawberry Maturity']",,"In strawberry cultivation, maturity classification plays an important role in ensuring the efficiency and quality of harvesting. In this study, we propose an Improved Faster R-CNN model to address these challenges, using MobileNetV3-Large as the backbone network to achieve a lightweight model, and introducing RoI Align to improve the spatial accuracy of the feature map. Experiments are conducted using the KGCV_Strawberry dataset, with precision, recall, F1 score, and mean average precision (mAP) measured for performance evaluation. The experimental results show that the proposed model achieves an average precision of 71.35%, recall of 71.07%, and F1 score of 71.21% across all classes. In particular, the proposed model achieves 63% performance on mAP0.5 and 58% performance on mAP0.5:0.95, which is comparable to existing ResNet-based models while achieving faster inference speed. The proposed model achieves a processing speed of 27.6543 ms, which is about 2 ms faster than existing ResNet-based models. This indicates that the goal of creating a lightweight model with improved image processing capability was achieved with minimal performance degradation. This research is expected to contribute to the development of automated strawberry cultivation systems in greenhouse environments and has the potential to be applied to various agricultural environments in the future."
시간-주파수 도메인 변환 및 W2GAN-GP 모델 기반의 향상된 오디오 데이터 증강,2024,"['Audio augmentation', 'WGAN-GP', 'Time-frequency transformation', 'Speech classification', 'Imbalanced data']","최근 딥러닝 기술은 다양한 분야의 분류 시스템에 활용됨에 따라 점차 딥러닝 모델의 성능을 극대화하기 위한 연구가 활발하게 진행되고 있다. 딥러닝 모델의 성능은 학습 데이터의 양과 품질에 따라 많은 영향을 받으며, 딥러닝 모델은 깊고 복잡하게 설계될수록 더 많은 학습 데이터가 요구된다. 또한 학습 데이터가 부족하거나 클래스 간의 데이터 불균형이 존재할 경우, 과 적합 현상이 발생하며 성능이 저하되는 문제가 발생한다. 음성 및 오디오를 활용하는 분야에서 분류 성능을 높이기 위해서 학습 데이터 확장 및 클래스 간 불균형 문제는 중요한 이슈이며, 이를 해결하기 위한 연구는 반드시 필요하다. 본 논문에서는 2차원 이미지 증강을 위해 고안된 WGAN 모델을 개선하여 1차원 오디오 신호를 효과적으로 증강하는 W2GAN-GP 증강 모델을 제안한다. 원 신호 데이터를 입력 받아 시간-주파수 변환 기법을 이용하여 1차 증강을 수행하고, 제안된 W2GAN-GP 모델을 이용하여 2차 증강을 통한 듀얼 오디오 신호 증강 기법을 제안한다. 또한 제안한 증강기법으로 생성된 오디오 데이터의 유효성을 검증하기 위해서 ResNet50 및 DenseNet 분류 모델을 이용하여 분류 정확도를 측정하였다. 분류 모델을 통한 검증 결과, 증강을 수행하지 않은 경우보다 약 27~30% 의 정확도가 높아진 것을 확인할 수 있었다.","Recently, as deep learning technology is being utilized in classification systems in various fields, research is being actively conducted to maximize the performance of deep learning models. The performance of deep learning models is greatly affected by the amount and quality of training data, and the deeper and more complex the design of a deep learning model, the more training data is required. Additionally, if training data is insufficient or there is data imbalance between classes, over-fitting occurs and performance deteriorates. In order to improve classification performance in fields that utilize voice and audio, learning data expansion and imbalance between classes are important issues, and research to resolve these issues is essential. In this paper, we propose a W2GAN-GP augmentation model that effectively augments one-dimensional audio signals by improving the WGAN model designed for two-dimensional image augmentation. We propose a dual audio signal augmentation technique by performing the first augmentation on the original signal data using the time-frequency transform technique, and then performing the second augmentation using the proposed W2GAN-GP model. To verify the validity of the audio data generated by the proposed augmentation technique, classification accuracy was measured using ResNet50 and DenseNet classification models. As a result of verification through the classification model, it was confirmed that the accuracy increased by about 27 to 30% compared to the case where augmentation was not performed."
패션 카테고리 오버샘플링 자동화 시스템,2024,"['CNN', 'Deep learning', 'Oversampling', 'ResNet50', 'YOLOv8', 'Fashion category', '컨볼루션 신경망', '딥러닝', '오버샘플링', '레즈넷50', '욜로 버전 8', '패션 카테고리']",,
An Improved Classification Model Based on Feature Fusion for Orchid Species,2024,['Orchid species · Classifcation model · Feature fusion · ResNet34'],,"Orchid is a kind of terrestrial herb and it has elegant fower posture, quiet fower fragrance, rich colors and noble moral, therefore it has high ornamental value and is deeply loved by people. There are many kinds of orchids, and some of them are similar in shape, texture and color, which make people difcult to quickly and correctly distinguish them. As the existing classifcation model of orchid species have the problems of low accuracy rate and long classifcation time because of the inter species similarities and intra species diferences in orchid species, thus infuencing its wide application. In order to solve the problem above, in this paper, an improved classifcation model based on feature fusion is proposed for orchid species. The achievement of the paper lies in the fact that we successfully developed a classifcation model based on feature fusion to realize the high-efcient classifcation for orchid species. Specifcally, in our scheme, frstly we obtained 12 orchid image sets with number of 12,227 images by network and feld photography; Secondly we analyzed and studied the semantic relationship of diferent scale features from acquired orchid images above; Thirdly we designed an improved classifcation model based on feature fusion on the basis of the semantic relationship above; At last, we used the classifcation model above to realize the high-efcient classifcation for 12 orchid species. The experimental results showed that our proposed classifcation model based on feature fusion in this paper can realize 92.98% classifcation accuracy rate compared with classifcation models without using feature fusion technology, which can greatly improve the classifcation efciency for orchid species."
머신러닝기반 오이 생육 최적 예측 모델에 관한 연구,2024,"['작물 질병 진단', '딥러닝 모델 비교', '레스넷50', '스마트 농업', '욜로v8', 'Crop Disease Diagnosis', 'Deep Learning Model Comparison', 'ResNet50', 'Smart Agriculture', 'YOLOv8']",,
딥러닝 기반의 빛간섭단층촬영 이미지에서 안구 방향 분류 모델,2024,"['Deep learning', 'Eye', 'Optical coherence tomography']",,"Purpose: To develop a deep learning model classifying the laterality of optical coherence tomography (OCT) images.Methods: The study included two-dimensional OCT images (horizontal/vertical macular section) from Seoul National University Hospital.A deep learning model based on ResNet-18 was developed and trained to classify whether OCT images were horizontal or vertical sections and to predict the laterality of the images. Analysis of the results included calculating a mean area under the receiver operating characteristic curve (AUROC) and evaluating accuracy, specificity, and sensitivity. Gradient-weighted class activation for mapping visualization highlighted critical regions for classification.Results: A total of 5,000 eyes of 2,500 patients (10,000 images) was included in the development process. The test dataset consisted of 1,000 eyes of 500 patients (590 eyes without macular abnormalities, 208 epiretinal membranes, 111 age-related macular degenerations, 56 central macular edemas, 23 macular holes, and 12 other macular abnormalities). The deep learning model predicted the OCT section of the eyes in the test dataset with a mean AUROC of 0.9967. The accuracy, sensitivity, and specificity were 0.9835, 0.9870, and 0.9800, respectively.The model predicted the laterality of the eyes in horizontal OCT images with a mean AUROC of 1.0000. The accuracy, sensitivity, and specificity were 0.9970, 1.0000, and 0.9940, respectively. Using vertical OCT images, deep learning models failed to demonstrate any predictive performance in laterality classification.Conclusions: We developed a deep learning model to classify the horizontal/vertical sections of OCT images and predict the laterality of horizontal OCT images with high accuracy, sensitivity, and specificity."
An Ensemble Deep Transfer Learning Model for Multi-dimensional Image Classification of Histological Prostate Biopsy Patterns,2024,"['Multi-dimensional', 'Histological', 'Prostate Cancer', 'Ensemble Deep Transfer Learning', 'Image Classification']",,"Early prostate cancer diagnosis by pathologists remains challenging. Recent advances in computer-aided detection (CAD), artificial intelligence (AI), and machine learning (ML) allow prostate cancer grading. This study explored the accuracy of prostate cancer detection by deep learning techniques, particularly convolutional neural networks (CNNs). We performed three-way binary classification based on images cropped to 256 × 256 and 512 × 512 pixels using an ensemble deep CNN model. Six pre-trained CNN models (MobileNet, VGG-16, ResNet-50, DenseNet-121, Inception-V3, and EfficientNet-B0) were integrated to classify histopathological features. The overall accuracy for the combined 256 × 256 and 512×512 pixel images was 94.9%. Additionally, in separate classifications of 256×256 and 512×512 images, we achieved overall accuracies of 90.8% and 94.3%, respectively. Consequently, our method effectively distinguishes benign from malignant samples, approaching near-perfect accuracy."
Automated Detection of COVID-19 in Chest Radiographs: Leveraging Machine Learning Approaches,2024,"['COVID-19 pandemic', 'Machine learning models', 'Chest X-ray classification', 'Automated identification', 'Medical diagnosis']",,"The World Health Organization (WHO) has designated the COVID-19 pandemic a global health emergency, prompting responses all over the world. The fatality rate is between 2% and 5%, and millions of people around the world have been infected. While the WHO recommends tests, resource-intensive testing has motivated the development of CNN technology for automated identification. Research employing machine learning models shows great accuracy in classifying X-ray and CT images for COVID-19 detection. These models include denseNet201, resnet50V2, inceptionv3, mobile net, and custom CNNs. The interpretation of chest X-rays has come a long way, yet there are still obstacles to overcome. In this paper, we present a way for using a machine learning model to categorize chest X-ray pictures into normal, COVID-19, viral pneumonia, and lung opacity, demonstrating the model's efficacy in assisting medical diagnosis, especially in time-sensitive situations like COVID-19."
Deep learning to assess bone quality from panoramic radiographs: the feasibility of clinical application through comparison with an implant surgeon and cone-beam computed tomography,2024,"['Artificial intelligence', 'Bone density', 'Deep learning', 'Dental implant', 'Dental radiography']",,"Purpose: Bone quality is one of the most important clinical factors for the primary stability and successful osseointegration of dental implants. This preliminary pilot study aimed to evaluate the clinical applicability of deep learning (DL) for assessing bone quality using panoramic (PA) radiographs compared with an implant surgeon's subjective tactile sense and cone-beam computed tomography (CBCT) values. Methods: In total, PA images of 2,270 edentulous sites for implant placement were selected, and the corresponding CBCT relative gray value measurements and bone quality classification were performed using 3-dimensional dental image analysis software. Based on the pre-trained and fine-tuned ResNet-50 architecture, the bone quality classification of PA images was classified into 4 levels, from D1 to D4, and Spearman correlation analyses were performed with the implant surgeon's tactile sense and CBCT values. Results: The classification accuracy of DL was evaluated using a test dataset comprising 454 cropped PA images, and it achieved an area under the receiving characteristic curve of 0.762 (95% confidence interval [CI], 0.714-0.810). Spearman correlation analysis of bone quality showed significant positive correlations with the CBCT classification (r=0.702; 95% CI, 0.651-0.747; P<0.001) and the surgeon's tactile sense (r=0.658; 95% CI, 0.600-0.708, P<0.001) versus the DL classification. Conclusions: DL classification using PA images showed a significant and consistent correlation with CBCT classification and the surgeon's tactile sense in classifying the bone quality at the implant placement site. Further research based on high-quality quantitative datasets is essential to increase the reliability and validity of this method for actual clinical applications."
연속 웨이블릿 변환을 이용한 On-board Charger의 전이 학습 기반 고장 진단 알고리즘,2024,"['on-board charger', 'fault diagnosis', 'wavelet transform', 'transfer learning', 'convolutional neural network', '.']",,"In this study, a method is proposed for diagnosing the normal operation and faults of an on-board charger (OBC), the battery charging device of a vehicle, using the convolutional neural network algorithm. In the conducted experiments, faults were defined as short and open states of the switching component in the power factor correction (PFC) stage of the OBC topology. To achieve this, the current data of the PFC boost inductor collected through PSIM simulations were reconstructed into images using continuous wavelet transform. In the MATLAB environment, transfer learning was implemented using pre-trained models such as GoogLeNet, ResNet50, ShuffleNet, and MobileNetV2 for diagnosing faults in the OBC. Experimental results revealed that MobileNetV2 achieved a validation accuracy of 95.91%. Subsequently, the performance of the model was analyzed using a confusion matrix on the test set, yielding an accuracy of 96.53%. These findings underscore the effectiveness of the proposed approach in classifying normal and faulty data."
주행 시스템을 위한 그리드 레벨 다중 클래스 이뮬 분류 모델,2024,"['soiling detection', 'woodscape dataset', 'image classification', 'autonomous driving', '.']",,"This study proposes a soiling detection algorithm to identify and locate contamination in vehicle camera lenses. Research on AI applications that utilize cameras and distance sensors in driving systems is directly related to the advancement of autonomous driving systems., Detecting soiling, such as mud and water droplets, is a particularly critical issue. Traditional methods using piezoelectric, ultrasonic, and thermal sensors can introduce additional design and maintenance complexity. Therefore, this study aims to reduce this complexity by utilizing existing surround-view cameras installed on vehicles without additional sensors. The proposed algorithm employs image-processing techniques and a lightweight neural network architecture based on ResNet18 to detect lens contamination in real-time across various driving environments. Experiments were conducted using 5,000 images from the WoodScape Soiling Dataset. The input images were divided into a 16 × 16 grid and classified into four labels: opaque, semi-transparent, transparent, and clean. The proposed grid-level multiclass soiling classification model demonstrated effective performance in detecting contamination, including mud, water droplets, and foggy dust. This study is expected to enhance the safety and convenience of driving systems."
Deep learning to assess bone quality  from panoramic radiographs: the  feasibility of clinical application  through comparison with an implant  surgeon and cone-beam computed  tomography,2024,"['Artificial intelligence', 'Bone density', 'Deep learning', 'Dental implant', 'Dental radiography']",,"Purpose: Bone quality is one of the most important clinical factors for the primary stability and successful osseointegration of dental implants. This preliminary pilot study aimed to evaluate the clinical applicability of deep learning (DL) for assessing bone quality using panoramic (PA) radiographs compared with an implant surgeon’s subjective tactile sense and cone-beam computed tomography (CBCT) values.Methods: In total, PA images of 2,270 edentulous sites for implant placement were selected, and the corresponding CBCT relative gray value measurements and bone quality classification were performed using 3-dimensional dental image analysis software. Based on the pre-trained and fine-tuned ResNet-50 architecture, the bone quality classification of PA images was classified into 4 levels, from D1 to D4, and Spearman correlation analyses were performed with the implant surgeon’s tactile sense and CBCT values.Results: The classification accuracy of DL was evaluated using a test dataset comprising 454 cropped PA images, and it achieved an area under the receiving characteristic curve of 0.762 (95% confidence interval [CI], 0.714–0.810). Spearman correlation analysis of bone quality showed significant positive correlations with the CBCT classification (r=0.702; 95% CI, 0.651–0.747; P<0.001) and the surgeon’s tactile sense (r=0.658; 95% CI, 0.600–0.708, P<0.001) versus the DL classification.Conclusions: DL classification using PA images showed a significant and consistent correlation with CBCT classification and the surgeon’s tactile sense in classifying the bone quality at the implant placement site. Further research based on high-quality quantitative datasets is essential to increase the reliability and validity of this method for actual clinical applications."
Sharpness-Aware Minimization을 적용한 Separable Vision Transformer 기반 악성코드 유형 분류 기법,2024,"['Malware', 'Classification Method', 'Separable Vision Transformer', 'SharpnessAware Minimization', '악성코드', '유형 분류 기법', 'Separable Vision Transformer', 'Sharpness-Aware Minimization']",,"""The methods of classifying malware family through malware visualization generate malware images and then classify malware family using artificial intelligence models such as Convolutional Neural Networks(CNNs). However, such methods are vulnerable to malware obfuscation techniques. In this paper, we propose a malware classification method based on the Separable Vision Transformer(SepViT) that is robust against obfuscation techniques. The proposed method performs malware family classification using a SepViT model enhanced with Sharpness-Aware Minimization(SAM) after visualizing malware as grayscale images. From the experimental results using the Microsoft Malware Classification Challenge dataset, we show that SAM Optimizer-based SepViT used in the proposed method can classify malware family more accurately than four methods(ResNet18, ViT, CrossViT, SepViT). We also analyze the basis for classification of the proposed method using Grad-Cam. In addition, from the experiments using AndroDex dataset, we show that the proposed method shows good detection performance even in the presence of obfuscation in malware."""
Convolutional Neural Network Technique for Distinguishing Nine Varieties of Vegetable Crops,2024,"['Confusion matrix', 'Convolutional neural network', 'DenseNet201', 'Image classification', 'Vegetables']",,"In the field of agriculture, conducting research on neural network models for image classification is necessary to accurately categorize crops based on their types and health conditions and distinguish them from other species, to minimize crop losses. This study aimed to compare multiple neural network models to select the optimal model that can classify the images of nine vegetable seedlings, such as carrot, Kimchi cabbage, kohlrabi, lettuce, mallow, mustard, pak-choi, spinach, and sweet pepper. The best model was selected based on its accuracy (precision, recall, and F1 score) from eight trained models, namely DenseNet201, InceptionResNetV2, InceptionV3, MobileNetV2, ResNet152V2, VGG16, VGG19, and Xception. To train the models, a 9-class dataset, 20 epochs, 32 batch sizes, Adam optimizer, and a learning rate of 0.001 were used. The DenseNet201 model exhibited the highest accuracy and was, therefore, selected as the optimal model. With a batch size of 128, Adam optimizer, and a learning rate of 0.001, this model exhibited high precision, recall, and F1 score, and its superiority was confirmed using a confusion matrix. As a result, the DenseNet201 model is expected to improve the recognition performance of the model by using images of various plant species, exploring more networks, and optimizing the hyperparameters to achieve higher recognition accuracy."
Inceptionv3-LSTM-COV: A multi-label framework for identifying adverse reactions to COVID medicine from chemical conformers based on Inceptionv3 and long short-term memory,2024,"['adverse medicine reactions', 'COVID medicine development', 'Inceptionv3', 'LSTM', 'multi-label']",,"Due to the global COVID-19 pandemic, distinct medicines have been devel-oped for treating the coronavirus disease (COVID). However, predicting and identifying potential adverse reactions to these medicines face significant chal-lenges in producing effective COVID medication. Accurate prediction of adverse reactions to COVID medications is crucial for ensuring patient safety and medicine success. Recent advancements in computational models used in pharmaceutical production have opened up new possibilities for detecting such adverse reactions. Due to the urgent need for effective COVID medication development, this research presents a multi-label Inceptionv3 and long short-term memory methodology for COVID (Inceptionv3-LSTM-COV) medicine development. The presented experimental evaluations were conducted using the chemical conformer image of COVID medicine. The features of the chemi-cal conformer are denoted utilizing the RGB color channel, which is extracted using Inceptionv3, GlobalAveragePooling2D, and long short-term memory (LSTM) layers. The results demonstrate that the efficiency of the Inceptionv3-LSTM-COV model outperformed the previous study’s perfor-mance and achieved better results compared to MLCNN-COV, Inceptionv3, ResNet50, MobileNetv2, VGG19, and DenseNet201 models. The proposed model reported the highest accuracy value of 99.19% in predicting adverse reactions to COVID medicine."
MLCNN-COV: A multilabel convolutional neural network-based framework to identify negative COVID medicine responses from the chemical three-dimensional conformer,2024,"['chemical three-dimensional conformers', 'convolutional neural network', 'COVID medicine development', 'negative medicine reactions', 'transfer-learning']",,"To treat the novel COronaVIrus Disease (COVID), comparatively fewer medicines have been approved. Due to the global pandemic status of COVID, several medicines are being developed to treat patients. The modern COVID medicines development process has various challenges, including predicting and detecting hazardous COVID medicine responses. Moreover, correctly pre-dicting harmful COVID medicine reactions is essential for health safety. Significant developments in computational models in medicine development can make it possible to identify adverse COVID medicine reactions. Since the beginning of the COVID pandemic, there has been significant demand for developing COVID medicines. Therefore, this paper presents the transfer-learning methodology and a multilabel convolutional neural network for COVID (MLCNN-COV) medicines development model to identify negative responses of COVID medicines. For analysis, a framework is proposed with five multilabel transfer-learning models, namely, MobileNetv2, ResNet50, VGG19, DenseNet201, and Inceptionv3, and an MLCNN-COV model is designed with an image augmentation (IA) technique and validated through experiments on the image of three-dimensional chemical conformer of 17 number of COVID medicines. The RGB color channel is utilized to represent the feature of the image, and image features are extracted by employing the Convolution2D and MaxPooling2D layer. The findings of the current MLCNN-COV are promising, and it can identify individual adverse reactions of medicines, with the accuracy ranging from 88.24% to 100%, which outper-formed the transfer-learning model’s performance. It shows that three-dimensional conformers adequately identify negative COVID medicine responses."
Improving the Recognition of Known and Unknown Plant Disease Classes Using Deep Learning,2024,"['deep learning', 'plant disease recognition', 'transfer learning', 'unknown disease recognition']",,"Recently, there has been a growing emphasis on identifying both known and unknown diseases in plant disease recognition. In this task, a model trained only on images of known classes is required to classify an input image into either one of the known classes or into an unknown class. Consequently, the capability to recognize unknown diseases is critical for model deployment. To enhance this capability, we are considering three factors. Firstly, we propose a new logits-based scoring function for unknown scores. Secondly, initial experiments indicate that a compact feature space is crucial for the effectiveness of logits-based methods, leading us to employ the AM-Softmax loss instead of Cross-entropy loss during training. Thirdly, drawing inspiration from the efficacy of transfer learning, we utilize a large plant-relevant dataset, PlantCLEF2022, for pre-training a model. The experimental results suggest that our method outperforms current algorithms. Specifically, our method achieved a performance of 97.90 CSA, 91.77 AUROC, and 90.63 OSCR with the ResNet50 model and a performance of 98.28 CSA, 92.05 AUROC, and 91.12 OSCR with the ConvNext base model. We believe that our study will contribute to the community."
Dental Age Estimation in Children Using Convolution Neural Network Algorithm: A Pilot Study,2024,"['Convolutional neural networks', 'Deep learning', 'Dental age estimation']",,"Purpose: Recently, deep learning techniques have been introduced for age estimation, with automated methods based on radiographic analysis demonstrating high accuracy. In this study, we applied convolutional neural network (CNN) techniques to the lower dentition area on orthopantomograms (OPGs) of children to develop an automated age estimation model and evaluate its accuracy for use in forensic dentistry.Methods: In this study, OPGs of 2,856 subjects aged 3-14 years were analyzed. The You Only Look Once (YOLO) V8 object detection technique was applied to extract the mandibular dentition area on OPGs, designating it as the region of interest (ROI). First, 200 radiographs were randomly selected, and were used to train a model for extracting the ROI. The trained model was then applied to the entire dataset. For the CNN image classification task, 80% of OPGs were allocated to the training set, while the remaining 20% were used as the test set. A transfer learning approach was employed using the ResNet50 and VGG19 backbone models, with an ensemble technique combining these models to improve performance. The mean absolute error (MAE) on the test set was used as the validation metric, and the model with the lowest MAE was selected.Results: In this study, the age estimation model developed using mandibular dentition region from OPGs achieved MAE and root mean squared error (RMSE) values of 0.501 and 0.742, respectively, on the test set, and MAE and RMSE values of 0.273 and 0.354, respectively, on the training set.Conclusions: The automated age estimation model developed in this study demonstrated accuracy comparable to that of previous research and shows potential for applications in forensic investigations. Increasing the sample size and incorporating diverse deep learning techniques are expected to further enhance the accuracy of future age estimation models."
Classification of Pulmonary Nodules in 2-[ 18 F]FDG PET/CT Images with a 3D Convolutional Neural Network,2024,['Convolutional neural networks · Positron emission tomography · 2-[18F]FDG PET/CT · Pulmonary nodules · Artificial intelligence'],,"Purpose 2-[18F]FDG PET/CT plays an important role in the management of pulmonary nodules. Convolutional neuralnetworks (CNNs) automatically learn features from images and have the potential to improve the discrimination betweenmalignant and benign pulmonary nodules. The purpose of this study was to develop and validate a CNN model for classificationof pulmonary nodules from 2-[18F]FDG PET images.Methods One hundred thirteen participants were retrospectively selected. One nodule per participant. The 2-[18F]FDG PETimages were preprocessed and annotated with the reference standard. The deep learning experiment entailed random datasplitting in five sets. A test set was held out for evaluation of the final model. Four-fold cross-validation was performed fromthe remaining sets for training and evaluating a set of candidate models and for selecting the final model. Models of threetypes of 3D CNNs architectures were trained from random weight initialization (Stacked 3D CNN, VGG-like and Inceptionv2-like models) both in original and augmented datasets. Transfer learning, from ImageNet with ResNet-50, was also used.Results The final model (Stacked 3D CNN model) obtained an area under the ROC curve of 0.8385 (95% CI: 0.6455–1.0000)in the test set. The model had a sensibility of 80.00%, a specificity of 69.23% and an accuracy of 73.91%, in the test set, foran optimised decision threshold that assigns a higher cost to false negatives.Conclusion A 3D CNN model was effective at distinguishing benign from malignant pulmonary nodules in 2-[18F]FDGPET images."
다층 다요소 시스템의 최적화를 위한 진화연산의 탐색공간 축소 - 딥러닝 가지치기 사례 연구,2024,"['Multilayers Multi-elements System', 'Evolutionary Computation', 'Gene Expression by Rules', 'CNN', 'Filter Pruning']",,"Optimization of multi-layer, multi-element systems such as deep learning is an NP-hard problem that requires determining the number of layers, the number of elements in a layer, and the types of elements. In such a system, deleting redundant elements to reduce the size while maintaining performance is crucial to conserve resources and improve the efficiency of the system. This is a very complex and challenging problem because it consists of a large number of multi-layered and multi-element systems with a huge search space. Evolutionary computation is widely used for large-scale optimization problems due to its high efficiency, but it is difficult to apply due to the characteristics of evolutionary computation when the calculation of the fitness function is complex. To solve this problem, we propose a technique that dramatically reduces the search space by improving the representation of the gene.We verify its feasibility by applying it to a case study, CNN pruning. We use the ResNet56 model for the CIFAR10 dataset and compare it with existing pruning approaches."
재난약자 및 취약시설에 대한 APC실증에 관한 연구,2024,"['재난취약시설', '요구조자', '자동계수기', '인공지능', '영상인식', 'Disaster-vulnerable Facilities', 'Victims', 'Auto People Counter', 'AI', 'Image Recognition']",,"Purpose: The purpose of this study is to improve the recognition rate of APC (Auto People Counting), which accurately identifies the remaining claimants and provides them to response agencies such as fire departments when a disaster occurs in a disaster-vulnerable facility such as a nursing hospital. Currently, when a disaster occurs, response agencies arrive at the disaster site and ask building officials directly to determine the status of those in need within the building. This may be inaccurate information about the rescuer, which may expand the scope of work of the response agency and pose a risk to the safety of the rescuer. APC automatically counts the number of people entering and leaving the building and provides real-time information on the number of people remaining, making it possible to accurately determine the status of those in need in the event of a disaster. The purpose of this study is to select the optimal artificial intelligence algorithm so that APC can more accurately count the number of people entering.Method: In this study, baseline modeling was performed using a CNN model to improve the algorithm that recognizes images of entering personnel through cameras targeting APCs installed and operated in actual disaster-vulnerable facilities. The study was conducted by analyzing the performance of various algorithms to select the top seven candidates and using a transfer learning model to select the optimal algorithm with the best performance.Research Results: As a result of the experiment, the precision and recall of the Densenet201 and Resnet152v2 models, which had the best time and performance, were confirmed to show 100% accuracy for all labels. Among these, the Densenet201 model showed higher performance.Conclusion: Among various artificial intelligence algorithms, the optimal algorithm that can be applied to APC was selected. This will improve the recognition rate of APC and enable quick and safe rescue operations by accurately identifying the information of rescuers in the event of a disaster. This is expected to contribute to ensuring the safety of rescuers performing rescue operations as well as the safe rescue of rescuers. In the future, additional research on algorithm analysis and learning is required to accurately identify the number of people entering disaster-vulnerable facilities in various disaster situations such as haze."
Crop Leaf Disease Identification Using Deep Transfer Learning,2024,"['Agricultural Artificial Intelligence', 'Crop Leaf Disease Identification', 'Plant Protection', 'Transfer Learning']",,"Traditional manual identification of crop leaf diseases is challenging. Owing to the limitations in manpower and resources, it is challenging to explore crop diseases on a large scale. The emergence of artificial intelligence technologies, particularly the extensive application of deep learning technologies, is expected to overcome these challenges and greatly improve the accuracy and efficiency of crop disease identification. Crop leaf disease identification models have been designed and trained using large-scale training data, enabling them to predict different categories of diseases from unlabeled crop leaves. However, these models, which possess strong feature representation capabilities, require substantial training data, and there is often a shortage of such datasets in practical farming scenarios. To address this issue and improve the feature learning abilities of models, this study proposes a deep transfer learning adaptation strategy. The novel proposed method aims to transfer the weights and parameters from pre-trained models in similar large-scale training datasets, such as ImageNet. ImageNet pre-trained weights are adopted and fine-tuned with the features of crop leaf diseases to improve prediction ability. In this study, we collected 16,060 crop leaf disease images, spanning 12 categories, for training. The experimental results demonstrate that an impressive accuracy of 98% is achieved using the proposed method on the transferred ResNet-50 model, thereby confirming the effectiveness of our transfer learning approach."
아음속 수송체 알루미늄 프레임의 비선형 유도초음파 주파수 응답 – 합성곱 신경망 분석 기반 미세 감육 진단 가능성 연구,2024,"['구조 진단', '유도초음파', '심층학습', '합성곱 신경망', '객체검출', 'Structural Health Monitoring', 'Guided Wave', 'Deep Learning', 'Convolution Neural Network', 'Object Detection']",,
Deep Learning-based Arabic Sign Recognition System for Automated Communication with Hearing Impaired Individuals,2024,"['Arabic Sign Language', 'Deep Neural Networks', 'ResNet', 'CNN', 'Automatic Recognition', 'Hearing Impaired']",,"Arabic Sign Language (ArSL) is used by individuals who are hard of hearing or deaf in Arab countries, as well as others around the world who use it for religious purposes. for the need for automated systems to facilitate the learning and communication of ArSL is therefore significant. Such systems would allow people to learn Arabic Sign Language and use it to communicate among themselves and with the surrounding community. This paper presents the development of an automatic recognition system capable of accurately identifying Arabic signs through hand gestures. In this paper, two Residual Network (ResNet) Configurations, Version 1 (V1) and Version 2 (V2), are proposed and detailed. The proposed ResNet V1 achieved an average accuracy of 98.83%, while ResNet V2 achieved an average accuracy of 98.84%. The results described in this paper far exceed those reported in the extant literature. The high accuracy of the proposed system shows the potential for integrating the system with education tools and assistive technologies for people with special needs."
Deep Learning Driven Human Posture Location  in Physical Education Teaching,2024,"['Human Posture', 'Physical Education Teaching', 'Deep Learning', 'ResNet.']",,"The study of human posture is widely applied in physical education teaching, human motion recognition, and other aspects. With the rise of online teaching, the lack of convenient physical education teaching methods has been able to improve. However, due to the complex structure of human body, the study of human posture is a hard problem of consciousness problem in the area of computer vision. This article mainly studies human posture research algorithms based on deep learning. It uses 101-layer network of ResNet to detect the key points of human body in the image and obtains the categories and coordinates of these key points. In this article, a 101-layer network of ResNet model is constructed to fully learn the visual features of key points in human posture. Secondly, the key point location loss function is improved, and the human posture research is realized by using huber loss function instead of mean square error (MSE) loss function. Finally, experimental analysis shows that compared to traditional integral pose regression (IPR) and location adaptive integral pose regression (LAIPR), the use of ResNet based human posture estimation method for human posture recognition improves precision. It has practical significance for physical education teaching applications."
Deep Learning-Based Plant Health State Classification Using Image  Data,2024,"['Deep Learning', 'Convolutional Neural Networks', 'Channel-wise Attention', 'Depthwise Separable Convolution', 'Attention-Enhanced ResNet', 'Plant Health State Classification', '딥러닝', '합성곱 신경망', '채널 어텐션', '깊이 분리 합성곱', 'Attention-Enhanced ResNet', '작물 건강 상태 분류']",,"Tomatoes are rich in nutrients like lycopene, β-carotene, and vitamin C. However, they often suffer from biological and environmental stressors, resulting in significant yield losses. Traditional manual plant health assessments are error-prone and inefficient for large-scale production. To address this need, we collected a comprehensive dataset covering the entire life span of tomato plants, annotated across 5 health states from 1 to 5. Our study introduces an Attention-Enhanced DS-ResNet architecture with Channel-wise attention and Grouped convolution, refined with new training techniques. Our model achieved an overall accuracy of 80.2% using 5-fold cross-validation, showcasing its robustness in precisely classifying the health states of tomato plants"
상품 카테고리 자동분류를 위한 BERT-분류기 아키텍처 연구,2024,"['문장 분류', '문장 유사도', 'Sentence BERT', 'CNN', 'ResNet', 'Transformer', 'Classification', 'Sentence classification', 'Sentence similarity', 'Sentence BERT', 'CNN', 'ResNet', 'Transformer', 'Classification']","본 연구는 생활 속 존재하는 다양한 상품들의 명칭을 BERT를 통해 임베딩 벡터화한 다음 이를 기반으로 상품 카테고리 예측을 수행하는 아키텍처에 대한 연구이다. 아키텍처의 성능은 상품 명칭으로부터 임베딩 추출을 수행하는 BERT 모델과, 추출된 임베딩으로 카테고리 예측을 수행하는 분류기에 의해 결정된다. 따라서 본 연구는 우선 상품 명칭 분류에 적합한 BERT 모델을 선정하고, 선정된 BERT 모델에 다양한 분류기를 적용하여 가장 높은 성능을 달성하는 BERT-분류기 조합을 찾고자 하였다. 최초 적합한 BERT 모델 선정에는 단순한 CNN 분류기를 사용하였으며 이를 baseline으로 다른 분류기와 성능을 비교하였다. 아키텍처의 성능은 카테고리 정답에 대한 precision, recall, f1 score, accuracy로 정량화하여 평가하였다. 실험 결과 BERT 측면에서는, Sentence BERT 모델이 비교 대상인 일반 BERT 모델보다 적합함을 확인하였다. 그리고 분류기 측면에서는, Sentence BERT와 CNN으로 구성된 baseline 대비하여 Residual Block이 추가 적용된 분류기가 더 높은 성능을 보였다. 본 연구에 사용된 Sentence BERT 모델의 경우 한국어 데이터가 학습되지 않은 단순 모델로, 향후 추가적 연구를 통해 다양한 한국어 데이터를 학습시켜 Domain Adaptation을 수행할 경우 추가적 성능 향상이 기대된다.","This research focuses on an architecture that vectorizes the names of various products found in daily life using BERT, followed by predicting product categories based on these embeddings. The architecture's performance is determined by the BERT model, which extracts embeddings from product names, and the classifier that predicts categories from these embeddings. Consequently, this research initially aimed to identify a BERT model suitable for classifying product names and then find the most efficient combination of BERT model and classifier by applying various classifiers to the chosen BERT model. A simple CNN classifier was employed for the initial selection of a suitable BERT model, serving as a baseline for performance comparison with other classifiers. The architecture's effectiveness was quantified using precision, recall, f1 score, and accuracy for category predictions. Experimental results showed that the Sentence BERT model was more suitable for this task than a conventional BERT model. Additionally, classifiers enhanced with Residual Blocks demonstrated superior performance compared to the baseline combination of Sentence BERT and CNN. The Sentence BERT model used in this study, not trained on Korean data, suggests that further improvements could be achieved through Domain Adaptation by training with diverse Korean datasets."
모션블러 이미지에 대한 CNN 모델의 균열 검출 성능,2024,"['CNN (convolutional neural network)', 'Motion blur', 'Dataset', 'Crack detection', 'F1 score', 'CNN (convolutional neural network)', '모션블러', '데이터셋', '균열 검출', 'F1 score']",,"In this study, we analyzed the effect of motion blur on images used for detecting cracksin concrete tunnel linings on the performance of CNN models. Motion-blurred imageswith intensities ranging from 10 to 50 were generated on the Kaggle and KICT datasets.A semantic segmentation model with ResNet 18, ResNet 34, VGG 11, and Alex-Net as backbones for feature extraction was employed, all pre-trained on the U-Netarchitecture. The performance of these models in crack detection was then assessed.It was observed that detection accuracy decreased across all models as the intensity ofmotion blur increased for each dataset. Within the same model, the F1-score on theKICT dataset showed over 20% higher performance than on the Kaggle dataset. Thisstudy demonstrates that CNN-based crack detection performance is affected by thequality of the image data and that the crack detection accuracy of CNN models canvary depending on the quality of the dataset used in training."
SERN 기반 운전자의 다중 행동 특징을 이용한 졸음 검출 시스템,2024,"['졸음 인식', '운전자 행동 특징', 'SERN', '다중 특징', '딥러닝', 'drowsiness detection', 'driver behavior features', 'SERN', 'Multiple features', 'deap learning']","최근 교통사고의 주요한 원인 중 하나인 운전자 졸음으로 인한 교통사고를 예방하기 위해 졸음 인식 연구가 활발히 진행되는 중이다. 기존 졸음 인식 시스템은 운전자의 신체적 특징을 이용하여 졸음 상태를 인식하지만 신체 부위 폐 색에 의한 가려짐으로 제한되는 한계가 있다. 본 논문에서는 운전자의 다중 행동 특징을 이용한 SERN(Squeeze and Excitation Resnet Network) 기반 졸음 인식 시스템을 제안한다. 제안한 시스템은 다중 행동적 특징 기반 특징 추출 과정, 데이터의 계층적 레이블링 세분화 과정, SERN 모델에 의한 졸음 인식 과정으로 구성된다. 공개 DB인 NTHU-DDD를 사용한 실험 결과, 제안하는 SERN 모델 기반 운전자 졸음 검출 성능이 기존 네트워크 모델 보다 정확도 1.03% 우수함을 확인했다.","Recently, driver drowsiness has been one of the major causes of traffic accidents, and study on drowsiness detection has been actively conducted to prevent drowsiness-related accidents. Existing drowsiness detection systems recognize the drowsy state of the driver using the driver's physical features, but they have limitations due to occlusion caused by obstructed body parts. In this paper, we propose a drowsiness detection system based on the squeeze and excitation resnet network (SERN) using the driver's multi-behavioral features. The proposed system consists of a multibehavioral feature extraction process, a hierarchical data labeling refinement process, and a drowsiness detection process using the SERN model. As a result of an experiment using public DB’s NTHU-DDD, it was confirmed that the proposed SERN model based driver drowsiness detection performance was 1.03% better than the existing network model."
합성곱 신경망 기반 화재 인식 모델 최적화 연구: Layer Importance Evaluation 기반 접근법,2024,"['레이어 중요도 평가', '전이 학습 모델', '합성곱 신경망 최적화', '실시간 화재 감지', '기여도', 'Layer Importance Evaluation', 'Transfer Learning Model', 'CNN Optimization', 'Real-Time Fire Detection', 'Contribution']","본 연구는 Layer Importance Evaluation을 통해 도출된 화재 감지에 최적화된 딥러닝 아키텍처를 제안한다. 기존의 합성곱 신경망(Convolutional Neural Network, CNN) 기반 화재 감지 시스템의 불필요한 복잡성과 연산을 초래하는 문제점을 해결하기 위해, Layer Importance Evaluation 기법을 통해 가중치 및 활성화 값에 근거한 모델의 내부 레이어의 동작을 분석하고, 화재 감지에 기여도가 높은 레이어를 식별한 뒤, 식별한 레이어만으로 모델을 재구성하여, 기존 모델과의 성능 지표를 비교 분석하였다. Xception, VGG19, ResNet, EfficientNetB5 등 네 가지 전이 학습 모델을 사용하여 화재 데이터를 학습시킨 후, Layer Importance Evaluation기법을 적용하여 각 레이어의 가중치와 활성화 값을 분석한 뒤 기여도가 가장 높은 상위 랭크 레이어들을 선별하여 새로운 모델을 구축하였다. 연구 결과, 구현된 아키텍처는 기존 모델 대비 약 80% 가량 경량화 된 파라미터로도 동등한 성능을 유지하며, 약 3~5배가량 신속한 학습 속도를 가지면서도 기존의 복잡한 전이학습 모델에 비해 정확도, 손실, 혼동행렬 지표에서 동등한 성능을 출력함으로써, 화재 감시 장비의 효율성을 높이는 데 기여할 수 있음을 확인하였다.","This study proposes a deep learning architecture optimized for fire detection derived through Layer Importance Evaluation. In order to solve the problem of unnecessary complexity and operation of the existing Convolutional Neural Network (CNN)-based fire detection system, the operation of the inner layer of the model based on the weight and activation values was analyzed through the Layer Importance Evaluation technique, the layer with a high contribution to fire detection was identified, and the model was reconstructed only with the identified layer, and the performance indicators were compared and analyzed with the existing model. After learning the fire data using four transfer learning models: Xception, VGG19, ResNet, and EfficientNetB5, the Layer Importance Evaluation technique was applied to analyze the weight and activation value of each layer, and then a new model was constructed by selecting the top rank layers with the highest contribution. As a result of the study, it was confirmed that the implemented architecture maintains the same performance with parameters that are about 80% lighter than the existing model, and can contribute to increasing the efficiency of fire monitoring equipment by outputting the same performance in accuracy, loss, and confusion matrix indicators compared to conventional complex transfer learning models while having a learning speed of about 3 to 5 times faster."
합성곱 신경망 기반 화재 인식 모델 최적화 연구: Layer Importance Evaluation 기반 접근법,2024,,"본 연구는 Layer Importance Evaluation을 통해 도출된 화재 감지에 최적화된 딥러닝 아키텍처를 제안한다. 기존의 합성곱 신경망(Convolutional Neural Network, CNN) 기반 화재 감지 시스템의 불필요한 복잡성과 연산을 초래하는 문제점을 해결하기 위해, Layer Importance Evaluation 기법을 통해 가중치 및 활성화 값에 근거한 모델의 내부 레이어의 동작을 분석하고, 화재 감지에 기여도가 높은 레이어를 식별한 뒤, 식별한 레이어만으로 모델을 재구성하여, 기존 모델과의 성능 지표를 비교 분석하였다. Xception, VGG19, ResNet, EfficientNetB5 등 네 가지 전이 학습 모델을 사용하여 화재 데이터를 학습시킨 후, Layer Importance Evaluation기법을 적용하여 각 레이어의 가중치와 활성화 값을 분석한 뒤 기여도가 가장 높은 상위 랭크 레이어들을 선별하여 새로운 모델을 구축하였다. 연구 결과, 구현된 아키텍처는 기존 모델 대비 약 80% 가량 경량화 된 파라미터로도 동등한 성능을 유지하며, 약 3~5배가량 신속한 학습 속도를 가지면서도 기존의 복잡한 전이학습 모델에 비해 정확도, 손실, 혼동행렬 지표에서 동등한 성능을 출력함으로써, 화재 감시 장비의 효율성을 높이는 데 기여할 수 있음을 확인하였다.","This study proposes a deep learning architecture optimized for fire detection derived through Layer Importance Evaluation. In order to solve the problem of unnecessary complexity and operation of the existing Convolutional Neural Network (CNN)-based fire detection system, the operation of the inner layer of the model based on the weight and activation values was analyzed through the Layer Importance Evaluation technique, the layer with a high contribution to fire detection was identified, and the model was reconstructed only with the identified layer, and the performance indicators were compared and analyzed with the existing model. After learning the fire data using four transfer learning models: Xception, VGG19, ResNet, and EfficientNetB5, the Layer Importance Evaluation technique was applied to analyze the weight and activation value of each layer, and then a new model was constructed by selecting the top rank layers with the highest contribution. As a result of the study, it was confirmed that the implemented architecture maintains the same performance with parameters that are about 80% lighter than the existing model, and can contribute to increasing the efficiency of fire monitoring equipment by outputting the same performance in accuracy, loss, and confusion matrix indicators compared to conventional complex transfer learning models while having a learning speed of about 3 to 5 times faster."
전이학습을 이용한 신발 이미지 스타일 분류모델 연구,2024,"['전이학습', '신발 스타일 분류', '신발 아웃솔', '신발 갑피', 'Transfer Learning', 'Shoes Style Classification', 'Shoes Outsole', 'Shoes Upper', 'ConvNeXt']","전자상거래의 성장과 4차산업혁명 기술 발전에 따라 패션 산업에서 인공지능을 접목한 서비스가 활발히 도입되고 있으나 신발 산업은 아직 관련 연구가 깊게 되어 있지 않아 활용 사례 및 데이터셋이 부족하다. 본 논문에서는 웹크롤링으로 운동화, 스니커즈 이미지를 수집하고 디자인 및 제조 관점을 반영하여 신발 스타일과 아웃솔, 갑피에 대해서 라벨링하였다. 구축한 약 2만건의 데이터셋을 대상으로 ResNet, ConvNeXt, ViT, Swin Transfomer를 전이학습하고 각 모델의 결과를 비교하였다. 그 결과 ConvNeXt 모델에서 가장 우수한 결과를 얻었고 과적합을 방지하기 위해 추가로 파인튜닝하여 테스트 데이터셋 대상으로 정확도 87%의 결과를 얻었다. 본 연구를 바탕으로 신발 산업에서 디자인 및 제조 기술에 딥러닝 모델을 활용하여 생산성을 향상시킬 수 있을 것이라 기대한다.","With the growth of e-commerce and the development of the 4th Industrial Revolution technologies, services using AI are being actively introduced in the fashion industry. However, the shoes industry has not yet been studied in-depth on AI and there is a lack of datasets and use cases. In this paper, we collected images of sneakers and running shoes by web crawling and labeled shoe styles, outsoles, and uppers from design and manufacturing perspectives. We trained ResNet, ConvNeXt, ViT and Swin Transformer on our dataset and compared the results of each model. As a result, the ConvNeXt model obtained the best results and was further fine-tuned to prevent overfitting, resulting in 87% accuracy on the test dataset. Based on this study, We expect that deep learning will be used in design and manufacturing process to improve productivity in the shoe industry."
다중 시멘틱 세그멘테이션 AI 기반의 가전용 크림프 하네스 검사 모델 개발,2024,"['Wire harness', 'Crimp harness inspection', 'Multi class semantic segmentation', 'U-Net']","와이어 하네스는 가전제품, 전기자동차, 자율주행 자동차에서 혈류의 역할을 하는 임의의 전기적회로의 상호 연결을 제공하는 와이어의 그룹으로 정의되며 품질이 가장 중요한 척도이다. 와이어 하네스 품질 불량은 제품 고장, 화재, 인명사고와 직결되기 때문이다. 본 논문은 와이어 하네스 압착공정 결과물인 크림프 하네스의 불량을 판정하기 위해, 다중 시멘틱 세그멘테이션 기법을 활용하는 방법을 제안하였다. 크림프 하네스의 불량 판정 문제는 전처리된 하네스 데이터로부터 연속된 5개 세그먼트들의 높이 및 폭의 길이를 정확히 측정하면 가능한 것으로 판단되었다. 이에 착안하여 대표적인 시멘틱 세그멘테이션 모델인 U-Net을 기반으로 다중 세그먼트 식별에 적합한 AI 모델의 개발을 목표로 하였다. 다중 세그먼트 식별을 위해 U-Net의 인코더 부분을 ResNet 34, EfficientNet B1 및 Mix Transformer B0로 변형한 모델을 제안하였다. 인공지능 모델 개발을 위해, 데이터셋 구축에는 와이어 하네스 제조공장에서 수집된 크림프 하네스 이미지들을 사용하였다. 개발된 다중 시멘틱 세그멘테이션 AI 모델은 테스트 데이터셋에 대해 95.14%의 판별 정확도를 나타내었다. 제안된 방법은 종래의 방법(수작업, 압착센서 측정값 및 규칙 기반의 영상처리)의 단점을 개선한, 균일한 고품질 유지와 인건비 절감이 가능하다.","Wire harness is defined as groups of wires providing interconnections for arbitrary electrical circuits that serve as the bloodstream in consumer electronics, electric vehicles, and autonomous cars. Poor wire harness quality is directly related to produce product failures, fires, and human casualties. This paper proposes a method that utilizes multi-class semantic segmentation techniques to determine the defects of the crimp harness, which is the result of a product of the wire harness crimp process. The problem of defect detection of crimp harness can be solved by accurately measuring the height and width of five consecutive segments from the preprocessed harness data. With this insight, we aimed to develop an AI model suitable for multi-segment identification based on U-Net, a representative semantic segmentation model. To identify multiple segments, we proposed a model that modifies the encoder part of U-Net using Resnet 34, EfficientNet B1, and Mix Transformer B0. For AI model development, images of crimp harnesses collected from a wire harness manufacturing plant were used to build the dataset. The developed multi-class semantic segmentation AI model showed a discernment accuracy of 95.14% on the test dataset. Through the method proposed in this paper, it is possible to maintain uniform high quality and reduce labor costs, which improves the shortcomings of the existing crimp harness quality inspection(manual, crimp sensor measurements, and rule-based image processing)."
UAV Imagery-based Automatic Classification of Ground Surface Types for Earthworks,2024,"['Automated construction equipment', 'Ground surface', 'Unmanned aerial vehicle', 'Multi-label classification', 'Computer vision']",,"The construction industry is introducing autonomous heavy equipment to overcome labor shortages and improve productivity. For autonomous heavy equipment to work on earthmoving at sites, the equipment needs to recognize and understand ground surface types. However, the ground surface types are manually inspected in practice, and related studies are lacking. To address this issue, the authors developed and tested models that automatically classify ground surface types from images acquired by an unmanned aerial vehicle using a deep learning-based multi-label classification method that applies Binary Relevance (BR) and Label Powerset (LP) methods with Residual Neural Network (ResNet) and Vision Transformer classification network (VIT). The model performances were comparatively evaluated through experiments conducted on actual construction sites. The results showed that the BR model with ResNet is the best model in terms of automated ground surface type identification during earthmoving. The results are expected to broaden the understanding of complex and expansive construction sites for autonomous vehicles and thus facilitate deployment of autonomous heavy equipment by helping them to understand working areas and any obstacles on construction sites quickly and effectively, which will reduce the cost and time needed for on-site ground surface management."
시퀀스 데이터 기반 단일 및 복합 변조 레이더 신호 변조 식별,2024,"['Radar signal modulation identification', 'Composite modulated radar signal', 'Deep learning model']","상대방에 대한 사전 정보 없이 수집된 레이더 신호의 변조를 식별하는 기술은 전자기전에서 매우 중요한 역할을 수행하며, 이를 통해 획득한 변조 방식에 관한 정보는 전자기전에서 전략 수립과 우위 확보를 위해 활용될 수 있다. 본 논문에서는 딥러닝 모델을 이용하여 37종의 단일 및 복합 변조 레이더 신호를 식별하는 시퀀스 데이터 기반의 레이더 신호 변조 식별 기법을 제안하고 시퀀스 데이터와 딥러닝 모델에 따른 변조 식별 성능을 분석한다. 제안하는 기법은 수신 레이더 신호를 시간 또는 주파수 영역에서 분석하여 시퀀스 데이터를 생성한 후, 이를 딥러닝 모델에 입력하여 변조 방식을 식별한다. 이때, 모델에 입력되는 시퀀스 데이터로 레이더 신호의 실수부 및 허수부로 구성한 시간 영역 데이터와 이산 푸리에 변환의 실수부 및 허수부로 구성한 주파수 영역 데이터를 고려하며, 딥러닝 모델로는 ResNet, DenseNet, Inception-v3를 고려한다. 컴퓨터 모의실험을 통해 주파수 영역 데이터를 모델 입력으로 사용하는 것이 시간 영역 데이터를 사용하는 것보다 변조 식별 성능이 우수함을 보이며, 본 논문에서 고려한 딥러닝 모델이 기존에 제안된 시퀀스 데이터 기반 레이더 변조 식별 딥러닝 모델보다 우수한 변조 식별 성능을 가짐을 확인한다. 또한, 딥러닝 모델별 변조 식별 성능을 비교하여 DenseNet201을 사용하는 경우에 가장 높은 변조 식별 성능을 보임을 입증한다.","Identifying the modulation scheme of collected radar signals without prior information plays an important role in electromagnetic warfare, and the information on the modulation scheme obtained through modulation identification of radar signal can be used to establish strategies and secure superiority in electromagnetic warfare. In this paper, using a deep learning model, we propose a sequence data-based modulation identification method for 37 types of single and composite modulated radar signals. In addition, we analyze the modulation identification performance according to the sequence data and the deep learning model. The proposed method generates sequence data by analyzing received radar signals in the time or frequency domain, and then using this data as input to the deep learning model, it identifies the modulation scheme. As sequence data, we consider the time domain data consisting of real and imaginary parts of the radar signal and the frequency domain data consisting of real and imaginary parts of the discrete Fourier transform. For deep learning models, we consider ResNet, DenseNet, and Inception-v3. Through computer simulations, we show that using the frequency domain data as the input of the model, the modulation identification performance is better compared to using the time domain data. It is also confirmed that the deep learning models considered in this paper show better performance than the existing deep learning model of the sequence data-based modulation identification method. Furthermore, by comparing the performance of each deep learning model, we demonstrate that the DenseNet201 exhibits the best modulation identification performance."
Drone Detection Using Dynamic-DBSCAN and Deep Learning in an Indoor Environment,2024,"['DBSCAN', 'Deep Learning', 'FMCW Radar', 'Object Detection', 'UAV']",,"Drones have found extensive utility in both public and personal places. Consequently, the accurate detection and tracking of drones have emerged as pivotal endeavors in terms of ensuring their optimal performance. This research paper introduces a novel application for discerning the movements of humans and drones from cloud points through the utilization of frequency-modulated continuous wave radar. The dynamic density-based spatial clustering of applications with noise (Dynamic-DBSCAN) algorithm was employed to classify cloud points into separate groups corresponding to the number of objects within the tracking area. Compared to the original DBSCAN algorithm, this method increased accuracy by about 16.8%, achieving an accuracy of up to 93.99%. Subsequently, a trio of deep learning algorithms-long short-term memory, deep neural network, and residual network (ResNet)—were harnessed to ascertain the categorization of each group as either human or drone. According to the results, ResNet achieved the best accuracy rate of 97.72%. Overall, this study underscores the efficacy of the proposed method in accurately and efficiently distinguishing between human and drone entities for effective monitoring and management."
잔차 신경망을 활용한 펫 로봇용 화자인식 경량화,2024,"['Speaker recognition', 'CNN', 'Resnet', 'Lightweight', 'Classification']","화자인식은 개개인마다 다른 음성 주파수를 분석하여 미리 저장된 음성과 비교해 본인 여부를 판단하는 하나의 기술을 의미한다.딥러닝 기반의 화자인식은 여러 분야에 적용되고 있으며, 펫 로봇도 그 중 하나이다. 하지만 펫 로봇의 하드웨어 성능은 딥러닝 기술의 많은 메모리 공간과 연산에 있어 매우 제한적인 상황이다. 이는 펫 로봇이 사용자와 실시간 상호작용에 있어 해결해야 할 중요한 문제점이다. 딥러닝 모델의 경량화는 위와 같은 문제를 해결하기 위한 하나의 중요한 방법으로 자리하였으며, 최근 많은 연구가진행되고 있다. 이 논문에서는 특정한 명령어 형태인 펫 로봇용 음성 데이터 세트를 구축하고 잔차(Residual)를 활용한 모델들의 결과를 비교해 펫 로봇용 화자인식의 경량화 연구의 결과를 서술하며, 결론에서는 제안한 방법에 대한 결과와 향후 연구방안에 대해서술한다.","Speaker recognition refers to a technology that analyzes voice frequencies that are different for each individualand compares them with pre-stored voices to determine the identity of the person. Deep learning-based speakerrecognition is being applied to many fields, and pet robots are one of them. However, the hardware performance ofpet robots is very limited in terms of the large memory space and calculations of deep learning technology. This isan important problem that pet robots must solve in real-time interaction with users. Lightening deep learning modelshas become an important way to solve the above problems, and a lot of research is being done recently. In thispaper, we describe the results of research on lightweight speaker recognition for pet robots by constructing a voicedata set for pet robots, which is a specific command type, and comparing the results of models using residuals. Inthe conclusion, we present the results of the proposed method and Future research plans are described."
잔차 신경망을 활용한 펫 로봇용 화자인식 경량화,2024,"['Speaker recognition', 'CNN', 'Resnet', 'Lightweight', 'Classification']",,
Few-Shot 기반 작물 기상피해 판별 시스템,2024,"['Smart Farm', 'Few-Shot', 'Classification', 'ResNet', 'DenseNet']",,"This study introduces a model utilizing Few-shot Learning to effectively classify and discern weather-induced damages in crops, focusing on cold and heat damages affecting peaches, apples, and pears. It addresses the challenge of limited data availability in agriculture by leveraging Few-shot Learning, offering a promising solution for data scarcity issues. The model demonstrates robust classification capabilities under constrained data conditions, highlighting the potential of AI and machine learning technologies to tackle significant challenges in modern agriculture related to weather damage. The research suggests avenues for future work, including model performance enhancement, integration with real-time monitoring systems, and broader application across various crops and weather conditions, aiming to contribute to sustainable agriculture and food security."
패션 이미지 데이터를 활용한 딥러닝 기반의 의류속성 분류,2024,"['패션 이미지', '의류 속성 분류', '딥러닝', 'Fashion Image', 'Clothing Attribute Classification', 'Deep Learning', 'ResNet', 'EfficientNet']",,
Design of Automatic Defect Classification System  for Wafer Edge Defect Inspection,2024,"['CNN', 'Deep Learning', 'Image Processing', 'Auto Defect Classification', 'Wafer Edge Defect']",,"This paper proposes an automatic defect classification (ADC) system to detect and classify bare wafer edge defects that occur during the extreme wafer thinning process required for advanced chip stacking technologies such as TSV and HBM. The proposed system combines a convolutional neural network (ResNet) with traditional image processing techniques (OpenCV-based frequency domain filtering) to effectively classify and visualize wafer edge defects. Experimental results demonstrate that the system achieves high accuracy in defect classification and detection, providing an efficient solution to prevent wafer damage and yield reduction."
Attention BiFPN 기반 화재 검출 딥러닝 알고리즘에 대한 연구,2024,"['fire detection', 'neural network', 'attention mechanism', 'bi-directional pyramid', 'feature pyramid network', '.']","최근 빈번하게 발생하는 화재를 인공지능 및 머신러닝 기법을 활용하여 조기에 감지하고 효과적인 대응에 초점을 맞춘 연구들이 활발히 진행되고 있다. 화재 영상에서 복잡한 배경 및 환경 요인의 불확실성으로 인해 불꽃 또는 연기 영역을 더욱 정확하게 검출할 수 있는 기법이 요구된다. 본 논문에서는 Attention 기반 BiFPN 구조의 ResNet 알고리즘을 제안한다. 화재영역을 정확하게 검출하기 위하여 어텐션 메커니즘과 척도 불변 BiFPN 구조를 유기적으로 결합하여 객체와 배경정보를 분리함과 동시에 서로간의 연관성 유지를 통해 불필요한 정보를 제거하고, 필요한 정보만을 강조함으로써 불꽃 및 연기 영역검출 결과가 객체 크기와 상관없이 일관된 정확도를 유지하는 방법을 제안한다. 실험을 통해 불꽃은 98%의 정확도와 98.67%의 정밀도, 연기는 96.67%의 정확도와 96.67%의 정밀도의 화재 검출에 대한 평가 결과를 확인하였다.","Recent research has been actively focused on utilizing artificial intelligence and machine learning techniques to detect fires early and respond effectively, given the frequent occurrences of fires. There is a demand for techniques that can accurately detect flame or smoke areas in fire images due to the uncertainty of complex backgrounds and environmental factors. In this paper, to accurately detect fire areas, we propose a method that seamlessly combines attent ion mechanisms and scale-invariant Bidirectional Feature Pyramid Network(BiFPN) structures to separate object and background information while maintaining their correlations. It removes unnecessary information and emphasizes only the necessary information to maintain consistent accuracy in flame and smoke areas regardless of their sizes. Evaluation results showed that fires could be detected with 98% accuracy and 98.67% precision for flames and 96.67% accuracy and 96.67% precision for smoke."
자율주행 자동차 인지 성능 향상을 위한 복수의 인공신경망 결과 값 합성법,2024,"['딥러닝', '전이 학습', '인공신경망', '이미지 인식', '모델 합성', 'Deep learning', 'Transfer learning', 'Neural network', 'Image classification', 'Model fusion']","전이 학습은 이미 학습된 딥러닝 모델을 초기 모델로 활용하여, 다른 데이터에서 높은 성능 을 발휘하는 기술이다. 특히, 학습에 사용할 데이터의 질과 양이 충분하지 않을 때 전이 학습은 매우 유용한 것으로 알려져 있기에 높은 성능 안정성과 많은 데이터를 필요로 하는 자율주행 차 인식 분야에 응용될 수 있다. 그러나 많은 선행 연구들은 전이 학습 알고리즘 자체의 성능 향상에 초점을 맞추었다. 본 연구는 기존의 알고리즘 중심 접근 방식에서 벗어나, 여러 데이터 셋의 특징 추출기 출력을 융합한 심층 모델 융합 기법을 전이 학습에 적용하고자 한다. 실험 결과, 이러한 심층 모델 융합 기법이 전이 학습의 성능을 향상시킬 수 있음을 확인했다. 이 결 과는 앞으로 데이터가 부족한 자율주행 자동차 분야에서 전이 학습에 활용되어 물체 인지 성 능의 향상을 달성할 수 있을 것으로 기대된다.","Transfer learning is a technique that leverages a deep learning model trained on a specific dataset as an initial model that allows fast training of a high-performing model on another dataset. Because a pre-trained model already learns how to extract the features from previously trained data, it allows for faster and better performance on new datasets compared to models that are initialized randomly. Transfer learning is particularly useful when the quality and quantity of the data to be learned are insufficient. On the other hand, most studies focused on improving the performance of transfer learning algorithms themselves. This study departs from the existing algorithm-centric research approach and aims to incorporate deep model fusion techniques that combine the outputs of feature extractors from different datasets into transfer learning. These experiments show that the application of deep model fusion improves the performance of transfer learning. These findings will be applicable to transfer learning in various domains with limited data."
Automatic cancer nuclei segmentation on histological images: comparison study of deep learning methods,2024,['Cancer · Medical segmentation · Histology · Convolutional neural networks · Augmentation'],,"Cancer is one of the most common health problems aff ecting individuals worldwide. In the fi eld of biomedical engineering, one of the main methods for cancer diagnosis is the analysis of histological images of tissue structures and cell nuclei using artifi cial intelligence. Here, we compared the performance of 15 deep learning methods viz: UNet, Deep-UNet, UNet-CBAM, RA-UNet, SA-Unet and Nuclei-SegNet, UNet-VGG2016, UNet-Resnet-101, TransResUNet, Inception-UNet, Att-UNet++ , FF-UNet, Att-UNet, Res-UNet and a new model, DanNucNet, in pathological nuclei segmentation on tissue slices from different organs on fi ve open datasets: MoNuSeg, CoNSeP, CryoNuSeg, Data Science Bowl, and NuInsSeg. Before training on the data, the pixel intensity and color distribution were analyzed, and diff erent augmentation techniques were applied. The results showed that the UNet-based model with 34.57 million Deep-UNet parameters performed the best, outperforming all models in terms of the Dice coeffi cient from 3.13 to 22.91%. The implementation of Deep-UNet in this context provides a valuable tool for accurate extraction of cancer cell nuclei from histological images, which in turn will contribute to further developments in cancer pathology and digital histology."
A Dynamic Head Gesture Recognition Method for Real-time Intention Inference and Its Application to Visual Human-robot Interaction,2024,"['Computer vision', 'deep learning', 'head gesture', 'human-robot interaction.']",,"Head gesture is a natural and non-verbal communication method for human-computer and human-robot interaction, conveying attitudes and intentions. However, the existing vision-based recognition methods cannot meet the precision and robustness of interaction requirements. Due to the limited computational resources, applying most high-accuracy methods to mobile and onboard devices is challenging. Moreover, the wearable device-based approach is inconvenient and expensive. To deal with these problems, an end-to-end two-stream fusion network named TSIR3D is proposed to identify head gestures from videos for analyzing human attitudes and intentions. Inspired by Inception and ResNet architecture, the width and depth of the network are increased to capture motion features sufficiently. Meanwhile, convolutional kernels are expanded from the spatial domain to the spatiotemporal domain for temporal feature extraction. The fusion position of the two-stream channel is explored under an accuracy/complexity trade-off to a certain extent. Furthermore, a dynamic head gesture dataset named DHG and a behavior tree are designed for human-robot interaction. Experimental results show that the proposed method has advantages in real-time performance on the remote server or the onboard computer. Furthermore, its accuracy on the DHG can surpass most state-of-the-art vision-based methods and is even better than most previous approaches based on head-mounted sensors. Finally, TSIR3D is applied on Pepper Robot equipped with Jetson TX2."
IRAE-UNet: InceptionResNetV2 -Attention Encoder based UNet Semantic Segmentation of Aerial Imagery,2024,"['remote sensing', 'attention', 'image segmentation', 'inception', '원격 감지', '어텐션', '영상 분할', 'Inception']",,"Remote sensing applications play a vital role in various areas such as urban planning, agriculture, and environmental monitoring. Remote sensing image segmentation, in particular, is a prominent domain that aims to address the challenges in these applications. Deep learning has significantly improved the efficiency and accuracy of remote sensing image segmentation by automating the identification of regions of interest. However, most existing methods struggle with capturing both global and local information in the images, which is crucial for accurate pixel classification. To overcome this limitation, this paper presents an enhanced version of the U-Net architecture that incorporates the InceptionResNetV2-Attention based encoder. This proposed method effectively combines the strengths of the Inception and ResNet architectures, along with the attention mechanism. The efficacy of the proposed network is verified using two publicly available datasets. The Semantic Drone Dataset consists of satellite images, while the NITRDrone dataset comprises images captured from Unmanned Aerial Vehicles (UAVs). The results demonstrate that the proposed architecture performs well on imagery obtained from different platforms, achieving  a dice-coefficient of 85.04% and 88.70% for each dataset respectively, outperforming other networks."
Anchor-Free 기반 3D 객체 검출을 이용한 과수 꼭지 검출 시스템 구현,2024,"['딥러닝', '디지털 농업', '3D 객체 검출', '과수 꼭지 검출', 'Deep learning', 'Digital agriculture', '3D Object detection', 'Fruit stem detection']",,"This paper proposes a fruit stem detection system using Anchor-Free-based 3D object detection model for fruit stem removal. The fruit stem is an important part that affects the hygiene, food safety, quality, and freshness of fruits. It can save manpower and time by automating existing manual-dependent tap removal operations, and increase efficiency in related agricultural fields. The FCAF3D model is a 3D object detection algorithm that predicts the skin and stem of the fruit, respectively, showing high detection performance even in the small size of the fruit stem. The network structure of the model consists of ResNet, GSDN, and FCOS networks, which handle various object scales through the FPN structure. In this paper, model training was performed using apples as an example, and the learned model showed high accuracy in the apple dataset, and the bounding box coordinate value of the test results can be used in the fruit stem removal system.Experimental results showed that the FCAF3D model showed high performance in detecting fruit stem."
EfficientNet과 LSTM을 활용한 병해  진행도 예측 시스템,2024,"['Image Classification', 'Time Series Data Analysis', 'Time Series Forecasting', 'Disease', 'XAI', 'EfficientNet', 'LSTM']",,"Modern agriculture leverages various smart farming technologies to manage crops efficiently, but ensuring optimal growth for each crop remains challenging. This paper proposes a novel approach that integrates an image classification model with a time series analysis model to address this issue. The proposed system uses EfficientNet to classify crop diseases and LSTM (Long Short-Term Memory) to analyze time series data to predict the progression of the disease based on its presence. The model performance was evaluated using Accuracy, Precision, Recall, and F1 Score metrics. The image classification model achieved 91% Precision, 92% Recall, 97% Accuracy, and 94% F1 Score, outperforming ResNet, DenseNet, and SENet. Additionally, the model's reliability was verified using Grad-CAM (Gradient-weighted Class Activation Map), an XAI technique. The time series analysis model demonstrated a Recall of 88% and Precision and F1 Score of 86%. The proposed system is accessible via a web page, allowing easy access for users, and includes features for writing posts and comments to facilitate a user community. This system aims to advance smart farming technology, increase crop yields, and minimize agricultural losses, ultimately contributing to improved crop management efficiency."
Benchmark study on a novel online dataset for standard evaluation of deep learning-based pavement cracks classification models,2024,"['Convolutional neural networks', 'Pavement cracks', 'Deep learning', 'Benchmark study', 'Crack classification']",,"Highway agencies and practitioners expect to have the most efficient method with adequate accuracy when choosing a deep learning-based model for pavement crack classification. However, many works are implemented on their own dataset, making them hard to compare with each other, and also less persuasive and robust. Therefore, a Road Cracks Classification Dataset is proposed to serve as a standard and open-source dataset. Based on this dataset, a benchmark study of fourteen deep learning classification methods is evaluated. Two parameters, the Ratio of F1 and Training Time (RFT) and Ratio of F1 and Prediction Time (RFP), are proposed to quantify the efficiency of networks. The results show that ConvNeXt_base reaches the highest accuracy among all models but requires the longest training time. AlexNet takes the least training time among all models, but gains the lowest accuracy. Of the four crack types, the block crack has the lowest accuracy, which means it is the most difficult to detect. SqueezeNet1_0 has the highest efficiency among all models in converting the computing power to accuracy. Wide ResNet 50_2 consumes the longest prediction time among CNN models, while the ConvNeXt_base has the highest feasibility on real-time tasks. To implement a suitable deep learning-based pavement crack inspection, we recommend a good balance between computational cost and accuracy. Based on this, we provide practical recommendations according to different user groups."
An Implementation of Effective CNN Model for AD Detection,2024,"[""Alzheimer's disease (AD)"", 'Magnetic Resonance Imaging (MRI)', 'Convolution Neural Network (CNN)']",,"This paper focuses on detecting Alzheimer’s Disease (AD). The most usual form of dementia is Alzheimer's disease, which causes permanent cause memory cell damage. Alzheimer's disease, a neurodegenerative disease, increases slowly over time. For this matter, early detection of Alzheimer's disease is important.  The purpose of this work is using Magnetic Resonance Imaging (MRI) to diagnose AD.  A Convolution Neural Network (CNN) model, Reset, and VGG the pre-trained learning models are used.  Performing analysis and validation of layers affects the effectiveness of the model. T1-weighted MRI images are taken for preprocessing from ADNI. The Dataset images are taken from the Alzheimer's Disease Neuroimaging Initiative (ADNI). 3D MRI scans into 2D image slices shows the optimization method in the training process while achieving 96% and 94% accuracy in VGG 16 and ResNet 18 respectively. This study aims to classify AD from brain 3D MRI images and obtain better results."
A deep learning model for estimating sedation levels using heart rate variability and vital signs: a retrospective cross-sectional study at a center in South Korea,2024,"['conscious sedation', 'deep learning', 'heart rate', 'machine learning', 'patient monitoring', 'pediatric intensive care unit', 'vital signs']",,"Background:Optimal sedation assessment in critically ill children remains challenging due to the subjective nature of behavioral scales and intermittent evaluation schedules. This study aimed to develop a deep learning model based on heart rate variability (HRV) parameters and vital signs to predict effective and safe sedation levels in pediatric patients.Methods: This retrospective cross-sectional study was conducted in a pediatric intensive care unit at a tertiary children’s hospital. We developed deep learning models incorporating HRV parameters extracted from electrocardiogram waveforms and vital signs to predict Richmond Agitation-Sedation Scale (RASS) scores. Model performance was evaluated using the area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). The data were split into training, validation, and test sets (6:2:2), and the models were developed using a 1D ResNet architecture.Results: Analysis of 4,193 feature sets from 324 patients achieved excellent discrimination ability, with AUROC values of 0.867, 0.868, 0.858, 0.851, and 0.811 for whole number RASS thresholds of −5 to −1, respectively. AUPRC values ranged from 0.928 to 0.623, showing superior performance in deeper sedation levels. The HRV metric SDANN2 showed the highest feature importance, followed by systolic blood pressure and heart rate.Conclusions: A combination of HRV parameters and vital signs can effectively predict sedation levels in pediatric patients, offering the potential for automated and continuous sedation monitoring in pediatric intensive care settings. Future multi-center validation studies are needed to establish broader applicability."
국방 데이터를 활용한 인셉션 네트워크 파생 이미지 분류 AI의 설명 가능성 연구,2024,"['Deep Learning(딥러닝)', 'Image Classification(이미지 분류)', 'AI(인공지능)', 'XAI(설명 가능한 인공지능)', 'Inception Network(인셉션 네트워크)', 'LIME Algorithm(라임 알고리즘)']",,"In the last 10 years, AI has made rapid progress, and image classification, in particular, are showing excellentperformance based on deep learning. Nevertheless, due to the nature of deep learning represented by a black box,it is difficult to actually use it in critical decision-making situations such as national defense, autonomous driving,medical care, and finance due to the lack of explainability of judgement results. In order to overcome theselimitations, in this study, a model description algorithm capable of local interpretation was applied to the inceptionnetwork-derived AI to analyze what grounds they made when classifying national defense data. Specifically, weconduct a comparative analysis of explainability based on confidence values by performing LIME analysis from theInception v2_resnet model and verify the similarity between human interpretations and LIME explanations.Furthermore, by comparing the LIME explanation results through the Top1 output results for Inception v3,Inception v2_resnet, and Xception models, we confirm the feasibility of comparing the efficiency and availabilityof deep learning networks using XAI."
