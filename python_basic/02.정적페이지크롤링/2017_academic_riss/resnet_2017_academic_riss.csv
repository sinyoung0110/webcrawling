title,date,keywords,abstract,multilingual_abstract
Deep neural networks perform equally and superiorly to dermatologists in onychomycosis diagnosis,2017,"['Deep learning', 'Region-based convolutional neural network', 'Onychomycosis', 'Nail dystrophy', 'Telemedicine']",국문 초록 정보 없음,"Background: Although there have been reports of the successful diagnosis of skin disorders using deep learning, unrealistically large clinical image datasets are required for artificial intelligence (AI) training.Objectives: We created datasets of standardized nail images using region-based convolutional neural network (R-CNN) trained to distinguish the nail from the background.Methods: We used R-CNN to generate training datasets of 49,567 images, which we then used to fine-tune the ResNet-152 and VGG-19 models. The validation datasets comprised 100 and 194 images from Inje University (B1 and B2 datasets, respectively), 125 images from Hallym University (C dataset), and 939 images from Seoul National University (D dataset).Results: AI’s results showed test sensitivity/specificity/ area under the curve of (96.0 / 94.7 / 0.98), (82.7 / 96.7 / 0.95), (92.3 / 79.3 / 0.93), (87.7 / 69.3 / 0.82) for the B1, B2, C, and D datasets. For the B1 and C datasets, the AI’s Youden index was significantly (p = 0.01) higher than that of dermatologists.Conclusion: By training with a dataset comprising 49,567 images, we achieved a diagnostic accuracy for onychomycosis superior to that of most dermatologists who participated in this study."
비디오 캡션 생성을 위한 의미 특징 학습과 선택적 주의집중,2017,[],"일반적으로 비디오로부터 캡션을 생성하는 작업은 입력 비디오로부터 특징을 추출해내는 과정과 추출한 특징을 이용하여 캡션을 생성해내는 과정을 포함한다. 본 논문에서는 효과적인 비디오 캡션 생성을 위한 심층 신경망 모델과 그 학습 방법을 소개한다. 본 논문에서는 입력 비디오를 표현하는 시각특징 외에, 비디오를 효과적으로 표현하는 동적 의미 특징과 정적 의미 특징을 입력 특징으로 이용한다. 본 논문에서 입력 비디오의 시각 특징들은 C3D, ResNet과 같은 합성곱 신경망을 이용하여 추출하지만, 의미 특징은 본 논문에서 제안하는 의미 특징 추출 네트워크를 활용하여 추출한다. 그리고 이러한 특징들을 기반으로 비디오 캡션을 효과적으로 생성하기 위하여 선택적 주의집중 캡션 생성 네트워크를 제안한다. Youtube 동영상으로부터 수집된 MSVD 데이터 집합을 이용한 다양한 실험을 통해, 본 논문에서 제안한 모델의 성능과 효과를 확인할 수 있었다.",다국어 초록 정보 없음
딥러닝 기반 병충해 작물 이미지 분류에 관한 연구,2022,[],세계자원연구소에 따르면 꾸준히 증가하는 인구를 위해선 추후에 기존 생산량의 약 70% 이상의 식량을 확보해야 한다고 예상한다. 그러나 농업인들은 자연재해보다 자주 발생하는 병충해로 인한 피해로 원활한 작물 생산에 고충을 겪고 있다. 사람이 작물들의 상태를 매번 확인하는 데에 어려움이 있기에 병충해를 미리 예방하기란 쉽지 않다. 그래서 농가는 대체로 전문가에게 병충해 분류의 자문을 구한다. 하지만 시간이 오래 걸리고 병충해는 빠르게 번지기 때문에 피해가 커지는 경우가 있다. 이와 같은 농업의 고질적인 문제를 해결하고자 본 논문에서는 딥러닝을 기반으로 작물의 이미지 데이터를 이용해 병충해 피해를 입은 작물을 분류하는 연구를 제안한다. 이미지 분류 분야에 자주 쓰이는 컨볼루션 신경망(CNN)을 이용할 것이다. 그중에서도 기본적인 모델과 전이 학습 기반인 사전 훈련 모델 ResNet50으로 병충해 작물 이미지분류 성능을 평가할 것이다. 이러한 연구로 농가는 병충해 작물을 조기에 파악하고 이는 신속한 방제로 이어져 큰 피해를 예방할 수 있을 것이다.,다국어 초록 정보 없음
Erythema severity scoring by deep neural network,2017,"['Erythema', 'Score', 'Deep convolution neural network', 'Artificial intelligence']",국문 초록 정보 없음,"Background: Erythema is one of common signs of inflammatory dermatologic diseases. It is one of the measured values which are needed to calculate the Eczema Area and Severity Index score or Psoriasis Area and Severity Index score. However, automated standardization of erythema severity using images has not been investigated yet.Objectives: Our aim was to determine whether the deep convolutional neural networks (CNNs) could assess erythema severity at the level of competence comparable to dermatologists’ scoring.Methods: We made a standard dataset of 4,000 clinical images showing erythema. These images were scored 0 to 4 by three dermatologists. First of all, we trained four CNNs (ResNet V1, ResNet V2, GoogLnet and VGG-Net) with the image dataset, and then examined which CNN was most suitable for erythema scoring.Results: : Among the 4 CNNs, ResNet V1 showed the highest accuracy. Compared to dermatologists’ scoring, the accuracy rates of ResNet V1, ResNet V2, GoogLnet and VGG-Net were 95.33%, 95.12%, 93.59% and 21.55%, respectively.Conclusion: These results suggest some CNNs have a performance capacity for erythema scoring at the level of competence comparable to dermatologists."
Instagram image classification with Deep Learning,2017,[],국문 초록 정보 없음,"In this paper we introduce two experimental results from classification of Instagram images and some valuable lessons from them. We have tried some experiments for evaluating the competitive power of Convolutional Neural Network(CNN) in classification of real social network images such as Instagram images. We used AlexNet and ResNet, which showed the most outstanding capabilities in ImageNet Large Scale Visual Recognition Challenge(ILSVRC) 2012 and 2015, respectively. And we used 240 Instagram images and 12 pre-defined categories for classifying social network images. Also, we performed fine-tuning using Inception V3 model, and compared those results. In the results of four cases of AlexNet, ResNet, Inception V3 and fine-tuned Inception V3, the Top-1 error rates were 49.58%, 40.42%, 30.42%, and 5.00%. And the Top-5 error rates were 35.42%, 25.00%, 20.83%, and 0.00% respectively."
Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification,2017,"['Clipart Classification', 'Convolutional neural network', 'Computer vision', 'Clipart', 'style search', 'Fine tuning', 'Deep learning']",국문 초록 정보 없음,"Clipart is artificial visual contents that are created using various tools such as Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how it looks. However, previous studies on clipart are focused only on the object recognition [16], segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification researches based on the style similarity using CNN have been proposed, however, they have used different CNN-models and experimented with different benchmark dataset so that it is very hard to compare their performances. This paper presents an experimental analysis of the clipart classification based on the style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end training can improve the accuracy more than 20% in both CNN models."
Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification,2017,"['Clipart Classification', 'Convolutional neural network', 'Computer vision', 'Clipart', 'style search', 'Fine tuning', 'Deep learning']",국문 초록 정보 없음,"Clipart is artificial visual contents that are created using various tools such as Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how it looks. However, previous studies on clipart are focused only on the object recognition [16], segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification researches based on the style similarity using CNN have been proposed, however, they have used different CNN-models and experimented with different benchmark dataset so that it is very hard to compare their performances. This paper presents an experimental analysis of the clipart classification based on the style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end training can improve the accuracy more than 20% in both CNN models."
Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification,2017,"['Clipart Classification', 'Convolutional neural network', 'Computer vision', 'Clipart', 'style search', 'Finetuning', 'Deep learning']",국문 초록 정보 없음,"Clipart is artificial visual contents that are created using various tools such as  Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how  it looks. However, previous studies on clipart are focused only on the object recognition [16],  segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification  researches based on the style similarity using CNN have been proposed, however, they have used different  CNN-models and experimented with different benchmark dataset so that it is very hard to compare  their performances. This paper presents an experimental analysis of the clipart classification based on the  style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers  learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out  that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its  deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end  training can improve the accuracy more than 20% in both CNN models."
합성곱 신경망을 이용한 복잡한 형상을 가진 공의 인식,2017,"['deep learning', 'R-CNN', 'RoboCup', 'object classification', 'object localization']",국문 초록 정보 없음,"Image processing is widely used not only in manufacturing industries, but also in advanced contexts such as RoboCup, an international robotics competition. Recently, in the image processing field, convolutional neural networks, which is a deep learning approach, became the mainstream of object recognition algorithms. In this paper, a convolutional neural network is designed and used to learn the features of the ball in the RoboCup soccer game. The convolutional neural network was named JeoNet. JeoNet was modified to learn fewer features than VGGNet and ResNet, but shows the same performance. In order to obtain the detected ball’s position, the Single Shot Multibox Detector was applied. To verify JeoNet, an experimental environment was constructed in which a soccer robot finds a ball in the sight. The benefits of JeoNet were shown by comparing the tracking time of the ball between JeoNet and conventional machine vision based ball finding algorithms."
딥 residual network를 이용한 선생-학생 프레임워크에서 힌트-KD 학습 성능 분석,2017,"['Knowledge distillation', 'Hint training', 'Deep residual networks', 'Caffe']",국문 초록 정보 없음,다국어 초록 정보 없음
합성곱 신경망(Convolutional Neural Network)을 활용한 지능형 아토피피부염 중증도 진단 모델 개발,2017,"['아토피피부염', '딥러닝', '이미지 인식 알고리즘', 'Convolutional Neural Network', 'Convolutional Neural Network', 'Atopic Dermatitis', 'Deep Learning', 'Image Recognition Algorithm']","제4차 산업혁명의 등장과 경제성장으로 인한 ‘국민 삶의 질 향상’ 요구 증대로 인해 의료서비스의 질과 의료비용에 대한 국민들의 요구수준이 향상되고 있으며, 이로 인해 인공지능이 의료현장에 도입되고 있다. 하지만 인공지능이 의료분야에 활용된 사례를 살펴보면 ‘삶의 질’에 직접적인 영향을 끼치는 만성피부질환에 활용된 사례는 부족한 실정이며, 만성피부질환 중 대표적 질병인 아토피피부염은 정성적 진단 방법으로 인해 진단의 객관성을 확보할 수 없다는 한계가 존재한다. 본 연구에서는 아토피피부염의 객관적 중증도 평가 방법을 마련하여 아토피피부염 환자의 삶의 질을 향상시키고자 다음과 같은 연구를 수행하였다. 첫째, 가톨릭대학교 의과대학 성모병원의 데이터베이스로부터 아토피피부염 환자의 이미지 데이터를 수집했으며, 수집된 이미지 데이터에 대한 정제 및 라벨링 작업을 수행하여 모델 학습과 검증에 적합한 데이터를 확보했다. 둘째, 지능형 아토피피부염 중증도 진단 모형에 적합한 이미지 인식 알고리즘을 파악하기 위해 다양한 CNN 알고리즘들을 병변별 학습용 데이터로 학습시키고, 검증용 데이터를 활용하여 해당 모델의 이미지 인식 정확도를 측정했다. 실증분석 결과 홍반(Erythema)의 경우 ‘ResNet V1 101’, 긁은 정도(Excoriation)의 경우 ‘ResNet V2 50’이 90% 이상의 정확도를 기록하였으며, 태선화(Lichenification)의 경우 학습용 데이터 부족의 한계로 인해 두 병변보다 낮은 89%의 정확도를 보였다. 해당 결과를 통해 이미지 인식 알고리즘이 단순한 사물 인식 분야뿐만 아니라 전문적 지식이 요구되는 분야에도 높은 성능을 나타낸다는 것을 실증적으로 입증했으며, 본 연구는 실제 아토피피부염 환자의 이미지 데이터를 활용했다는 측면에서 실제 임상환경에서 활용성이 높을 것으로 사료된다.","With the advent of ‘The Forth Industrial Revolution’ and the growing demand for quality of life due to economic growth, needs for the quality of medical services are increasing. Artificial intelligence has been introduced in the medical field, but it is rarely used in chronic skin diseases that directly affect the quality of life. Also, atopic dermatitis, a representative disease among chronic skin diseases, has a disadvantage in that it is difficult to make an objective diagnosis of the severity of lesions. The aim of this study is to establish an intelligent severity recognition model of atopic dermatitis for improving the quality of patient’s life. For this, the following steps were performed. First, image data of patients with atopic dermatitis were collected from the Catholic University of Korea Seoul Saint Mary’s Hospital. Refinement and labeling were performed on the collected image data to obtain training and verification data that suitable for the objective intelligent atopic dermatitis severity recognition model. Second, learning and verification of various CNN algorithms are performed to select an image recognition algorithm that suitable for the objective intelligent atopic dermatitis severity recognition model. Experimental results showed that ‘ResNet V1 101’ and ‘ResNet V2 50’ were measured the highest performance with Erythema and Excoriation over 90% accuracy, and ‘VGG-NET’ was measured 89% accuracy lower than the two lesions due to lack of training data. The proposed methodology demonstrates that the image recognition algorithm has high performance not only in the field of object recognition but also in the medical field requiring expert knowledge. In addition, this study is expected to be highly applicable in the field of atopic dermatitis due to it uses image data of actual atopic dermatitis patients."
비분할 비디오로부터 행동 탐지를 위한 순환 신경망 학습,2017,[],"본 논문에서는 비분할 비디오로부터 이 비디오에 담긴 사람의 행동을 효과적으로 탐지해내기 위한 심층 신경망 모델을 제안한다. 일반적으로 비디오에서 사람의 행동을 탐지해내는 작업은 크게 비디오에서 행동 탐지에 효과적인 특징들을 추출해내는 과정과 이 특징들을 토대로 비디오에 담긴 행동을 탐지해내는 과정을 포함한다. 본 논문에서는 특징 추출 과정과 행동 탐지 과정에 이용할 심층 신경망모델을 제시한다. 특히 비디오로부터 각 행동별 시간적, 공간적 패턴을 잘 표현할 수 있는 특징들을 추출해내기 위해서는 C3D 및 I-ResNet 합성곱 신경망 모델을 이용하고, 시계열 특징 벡터들로부터 행동을 자동 판별해내기 위해서는 양방향 BI-LSTM 순환 신경망 모델을 이용한다. 대용량의 공개 벤치마크 데이터 집합인 ActivityNet 비디오 데이터를 이용한 실험을 통해, 본 논문에서 제안하는 심층 신경망 모델의 성능과 효과를 확인할 수 있었다.",다국어 초록 정보 없음
3D CNN 기반 회전근개 파열 진단 및 활성화 맵 시각화,2017,"['Rotator Cuff Tear(회전근개 파열)', '3D CNN', 'Class Activation Map(활성화 맵)']",국문 초록 정보 없음,"When diagnosing Rotator Cuff Tear(RCT), magnetic resonance imaging(MRI) scanned 3D data is largely used. Compare to 2D-based medical image, 3D data can offer more detail information and intuitive visualization of patients condition. For example, three-dimensionally visualized MRI volume data of rotator cuff area can reveal not only presence or absence of the rupture, but also clear position and shape of it. However, only 2D-based slice images are used to diagnose RCT without taking advantage of 3D volume at the general medical field. Most of Convolutional Neural Network(CNN)-based medical image diagnosing methods are also using 2D data. We have proposed a RCT diagnosis method using 3D CNN that uses 3D information and take advantages of it to do the same task. The Voxception-Resnet(VRN) network was used to classify if volume has RCT or not. The data was preprocessed and resampled to 64x64x64 volume. and showed about 80 percent of accuracy when diagnosing test data. The 3D Class Activation Map(CAM) was also applied to visualize approximate location and shape of RCT. Our proposed method can automatically diagnose the presence of RCT using 3D data, and also visualize the activation map in 3D.This is less onerous and timeconsuming than using 2D data."
