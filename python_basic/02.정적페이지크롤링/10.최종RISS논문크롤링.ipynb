{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딥러닝을 이용한 고압 수소에 의하여 손상된 Acrylonitrile Butadiene Rubber (NBR) 단면 균열 탐지 2021 ['딥러닝(Deep learning)', '균열 탐지(Crack Detection)', '인공신경망(Artificial neural network)', '객체인식(Object detection)', '반 자동화 라벨링(Semi-automated labeling)'] None Demand for understanding material behaviors in hydrogen rich environment rises due to the extended use of hydrogen gas. Organic material such as rubber used in O-rings for hydrogen containers displays pore-like cracks when it is exposed extensively to high pressure hydrogen. To find the optimal organic material in a certain hydrogen rich environment, a method for localizing and analyzing pore-like cracks must be devised. This paper deals with a new method to detect high pressure hydrogen induced cracksusing deep learning algorithms. In this study, acrylonitrile Butadiene Rubber (NBR) was exposed to hydrogen of 96.6MPa for 24 hours. Images were taken of the appearing pore like cracks magnified by 100. For bounding box labeling a semi-automated labeling was used using maximally. stable extremal region (MSER) features on graphical contours. The crack detection model was trained adopting the structure of faster R-CNN using ResNet50 as its backbone network. The resulting artificial intelligence model showed robust and accurate detecting ability.\n",
      "망막질환진단을 위한 CNN모델들의 성능비교 2022 ['딥러닝', '망막 OCT', 'Deep Learning', 'VGG-16', 'VGG-19', 'ResNet', 'DenseNet', 'Retinal OCT'] None None\n",
      "2015년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                   title  \\\n",
      "0  딥러닝을 이용한 고압 수소에 의하여 손상된 Acrylonitrile Butadiene Rubber (NBR) 단면 균열 탐지   \n",
      "1                                                망막질환진단을 위한 CNN모델들의 성능비교   \n",
      "\n",
      "   date  \\\n",
      "0  2021   \n",
      "1  2022   \n",
      "\n",
      "                                                                                                                                     keywords  \\\n",
      "0  [딥러닝(Deep learning), 균열 탐지(Crack Detection), 인공신경망(Artificial neural network), 객체인식(Object detection), 반 자동화 라벨링(Semi-automated labeling)]   \n",
      "1                                                                 [딥러닝, 망막 OCT, Deep Learning, VGG-16, VGG-19, ResNet, DenseNet, Retinal OCT]   \n",
      "\n",
      "  abstract  \\\n",
      "0     None   \n",
      "1     None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              multilingual_abstract  \n",
      "0  Demand for understanding material behaviors in hydrogen rich environment rises due to the extended use of hydrogen gas. Organic material such as rubber used in O-rings for hydrogen containers displays pore-like cracks when it is exposed extensively to high pressure hydrogen. To find the optimal organic material in a certain hydrogen rich environment, a method for localizing and analyzing pore-like cracks must be devised. This paper deals with a new method to detect high pressure hydrogen induced cracksusing deep learning algorithms. In this study, acrylonitrile Butadiene Rubber (NBR) was exposed to hydrogen of 96.6MPa for 24 hours. Images were taken of the appearing pore like cracks magnified by 100. For bounding box labeling a semi-automated labeling was used using maximally. stable extremal region (MSER) features on graphical contours. The crack detection model was trained adopting the structure of faster R-CNN using ResNet50 as its backbone network. The resulting artificial intelligence model showed robust and accurate detecting ability.  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                               keyword  count\n",
      "0                   딥러닝(Deep learning)      1\n",
      "1               균열 탐지(Crack Detection)      1\n",
      "2     인공신경망(Artificial neural network)      1\n",
      "3               객체인식(Object detection)      1\n",
      "4   반 자동화 라벨링(Semi-automated labeling)      1\n",
      "5                                  딥러닝      1\n",
      "6                               망막 OCT      1\n",
      "7                        Deep Learning      1\n",
      "8                               VGG-16      1\n",
      "9                               VGG-19      1\n",
      "10                              ResNet      1\n",
      "11                            DenseNet      1\n",
      "12                         Retinal OCT      1\n",
      "데이터가 저장되었습니다: resnet_2015_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2015_academic_riss.csv\n",
      "2016년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "Empty DataFrame\n",
      "Columns: [title, date, keywords, abstract, multilingual_abstract]\n",
      "Index: []\n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "Empty DataFrame\n",
      "Columns: [keyword, count]\n",
      "Index: []\n",
      "데이터가 저장되었습니다: resnet_2016_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2016_academic_riss.csv\n",
      "딥러닝 기반 병충해 작물 이미지 분류에 관한 연구 2022 None 세계자원연구소에 따르면 꾸준히 증가하는 인구를 위해선 추후에 기존 생산량의 약 70% 이상의 식량을 확보해야 한다고 예상한다. 그러나 농업인들은 자연재해보다 자주 발생하는 병충해로 인한 피해로 원활한 작물 생산에 고충을 겪고 있다. 사람이 작물들의 상태를 매번 확인하는 데에 어려움이 있기에 병충해를 미리 예방하기란 쉽지 않다. 그래서 농가는 대체로 전문가에게 병충해 분류의 자문을 구한다. 하지만 시간이 오래 걸리고 병충해는 빠르게 번지기 때문에 피해가 커지는 경우가 있다. 이와 같은 농업의 고질적인 문제를 해결하고자 본 논문에서는 딥러닝을 기반으로 작물의 이미지 데이터를 이용해 병충해 피해를 입은 작물을 분류하는 연구를 제안한다. 이미지 분류 분야에 자주 쓰이는 컨볼루션 신경망(CNN)을 이용할 것이다. 그중에서도 기본적인 모델과 전이 학습 기반인 사전 훈련 모델 ResNet50으로 병충해 작물 이미지분류 성능을 평가할 것이다. 이러한 연구로 농가는 병충해 작물을 조기에 파악하고 이는 신속한 방제로 이어져 큰 피해를 예방할 수 있을 것이다. None\n",
      "Instagram image classification with Deep Learning 2017 None None In this paper we introduce two experimental results from classification of Instagram images and some valuable lessons from them. We have tried some experiments for evaluating the competitive power of Convolutional Neural Network(CNN) in classification of real social network images such as Instagram images. We used AlexNet and ResNet, which showed the most outstanding capabilities in ImageNet Large Scale Visual Recognition Challenge(ILSVRC) 2012 and 2015, respectively. And we used 240 Instagram images and 12 pre-defined categories for classifying social network images. Also, we performed fine-tuning using Inception V3 model, and compared those results. In the results of four cases of AlexNet, ResNet, Inception V3 and fine-tuned Inception V3, the Top-1 error rates were 49.58%, 40.42%, 30.42%, and 5.00%. And the Top-5 error rates were 35.42%, 25.00%, 20.83%, and 0.00% respectively.\n",
      "Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification 2017 ['Clipart Classification', 'Convolutional neural network', 'Computer vision', 'Clipart', 'style search', 'Fine tuning', 'Deep learning'] None Clipart is artificial visual contents that are created using various tools such as Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how it looks. However, previous studies on clipart are focused only on the object recognition [16], segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification researches based on the style similarity using CNN have been proposed, however, they have used different CNN-models and experimented with different benchmark dataset so that it is very hard to compare their performances. This paper presents an experimental analysis of the clipart classification based on the style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end training can improve the accuracy more than 20% in both CNN models.\n",
      "Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification 2017 ['Clipart Classification', 'Convolutional neural network', 'Computer vision', 'Clipart', 'style search', 'Fine tuning', 'Deep learning'] None Clipart is artificial visual contents that are created using various tools such as Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how it looks. However, previous studies on clipart are focused only on the object recognition [16], segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification researches based on the style similarity using CNN have been proposed, however, they have used different CNN-models and experimented with different benchmark dataset so that it is very hard to compare their performances. This paper presents an experimental analysis of the clipart classification based on the style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end training can improve the accuracy more than 20% in both CNN models.\n",
      "Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification 2017 ['Clipart Classification', 'Convolutional neural network', 'Computer vision', 'Clipart', 'style search', 'Finetuning', 'Deep learning'] None Clipart is artificial visual contents that are created using various tools such as  Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how  it looks. However, previous studies on clipart are focused only on the object recognition [16],  segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification  researches based on the style similarity using CNN have been proposed, however, they have used different  CNN-models and experimented with different benchmark dataset so that it is very hard to compare  their performances. This paper presents an experimental analysis of the clipart classification based on the  style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers  learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out  that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its  deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end  training can improve the accuracy more than 20% in both CNN models.\n",
      "합성곱 신경망을 이용한 복잡한 형상을 가진 공의 인식 2017 ['deep learning', 'R-CNN', 'RoboCup', 'object classification', 'object localization'] None Image processing is widely used not only in manufacturing industries, but also in advanced contexts such as RoboCup, an international robotics competition. Recently, in the image processing field, convolutional neural networks, which is a deep learning approach, became the mainstream of object recognition algorithms. In this paper, a convolutional neural network is designed and used to learn the features of the ball in the RoboCup soccer game. The convolutional neural network was named JeoNet. JeoNet was modified to learn fewer features than VGGNet and ResNet, but shows the same performance. In order to obtain the detected ball’s position, the Single Shot Multibox Detector was applied. To verify JeoNet, an experimental environment was constructed in which a soccer robot finds a ball in the sight. The benefits of JeoNet were shown by comparing the tracking time of the ball between JeoNet and conventional machine vision based ball finding algorithms.\n",
      "딥 residual network를 이용한 선생-학생 프레임워크에서 힌트-KD 학습 성능 분석 2017 ['Knowledge distillation', 'Hint training', 'Deep residual networks', 'Caffe'] None None\n",
      "합성곱 신경망(Convolutional Neural Network)을 활용한 지능형 아토피피부염 중증도 진단 모델 개발 2017 ['아토피피부염', '딥러닝', '이미지 인식 알고리즘', 'Convolutional Neural Network', 'Convolutional Neural Network', 'Atopic Dermatitis', 'Deep Learning', 'Image Recognition Algorithm'] 제4차 산업혁명의 등장과 경제성장으로 인한 ‘국민 삶의 질 향상’ 요구 증대로 인해 의료서비스의 질과 의료비용에 대한 국민들의 요구수준이 향상되고 있으며, 이로 인해 인공지능이 의료현장에 도입되고 있다. 하지만 인공지능이 의료분야에 활용된 사례를 살펴보면 ‘삶의 질’에 직접적인 영향을 끼치는 만성피부질환에 활용된 사례는 부족한 실정이며, 만성피부질환 중 대표적 질병인 아토피피부염은 정성적 진단 방법으로 인해 진단의 객관성을 확보할 수 없다는 한계가 존재한다. 본 연구에서는 아토피피부염의 객관적 중증도 평가 방법을 마련하여 아토피피부염 환자의 삶의 질을 향상시키고자 다음과 같은 연구를 수행하였다. 첫째, 가톨릭대학교 의과대학 성모병원의 데이터베이스로부터 아토피피부염 환자의 이미지 데이터를 수집했으며, 수집된 이미지 데이터에 대한 정제 및 라벨링 작업을 수행하여 모델 학습과 검증에 적합한 데이터를 확보했다. 둘째, 지능형 아토피피부염 중증도 진단 모형에 적합한 이미지 인식 알고리즘을 파악하기 위해 다양한 CNN 알고리즘들을 병변별 학습용 데이터로 학습시키고, 검증용 데이터를 활용하여 해당 모델의 이미지 인식 정확도를 측정했다. 실증분석 결과 홍반(Erythema)의 경우 ‘ResNet V1 101’, 긁은 정도(Excoriation)의 경우 ‘ResNet V2 50’이 90% 이상의 정확도를 기록하였으며, 태선화(Lichenification)의 경우 학습용 데이터 부족의 한계로 인해 두 병변보다 낮은 89%의 정확도를 보였다. 해당 결과를 통해 이미지 인식 알고리즘이 단순한 사물 인식 분야뿐만 아니라 전문적 지식이 요구되는 분야에도 높은 성능을 나타낸다는 것을 실증적으로 입증했으며, 본 연구는 실제 아토피피부염 환자의 이미지 데이터를 활용했다는 측면에서 실제 임상환경에서 활용성이 높을 것으로 사료된다. With the advent of ‘The Forth Industrial Revolution’ and the growing demand for quality of life due to economic growth, needs for the quality of medical services are increasing. Artificial intelligence has been introduced in the medical field, but it is rarely used in chronic skin diseases that directly affect the quality of life. Also, atopic dermatitis, a representative disease among chronic skin diseases, has a disadvantage in that it is difficult to make an objective diagnosis of the severity of lesions. The aim of this study is to establish an intelligent severity recognition model of atopic dermatitis for improving the quality of patient’s life. For this, the following steps were performed. First, image data of patients with atopic dermatitis were collected from the Catholic University of Korea Seoul Saint Mary’s Hospital. Refinement and labeling were performed on the collected image data to obtain training and verification data that suitable for the objective intelligent atopic dermatitis severity recognition model. Second, learning and verification of various CNN algorithms are performed to select an image recognition algorithm that suitable for the objective intelligent atopic dermatitis severity recognition model. Experimental results showed that ‘ResNet V1 101’ and ‘ResNet V2 50’ were measured the highest performance with Erythema and Excoriation over 90% accuracy, and ‘VGG-NET’ was measured 89% accuracy lower than the two lesions due to lack of training data. The proposed methodology demonstrates that the image recognition algorithm has high performance not only in the field of object recognition but also in the medical field requiring expert knowledge. In addition, this study is expected to be highly applicable in the field of atopic dermatitis due to it uses image data of actual atopic dermatitis patients.\n",
      "2017년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                     title  \\\n",
      "0                                                              딥러닝 기반 병충해 작물 이미지 분류에 관한 연구   \n",
      "1                                        Instagram image classification with Deep Learning   \n",
      "2  Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification   \n",
      "3  Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification   \n",
      "4  Comparison of Fine-Tuned Convolutional Neural Networks for Clipart Style Classification   \n",
      "5                                                            합성곱 신경망을 이용한 복잡한 형상을 가진 공의 인식   \n",
      "6                                     딥 residual network를 이용한 선생-학생 프레임워크에서 힌트-KD 학습 성능 분석   \n",
      "7                       합성곱 신경망(Convolutional Neural Network)을 활용한 지능형 아토피피부염 중증도 진단 모델 개발   \n",
      "\n",
      "   date  \\\n",
      "0  2022   \n",
      "1  2017   \n",
      "2  2017   \n",
      "3  2017   \n",
      "4  2017   \n",
      "5  2017   \n",
      "6  2017   \n",
      "7  2017   \n",
      "\n",
      "                                                                                                                                                keywords  \\\n",
      "0                                                                                                                                                     []   \n",
      "1                                                                                                                                                     []   \n",
      "2                             [Clipart Classification, Convolutional neural network, Computer vision, Clipart, style search, Fine tuning, Deep learning]   \n",
      "3                             [Clipart Classification, Convolutional neural network, Computer vision, Clipart, style search, Fine tuning, Deep learning]   \n",
      "4                              [Clipart Classification, Convolutional neural network, Computer vision, Clipart, style search, Finetuning, Deep learning]   \n",
      "5                                                                            [deep learning, R-CNN, RoboCup, object classification, object localization]   \n",
      "6                                                                                 [Knowledge distillation, Hint training, Deep residual networks, Caffe]   \n",
      "7  [아토피피부염, 딥러닝, 이미지 인식 알고리즘, Convolutional Neural Network, Convolutional Neural Network, Atopic Dermatitis, Deep Learning, Image Recognition Algorithm]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       abstract  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                         세계자원연구소에 따르면 꾸준히 증가하는 인구를 위해선 추후에 기존 생산량의 약 70% 이상의 식량을 확보해야 한다고 예상한다. 그러나 농업인들은 자연재해보다 자주 발생하는 병충해로 인한 피해로 원활한 작물 생산에 고충을 겪고 있다. 사람이 작물들의 상태를 매번 확인하는 데에 어려움이 있기에 병충해를 미리 예방하기란 쉽지 않다. 그래서 농가는 대체로 전문가에게 병충해 분류의 자문을 구한다. 하지만 시간이 오래 걸리고 병충해는 빠르게 번지기 때문에 피해가 커지는 경우가 있다. 이와 같은 농업의 고질적인 문제를 해결하고자 본 논문에서는 딥러닝을 기반으로 작물의 이미지 데이터를 이용해 병충해 피해를 입은 작물을 분류하는 연구를 제안한다. 이미지 분류 분야에 자주 쓰이는 컨볼루션 신경망(CNN)을 이용할 것이다. 그중에서도 기본적인 모델과 전이 학습 기반인 사전 훈련 모델 ResNet50으로 병충해 작물 이미지분류 성능을 평가할 것이다. 이러한 연구로 농가는 병충해 작물을 조기에 파악하고 이는 신속한 방제로 이어져 큰 피해를 예방할 수 있을 것이다.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          None   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          None   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          None   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          None   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          None   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          None   \n",
      "7  제4차 산업혁명의 등장과 경제성장으로 인한 ‘국민 삶의 질 향상’ 요구 증대로 인해 의료서비스의 질과 의료비용에 대한 국민들의 요구수준이 향상되고 있으며, 이로 인해 인공지능이 의료현장에 도입되고 있다. 하지만 인공지능이 의료분야에 활용된 사례를 살펴보면 ‘삶의 질’에 직접적인 영향을 끼치는 만성피부질환에 활용된 사례는 부족한 실정이며, 만성피부질환 중 대표적 질병인 아토피피부염은 정성적 진단 방법으로 인해 진단의 객관성을 확보할 수 없다는 한계가 존재한다. 본 연구에서는 아토피피부염의 객관적 중증도 평가 방법을 마련하여 아토피피부염 환자의 삶의 질을 향상시키고자 다음과 같은 연구를 수행하였다. 첫째, 가톨릭대학교 의과대학 성모병원의 데이터베이스로부터 아토피피부염 환자의 이미지 데이터를 수집했으며, 수집된 이미지 데이터에 대한 정제 및 라벨링 작업을 수행하여 모델 학습과 검증에 적합한 데이터를 확보했다. 둘째, 지능형 아토피피부염 중증도 진단 모형에 적합한 이미지 인식 알고리즘을 파악하기 위해 다양한 CNN 알고리즘들을 병변별 학습용 데이터로 학습시키고, 검증용 데이터를 활용하여 해당 모델의 이미지 인식 정확도를 측정했다. 실증분석 결과 홍반(Erythema)의 경우 ‘ResNet V1 101’, 긁은 정도(Excoriation)의 경우 ‘ResNet V2 50’이 90% 이상의 정확도를 기록하였으며, 태선화(Lichenification)의 경우 학습용 데이터 부족의 한계로 인해 두 병변보다 낮은 89%의 정확도를 보였다. 해당 결과를 통해 이미지 인식 알고리즘이 단순한 사물 인식 분야뿐만 아니라 전문적 지식이 요구되는 분야에도 높은 성능을 나타낸다는 것을 실증적으로 입증했으며, 본 연구는 실제 아토피피부염 환자의 이미지 데이터를 활용했다는 측면에서 실제 임상환경에서 활용성이 높을 것으로 사료된다.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this paper we introduce two experimental results from classification of Instagram images and some valuable lessons from them. We have tried some experiments for evaluating the competitive power of Convolutional Neural Network(CNN) in classification of real social network images such as Instagram images. We used AlexNet and ResNet, which showed the most outstanding capabilities in ImageNet Large Scale Visual Recognition Challenge(ILSVRC) 2012 and 2015, respectively. And we used 240 Instagram images and 12 pre-defined categories for classifying social network images. Also, we performed fine-tuning using Inception V3 model, and compared those results. In the results of four cases of AlexNet, ResNet, Inception V3 and fine-tuned Inception V3, the Top-1 error rates were 49.58%, 40.42%, 30.42%, and 5.00%. And the Top-5 error rates were 35.42%, 25.00%, 20.83%, and 0.00% respectively.  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Clipart is artificial visual contents that are created using various tools such as Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how it looks. However, previous studies on clipart are focused only on the object recognition [16], segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification researches based on the style similarity using CNN have been proposed, however, they have used different CNN-models and experimented with different benchmark dataset so that it is very hard to compare their performances. This paper presents an experimental analysis of the clipart classification based on the style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end training can improve the accuracy more than 20% in both CNN models.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Clipart is artificial visual contents that are created using various tools such as Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how it looks. However, previous studies on clipart are focused only on the object recognition [16], segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification researches based on the style similarity using CNN have been proposed, however, they have used different CNN-models and experimented with different benchmark dataset so that it is very hard to compare their performances. This paper presents an experimental analysis of the clipart classification based on the style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end training can improve the accuracy more than 20% in both CNN models.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Clipart is artificial visual contents that are created using various tools such as  Illustrator to highlight some information. Here, the style of the clipart plays a critical role in determining how  it looks. However, previous studies on clipart are focused only on the object recognition [16],  segmentation, and retrieval of clipart images using hand-craft image features. Recently, some clipart classification  researches based on the style similarity using CNN have been proposed, however, they have used different  CNN-models and experimented with different benchmark dataset so that it is very hard to compare  their performances. This paper presents an experimental analysis of the clipart classification based on the  style similarity with two well-known CNN-models (Inception Resnet V2 [13] and VGG-16 [14] and transfers  learning with the same benchmark dataset (Microsoft Style Dataset 3.6K). From this experiment, we find out  that the accuracy of Inception Resnet V2 is better than VGG for clipart style classification because of its  deep nature and convolution map with various sizes in parallel. We also find out that the end-to-end  training can improve the accuracy more than 20% in both CNN models.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Image processing is widely used not only in manufacturing industries, but also in advanced contexts such as RoboCup, an international robotics competition. Recently, in the image processing field, convolutional neural networks, which is a deep learning approach, became the mainstream of object recognition algorithms. In this paper, a convolutional neural network is designed and used to learn the features of the ball in the RoboCup soccer game. The convolutional neural network was named JeoNet. JeoNet was modified to learn fewer features than VGGNet and ResNet, but shows the same performance. In order to obtain the detected ball’s position, the Single Shot Multibox Detector was applied. To verify JeoNet, an experimental environment was constructed in which a soccer robot finds a ball in the sight. The benefits of JeoNet were shown by comparing the tracking time of the ball between JeoNet and conventional machine vision based ball finding algorithms.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None  \n",
      "7  With the advent of ‘The Forth Industrial Revolution’ and the growing demand for quality of life due to economic growth, needs for the quality of medical services are increasing. Artificial intelligence has been introduced in the medical field, but it is rarely used in chronic skin diseases that directly affect the quality of life. Also, atopic dermatitis, a representative disease among chronic skin diseases, has a disadvantage in that it is difficult to make an objective diagnosis of the severity of lesions. The aim of this study is to establish an intelligent severity recognition model of atopic dermatitis for improving the quality of patient’s life. For this, the following steps were performed. First, image data of patients with atopic dermatitis were collected from the Catholic University of Korea Seoul Saint Mary’s Hospital. Refinement and labeling were performed on the collected image data to obtain training and verification data that suitable for the objective intelligent atopic dermatitis severity recognition model. Second, learning and verification of various CNN algorithms are performed to select an image recognition algorithm that suitable for the objective intelligent atopic dermatitis severity recognition model. Experimental results showed that ‘ResNet V1 101’ and ‘ResNet V2 50’ were measured the highest performance with Erythema and Excoriation over 90% accuracy, and ‘VGG-NET’ was measured 89% accuracy lower than the two lesions due to lack of training data. The proposed methodology demonstrates that the image recognition algorithm has high performance not only in the field of object recognition but also in the medical field requiring expert knowledge. In addition, this study is expected to be highly applicable in the field of atopic dermatitis due to it uses image data of actual atopic dermatitis patients.  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                         keyword  count\n",
      "0         Clipart Classification      3\n",
      "2                Computer vision      3\n",
      "3                        Clipart      3\n",
      "4                   style search      3\n",
      "6                  Deep learning      3\n",
      "1   Convolutional neural network      3\n",
      "5                    Fine tuning      2\n",
      "20  Convolutional Neural Network      2\n",
      "15        Deep residual networks      1\n",
      "22                 Deep Learning      1\n",
      "21             Atopic Dermatitis      1\n",
      "19                   이미지 인식 알고리즘      1\n",
      "18                           딥러닝      1\n",
      "17                        아토피피부염      1\n",
      "16                         Caffe      1\n",
      "12           object localization      1\n",
      "14                 Hint training      1\n",
      "13        Knowledge distillation      1\n",
      "11         object classification      1\n",
      "10                       RoboCup      1\n",
      "9                          R-CNN      1\n",
      "8                  deep learning      1\n",
      "7                     Finetuning      1\n",
      "23   Image Recognition Algorithm      1\n",
      "데이터가 저장되었습니다: resnet_2017_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2017_academic_riss.csv\n",
      "슈퍼픽셀 이미지 분할을 이용한 ResNet 기반 백혈구 감별 알고리즘 개발 2018 ['Super-pixel', 'Residual Network', 'Segmentation', 'Classification', 'White Blood Cell'] None In this paper, we propose an efficient WBC 14-Diff classification which performs using the WBC-ResNet-152, a type of CNN model. The main point of view is to use Super-pixel for the segmentation of the image of WBC, and to use ResNet for the classification of WBC.  A total of 136,164 blood image samples (224x224) were grouped for image segmentation, training, training verification, and final test performance analysis.  Image segmentation using super-pixels have different number of images for each classes, so weighted average was applied and therefore image segmentation error was low at 7.23%.  Using the training data-set for training 50 times, and using soft-max classifier, TPR average of 80.3% for the training set of 8,827 images was achieved. Based on this, using verification data-set of 21,437 images, 14-Diff classification TPR average of normal WBCs were at 93.4% and TPR average of abnormal WBCs were at 83.3%. The result and methodology of this research demonstrates the usefulness of artificial intelligence technology in the blood cell image classification field.  WBC-ResNet-152 based morphology approach is shown to be meaningful and worthwhile method. And based on stored medical data, in-depth diagnosis and early detection of curable diseases is expected to improve the quality of treatment.\n",
      "ResNet-50 합성곱 신경망을 위한 고정 소수점 표현 방법 2018 ['ASIC'] None Recently, the convolutional neural network shows high performance in many computer vision tasks. However, convolutional neural networks require enormous amount of operation, so it is difficult to adopt them in the embedded environments. To solve this problem, many studies are performed on the ASIC or FPGA implementation, where an efficient representation method is required. The fixed-point representation is adequate for the ASIC or FPGA implementation but causes a performance degradation. This paper proposes a separate optimization of representations for the convolutional layers and the batch normalization layers. With the proposed method, the required bit width for the convolutional layers is reduced from 16 bits to 10 bits for the ResNet-50 neural network. Since the computation amount of the convolutional layers occupies the most of the entire computation, the bit width reduction in the convolutional layers enables the efficient implementation of the convolutional neural networks.\n",
      "합성곱 신경망을 활용한 위내시경 이미지 분류에서 전이학습의 효용성 평가 2018 ['Gastroscope', 'Convolutional Neual Network', 'Transfer learning', 'Resnet', 'Inception', 'VGGnet'] None Stomach cancer is the most diagnosed cancer in Korea. When gastric cancer is detected early, the 5-year survival rate is as high as 90%. Gastroscopy is a very useful method for early diagnosis. But the false negative rate of gastric cancer in the gastroscopy was 4.6~25.8% due to the subjective judgment of the physician. Recently, the image classification performance of the image recognition field has been advanced by the convolutional neural network. Convolutional neural networks perform well when diverse and sufficient amounts of data are supported. However, medical data is not easy to access and it is difficult to gather enough high-quality data that includes expert annotations. So This paper evaluates the efficacy of transfer learning in gastroscopy classification and diagnosis. We obtained 787 endoscopic images of gastric endoscopy at Gil Medical Center, Gachon University. The number of normal images was 200, and the number of abnormal images was 587. The image size was reconstructed and normalized. In the case of the ResNet50 structure, the classification accuracy before and after applying the transfer learning was improved from 0.9 to 0.947, and the AUC was also improved from 0.94 to 0.98. In the case of the InceptionV3 structure, the classification accuracy before and after applying the transfer learning was improved from 0.862 to 0.924, and the AUC was also improved from 0.89 to 0.97. In the case of the VGG16 structure, the classification accuracy before and after applying the transfer learning was improved from 0.87 to 0.938, and the AUC was also improved from 0.89 to 0.98. The difference in the performance of the CNN model before and after transfer learning was statistically significant when confirmed by T-test (p < 0.05). As a result, transfer learning is judged to be an effective method of medical data that is difficult to collect good quality data.\n",
      "전이학습에 방법에 따른 컨벌루션 신경망의영상 분류 성능 비교 2018 ['Deep Learning', 'Computer Vision', 'Convolutional Neural Network', 'Transfer Learnin'] None Core algorithm of deep learning Convolutional Neural Network(CNN) shows better performance than other machine learning algorithms. However, if there is not sufficient data, CNN can not achieve satisfactory performance even if the classifier is excellent. In this situation, it has been proven that the use of transfer learning can have a great effect. In this paper, we apply two transition learning methods(freezing, retraining) to three CNN models(ResNet-50, Inception-V3, DenseNet-121) and compare and analyze how the classification performance of CNN changes according to the methods. As a result of statistical significance test using various evaluation indicators, ResNet-50, Inception-V3, and DenseNet-121 differed by 1.18 times, 1.09 times, and 1.17 times, respectively. Based on this, we concluded that the retraining method may be more effective than the freezing method in case of transition learning in image classification problem.\n",
      "보안 감시를 위한 심층학습 기반 다채널 영상 분석 2018 ['Video Surveillance', 'Object detection', 'Multi-object tracking', 'Deep learning', 'Probabilistic data association filter', '영상보안감시', '객체 검출', '다중 객체 추적', '심층학습', '확률적 데이터 연관 필터'] 본 논문에서는 영상 보안 감시를 위한 심층학습 객체 검출과 다중 객체 추적을 위한 확률적 데이터연관 필터를 연계한 영상분석 기법을 제안하고, GPU를 이용하여 구현하는 방안을 제시한다. 제안하는 영상분석 기법은 객체 검출과 추적으로 순차적으로 수행한다. 객체 검출을 위한 심층학습은 ResNet을 이용하고, 다중 객체 추적을 위하여 확률적 데이터 연관 필터를 적용한다. 제안하는 영상분석 기법은 임의의 영역으로 불법으로 침입하는 사람을 검출하거나 특정 공간에 출입하는 사람을 계수하는데 응용할 수 있다. 시뮬레이션을 통하여 약 27fps의 속도로 48채널의 영상을 분석할 수 있음을 보이고, RTSP 프로토콜을 통하여 실시간 영상분석이 가능함을 보인다. In this paper, a video analysis is proposed to implement video surveillance system with deep learning object detection and probabilistic data association filter for tracking multiple objects and suggests its implementation using GPU. The proposed video analysis technique involves object detection and object tracking sequentially. The deep learning network architecture uses ResNet for object detection and applies probabilistic data association filter for multiple objects tracking. The proposed video analysis technique can be used to detect intruders illegally trespassing any restricted area or to count the number of people entering a specified area. As results of simulations and experiments, 48 channels of videos can be analyzed at a speed of about 27 fps and real-time video analysis is possible through RTSP protocol.\n",
      "이진화된 컨벌루션 신경망의 효율적인 SIMD 구현 2018 ['인공 신경망', '컨벌루션 신경망', '이진화', '이미지 분류', '임베디드 시스템'] 본 논문에서는 이진화된 컨벌루션 신경망 (Convolutional Neural Network; CNN)의 효율적인 구현을 제시한다. 이진화된 CNN은 기존 CNN에 이진화 과정을 추가하여 각각의 파라미터와 컨벌루션의 입력이 단일 비트로 표현될 수 있도록 변형한 것이다. 제안하는 구현에서는, 다수의 이진화된 파라미터들과 컨벌루션의 입력들을 하나의 워드로 묶어서 저장하고, CNN에서 연산 량 대부분을 차지하는 기존 컨벌루션을 이진화된 컨벌루션으로 대체하여, Bitwise XNOR-Bitcount으로 구현하였다. 이러한 SIMD 처리 방식의 구현은 CNN의 전체적인 메모리 요구량과 연산 량을 크게 감소시킬 수 있다. 실제로 LeNet-5와 ResNet-18을 대상으로 제안하는 구현은 분석 성능에서 기존의 결과와 비교하여 대등한 수준을 유지하면서도, 수행 시간을 기존 구현의 결과 대비 최대 89% 단축하고, 메모리 요구량은 기존 구현의 결과 대비 최대 95% 축소한다. This paper presents the efficient implementation of the binarized convolutional neural network (CNN). The binarized CNN is designed by modifying the conventional CNN so as to include the binarization processes for the parameters and the activation outputs. In the proposed implementation, multiple binarized parameters and multiple activation outputs are packed into a single word, and the inner-products are calculated by performing simple bitwise XNOR followed by bit-counting operations. Owing to such SIMD optimization, both the overall number of the computations and the memory footprint are reduced significantly. LeNet-5 and ResNet-18 are implemented based on the proposed SIMD optimization. When compared to the straightforward implementation of the non-binarized CNN model, the proposed implementation shows the significant reduction in terms of the inference time and the memory footprint, while maintaining the analysis performance.\n",
      "Breast cancer histology images classification: Training from scratch or transfer learning? 2018 ['Breast cancer', 'Histopathological images', 'Convolutional neural network', 'Full training', 'Transfer learning'] None We demonstrated the ability of transfer learning in comparison with the fully-trained network on the histopathological imaging modality by considering three pre-trained networks: VGG16, VGG19, and ResNet50 and analyzed their behavior for magnification independent breast cancer classification. Concurrently, we examined the effect of training–testing data size on the performance of considered networks. A fine-tuned pre-trained VGG16 with logistic regression classifier yielded the best performance with 92.60% accuracy, 95.65% area under ROC curve (AUC), and 95.95% accuracy precision score (APS) for 90%–10% training–testing data splitting. Layer-wise fine-tuning and different weight initialization schemes can be a future aspect of this study.\n",
      "심층신경망 기반 총채벌레 탐색에 관한 연구 2018 ['객체 탐색', '볼록총채벌레', '빠른 지역기반 객체 탐색', '심층신경망', '합성곱 신경망', 'Convolutinal network', 'deep learning', 'Faster R-CNN', 'object detection', 'Scirtothrips dorsalis Hood'] 최근 감귤농업에서 주요해층으로 분류되는 미소 객체 (tiny object)인 볼록총채벌레 (Scirtothrips dorsalis Hood)의 탐색은 관심이 많고 어려운 작업으로 알려져 있다. 본 논문에서는 심층신경망을 이용하여 볼록총채벌레를 탐색 (detection)하고자 한다. 분석자료는 황색끈끈이트랩 이미지자료 (250×150mm, 5472×3648픽셀)이며 합성곱 신경망 (convolutional neural network, CNN)인 ResNet을 기반으로 하는 Faster R-CNN (faster regions with CNN) 탐색모형을 사용하였다. 이미지넷(ImageNet)을 사전 학습한 가중치를 사용하고 초모수 (hyperparameter)를 격자탐색법(grid search)으로 선택한 모형을 제안한다. 제안된 모형의 AUC (area under curve)는 0.91로 아주 좋은 결과를 보이는데, 제안된 모형으로 볼록총채벌레의 생태를 파악하여 보다 더 정밀한 방제가 이뤄질 수 있을 것으로 기대한다. In this paper, we study on a detection of Scirtothrips dorsalis Hood, which is classified as a major insect in citrus farming. The detection is based on the deep neural networks, specifically the Faster R-CNN (faster regions with CNN) model based on CNN (convolutional neural network), with the yellow sticky trap image data (250×150mm, 5472×3648pixels). It was found that the model performance becomes unstable when the object is too small and rare. In order to solve this problem, we use pretrained weights to set the initial value of the model, as well as we select hyperparameters by grid search. Result shows that our proposed model has an high AUC (area under curve) value 0.91. We expect that it would be possible to know more precisely the lifespan of the Scirtothrips dorsalis Hood and to control them more precisely through our proposed model.\n",
      "CNN 소실점 검출을 이용한 차선 검출 2018 ['lane detection', 'deep learning', 'convolutional neural networks', 'transfer learning'] None Lane detection is essential in autonomous navigation. Conventional algorithms use hand crafted features which produce difficulties because of diverse image variations from illumination variations, occlusions and shadows. Recently, deep learning based approaches have provided more robust results. In this paper, we present an algorithm for the robust detection of lanes by finding vanishing points with convolutional neural networks. We use two modified CNN architectures, where the final output layer consists of four elements. The epipole and the angles of the current driving lane each have two elements. Experiments are performed by using two modified structures of the NVIDIA end-to-end model[9] and the ResNet-50 model[10].\n",
      "딥 러닝 기반의 악성흑색종 분류를 위한 컴퓨터 보조진단 알고리즘 2018 ['Deep Learning', 'Machine Learning', 'Malignant Melanoma', 'Convolutional Neural Network', 'Computer Aided Diagnosis'] None The malignant melanoma accounts for about 1 to 3% of the total malignant tumor in the West, especially in the US, it is a disease that causes more than 9,000 deaths each year. Generally, skin lesions are difficult to detect the features through photography. In this paper, we propose a computer-aided diagnosis algorithm based on deep learning for classification of malignant melanoma and benign skin tumor in RGB channel skin images. The proposed deep learning model configures the tumor lesion segmentation model and a classification model of malignant melanoma. First, U-Net was used to segment a skin lesion area in the dermoscopic image. We could implement algorithms to classify malignant melanoma and benign tumor using skin lesion image and results of expert’s labeling in ResNet. The U-Net model obtained a dice similarity coefficient of 83.45% compared with results of expert’s labeling. The classification accuracy of malignant melanoma obtained the 83.06%. As the result, it is expected that the proposed artificial intelligence algorithm will utilize as a computer-aided diagnosis algorithm and help to detect malignant melanoma at an early stage.\n",
      "임베디드 보드에서 실시간 의미론적 분할을 위한 심층 신경망 구조 2018 ['딥 러닝', '심층 신경망', '의미론적 분할', '자율주행', '임베디드 보드', 'deep learning', 'neural network', 'semantic segmentation', 'autonomous driving', 'embedded board'] 본 논문은 자율주행을 위한 실시간 의미론적 분할 방법으로 최적화된 심층 신경망 구조인Wide Inception ResNet (WIR Net)을 제안한다. 신경망 구조는 Residual connection과 Inception module 을 적용하여 특징을 추출하는 인코더와 Transposed convolution과 낮은 층의 특징 맵을 사용하여 해상도를 높이는 디코더로 구성하였고 ELU 활성화 함수를 적용함으로써 성능을 올렸다. 또한 신경망의 전체 층수를 줄이고 필터 수를 늘리는 방법을 통해 성능을 최적화하였다. 성능평가는 NVIDIA Geforce gtx 1080 과 TX1 보드를 사용하여 주행환경의 Cityscapes 데이터에 대해 클래스와 카테고리별 IoU를 평가하였다.실험 결과를 통해 클래스 IoU 53.4, 카테고리 IoU 81.8의 정확도와 TX1 보드에서 640×360, 720×480 해상도 영상처리에 17.8fps, 13.0fps의 실행속도를 보여주는 것을 확인하였다. We propose Wide Inception ResNet (WIR Net) an optimized neural network architecture as a real-time semantic segmentation method for autonomous driving. The neural network architecture consists of an encoder that extracts features by applying a residual connection and inception module, and a decoder that increases the resolution by using transposed convolution and a low layer feature map. We also improved the performance by applying an ELU activation function and optimized the neural network by reducing the number of layers and increasing the number of filters. The performance evaluations used an NVIDIA Geforce GTX 1080 and TX1 boards to assess the class and category IoU for cityscapes data in the driving environment. The experimental results show that the accuracy of class IoU 53.4, category IoU 81.8 and the execution speed of 640x360, 720x480 resolution image processing 17.8fps and 13.0fps on TX1 board.\n",
      "메모리 추가 신경망을 이용한 희소 악성코드 분류 2018 ['Malware Classification', 'Visualization', 'Memory Augmented Neural Network'] 악성코드의 수가 가파르게 증가하면서 기업 및 공공기관, 금융기관, 병·의원 등을 타깃으로 한 사이버 공격 피해사례가 늘어나고 있다. 이러한 흐름에 따라 학계와 보안 업계에서는 악성코드 탐지를 위한 다양한 연구를 진행하고 있다. 최근 들어서는 딥러닝을 비롯해 머신러닝 기법을 적용하는 형태의 연구가 많이 진행되는 추세다. 이 중 합성곱 신경망(CNN: Convolutional Neural Network), ResNet 등을 이용한 악성코드 분류 연구의 경우에는 기존의 분류 방법에 비해 정확도가 크게 향상된 것을 확인할 수 있다. 그러나 타깃 공격의 특징 중 하나는 사용된 악성코드가 불특정 다수를 상대로 광범위하게 퍼뜨리는 형태가 아닌, 특정 대상을 타깃으로 한 맞춤형 악성코드라는 점이다. 이러한 유형의 악성코드는 그 수가 많지 않기 때문에 기존에 연구되어온 머신러닝이나 딥러닝 기법을 적용하기에 한계가 있다. 본 논문은 타깃형 악성코드와 같이 샘플의 양이 부족한 상황에서 악성코드를 분류하는 방법에 대해 다루고 있다. 메모리가 추가된 신경망(MANN: Memory Augmented Neural Networks) 모델을 이용하였고 각 그룹별 20개의 소량 데이터로 구성되어 있는 악성코드 데이터셋에 대해 최대 97%까지 정확도로 분류할 수있음을 확인하였다. As the number of malicious code increases steeply, cyber attack victims targeting corporations, public institutions, financial institutions, hospitals are also increasing. Accordingly, academia and security industry are conducting various researches on malicious code detection. In recent years, there have been a lot of researches using machine learning techniques including deep learning. In the case of research using Convolutional Neural Network, ResNet, etc. for classification of malicious code, it can be confirmed that the performance improvement is higher than the existing classification method. However, one of the characteristics of the target attack is that it is custom malicious code that makes it operate only for a specific company, so it is not a form spreading widely to a large number of users. Since there are not many malicious codes of this kind, it is difficult to apply the previously studied machine learning or deep learning techniques. In this paper, we propose a method to classify malicious codes when the amount of samples is insufficient such as targeting type malicious code. As a result of the study, we confirmed that the accuracy of 97% can be achieved even with a small amount of data by applying the Memory Augmented Neural Networks model.\n",
      "자동차 주행환경에서 보행자 분류를 위한 딥러닝 모델의 전이학습 및 성능비교 2018 ['pedestrian classification', 'deep-learning', 'automobile driving environment', 'transer learning', 'INRIA database'] None In this paper, a performance comparison of deep-learning models for pedestrian classification under automobile driving environment is performed. Most automobiles nowadays are equipped with black boxes, and driver assistance systems are also applied to camera based image processing technologies. Pedestrian classification plays an important role in determining the final decision whether a candidate region is a person or not. We perform the transfer learning based on AlexNet, GoogLeNet, and ResNet that are well known as deep-learning models. For comparison experiments of the deep learning models, we used INRIA database and Chosun University (CU) database constructed under automobile driving environment. The INRIA training data set is used for transfer learning and performance validation is used with INRIA testing data set and CU database. The experimental results showed that the performance of ResNet based on transfer learning outperformed AlexNet and GoogLeNet.\n",
      "Video Captioning with Visual and Semantic Features 2018 ['Attention-Based Caption Generation', 'Deep Neural Networks', 'Semantic Feature', 'Video Captioning'] None Video captioning refers to the process of extracting features from a video and generating video captions using the extracted features. This paper introduces a deep neural network model and its learning method for effective video captioning. In this study, visual features as well as semantic features, which effectively express the video, are also used. The visual features of the video are extracted using convolutional neural networks, such as C3D and ResNet, while the semantic features are extracted using a semantic feature extraction network proposed in this paper. Further, an attention-based caption generation network is proposed for effective generation of video captions using the extracted features. The performance and effectiveness of the proposed model is verified through various experiments using two large-scale video benchmarks such as the Microsoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT).\n",
      "Video Captioning with Visual and Semantic Features 2018 ['Attention-Based Caption Generation', 'Deep Neural Networks', 'Semantic Feature', 'Video Captioning'] None Video captioning refers to the process of extracting features from a video and generating video captions usingthe extracted features. This paper introduces a deep neural network model and its learning method foreffective video captioning. In this study, visual features as well as semantic features, which effectively expressthe video, are also used. The visual features of the video are extracted using convolutional neural networks,such as C3D and ResNet, while the semantic features are extracted using a semantic feature extractionnetwork proposed in this paper. Further, an attention-based caption generation network is proposed foreffective generation of video captions using the extracted features. The performance and effectiveness of theproposed model is verified through various experiments using two large-scale video benchmarks such as theMicrosoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT).\n",
      "Video Captioning with Visual and Semantic Features 2018 ['Attention-Based Caption Generation', 'Deep Neural Networks', 'Semantic Feature', 'Video Captioning'] None Video captioning refers to the process of extracting features from a video and generating video captions using the extracted features. This paper introduces a deep neural network model and its learning method for effective video captioning. In this study, visual features as well as semantic features, which effectively express the video, are also used. The visual features of the video are extracted using convolutional neural networks, such as C3D and ResNet, while the semantic features are extracted using a semantic feature extraction network proposed in this paper. Further, an attention-based caption generation network is proposed for effective generation of video captions using the extracted features. The performance and effectiveness of the proposed model is verified through various experiments using two large-scale video benchmarks such as the Microsoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT).\n",
      "DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos 2018 ['Activity Detection', 'Bi-directional LSTM', 'Deep Neural Networks', 'Untrimmed Video'] None We propose a novel deep neural network model for detecting human activities in untrimmed videos. Theprocess of human activity detection in a video involves two steps: a step to extract features that are effective inrecognizing human activities in a long untrimmed video, followed by a step to detect human activities fromthose extracted features. To extract the rich features from video segments that could express unique patternsfor each activity, we employ two different convolutional neural network models, C3D and I-ResNet. Fordetecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directionalrecurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmarkdataset, we show the high performance of the proposed DeepAct model.\n",
      "단백질 이차 구조 예측을 위한 합성곱 신경망의 구조 2018 None None Deep learning has been actively studied for predicting protein secondary structure based only on the sequence information of the amino acids constituting the protein. In this paper, we compared the performances of the convolutional neural networks of various structures to predict the protein secondary structure. To investigate the optimal depth of the layer of neural network for the prediction of protein secondary structure, the performance according to the number of layers was investigated. We also applied the structure of GoogLeNet and ResNet which constitute building blocks of many image classification methods. These methods extract various features from input data, and smooth the gradient transmission in the learning process even using the deep layer. These architectures of convolutional neural networks were modified to suit the characteristics of protein data to improve performance.\n",
      "DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos 2018 ['Activity Detection', 'Bi-directional LSTM', 'Deep Neural Networks', 'Untrimmed Video'] None We propose a novel deep neural network model for detecting human activities in untrimmed videos. The process of human activity detection in a video involves two steps: a step to extract features that are effective in recognizing human activities in a long untrimmed video, followed by a step to detect human activities from those extracted features. To extract the rich features from video segments that could express unique patterns for each activity, we employ two different convolutional neural network models, C3D and I-ResNet. For detecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directional recurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmark dataset, we show the high performance of the proposed DeepAct model.\n",
      "DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos 2018 ['Activity Detection', 'Bi-directional LSTM', 'Deep Neural Networks', 'Untrimmed Video'] None We propose a novel deep neural network model for detecting human activities in untrimmed videos. The process of human activity detection in a video involves two steps: a step to extract features that are effective in recognizing human activities in a long untrimmed video, followed by a step to detect human activities from those extracted features. To extract the rich features from video segments that could express unique patterns for each activity, we employ two different convolutional neural network models, C3D and I-ResNet. For detecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directional recurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmark dataset, we show the high performance of the proposed DeepAct model.\n",
      "2018년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                         title  \\\n",
      "0                                                    슈퍼픽셀 이미지 분할을 이용한 ResNet 기반 백혈구 감별 알고리즘 개발   \n",
      "1                                                           ResNet-50 합성곱 신경망을 위한 고정 소수점 표현 방법   \n",
      "2                                                      합성곱 신경망을 활용한 위내시경 이미지 분류에서 전이학습의 효용성 평가   \n",
      "3                                                            전이학습에 방법에 따른 컨벌루션 신경망의영상 분류 성능 비교   \n",
      "4                                                                  보안 감시를 위한 심층학습 기반 다채널 영상 분석   \n",
      "5                                                                  이진화된 컨벌루션 신경망의 효율적인 SIMD 구현   \n",
      "6   Breast cancer histology images classification: Training from scratch or transfer learning?   \n",
      "7                                                                      심층신경망 기반 총채벌레 탐색에 관한 연구   \n",
      "8                                                                        CNN 소실점 검출을 이용한 차선 검출   \n",
      "9                                                          딥 러닝 기반의 악성흑색종 분류를 위한 컴퓨터 보조진단 알고리즘   \n",
      "10                                                         임베디드 보드에서 실시간 의미론적 분할을 위한 심층 신경망 구조   \n",
      "11                                                                  메모리 추가 신경망을 이용한 희소 악성코드 분류   \n",
      "12                                                   자동차 주행환경에서 보행자 분류를 위한 딥러닝 모델의 전이학습 및 성능비교   \n",
      "13                                          Video Captioning with Visual and Semantic Features   \n",
      "14                                          Video Captioning with Visual and Semantic Features   \n",
      "15                                          Video Captioning with Visual and Semantic Features   \n",
      "16             DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos   \n",
      "17                                                                단백질 이차 구조 예측을 위한 합성곱 신경망의 구조   \n",
      "18             DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos   \n",
      "19             DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos   \n",
      "\n",
      "    date  \\\n",
      "0   2018   \n",
      "1   2018   \n",
      "2   2018   \n",
      "3   2018   \n",
      "4   2018   \n",
      "5   2018   \n",
      "6   2018   \n",
      "7   2018   \n",
      "8   2018   \n",
      "9   2018   \n",
      "10  2018   \n",
      "11  2018   \n",
      "12  2018   \n",
      "13  2018   \n",
      "14  2018   \n",
      "15  2018   \n",
      "16  2018   \n",
      "17  2018   \n",
      "18  2018   \n",
      "19  2018   \n",
      "\n",
      "                                                                                                                                                             keywords  \\\n",
      "0                                                                                     [Super-pixel, Residual Network, Segmentation, Classification, White Blood Cell]   \n",
      "1                                                                                                                                                              [ASIC]   \n",
      "2                                                                            [Gastroscope, Convolutional Neual Network, Transfer learning, Resnet, Inception, VGGnet]   \n",
      "3                                                                                    [Deep Learning, Computer Vision, Convolutional Neural Network, Transfer Learnin]   \n",
      "4   [Video Surveillance, Object detection, Multi-object tracking, Deep learning, Probabilistic data association filter, 영상보안감시, 객체 검출, 다중 객체 추적, 심층학습, 확률적 데이터 연관 필터]   \n",
      "5                                                                                                                           [인공 신경망, 컨벌루션 신경망, 이진화, 이미지 분류, 임베디드 시스템]   \n",
      "6                                                           [Breast cancer, Histopathological images, Convolutional neural network, Full training, Transfer learning]   \n",
      "7                     [객체 탐색, 볼록총채벌레, 빠른 지역기반 객체 탐색, 심층신경망, 합성곱 신경망, Convolutinal network, deep learning, Faster R-CNN, object detection, Scirtothrips dorsalis Hood]   \n",
      "8                                                                                   [lane detection, deep learning, convolutional neural networks, transfer learning]   \n",
      "9                                                       [Deep Learning, Machine Learning, Malignant Melanoma, Convolutional Neural Network, Computer Aided Diagnosis]   \n",
      "10                                   [딥 러닝, 심층 신경망, 의미론적 분할, 자율주행, 임베디드 보드, deep learning, neural network, semantic segmentation, autonomous driving, embedded board]   \n",
      "11                                                                                           [Malware Classification, Visualization, Memory Augmented Neural Network]   \n",
      "12                                                       [pedestrian classification, deep-learning, automobile driving environment, transer learning, INRIA database]   \n",
      "13                                                                     [Attention-Based Caption Generation, Deep Neural Networks, Semantic Feature, Video Captioning]   \n",
      "14                                                                     [Attention-Based Caption Generation, Deep Neural Networks, Semantic Feature, Video Captioning]   \n",
      "15                                                                     [Attention-Based Caption Generation, Deep Neural Networks, Semantic Feature, Video Captioning]   \n",
      "16                                                                                   [Activity Detection, Bi-directional LSTM, Deep Neural Networks, Untrimmed Video]   \n",
      "17                                                                                                                                                                 []   \n",
      "18                                                                                   [Activity Detection, Bi-directional LSTM, Deep Neural Networks, Untrimmed Video]   \n",
      "19                                                                                   [Activity Detection, Bi-directional LSTM, Deep Neural Networks, Untrimmed Video]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None   \n",
      "4                                                                                                                                                                                                                                                                                                본 논문에서는 영상 보안 감시를 위한 심층학습 객체 검출과 다중 객체 추적을 위한 확률적 데이터연관 필터를 연계한 영상분석 기법을 제안하고, GPU를 이용하여 구현하는 방안을 제시한다. 제안하는 영상분석 기법은 객체 검출과 추적으로 순차적으로 수행한다. 객체 검출을 위한 심층학습은 ResNet을 이용하고, 다중 객체 추적을 위하여 확률적 데이터 연관 필터를 적용한다. 제안하는 영상분석 기법은 임의의 영역으로 불법으로 침입하는 사람을 검출하거나 특정 공간에 출입하는 사람을 계수하는데 응용할 수 있다. 시뮬레이션을 통하여 약 27fps의 속도로 48채널의 영상을 분석할 수 있음을 보이고, RTSP 프로토콜을 통하여 실시간 영상분석이 가능함을 보인다.   \n",
      "5                                                                                                                                                                     본 논문에서는 이진화된 컨벌루션 신경망 (Convolutional Neural Network; CNN)의 효율적인 구현을 제시한다. 이진화된 CNN은 기존 CNN에 이진화 과정을 추가하여 각각의 파라미터와 컨벌루션의 입력이 단일 비트로 표현될 수 있도록 변형한 것이다. 제안하는 구현에서는, 다수의 이진화된 파라미터들과 컨벌루션의 입력들을 하나의 워드로 묶어서 저장하고, CNN에서 연산 량 대부분을 차지하는 기존 컨벌루션을 이진화된 컨벌루션으로 대체하여, Bitwise XNOR-Bitcount으로 구현하였다. 이러한 SIMD 처리 방식의 구현은 CNN의 전체적인 메모리 요구량과 연산 량을 크게 감소시킬 수 있다. 실제로 LeNet-5와 ResNet-18을 대상으로 제안하는 구현은 분석 성능에서 기존의 결과와 비교하여 대등한 수준을 유지하면서도, 수행 시간을 기존 구현의 결과 대비 최대 89% 단축하고, 메모리 요구량은 기존 구현의 결과 대비 최대 95% 축소한다.   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None   \n",
      "7                                                                                                                                      최근 감귤농업에서 주요해층으로 분류되는 미소 객체 (tiny object)인 볼록총채벌레 (Scirtothrips dorsalis Hood)의 탐색은 관심이 많고 어려운 작업으로 알려져 있다. 본 논문에서는 심층신경망을 이용하여 볼록총채벌레를 탐색 (detection)하고자 한다. 분석자료는 황색끈끈이트랩 이미지자료 (250×150mm, 5472×3648픽셀)이며 합성곱 신경망 (convolutional neural network, CNN)인 ResNet을 기반으로 하는 Faster R-CNN (faster regions with CNN) 탐색모형을 사용하였다. 이미지넷(ImageNet)을 사전 학습한 가중치를 사용하고 초모수 (hyperparameter)를 격자탐색법(grid search)으로 선택한 모형을 제안한다. 제안된 모형의 AUC (area under curve)는 0.91로 아주 좋은 결과를 보이는데, 제안된 모형으로 볼록총채벌레의 생태를 파악하여 보다 더 정밀한 방제가 이뤄질 수 있을 것으로 기대한다.   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         None   \n",
      "10                                                                                                                                                        본 논문은 자율주행을 위한 실시간 의미론적 분할 방법으로 최적화된 심층 신경망 구조인Wide Inception ResNet (WIR Net)을 제안한다. 신경망 구조는 Residual connection과 Inception module 을 적용하여 특징을 추출하는 인코더와 Transposed convolution과 낮은 층의 특징 맵을 사용하여 해상도를 높이는 디코더로 구성하였고 ELU 활성화 함수를 적용함으로써 성능을 올렸다. 또한 신경망의 전체 층수를 줄이고 필터 수를 늘리는 방법을 통해 성능을 최적화하였다. 성능평가는 NVIDIA Geforce gtx 1080 과 TX1 보드를 사용하여 주행환경의 Cityscapes 데이터에 대해 클래스와 카테고리별 IoU를 평가하였다.실험 결과를 통해 클래스 IoU 53.4, 카테고리 IoU 81.8의 정확도와 TX1 보드에서 640×360, 720×480 해상도 영상처리에 17.8fps, 13.0fps의 실행속도를 보여주는 것을 확인하였다.   \n",
      "11  악성코드의 수가 가파르게 증가하면서 기업 및 공공기관, 금융기관, 병·의원 등을 타깃으로 한 사이버 공격 피해사례가 늘어나고 있다. 이러한 흐름에 따라 학계와 보안 업계에서는 악성코드 탐지를 위한 다양한 연구를 진행하고 있다. 최근 들어서는 딥러닝을 비롯해 머신러닝 기법을 적용하는 형태의 연구가 많이 진행되는 추세다. 이 중 합성곱 신경망(CNN: Convolutional Neural Network), ResNet 등을 이용한 악성코드 분류 연구의 경우에는 기존의 분류 방법에 비해 정확도가 크게 향상된 것을 확인할 수 있다. 그러나 타깃 공격의 특징 중 하나는 사용된 악성코드가 불특정 다수를 상대로 광범위하게 퍼뜨리는 형태가 아닌, 특정 대상을 타깃으로 한 맞춤형 악성코드라는 점이다. 이러한 유형의 악성코드는 그 수가 많지 않기 때문에 기존에 연구되어온 머신러닝이나 딥러닝 기법을 적용하기에 한계가 있다. 본 논문은 타깃형 악성코드와 같이 샘플의 양이 부족한 상황에서 악성코드를 분류하는 방법에 대해 다루고 있다. 메모리가 추가된 신경망(MANN: Memory Augmented Neural Networks) 모델을 이용하였고 각 그룹별 20개의 소량 데이터로 구성되어 있는 악성코드 데이터셋에 대해 최대 97%까지 정확도로 분류할 수있음을 확인하였다.   \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this paper, we propose an efficient WBC 14-Diff classification which performs using the WBC-ResNet-152, a type of CNN model. The main point of view is to use Super-pixel for the segmentation of the image of WBC, and to use ResNet for the classification of WBC.  A total of 136,164 blood image samples (224x224) were grouped for image segmentation, training, training verification, and final test performance analysis.  Image segmentation using super-pixels have different number of images for each classes, so weighted average was applied and therefore image segmentation error was low at 7.23%.  Using the training data-set for training 50 times, and using soft-max classifier, TPR average of 80.3% for the training set of 8,827 images was achieved. Based on this, using verification data-set of 21,437 images, 14-Diff classification TPR average of normal WBCs were at 93.4% and TPR average of abnormal WBCs were at 83.3%. The result and methodology of this research demonstrates the usefulness of artificial intelligence technology in the blood cell image classification field.  WBC-ResNet-152 based morphology approach is shown to be meaningful and worthwhile method. And based on stored medical data, in-depth diagnosis and early detection of curable diseases is expected to improve the quality of treatment.  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Recently, the convolutional neural network shows high performance in many computer vision tasks. However, convolutional neural networks require enormous amount of operation, so it is difficult to adopt them in the embedded environments. To solve this problem, many studies are performed on the ASIC or FPGA implementation, where an efficient representation method is required. The fixed-point representation is adequate for the ASIC or FPGA implementation but causes a performance degradation. This paper proposes a separate optimization of representations for the convolutional layers and the batch normalization layers. With the proposed method, the required bit width for the convolutional layers is reduced from 16 bits to 10 bits for the ResNet-50 neural network. Since the computation amount of the convolutional layers occupies the most of the entire computation, the bit width reduction in the convolutional layers enables the efficient implementation of the convolutional neural networks.  \n",
      "2   Stomach cancer is the most diagnosed cancer in Korea. When gastric cancer is detected early, the 5-year survival rate is as high as 90%. Gastroscopy is a very useful method for early diagnosis. But the false negative rate of gastric cancer in the gastroscopy was 4.6~25.8% due to the subjective judgment of the physician. Recently, the image classification performance of the image recognition field has been advanced by the convolutional neural network. Convolutional neural networks perform well when diverse and sufficient amounts of data are supported. However, medical data is not easy to access and it is difficult to gather enough high-quality data that includes expert annotations. So This paper evaluates the efficacy of transfer learning in gastroscopy classification and diagnosis. We obtained 787 endoscopic images of gastric endoscopy at Gil Medical Center, Gachon University. The number of normal images was 200, and the number of abnormal images was 587. The image size was reconstructed and normalized. In the case of the ResNet50 structure, the classification accuracy before and after applying the transfer learning was improved from 0.9 to 0.947, and the AUC was also improved from 0.94 to 0.98. In the case of the InceptionV3 structure, the classification accuracy before and after applying the transfer learning was improved from 0.862 to 0.924, and the AUC was also improved from 0.89 to 0.97. In the case of the VGG16 structure, the classification accuracy before and after applying the transfer learning was improved from 0.87 to 0.938, and the AUC was also improved from 0.89 to 0.98. The difference in the performance of the CNN model before and after transfer learning was statistically significant when confirmed by T-test (p < 0.05). As a result, transfer learning is judged to be an effective method of medical data that is difficult to collect good quality data.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Core algorithm of deep learning Convolutional Neural Network(CNN) shows better performance than other machine learning algorithms. However, if there is not sufficient data, CNN can not achieve satisfactory performance even if the classifier is excellent. In this situation, it has been proven that the use of transfer learning can have a great effect. In this paper, we apply two transition learning methods(freezing, retraining) to three CNN models(ResNet-50, Inception-V3, DenseNet-121) and compare and analyze how the classification performance of CNN changes according to the methods. As a result of statistical significance test using various evaluation indicators, ResNet-50, Inception-V3, and DenseNet-121 differed by 1.18 times, 1.09 times, and 1.17 times, respectively. Based on this, we concluded that the retraining method may be more effective than the freezing method in case of transition learning in image classification problem.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In this paper, a video analysis is proposed to implement video surveillance system with deep learning object detection and probabilistic data association filter for tracking multiple objects and suggests its implementation using GPU. The proposed video analysis technique involves object detection and object tracking sequentially. The deep learning network architecture uses ResNet for object detection and applies probabilistic data association filter for multiple objects tracking. The proposed video analysis technique can be used to detect intruders illegally trespassing any restricted area or to count the number of people entering a specified area. As results of simulations and experiments, 48 channels of videos can be analyzed at a speed of about 27 fps and real-time video analysis is possible through RTSP protocol.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         This paper presents the efficient implementation of the binarized convolutional neural network (CNN). The binarized CNN is designed by modifying the conventional CNN so as to include the binarization processes for the parameters and the activation outputs. In the proposed implementation, multiple binarized parameters and multiple activation outputs are packed into a single word, and the inner-products are calculated by performing simple bitwise XNOR followed by bit-counting operations. Owing to such SIMD optimization, both the overall number of the computations and the memory footprint are reduced significantly. LeNet-5 and ResNet-18 are implemented based on the proposed SIMD optimization. When compared to the straightforward implementation of the non-binarized CNN model, the proposed implementation shows the significant reduction in terms of the inference time and the memory footprint, while maintaining the analysis performance.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We demonstrated the ability of transfer learning in comparison with the fully-trained network on the histopathological imaging modality by considering three pre-trained networks: VGG16, VGG19, and ResNet50 and analyzed their behavior for magnification independent breast cancer classification. Concurrently, we examined the effect of training–testing data size on the performance of considered networks. A fine-tuned pre-trained VGG16 with logistic regression classifier yielded the best performance with 92.60% accuracy, 95.65% area under ROC curve (AUC), and 95.95% accuracy precision score (APS) for 90%–10% training–testing data splitting. Layer-wise fine-tuning and different weight initialization schemes can be a future aspect of this study.  \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In this paper, we study on a detection of Scirtothrips dorsalis Hood, which is classified as a major insect in citrus farming. The detection is based on the deep neural networks, specifically the Faster R-CNN (faster regions with CNN) model based on CNN (convolutional neural network), with the yellow sticky trap image data (250×150mm, 5472×3648pixels). It was found that the model performance becomes unstable when the object is too small and rare. In order to solve this problem, we use pretrained weights to set the initial value of the model, as well as we select hyperparameters by grid search. Result shows that our proposed model has an high AUC (area under curve) value 0.91. We expect that it would be possible to know more precisely the lifespan of the Scirtothrips dorsalis Hood and to control them more precisely through our proposed model.  \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Lane detection is essential in autonomous navigation. Conventional algorithms use hand crafted features which produce difficulties because of diverse image variations from illumination variations, occlusions and shadows. Recently, deep learning based approaches have provided more robust results. In this paper, we present an algorithm for the robust detection of lanes by finding vanishing points with convolutional neural networks. We use two modified CNN architectures, where the final output layer consists of four elements. The epipole and the angles of the current driving lane each have two elements. Experiments are performed by using two modified structures of the NVIDIA end-to-end model[9] and the ResNet-50 model[10].  \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The malignant melanoma accounts for about 1 to 3% of the total malignant tumor in the West, especially in the US, it is a disease that causes more than 9,000 deaths each year. Generally, skin lesions are difficult to detect the features through photography. In this paper, we propose a computer-aided diagnosis algorithm based on deep learning for classification of malignant melanoma and benign skin tumor in RGB channel skin images. The proposed deep learning model configures the tumor lesion segmentation model and a classification model of malignant melanoma. First, U-Net was used to segment a skin lesion area in the dermoscopic image. We could implement algorithms to classify malignant melanoma and benign tumor using skin lesion image and results of expert’s labeling in ResNet. The U-Net model obtained a dice similarity coefficient of 83.45% compared with results of expert’s labeling. The classification accuracy of malignant melanoma obtained the 83.06%. As the result, it is expected that the proposed artificial intelligence algorithm will utilize as a computer-aided diagnosis algorithm and help to detect malignant melanoma at an early stage.  \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We propose Wide Inception ResNet (WIR Net) an optimized neural network architecture as a real-time semantic segmentation method for autonomous driving. The neural network architecture consists of an encoder that extracts features by applying a residual connection and inception module, and a decoder that increases the resolution by using transposed convolution and a low layer feature map. We also improved the performance by applying an ELU activation function and optimized the neural network by reducing the number of layers and increasing the number of filters. The performance evaluations used an NVIDIA Geforce GTX 1080 and TX1 boards to assess the class and category IoU for cityscapes data in the driving environment. The experimental results show that the accuracy of class IoU 53.4, category IoU 81.8 and the execution speed of 640x360, 720x480 resolution image processing 17.8fps and 13.0fps on TX1 board.  \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          As the number of malicious code increases steeply, cyber attack victims targeting corporations, public institutions, financial institutions, hospitals are also increasing. Accordingly, academia and security industry are conducting various researches on malicious code detection. In recent years, there have been a lot of researches using machine learning techniques including deep learning. In the case of research using Convolutional Neural Network, ResNet, etc. for classification of malicious code, it can be confirmed that the performance improvement is higher than the existing classification method. However, one of the characteristics of the target attack is that it is custom malicious code that makes it operate only for a specific company, so it is not a form spreading widely to a large number of users. Since there are not many malicious codes of this kind, it is difficult to apply the previously studied machine learning or deep learning techniques. In this paper, we propose a method to classify malicious codes when the amount of samples is insufficient such as targeting type malicious code. As a result of the study, we confirmed that the accuracy of 97% can be achieved even with a small amount of data by applying the Memory Augmented Neural Networks model.  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this paper, a performance comparison of deep-learning models for pedestrian classification under automobile driving environment is performed. Most automobiles nowadays are equipped with black boxes, and driver assistance systems are also applied to camera based image processing technologies. Pedestrian classification plays an important role in determining the final decision whether a candidate region is a person or not. We perform the transfer learning based on AlexNet, GoogLeNet, and ResNet that are well known as deep-learning models. For comparison experiments of the deep learning models, we used INRIA database and Chosun University (CU) database constructed under automobile driving environment. The INRIA training data set is used for transfer learning and performance validation is used with INRIA testing data set and CU database. The experimental results showed that the performance of ResNet based on transfer learning outperformed AlexNet and GoogLeNet.  \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Video captioning refers to the process of extracting features from a video and generating video captions using the extracted features. This paper introduces a deep neural network model and its learning method for effective video captioning. In this study, visual features as well as semantic features, which effectively express the video, are also used. The visual features of the video are extracted using convolutional neural networks, such as C3D and ResNet, while the semantic features are extracted using a semantic feature extraction network proposed in this paper. Further, an attention-based caption generation network is proposed for effective generation of video captions using the extracted features. The performance and effectiveness of the proposed model is verified through various experiments using two large-scale video benchmarks such as the Microsoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT).  \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Video captioning refers to the process of extracting features from a video and generating video captions usingthe extracted features. This paper introduces a deep neural network model and its learning method foreffective video captioning. In this study, visual features as well as semantic features, which effectively expressthe video, are also used. The visual features of the video are extracted using convolutional neural networks,such as C3D and ResNet, while the semantic features are extracted using a semantic feature extractionnetwork proposed in this paper. Further, an attention-based caption generation network is proposed foreffective generation of video captions using the extracted features. The performance and effectiveness of theproposed model is verified through various experiments using two large-scale video benchmarks such as theMicrosoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT).  \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Video captioning refers to the process of extracting features from a video and generating video captions using the extracted features. This paper introduces a deep neural network model and its learning method for effective video captioning. In this study, visual features as well as semantic features, which effectively express the video, are also used. The visual features of the video are extracted using convolutional neural networks, such as C3D and ResNet, while the semantic features are extracted using a semantic feature extraction network proposed in this paper. Further, an attention-based caption generation network is proposed for effective generation of video captions using the extracted features. The performance and effectiveness of the proposed model is verified through various experiments using two large-scale video benchmarks such as the Microsoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT).  \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We propose a novel deep neural network model for detecting human activities in untrimmed videos. Theprocess of human activity detection in a video involves two steps: a step to extract features that are effective inrecognizing human activities in a long untrimmed video, followed by a step to detect human activities fromthose extracted features. To extract the rich features from video segments that could express unique patternsfor each activity, we employ two different convolutional neural network models, C3D and I-ResNet. Fordetecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directionalrecurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmarkdataset, we show the high performance of the proposed DeepAct model.  \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Deep learning has been actively studied for predicting protein secondary structure based only on the sequence information of the amino acids constituting the protein. In this paper, we compared the performances of the convolutional neural networks of various structures to predict the protein secondary structure. To investigate the optimal depth of the layer of neural network for the prediction of protein secondary structure, the performance according to the number of layers was investigated. We also applied the structure of GoogLeNet and ResNet which constitute building blocks of many image classification methods. These methods extract various features from input data, and smooth the gradient transmission in the learning process even using the deep layer. These architectures of convolutional neural networks were modified to suit the characteristics of protein data to improve performance.  \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We propose a novel deep neural network model for detecting human activities in untrimmed videos. The process of human activity detection in a video involves two steps: a step to extract features that are effective in recognizing human activities in a long untrimmed video, followed by a step to detect human activities from those extracted features. To extract the rich features from video segments that could express unique patterns for each activity, we employ two different convolutional neural network models, C3D and I-ResNet. For detecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directional recurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmark dataset, we show the high performance of the proposed DeepAct model.  \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We propose a novel deep neural network model for detecting human activities in untrimmed videos. The process of human activity detection in a video involves two steps: a step to extract features that are effective in recognizing human activities in a long untrimmed video, followed by a step to detect human activities from those extracted features. To extract the rich features from video segments that could express unique patterns for each activity, we employ two different convolutional neural network models, C3D and I-ResNet. For detecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directional recurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmark dataset, we show the high performance of the proposed DeepAct model.  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                                  keyword  count\n",
      "69                   Deep Neural Networks      6\n",
      "74                        Untrimmed Video      3\n",
      "41                          deep learning      3\n",
      "73                    Bi-directional LSTM      3\n",
      "72                     Activity Detection      3\n",
      "71                       Video Captioning      3\n",
      "70                       Semantic Feature      3\n",
      "68     Attention-Based Caption Generation      3\n",
      "14           Convolutional Neural Network      2\n",
      "12                          Deep Learning      2\n",
      "8                       Transfer learning      2\n",
      "42                           Faster R-CNN      1\n",
      "43                       object detection      1\n",
      "54                                   자율주행      1\n",
      "44             Scirtothrips dorsalis Hood      1\n",
      "45                         lane detection      1\n",
      "46          convolutional neural networks      1\n",
      "47                      transfer learning      1\n",
      "48                       Machine Learning      1\n",
      "49                     Malignant Melanoma      1\n",
      "50               Computer Aided Diagnosis      1\n",
      "51                                   딥 러닝      1\n",
      "52                                 심층 신경망      1\n",
      "53                                의미론적 분할      1\n",
      "59                         embedded board      1\n",
      "55                                임베디드 보드      1\n",
      "56                         neural network      1\n",
      "57                  semantic segmentation      1\n",
      "58                     autonomous driving      1\n",
      "39                                합성곱 신경망      1\n",
      "60                 Malware Classification      1\n",
      "61                          Visualization      1\n",
      "62        Memory Augmented Neural Network      1\n",
      "63              pedestrian classification      1\n",
      "64                          deep-learning      1\n",
      "65         automobile driving environment      1\n",
      "66                       transer learning      1\n",
      "67                         INRIA database      1\n",
      "40                   Convolutinal network      1\n",
      "0                             Super-pixel      1\n",
      "38                                  심층신경망      1\n",
      "10                              Inception      1\n",
      "18                  Multi-object tracking      1\n",
      "17                       Object detection      1\n",
      "16                     Video Surveillance      1\n",
      "15                       Transfer Learnin      1\n",
      "13                        Computer Vision      1\n",
      "11                                 VGGnet      1\n",
      "9                                  Resnet      1\n",
      "20  Probabilistic data association filter      1\n",
      "7             Convolutional Neual Network      1\n",
      "6                             Gastroscope      1\n",
      "5                                    ASIC      1\n",
      "4                        White Blood Cell      1\n",
      "3                          Classification      1\n",
      "2                            Segmentation      1\n",
      "19                          Deep learning      1\n",
      "21                                 영상보안감시      1\n",
      "1                        Residual Network      1\n",
      "30                               임베디드 시스템      1\n",
      "36                                 볼록총채벌레      1\n",
      "35                                  객체 탐색      1\n",
      "34                          Full training      1\n",
      "33           Convolutional neural network      1\n",
      "32               Histopathological images      1\n",
      "31                          Breast cancer      1\n",
      "29                                 이미지 분류      1\n",
      "22                                  객체 검출      1\n",
      "28                                    이진화      1\n",
      "27                               컨벌루션 신경망      1\n",
      "26                                 인공 신경망      1\n",
      "25                          확률적 데이터 연관 필터      1\n",
      "24                                   심층학습      1\n",
      "23                               다중 객체 추적      1\n",
      "37                          빠른 지역기반 객체 탐색      1\n",
      "데이터가 저장되었습니다: resnet_2018_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2018_academic_riss.csv\n",
      "ResNet 모델을 이용한 눈 주변 영역의 특징 추출 및 개인 인증 2019 ['Periocular Region', 'Authentication', 'CNN', 'MLP'] None Deep learning approach based on convolution neural network (CNN)  has extensively studied in the field of computer vision. However, periocular feature extraction using CNN was not well studied because it is practically impossible to collect large volume of biometric data. This study uses the ResNet model which was trained with the ImageNet dataset. To overcome the problem of insufficient training data, we focused on the training of multi-layer perception (MLP) having simple structure rather than training the CNN having complex structure. It first extracts features using the pretrained ResNet model and reduces the feature dimension by principle component analysis (PCA), then trains a MLP classifier. Experimental results with the public periocular dataset UBIPr show that the proposed method is effective in person authentication using periocular region. Especially it has the advantage which can be directly applied for other biometric traits.\n",
      "Multi-parametric MRIs based assessment of Hepatocellular Carcinoma Differentiation with Multi-scale ResNet 2019 ['Multi-parametric MRI', 'data fusion', 'transfer learning', 'deep learning', 'hepatocellular carcinoma differentiation'] None To explore an effective non-invasion medical imaging diagnostics approach for hepatocellular carcinoma (HCC), we propose a method based on adopting the multiple technologies with the multi-parametric data fusion, transfer learning, and multi-scale deep feature extraction. Firstly, to make full use of complementary and enhancing the contribution of different modalities viz. multi-parametric MRI images in the lesion diagnosis, we propose a data-level fusion strategy. Secondly, based on the fusion data as the input, the multi-scale residual neural network with SPP (Spatial Pyramid Pooling) is utilized for the discriminative feature representation learning. Thirdly, to mitigate the impact of the lack of training samples, we do the pre-training of the proposed multi-scale residual neural network model on the natural image dataset and the fine-tuning with the chosen multi-parametric MRI images as complementary data. The comparative experiment results on the dataset from the clinical cases show that our proposed approach by employing the multiple strategies achieves the highest accuracy of 0.847±0.023 in the classification problem on the HCC differentiation. In the problem of discriminating the HCC lesion from the non-tumor area, we achieve a good performance with accuracy, sensitivity, specificity and AUC (area under the ROC curve) being 0.981±0.002, 0.981±0.002, 0.991±0.007 and 0.999±0.0008, respectively.\n",
      "ResNet을 이용한 얼굴 인식 기반 출입관리시스템 개발 2019 ['Face Recognition', 'Access Control', 'Deep Learning', 'ResNet'] None None\n",
      "한국어 음성 명령어 인식을 위한 자동데이터 구축 2019 ['Korean Speech Command', 'Speech Recognition', 'Automatic Data Construction', 'ResNet', 'CNN', '한국어 명령어 인식', '음성인식', '자동 데이터 구축', '레스넷', '합성곱신경망'] 최근 화두가 되고 있는 AI분야에서 가장 큰 문제점은 학습데이터의 부족 문제를 꼽을 수 있다. 수동 데이터 구축에는 많은 시간과 노력이 소요되기에 개인이 손쉽게 필요 데이터를 구축하기는 매우 어렵다. 반면, 수동 데이터 구축에 비해 자동으로 구축하는 것은 높은 품질을 유지하는 것이 관건이다. 본 논문에서는 한국어 음성 명령어 인식기 개발에 필요한 데이터를 웹에서 자동으로 추출하고, 학습데이터로 사용할 수 있는 데이터를 자동으로 선별하는 방법을 소개한다. 특히, 자동 구축된 한국어 음성 데이터를 대상으로 우수한 성능을 보이는 ResNet기반의 수정 모델을 기반으로, 건강 및 일상생활도메인의 명령어 셋을 대상으로 적용가능성을 보이기 위한 실험을 진행하였다. 자동으로 구축된 데이터만을 사용한 일련의 실험에서 건강도메인은 ResNet15에서 89.5%, 일상생활도메인에서는 ResNet8에서 82%의 정확도를 보임으로써, 자동 수집 데이터의 활용 가능성을 검증하였다. The biggest problem in the AI field, which has become a hot topic in recent years, is how to deal with the lack of training data. Since manual data construction takes a lot of time and efforts, it is non-trivial for an individual to easily build the necessary data. On the other hand, automatic data construction needs to handle data quality issue. In this paper, we introduce a method to automatically extract the data required to develop Korean speech command recognizer from the web and to automatically select the data that can be used for training data. In particular, we propose a modified ResNet model that shows modest performance for the automatically constructed Korean speech command data. We conducted an experiment to show the applicability of the command set of the health and daily life domain. In a series of experiments using only automatically constructed data, the accuracy of the health domain was 89.5% in ResNet15 and 82% in ResNet8 in the daily lives domain, respectively.\n",
      "Layer‐wise hint‐based training for knowledge transfer in a teacher‐student framework 2019 ['knowledge transfer', 'layer‐wise hint training', 'residual networks', 'teacher‐student framework'] None We devise a layer‐wise hint training method to improve the existing hint‐based knowledge distillation (KD) training approach, which is employed for knowledge transfer in a teacher‐student framework using a residual network (ResNet). To achieve this objective, the proposed method first iteratively trains the student ResNet and incrementally employs hint‐based information extracted from the pretrained teacher ResNet containing several hint and guided layers. Next, typical softening factor‐based KD training is performed using the previously estimated hint‐based information. We compare the recognition accuracy of the proposed approach with that of KD training without hints, hint‐based KD training, and ResNet‐based layer‐wise pretraining using reliable datasets, including CIFAR‐10, CIFAR‐100, and MNIST. When using the selected multiple hint‐based information items and their layer‐wise transfer in the proposed method, the trained student ResNet more accurately reflects the pretrained teacher ResNet's rich information than the baseline training methods, for all the benchmark datasets we consider in this study.\n",
      "앙상블 학습 알고리즘을 이용한 컨벌루션 신경망의 분류 성능 분석에 관한 연구 2019 ['Deep Learning', 'Computer Vision', 'CNN', 'Ensemble Learning Algorithm'] None In this paper, we compare and analyze the classification performance of deep learning algorithm Convolutional Neural Network(CNN) ac cording to ensemble generation and combining techniques. We used several CNN models(VGG16, VGG19, DenseNet121, DenseNet169, DenseNet201, ResNet18, ResNet34, ResNet50, ResNet101, ResNet152, GoogLeNet) to create 10 ensemble generation combinations and applied 6 combine techniques(average, weighted average, maximum, minimum, median, product) to the optimal combination. Experimental results, DenseNet169-VGG16-GoogLeNet combination in ensemble generation, and the product rule in ensemble combination showed the best performance. Based on this, it was concluded that ensemble in different models of high benchmarking scores is another way to get good results.\n",
      "CNN Model Performance Analysis on MRI Images of an OASIS Dataset for Distinction Between Healthy and Alzheimer’s Patients 2019 ['Medical MRI', 'CNN', 'AlexNet', 'GoogLeNet', 'ResNet50', 'CAD'] None In this paper, we present the performance of a medical image classification model pretrained on natural images. In addition, another model is scratch trained from available medical magnetic resonance images in order to get a comparative analysis. We perform shallow tuning and fine-tuning of the pretrained model (AlexNet, GoogLeNet, and ResNet50) in a bunch of layers in order to find the impact of each section of layers in the classification result. We use 28 normal controls (NC) and 28 Alzheimer’s disease (AD) patients for classification, selecting 30 important slices from each patient. Once all the slices were collected, each model was trained, validated, and tested at a ratio of 6:2:2 on a random selection basis. The testing results are reported and analyzed so the final CNN model could be built with a minimal number of layers for optimal performance.\n",
      "영상기반 콘크리트 균열 탐지 딥러닝 모델의 유형별 성능 비교 2019 ['crack detection', 'deep learning', 'image classification', 'object detection', 'semantic segmentation', 'instance segmentation'] None In this study, various types of deep learning models that have been proposed recently are classified according to data input / output types and analyzed to find the deep learning model suitable for constructing a crack detection model. First the deep learning models are classified into image classification model, object segmentation model, object detection model, and instance segmentation model. ResNet-101, DeepLab V2, Faster R-CNN, and Mask R-CNN were selected as representative deep learning model of each type. For the comparison, ResNet-101 was implemented for all the types of deep learning model as a backbone network which serves as a main feature extractor. The four types of deep learning models were trained with 500 crack images taken from real concrete structures and collected from the Internet. The four types of deep learning models showed high accuracy above 94% during the training.Comparative evaluation was conducted using 40 images taken from real concrete structures. The performance of each type of deep learning model was measured using precision and recall. In the experimental result, Mask R-CNN, an instance segmentation deep learning model showed the highest precision and recall on crack detection.Qualitative analysis also shows that Mask R-CNN could detect crack shapes most similarly to the real crack shapes.\n",
      "A Novel Integrated Convolutional Neural Network via Deep Transfer Learning in Colorectal Images 2019 ['Colon Disease Classification', 'Convolutional Neural Networks', 'Transfer Learning'] None In this paper, we explore the use of current deep learning methods, convolutional neural networks (CNNs) in the field of computer-aided diagnosis systems to classify several endoscopic colon diseases. Transfer learning by fine-tuning deep convolutional neural networks (CNNs) is applied due to the limited amount of data. For this, state-of-the-art CNN architectures, such as VGG16, VGG19, InceptionV3, ResNet50, Inception-ResNet-V2, DenseNet169 were used for training and validating the dataset. However, these existing architectures cannot extract more dense endoscopic image features and have problem on similar-looking images of different category. Therefore, we propose a novel integrated convolutional neural network to develop a more accurate and highly efficient method for endoscopic image classification, which uses the features of earlier layers in the classification process and increases the receptive field of view at the end layers in the network. We compare and evaluate our performance using performance metrics Accuracy (ACC), Recall, Precision and F1-score. In our experimental results, the proposed method outperforms the existing architectures, obtaining an accuracy of about 92.4% on the test dataset.\n",
      "말벌 영상인식을 위한 심층 합성곱 신경망의 성능 평가 2019 ['Vespa hornets', 'Deep convolutional neural network', 'Deep learning', 'Classification'] None One of the serious factors for honeybee decline is due to the various attacks from Vespa hornets, indigenous and invaded. Population monitoring as well as the alerting systems is requested against the Vespa. Automated image recognition is the primary step for the unmanned autonomous monitoring system development. This study compared the recent deep convolutional neural network (DCNN) algorithms such as AlexNet, VGG19, GoogLeNet, and ResNet50 for the best model selection for classification of 3 Vespa species, V. mandarinia, V. crabro and V. velutina. To evaluate classification performance, accuracy was utilized after transfer learning on each DCNN. As a result, the ResNet50 showed the best in terms of accuracy after sufficient training of 100 epochs. If performance and speed are considered simultaneously, AlexNet could be the alternative. The real-time monitoring system for objects requires both localization and classification. And Vespa occurrence or population change would need rapid recognition for the objects. Therefore speedy image recognition based on the DCNN, which combines localization and classification for objects in an image, should be considered in the future works.\n",
      "Sentinel-1 A/B 위성 SAR 자료와 딥러닝 모델을 이용한 여름철 북극해 해빙 분류 연구 2019 ['Sentinel-1 A/B', 'sea ice', 'thermal noise', 'Deep Learning', 'classification'] 북극항로의 개척 가능성과 정확한 기후 예측 모델의 필요성에 의해 북극해 고해상도 해빙 지도의 중요성이 증가하고 있다. 그러나 기존의 북극 해빙 지도는 제작에 사용된 위성 영상 취득 센서의 특성에 따른 데이터의 취득과 공간해상도 등에서 그 활용도가 제한된다. 본 연구에서는 Sentinel-1 A/B SAR 위성자료로부터 고해상도 해빙 지도를 생성하기 위한 딥러닝 기반의 해빙 분류 알고리즘을 연구하였다. 북극해 Ice Chart를 기반으로 전문가 판독에 의해 Open Water, First Year Ice, Multi Year Ice의 세 클래스로 구성된 훈련자료를 구축하였으며, Convolutional Neural Network 기반의 두 가지 딥러닝 모델(Simple CNN, Resnet50)과 입사각 및 thermal noise가 보정된 HV 밴드를 포함하는 다섯 가지 입력 밴드 조합을 이용하여 총 10가지 케이스의 해빙 분류를 실시하였다. 이 케이스들에 대하여 Ground Truth Point를 사용하여 정확도를 비교하고, 가장 높은 정확도가 나온케이스에 대해 confusion matrix 및 Cohen의 kappa 분석을 실시하였다. 또한 전통적으로 분류를 위해 많이 활용되어 온 Maximum Likelihood Classifier 기법을 이용한 분류결과에 대해서도 같은 비교를 하였다. 그 결과Convolution 층 2개, Max Pooling 층 2개를 가진 구조의 Convolutional Neural Network에 [HV, 입사각] 밴드를 넣은 딥러닝 알고리즘의 분류 결과가 96.66%의 가장 높은 분류 정확도를 보였으며, Cohen의 kappa 계수는 0.9499 로 나타나 딥러닝에 의한 해빙 분류는 비교적 높은 분류 결과를 보였다. 또한 모든 딥러닝 케이스는 Maximum Likelihood Classifier 기법에 비해 높은 분류 정확도를 보였다. The importance of high-resolution sea ice maps of the Arctic Ocean is increasing due to the possibility of pioneering North Pole Routes and the necessity of precise climate prediction models. In this study, sea ice classification algorithms for two deep learning models were examined using Sentinel- 1 A/B SAR data to generate high-resolution sea ice classification maps. Based on current ice charts, three classes (Open Water, First Year Ice, Multi Year Ice) of training data sets were generated by Arctic sea ice and remote sensing experts. Ten sea ice classification algorithms were generated by combing two deep learning models (i.e. Simple CNN and Resnet50) and five cases of input bands including incident angles and thermal noise corrected HV bands. For the ten algorithms, analyses were performed by comparing classification results with ground truth points. A confusion matrix and Cohen’s kappa coefficient were produced for the case that showed best result. Furthermore, the classification result with the Maximum Likelihood Classifier that has been traditionally employed to classify sea ice. In conclusion, the Convolutional Neural Network case, which has two convolution layers and two max pooling layers, with HV and incident angle input bands shows classification accuracy of 96.66%, and Cohen’s kappa coefficient of 0.9499. All deep learning cases shows better classification accuracy than the classification result of the Maximum Likelihood Classifier.\n",
      "가상화 플랫폼을 통한 CNN기반 모니터링 애플리케이션의 안정적인 응답 속도 보장 방안 연구 2019 ['Virtualized Platform', 'Cloud Computing', 'OpenStack', 'Docker', 'IaaS', 'CNN', 'Monitoring', '가상화 플랫폼', '클라우드 컴퓨팅', 'OpenStack', 'Docker', 'IaaS', 'CNN', '모니터링'] 최근 가상화 기술이 적용된 가상화 플랫폼(Virtualized Platform)을 도입하게 되면서 단일 하드웨어 리소스의 파티셔닝을 통한 리소스 활용률 상승과 마이그레이션을 통한 확장성의 이점을 통해 서비스의 안정적인 응답 속도를 기대할 수 있게 되었다. 기존 단일 하드웨어 서버기반 모니터링 애플리케이션 서비스에서는 필요 이상의 리소스를 사용하거나 사용자의 요청에 비해 리소스가 부족하여 응답 속도가 저하되는 문제가 발생하였다. 본 논문에서는 이를 해결하기 위해 오픈 소스 가상화 플랫폼인 OpenBaton, OpenStack과 Docker를 통해 이미지에 대해 화재 및 연기 예측이 가능한 ResNet50 기반의 CNN 모델이 적용된 모니터링 애플리케이션을 구현하였다. 이를 통해 본 논문에서는 기존 단일 하드웨어 서버와 제안된 시스템의 응답 속도 비교를 통해 안정적인 응답 속도 보장 방안에 대해 연구하였다. With the recent introduction of a virtualized platform with virtualization technology, the benefits of increased resource utilization through partitioning of a single hardware resource and scalability through migration provide a reliable response rate for services. Traditional single hardware server-based monitoring application services have had problems with using more resources than needed or lack of resources compared to the user's request, resulting in slower response times. To address this, this paper implemented a monitoring application with a CNN model based on ResNet50 that enables fire and smoke prediction for images through open source virtualization platforms, OpenBaton, OpenStack and Docker. In this paper, we studied how to ensure a stable response rate through comparing the response speed of the existing single hardware server with the proposed system.\n",
      "위내시경 디지털 영상에서 정상과 위궤양 딥러닝 분류 모델 2019 ['위궤양', '내시경 영상', '딥러닝', '인공지능', 'Gastric Ulcer', 'Endoscopic Image', 'Deep Learning', 'Artificial Intelligent'] 내시경 장비의 발전으로 위장 질환의 조기 발견 및 치료에 많은 향상이 있다. 따라서, 많은 내시경 이미지과 함께 내시경 이미지 분류에 대한 연구가 활발히 증가하고 있다. 본 논문에서는 위내시경에서 많이 발견되는 질환인 위궤양을 분류하고자 한다. 위궤양은 초기에 적절한 치료를 하지 않으면 합병증을 일으킬 수 있으므로, 초기에 병변을 진단하는 것이 가장 중요하다. 본 연구에서는 ResNet-50 딥러닝 모델을 이용하여 정상과 위궤양을 분류하고자 하였다. 총 1,525개의 이미지를 이용하여 모델을 생성하였고, 효율적인 학습을 위해 데이터 증강을 적용하였다. 제안된 모델의 분류 성능은 정확도 0.9016, ROC 곡선은 0.83으로 확인하였다. Due to the development of endoscopic equipment, there has been much progress in the early detection and treatment of gastrointestinal diseases. Therefore, many endoscopic images have been provided, and studies on endoscopic image classification have been increased actively. In this paper, we aim to classify gastric ulcer, a disease frequently found in endoscopy.Gastric ulcers can lead to complications if not properly treated early on, so it is most important to diagnose the lesions early.Our study used the ResNet-50 in-depth learning model. A model was created using a total of 1,525 images, and data enhancement was applied for efficient learning. The classification performance of the proposed model is 0.9016 with accuracy and 0.83 with ROC curve.\n",
      "딥러닝 기반 포즈 변화에 강인한 귀 인식 연구 2019 ['Ear recognition', 'Convolutional neural networks', 'K-ear database', 'Ensemble'] None None\n",
      "Performance Comparison of the Optimizers in a Faster R-CNN Model for Object Detection of Metaphase Chromosomes 2019 None None In this paper, we compares the performance of the gredient descent optimizers of the Faster Region-based Convolutional Neural Network (R-CNN) model for the chromosome object detection in digital images composed of human metaphase chromosomes. In faster R-CNN, the gradient descent optimizer is used to minimize the objective function of the region proposal network (RPN) module and the classification score and bounding box regression blocks. The gradient descent optimizer. Through performance comparisons among these four gradient descent optimizers in our experiments, we found that the Adamax optimizer could achieve the mean average precision (mAP) of about 52% when considering faster R-CNN with a base network, VGG16. In case of faster R-CNN with a base network, ResNet50, the Adadelta optimizer could achieve the mAP of about 58%.\n",
      "Cody Recommendation System Using Deep Learning and User Preferences 2019 ['deep-learning', 'Fashion', 'Cody Recommendation', 'User Preferences'] None As AI technology is recently introduced into various fields, it is being applied to the fashion field. This paper proposes a system for recommending cody clothes suitable for a user's selected clothes. The proposed system consists of user app, cody recommendation module, and server interworking of each module and managing database data. Cody recommendation system classifies clothing images into 80 categories composed of feature combinations, selects multiple representative reference images for each category, and selects 3 full body cordy images for each representative reference image. Cody images of the representative reference image were determined by analyzing the user's preference using Google survey app. The proposed algorithm classifies categories the clothing image selected by the user into a category, recognizes the most similar image among the classification category reference images, and transmits the linked cody images to the user's app. The proposed system uses the ResNet-50 model to categorize the input image and measures similarity using ORB and HOG features to select a reference image in the category. We test the proposed algorithm in the Android app, and the result shows that the recommended system runs well.\n",
      "Cody Recommendation System Using Deep Learning and User Preferences 2019 ['deep-learning', 'Fashion', 'Cody Recommendation', 'User Preferences'] None As AI technology is recently introduced into various fields, it is being applied to the fashion field. This paper proposes a system for recommending cody clothes suitable for a user's selected clothes. The proposed system consists of user app, cody recommendation module, and server interworking of each module and managing database data. Cody recommendation system classifies clothing images into 80 categories composed of feature combinations, selects multiple representative reference images for each category, and selects 3 full body cordy images for each representative reference image. Cody images of the representative reference image were determined by analyzing the user's preference using Google survey app. The proposed algorithm classifies categories the clothing image selected by the user into a category, recognizes the most similar image among the classification category reference images, and transmits the linked cody images to the user's app. The proposed system uses the ResNet-50 model to categorize the input image and measures similarity using ORB and HOG features to select a reference image in the category. We test the proposed algorithm in the Android app, and the result shows that the recommended system runs well.\n",
      "Cody Recommendation System Using Deep Learning and User Preferences 2019 ['deep-learning', 'Fashion', 'Cody Recommendation', 'User Preferences'] None As AI technology is recently introduced into various fields, it is being applied to the fashion field. This paper proposes a system for recommending cody clothes suitable for a user's selected clothes. The proposed system consists of user app, cody recommendation module, and server interworking of each module and managing database data. Cody recommendation system classifies clothing images into 80 categories composed of feature combinations, selects multiple representative reference images for each category, and selects 3 full body cordy images for each representative reference image. Cody images of the representative reference image were determined by analyzing the user's preference using Google survey app. The proposed algorithm classifies categories the clothing image selected by the user into a category, recognizes the most similar image among the classification category reference images, and transmits the linked cody images to the user's app. The proposed system uses the ResNet-50 model to categorize the input image and measures similarity using ORB and HOG features to select a reference image in the category. We test the proposed algorithm in the Android app, and the result shows that the recommended system runs well.\n",
      "Training Data Sets Construction from Large Data Set for PCB Character Recognition 2019 ['PCB inspection', 'Optical character recognition', 'Deep learning', 'Data reduction', 'Sampling.'] None Deep learning has become increasingly popular in both academic and industrial areas nowadays. Various domains including pattern recognition, Computer vision have witnessed the great power of deep neural networks. However, current studies on deep learning mainly focus on quality data sets with balanced class labels, while training on bad and imbalanced data set have been providing great challenges for classification tasks. We propose in this paper a method of data analysis-based data reduction techniques for selecting good and diversity data samples from a large dataset for a deep learning model. Furthermore, data sampling techniques could be applied to decrease the large size of raw data by retrieving its useful knowledge as representatives. Therefore, instead of dealing with large size of raw data, we can use some data reduction techniques to sample data without losing important information. We group PCB characters in classes and train deep learning on the ResNet56 v2 and SENet model in order to improve the classification performance of optical character recognition (OCR) character classifier.\n",
      "A Comparative Study of Alzheimer’s Disease Classification using Multiple Transfer Learning Models 2019 ['Alzheimer’s disease', 'CNN', 'MR images', 'Transfer learning.'] None Over the past decade, researchers were able to solve complex medical problems as well as acquire deeper understanding of entire issue due to the availability of machine learning techniques, particularly predictive algorithms and automatic recognition of patterns in medical imaging. In this study, a technique called transfer learning has been utilized to classify Magnetic Resonance (MR) images by a pre-trained Convolutional Neural Network (CNN). Rather than training an entire model from scratch, transfer learning approach uses the CNN model by fine-tuning them, to classify MR images into Alzheimer’s disease (AD), mild cognitive impairment (MCI) and normal control (NC). The performance of this method has been evaluated over Alzheimer’s Disease Neuroimaging (ADNI) dataset by changing the learning rate of the model. Moreover, in this study, in order to demonstrate the transfer learning approach we utilize different pre-trained deep learning models such as GoogLeNet, VGG-16, AlexNet and ResNet-18, and compare their efficiency to classify AD. The overall classification accuracy resulted by GoogLeNet for training and testing was 99.84% and 98.25% respectively, which was exceptionally more than other models training and testing accuracies.\n",
      "Analyze weeds classification with visual explanation based on Convolutional Neural Networks 2019 ['Grad-CAM', 'CNN', 'visualization', 'Resnet'] None To understand how a Convolutional Neural Network (CNN) model captures the features of a pattern to determine which class it belongs to, in this paper, we use Gradient-weighted Class Activation Mapping (Grad-CAM) to visualize and analyze how well a CNN model behave on the CNU weeds dataset. We apply this technique to Resnet model and figure out which features this model captures to determine a specific class, what makes the model get a correct/wrong classification, and how those wrong label images can cause a negative effect to a CNN model during the training process. In the experiment, Grad-CAM highlights the important regions of weeds, depending on the patterns learned by Resnet, such as the lobe and limb on 미국가막사리, or the entire leaf surface on 단풍잎돼지풀. Besides, Grad-CAM points out a CNN model can localize the object even though it is trained only for the classification problem.\n",
      "Spatio-Temporal Residual Networks for Slide Transition Detection in Lecture Videos 2019 ['Lecture video', 'slide transition', '3D ConvNet', 'ResNet'] None In this paper, we present an approach for detecting slide transitions in lecture videos by introducing the spatio-temporal residual networks. Given a lecture video which records the digital slides, the speaker, and the audience by multiple cameras, our goal is to find keyframes where slide content changes. Since temporal dependency among video frames is important for detecting slide changes, 3D Convolutional Networks has been regarded as an efficient approach to learn the spatio-temporal features in videos. However, 3D ConvNet will cost much training time and need lots of memory. Hence, we utilize ResNet to ease the training of network, which is easy to optimize. Consequently, we present a novel ConvNet architecture based on 3D ConvNet and ResNet for slide transition detection in lecture videos. Experimental results show that the proposed novel ConvNet architecture achieves the better accuracy than other slide progression detection approaches.\n",
      "심층 신경회로망들을 사용한 기흉 진단 2019 ['Convolutional Neural Networks', 'U-Net', 'EfficientNet', 'ResNetM', 'ask-RCNN', 'Xecption'] 기흉은 가슴에 공기가 차는 것으로, 일반적으로 흉부 엑스레이를 사용하여 진단한다. 데이터 전처리를 하였고, 전처리방법으로는 결측값 제거 및 마스킹을 하였다. 최근에 와서 심층신경망의 성능이 개선됨에 따라서 활발히 사용되고 있고, 특히 영상인식에 적용되어 기존의 방법에 비하여 향상된 성능을 보이고 있다.기흉을 진단하는데 U-Net, Mask R-CNN, Resnet, EfficientNet, Xception을 사용하였으며, 이 심층 신경 회로망들의 성능을 비교하였다. U-Net은 Sementic Segmentation을사용하였고, Mask R-CNN은 Instance Segmentation을 사용하였다. Sementic Segmentation은 분할의 기본 단위를 클래스로하여, 동일한 클래스에 속하는 사물은 예측마스크 상에 동일한 색깔로 표시한다. Instance Segmentation은 분할의 기본단위를 사물로 하여, 동일한 클래스에 속하더 라도 다른 사물에 해당하면 예측 마스크 상에 다른 색깔로 표시한다. 실험 결과EfficientNet의 성능이 가장 좋았다. 이는 의사의 기흉 진단을 보조하기에 충분한 성능으로 의사를 도와 기흉을 진단하는데효율적으로 이용 될 수 있다. A Pneumothorax is an abnormal collection of air in the space between lung and chest wall and is diagnosed using chest X-ray. Preprocessings, such as the eliminaton of missing value and masking, were performed. Performances of U-Net, Mask R-CNN, Resnet, EfficientNet and Xception for diagnosing pneumothorax are compared. U-Net uses semantic segmentation and Mask R-CNN uses instance segmentation. Semantic segmentation uses a class as the unit of segmentation. Therefore, objects in the same class are denoted using the same color on the prediction mask. On the other hand, instance segmentation uses object as the unit of segmentation. Therefore, objects in the same class are denoted using the different color on the prediction mask, if they belong to the different objects. EfficientNet got the best result. It can be efficiently used because it performs well enough to assist for physicians to diagnose pneumothorax using the chest X-ray\n",
      "C++ 기반 범용 오픈소스 딥러닝 프레임워크 WICWIU 2019 ['딥러닝', '신경망', '프레임워크', '오픈소스', 'WICWIU', 'deep learning', 'neural networks', 'framework', 'open source'] 국내 대학으로는 최초로 공개한 오픈소스 딥러닝 프레임워크 WICWIU를 소개한다. WICWIU 는 다양한 연산자와 모듈, 그리고 일반적인 계산 그래프들을 표현할 수 있는 신경망 구조를 제공하여 Inception, ResNet, DenseNet 등 널리 사용되는 최신 딥러닝 모델들을 구성하기에 충분한 기능을 제공한다. 또한, GPU 기반 대규모 병렬 컴퓨팅을 지원해 빠른 학습이 가능하다. 모든 API가 C++로 제공되어 C++ 개발자들이 쉽게 적응할 수 있으며, C++환경에 기반하기 때문에 파이썬 기반의 프레임워크에 비해 메모리 및 성능 최적화에도 유리하다. 따라서, 프레임워크 자체를 자원이 제한된 환경에 맞도록 수정하기에도 용이하다. 일관성 높은 코드와 API로 구성되어 가독성과 확장성이 우수하며, 한국어 문서를 제공해 국내 개발자들이 쉽게 접근할 수 있다. WICWIU는 Apache 2.0 라이선스를 적용해 어떠한 연구 목적 및 상용 목적으로도 자유롭게 활용할 수 있다. In this paper, we introduce WICWIU, the first open source deep learning framework among Korean universities. WICWIU provides a variety of operators and modules together with a network structure that can represent an arbitrary general computational graph. The WICWIU features are sufficient to compose widely used deep learning models such as Inception, ResNet, and DenseNet.WICWIU also supports GPU-based massive parallel computing which significantly accelerates the training of neural networks. It is also easily accessible for C++ developers because the whole API is provided in C++. WICWIU has an advantage over Python-based frameworks in memory and performance optimization based on the C++ environment. This eases the customizability of WICWIU for environments with limited resources. WICWIU is readable and extensible because it is composed of C++ codes coupled with consistent APIs. With Korean documentation, it is particularly suitable for Korean developers. WICWIU applies the Apache 2.0 license which is available for any research or commercial purposes for free.\n",
      "합성곱 신경망(Convolutional Neural Network)을 활용한 지능형 유사상표 검색 모형 개발 2019 ['Deep Learning', 'Convolutional Neural Network', 'Trademark Retrieval System', 'Image retrieval Algorithm', '합성곱신경망', '딥 러닝', '상표 검색 시스템', '이미지 검색 알고리즘'] 전 세계적으로 온라인 상거래 시장 규모가 성장함에 따라 국제 및 국내 기업의 상표권이 침해되는 사례가 빈번하게 발생하고 있다. 다양한 연구 및 보고서에 따르면, 해외 기업 또는 개인이 국내 기업의 상표권을 침해한 사례와, 국내 기업 간 발생하는 상표권 분쟁 사례가 증가하고 있는 것으로 나타나고 있으며, 특허청의 보고서에 따르면 기업의 규모가 작을수록 상표보호를 위한 사전 예방활동을 수행하지 않는다고 응답한 비율이 높은 것으로 나타났다. 이러한 문제는 선등록 상표에 대한 사전조사 또는 자사의 상표보호를위해 소요되는 인력과 비용이 원인인 것으로 판단된다.한편, 국내에서 선등록상표에 대한 사전조사를 위해 상용되는 서비스를 살펴보면 상표 이미지를 활용한검색 서비스를 제공하고 있지 않은 상황이다. 이로 인해 국내 대다수의 기업은 자사의 상표 보호 및 선등록 상표에 대한 사전조사 수행 시 방대한 양의 선등록된 상표를 수작업으로 조사해야하는 문제가 발생한다.따라서 본 연구에서는 기업의 상표권 보호 및 선등록 상표에 대한 사전조사 수행 시 투입되는 인력 및비용절감과, 국내외에서 발생하고 있는 상표권 침해 문제를 해결하기 위해 합성곱 신경망 기법을 활용한지능형 유사 상표 검색 모델을 개발하고자 한다. 지적 재산권 전문가가 선정한 테스트 데이터를 활용하여지능형 유사 상표 검색 모델의 정확도를 측정한 결과 ResNet V1 101의 성능이 가장 높게 나타났다. 해당결과를 통해 이미지 분류 알고리즘이 단순한 사물 인식 분야뿐만 아니라 이미지 검색 분야에서도 높은 성능을 나타낸다는 것을 실증적으로 입증했으며, 본 연구는 실제 상표 이미지 데이터를 활용했다는 측면에서실제 산업 환경에서 활용성이 높을 것으로 사료된다. Recently, many companies improving their management performance by building a powerful brand value which is recognized for trademark rights. However, as growing up the size of online commerce market, the infringement of trademark rights is increasing. According to various studies and reports, cases of foreign and domestic companies infringing on their trademark rights are increased. As the manpower and the cost required for the protection of trademark are enormous, small and medium enterprises(SMEs) could not conduct preliminary investigations to protect their trademark rights.Besides, due to the trademark image search service does not exist, many domestic companies have a problem that investigating huge amounts of trademarks manually when conducting preliminary investigations to protect their rights of trademark.Therefore, we develop an intelligent similar trademark search model to reduce the manpower and cost for preliminary investigation. To measure the performance of the model which is developed in this study, test data selected by intellectual property experts was used, and the performance of ResNet V1 101 was the highest. The significance of this study is as follows. The experimental results empirically demonstrate that the image classification algorithm shows high performance not only object recognition but also image retrieval. Since the model that developed in this study was learned through actual trademark image data, it is expected that it can be applied in the real industrial environment.\n",
      "Fast R-CNN을 이용한 객체 인식 기반의 도로 노면 파손 탐지 기법 2019 ['도로 노면 파손', '심층 신경망', '유지보수', '영역 기반 합성곱', '객체 인식', 'Road surface damage', 'Deep neural network', 'Road maintenance', 'Region based convolutional neural networks', 'Object recognition'] None None\n",
      "심층신경망을 이용한 시간 영역 음향 이벤트 검출 알고리즘 2019 ['Sound Event Detection (SED)', 'Time-domain based DNN structure', 'ResGLU-SE', 'Data augmentation', 'pseudo-labeling'] 본 논문에서는 심층신경망을 이용한 시간 영역 음향 이벤트 검출 알고리즘을 제시한다. 본 시스템에서는 주파수 영역으로 변환되지 않은 시간 영역의 음향 데이터를 심층신경망의 입력으로 사용한다. 전반적인 구조는 CRNN 구조를 사용하였으며, GLU, ResNet, Squeeze- and-excitation 블럭을 적용하였다. 그리고 여러 계층에서 추출된 특징을 함께 고려하는 구조를 제안하였다. 또한 본 연구에서는 강한 라벨이 있는 훈련 데이터를 확보하는 것이 현실적으로 어렵다는 전제 아래에서 약한 라벨이 있는 훈련 데이터 약간 그리고 다수의 라벨이 없는 훈련 데이터를 활용하여 훈련을 수행하였다. 적은 수의 훈련 데이터를 효과적으로 사용하기 위해 타임 스트레칭, 피치 변화, 동적 영역 압축, 블럭 혼합 등의 데이터 증강 방법을 적용하였다. 라벨이 없는 데이터에는 의사 라벨을 붙여 부족한 훈련 데이터를 보완하였다. 본 논문에서 제안한 신경망과 데이터 증강 방법을 사용하는 경우, 종래의 방식으로 CRNN 구조의 신경망을 훈련하여 사용하는 경우보다, 음향 이벤트 검출 성능이 약 6 % (f-score 기준)가 개선되었다. None\n",
      "The development of food image detection and recognition model of Korean food for mobile dietary management 2019 ['Food recognition', 'deep convolutional neural networks (DCNN)', 'mobile device', 'dietary assessment'] None BACKGROUND/OBJECTIVES: The aim of this study was to develop Korean food image detection and recognition model for use in mobile devices for accurate estimation of dietary intake.MATERIALS/METHODS: We collected food images by taking pictures or by searching web images and built an image dataset for use in training a complex recognition model for Korean food. Augmentation techniques were performed in order to increase the dataset size. The dataset for training contained more than 92,000 images categorized into 23 groups of Korean food.All images were down-sampled to a fixed resolution of 150 × 150 and then randomly divided into training and testing groups at a ratio of 3:1, resulting in 69,000 training images and 23,000 test images. We used a Deep Convolutional Neural Network (DCNN) for the complex recognition model and compared the results with those of other networks: AlexNet, GoogLeNet, Very Deep Convolutional Neural Network, VGG and ResNet, for large-scale image recognition.RESULTS: Our complex food recognition model, K-foodNet, had higher test accuracy (91.3%) and faster recognition time (0.4 ms) than those of the other networks.CONCLUSION: The results showed that K-foodNet achieved better performance in detecting and recognizing Korean food compared to other state-of-the-art models.\n",
      "The development of food image detection and recognition model of Korean food for mobile dietary management 2019 ['Food recognition', 'deep convolutional neural networks (DCNN)', 'mobile device', 'dietary assessment'] None BACKGROUND/OBJECTIVES: The aim of this study was to develop Korean food image detection and recognition model for use in mobile devices for accurate estimation of dietary intake. MATERIALS/METHODS: We collected food images by taking pictures or by searching web images and built an image dataset for use in training a complex recognition model for Korean food. Augmentation techniques were performed in order to increase the dataset size. The dataset for training contained more than 92,000 images categorized into 23 groups of Korean food. All images were down-sampled to a fixed resolution of $150{\\times}150$ and then randomly divided into training and testing groups at a ratio of 3:1, resulting in 69,000 training images and 23,000 test images. We used a Deep Convolutional Neural Network (DCNN) for the complex recognition model and compared the results with those of other networks: AlexNet, GoogLeNet, Very Deep Convolutional Neural Network, VGG and ResNet, for large-scale image recognition. RESULTS: Our complex food recognition model, K-foodNet, had higher test accuracy (91.3%) and faster recognition time (0.4 ms) than those of the other networks. CONCLUSION: The results showed that K-foodNet achieved better performance in detecting and recognizing Korean food compared to other state-of-the-art models.\n",
      "자율주행 자동차 환경에서의 3D-LiDAR 와 딥러닝을 이용한 클러스터링 후보군 기반 실시간 객체 검출 2019 ['autonomous vehicle', '3d-LiDAR', 'clustering', 'deep learning', 'object detection'] None Recently, IT companies such as Google, NVIDIA, and NAVER have been also developing autonomous vehicle platform technologies. In particular, sensors for object detection in surrounding environments have been improved in recognition rates by applying multi-sensor systems using camera, LiDAR, and radar. With the increasing importance of recognition technology, 3D information-based recognition technologies have been actively advanced as a commercial product of 3D-LiDAR. In this paper, a candidate group of point-clouds from 3D-LiDAR is extracted using Euclidean clustering in order to reduce the processing time delay in RPN (Region Proposal Network), which is one of the basic schemes for existing object detection. Then, it proposes types of input slicing, based on the extracted candidates. In addition, the accuracy and the processing time using four CNN networks (Basic CNN, ResNet, VGG16, and MobileNet) are compared over not only the private data (CVLab dataset) obtained in actual road environment but also the publicly open KITTI dataset.\n",
      "자율운항선박의 국제해상충돌예방규칙 준수를 위한 합성곱 신경망 기반의 선박 분류에 관한 연구 2019 ['자율운항선박', '선박 분류', '국제해상충돌예방규칙', '합성곱 신경망', 'Autonomous Ships', 'Vessel Classification', 'COLREGs', 'Convolutional Neural Networks'] 최근 자율운항선박에 대한 관심이 증가하고 있으며, 바다를 항해하는 자율운항선박은 유인선과 같이 국제해상충돌방지규칙을 준수해야한다. 따라서 본 논문에서는 자율운항선박이 국제해상충돌예방규칙을 준수하기 위해서 필요한 선박 범주 및 합성곱 신경망 기반의 선박 분류 기술을 제안하였다. 먼저 국제해상충돌예방규칙을 분석하여 자율운항선박이 구별해야 되는 14개의 선박 범주를 정의하였다. 또한 본 논문에서 정의된 선박 범주에 맞도록 인터넷 영상검색 및 기존 데이터 셋 정제를 통하여 40,300장 규모의 선박 범주 분류 데이터 셋을 구축하였다. 마지막으로 최신 합성곱 신경망 모델을 구축된 선박 범주 분류 데이터 셋에 적용하여 선박 범주 분류 성능을 분석하였다. 실험결과 전이학습을 통하여 학습된 Inception-ResNet v2 모델은 14개 선박 범주를 91%의 높은 정확도로 분류함을 확인하였다. The interest in autonomous ships for marine industries has increased significantly over the past few years and autonomous ships also must follow maritime laws in the same way as regular ships operated by crews. Therefore, in this paper, we propose the vessel taxonomy for COLREGs compliance of autonomous ships and evaluate the performance of the vessel classification method using CNNs. First, we define the vessel taxonomy for complying with maritime laws by analyzing the COLREGs. And then, we build our dataset separated manually by the vessel taxonomy. For the dataset, 40,300 images are collected by image search on websites and refining the publicly available dataset. Finally, the state-of-the-art CNN model is applied to evaluate the recognition rate of our dataset. The experimental results show that the Inception-ResNet v2 model which is trained by transfer learning effectively classifies the ships with a high accuracy of 91%.\n",
      "2019년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                                                        title  \\\n",
      "0                                                                                       ResNet 모델을 이용한 눈 주변 영역의 특징 추출 및 개인 인증   \n",
      "1                  Multi-parametric MRIs based assessment of Hepatocellular Carcinoma Differentiation with Multi-scale ResNet   \n",
      "2                                                                                             ResNet을 이용한 얼굴 인식 기반 출입관리시스템 개발   \n",
      "3                                                                                                  한국어 음성 명령어 인식을 위한 자동데이터 구축   \n",
      "4                                        Layer‐wise hint‐based training for knowledge transfer in a teacher‐student framework   \n",
      "5                                                                                  앙상블 학습 알고리즘을 이용한 컨벌루션 신경망의 분류 성능 분석에 관한 연구   \n",
      "6   CNN Model Performance Analysis on MRI Images of an OASIS Dataset for Distinction Between Healthy and Alzheimer’s Patients   \n",
      "7                                                                                           영상기반 콘크리트 균열 탐지 딥러닝 모델의 유형별 성능 비교   \n",
      "8                             A Novel Integrated Convolutional Neural Network via Deep Transfer Learning in Colorectal Images   \n",
      "9                                                                                               말벌 영상인식을 위한 심층 합성곱 신경망의 성능 평가   \n",
      "10                                                                     Sentinel-1 A/B 위성 SAR 자료와 딥러닝 모델을 이용한 여름철 북극해 해빙 분류 연구   \n",
      "11                                                                         가상화 플랫폼을 통한 CNN기반 모니터링 애플리케이션의 안정적인 응답 속도 보장 방안 연구   \n",
      "12                                                                                            위내시경 디지털 영상에서 정상과 위궤양 딥러닝 분류 모델   \n",
      "13                                                                                                  딥러닝 기반 포즈 변화에 강인한 귀 인식 연구   \n",
      "14             Performance Comparison of the Optimizers in a Faster R-CNN Model for Object Detection of Metaphase Chromosomes   \n",
      "15                                                        Cody Recommendation System Using Deep Learning and User Preferences   \n",
      "16                                                        Cody Recommendation System Using Deep Learning and User Preferences   \n",
      "17                                                        Cody Recommendation System Using Deep Learning and User Preferences   \n",
      "18                                          Training Data Sets Construction from Large Data Set for PCB Character Recognition   \n",
      "19                          A Comparative Study of Alzheimer’s Disease Classification using Multiple Transfer Learning Models   \n",
      "20                                Analyze weeds classification with visual explanation based on Convolutional Neural Networks   \n",
      "21                                         Spatio-Temporal Residual Networks for Slide Transition Detection in Lecture Videos   \n",
      "22                                                                                                       심층 신경회로망들을 사용한 기흉 진단   \n",
      "23                                                                                            C++ 기반 범용 오픈소스 딥러닝 프레임워크 WICWIU   \n",
      "24                                                               합성곱 신경망(Convolutional Neural Network)을 활용한 지능형 유사상표 검색 모형 개발   \n",
      "25                                                                                   Fast R-CNN을 이용한 객체 인식 기반의 도로 노면 파손 탐지 기법   \n",
      "26                                                                                            심층신경망을 이용한 시간 영역 음향 이벤트 검출 알고리즘   \n",
      "27                 The development of food image detection and recognition model of Korean food for mobile dietary management   \n",
      "28                 The development of food image detection and recognition model of Korean food for mobile dietary management   \n",
      "29                                                                  자율주행 자동차 환경에서의 3D-LiDAR 와 딥러닝을 이용한 클러스터링 후보군 기반 실시간 객체 검출   \n",
      "30                                                                         자율운항선박의 국제해상충돌예방규칙 준수를 위한 합성곱 신경망 기반의 선박 분류에 관한 연구   \n",
      "\n",
      "    date  \\\n",
      "0   2019   \n",
      "1   2019   \n",
      "2   2019   \n",
      "3   2019   \n",
      "4   2019   \n",
      "5   2019   \n",
      "6   2019   \n",
      "7   2019   \n",
      "8   2019   \n",
      "9   2019   \n",
      "10  2019   \n",
      "11  2019   \n",
      "12  2019   \n",
      "13  2019   \n",
      "14  2019   \n",
      "15  2019   \n",
      "16  2019   \n",
      "17  2019   \n",
      "18  2019   \n",
      "19  2019   \n",
      "20  2019   \n",
      "21  2019   \n",
      "22  2019   \n",
      "23  2019   \n",
      "24  2019   \n",
      "25  2019   \n",
      "26  2019   \n",
      "27  2019   \n",
      "28  2019   \n",
      "29  2019   \n",
      "30  2019   \n",
      "\n",
      "                                                                                                                                                                  keywords  \\\n",
      "0                                                                                                                            [Periocular Region, Authentication, CNN, MLP]   \n",
      "1                                                          [Multi-parametric MRI, data fusion, transfer learning, deep learning, hepatocellular carcinoma differentiation]   \n",
      "2                                                                                                                [Face Recognition, Access Control, Deep Learning, ResNet]   \n",
      "3                                          [Korean Speech Command, Speech Recognition, Automatic Data Construction, ResNet, CNN, 한국어 명령어 인식, 음성인식, 자동 데이터 구축, 레스넷, 합성곱신경망]   \n",
      "4                                                                             [knowledge transfer, layer‐wise hint training, residual networks, teacher‐student framework]   \n",
      "5                                                                                                       [Deep Learning, Computer Vision, CNN, Ensemble Learning Algorithm]   \n",
      "6                                                                                                                    [Medical MRI, CNN, AlexNet, GoogLeNet, ResNet50, CAD]   \n",
      "7                                                   [crack detection, deep learning, image classification, object detection, semantic segmentation, instance segmentation]   \n",
      "8                                                                                         [Colon Disease Classification, Convolutional Neural Networks, Transfer Learning]   \n",
      "9                                                                                        [Vespa hornets, Deep convolutional neural network, Deep learning, Classification]   \n",
      "10                                                                                                 [Sentinel-1 A/B, sea ice, thermal noise, Deep Learning, classification]   \n",
      "11                                [Virtualized Platform, Cloud Computing, OpenStack, Docker, IaaS, CNN, Monitoring, 가상화 플랫폼, 클라우드 컴퓨팅, OpenStack, Docker, IaaS, CNN, 모니터링]   \n",
      "12                                                                        [위궤양, 내시경 영상, 딥러닝, 인공지능, Gastric Ulcer, Endoscopic Image, Deep Learning, Artificial Intelligent]   \n",
      "13                                                                                              [Ear recognition, Convolutional neural networks, K-ear database, Ensemble]   \n",
      "14                                                                                                                                                                      []   \n",
      "15                                                                                                         [deep-learning, Fashion, Cody Recommendation, User Preferences]   \n",
      "16                                                                                                         [deep-learning, Fashion, Cody Recommendation, User Preferences]   \n",
      "17                                                                                                         [deep-learning, Fashion, Cody Recommendation, User Preferences]   \n",
      "18                                                                               [PCB inspection, Optical character recognition, Deep learning, Data reduction, Sampling.]   \n",
      "19                                                                                                               [Alzheimer’s disease, CNN, MR images, Transfer learning.]   \n",
      "20                                                                                                                                  [Grad-CAM, CNN, visualization, Resnet]   \n",
      "21                                                                                                                   [Lecture video, slide transition, 3D ConvNet, ResNet]   \n",
      "22                                                                                       [Convolutional Neural Networks, U-Net, EfficientNet, ResNetM, ask-RCNN, Xecption]   \n",
      "23                                                                                 [딥러닝, 신경망, 프레임워크, 오픈소스, WICWIU, deep learning, neural networks, framework, open source]   \n",
      "24                              [Deep Learning, Convolutional Neural Network, Trademark Retrieval System, Image retrieval Algorithm, 합성곱신경망, 딥 러닝, 상표 검색 시스템, 이미지 검색 알고리즘]   \n",
      "25  [도로 노면 파손, 심층 신경망, 유지보수, 영역 기반 합성곱, 객체 인식, Road surface damage, Deep neural network, Road maintenance, Region based convolutional neural networks, Object recognition]   \n",
      "26                                                           [Sound Event Detection (SED), Time-domain based DNN structure, ResGLU-SE, Data augmentation, pseudo-labeling]   \n",
      "27                                                                        [Food recognition, deep convolutional neural networks (DCNN), mobile device, dietary assessment]   \n",
      "28                                                                        [Food recognition, deep convolutional neural networks (DCNN), mobile device, dietary assessment]   \n",
      "29                                                                                             [autonomous vehicle, 3d-LiDAR, clustering, deep learning, object detection]   \n",
      "30                                                   [자율운항선박, 선박 분류, 국제해상충돌예방규칙, 합성곱 신경망, Autonomous Ships, Vessel Classification, COLREGs, Convolutional Neural Networks]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               abstract  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                최근 화두가 되고 있는 AI분야에서 가장 큰 문제점은 학습데이터의 부족 문제를 꼽을 수 있다. 수동 데이터 구축에는 많은 시간과 노력이 소요되기에 개인이 손쉽게 필요 데이터를 구축하기는 매우 어렵다. 반면, 수동 데이터 구축에 비해 자동으로 구축하는 것은 높은 품질을 유지하는 것이 관건이다. 본 논문에서는 한국어 음성 명령어 인식기 개발에 필요한 데이터를 웹에서 자동으로 추출하고, 학습데이터로 사용할 수 있는 데이터를 자동으로 선별하는 방법을 소개한다. 특히, 자동 구축된 한국어 음성 데이터를 대상으로 우수한 성능을 보이는 ResNet기반의 수정 모델을 기반으로, 건강 및 일상생활도메인의 명령어 셋을 대상으로 적용가능성을 보이기 위한 실험을 진행하였다. 자동으로 구축된 데이터만을 사용한 일련의 실험에서 건강도메인은 ResNet15에서 89.5%, 일상생활도메인에서는 ResNet8에서 82%의 정확도를 보임으로써, 자동 수집 데이터의 활용 가능성을 검증하였다.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None   \n",
      "10  북극항로의 개척 가능성과 정확한 기후 예측 모델의 필요성에 의해 북극해 고해상도 해빙 지도의 중요성이 증가하고 있다. 그러나 기존의 북극 해빙 지도는 제작에 사용된 위성 영상 취득 센서의 특성에 따른 데이터의 취득과 공간해상도 등에서 그 활용도가 제한된다. 본 연구에서는 Sentinel-1 A/B SAR 위성자료로부터 고해상도 해빙 지도를 생성하기 위한 딥러닝 기반의 해빙 분류 알고리즘을 연구하였다. 북극해 Ice Chart를 기반으로 전문가 판독에 의해 Open Water, First Year Ice, Multi Year Ice의 세 클래스로 구성된 훈련자료를 구축하였으며, Convolutional Neural Network 기반의 두 가지 딥러닝 모델(Simple CNN, Resnet50)과 입사각 및 thermal noise가 보정된 HV 밴드를 포함하는 다섯 가지 입력 밴드 조합을 이용하여 총 10가지 케이스의 해빙 분류를 실시하였다. 이 케이스들에 대하여 Ground Truth Point를 사용하여 정확도를 비교하고, 가장 높은 정확도가 나온케이스에 대해 confusion matrix 및 Cohen의 kappa 분석을 실시하였다. 또한 전통적으로 분류를 위해 많이 활용되어 온 Maximum Likelihood Classifier 기법을 이용한 분류결과에 대해서도 같은 비교를 하였다. 그 결과Convolution 층 2개, Max Pooling 층 2개를 가진 구조의 Convolutional Neural Network에 [HV, 입사각] 밴드를 넣은 딥러닝 알고리즘의 분류 결과가 96.66%의 가장 높은 분류 정확도를 보였으며, Cohen의 kappa 계수는 0.9499 로 나타나 딥러닝에 의한 해빙 분류는 비교적 높은 분류 결과를 보였다. 또한 모든 딥러닝 케이스는 Maximum Likelihood Classifier 기법에 비해 높은 분류 정확도를 보였다.   \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              최근 가상화 기술이 적용된 가상화 플랫폼(Virtualized Platform)을 도입하게 되면서 단일 하드웨어 리소스의 파티셔닝을 통한 리소스 활용률 상승과 마이그레이션을 통한 확장성의 이점을 통해 서비스의 안정적인 응답 속도를 기대할 수 있게 되었다. 기존 단일 하드웨어 서버기반 모니터링 애플리케이션 서비스에서는 필요 이상의 리소스를 사용하거나 사용자의 요청에 비해 리소스가 부족하여 응답 속도가 저하되는 문제가 발생하였다. 본 논문에서는 이를 해결하기 위해 오픈 소스 가상화 플랫폼인 OpenBaton, OpenStack과 Docker를 통해 이미지에 대해 화재 및 연기 예측이 가능한 ResNet50 기반의 CNN 모델이 적용된 모니터링 애플리케이션을 구현하였다. 이를 통해 본 논문에서는 기존 단일 하드웨어 서버와 제안된 시스템의 응답 속도 비교를 통해 안정적인 응답 속도 보장 방안에 대해 연구하였다.   \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             내시경 장비의 발전으로 위장 질환의 조기 발견 및 치료에 많은 향상이 있다. 따라서, 많은 내시경 이미지과 함께 내시경 이미지 분류에 대한 연구가 활발히 증가하고 있다. 본 논문에서는 위내시경에서 많이 발견되는 질환인 위궤양을 분류하고자 한다. 위궤양은 초기에 적절한 치료를 하지 않으면 합병증을 일으킬 수 있으므로, 초기에 병변을 진단하는 것이 가장 중요하다. 본 연구에서는 ResNet-50 딥러닝 모델을 이용하여 정상과 위궤양을 분류하고자 하였다. 총 1,525개의 이미지를 이용하여 모델을 생성하였고, 효율적인 학습을 위해 데이터 증강을 적용하였다. 제안된 모델의 분류 성능은 정확도 0.9016, ROC 곡선은 0.83으로 확인하였다.   \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "22                                                                                                                                                                                                                                                                                                                                                      기흉은 가슴에 공기가 차는 것으로, 일반적으로 흉부 엑스레이를 사용하여 진단한다. 데이터 전처리를 하였고, 전처리방법으로는 결측값 제거 및 마스킹을 하였다. 최근에 와서 심층신경망의 성능이 개선됨에 따라서 활발히 사용되고 있고, 특히 영상인식에 적용되어 기존의 방법에 비하여 향상된 성능을 보이고 있다.기흉을 진단하는데 U-Net, Mask R-CNN, Resnet, EfficientNet, Xception을 사용하였으며, 이 심층 신경 회로망들의 성능을 비교하였다. U-Net은 Sementic Segmentation을사용하였고, Mask R-CNN은 Instance Segmentation을 사용하였다. Sementic Segmentation은 분할의 기본 단위를 클래스로하여, 동일한 클래스에 속하는 사물은 예측마스크 상에 동일한 색깔로 표시한다. Instance Segmentation은 분할의 기본단위를 사물로 하여, 동일한 클래스에 속하더 라도 다른 사물에 해당하면 예측 마스크 상에 다른 색깔로 표시한다. 실험 결과EfficientNet의 성능이 가장 좋았다. 이는 의사의 기흉 진단을 보조하기에 충분한 성능으로 의사를 도와 기흉을 진단하는데효율적으로 이용 될 수 있다.   \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                       국내 대학으로는 최초로 공개한 오픈소스 딥러닝 프레임워크 WICWIU를 소개한다. WICWIU 는 다양한 연산자와 모듈, 그리고 일반적인 계산 그래프들을 표현할 수 있는 신경망 구조를 제공하여 Inception, ResNet, DenseNet 등 널리 사용되는 최신 딥러닝 모델들을 구성하기에 충분한 기능을 제공한다. 또한, GPU 기반 대규모 병렬 컴퓨팅을 지원해 빠른 학습이 가능하다. 모든 API가 C++로 제공되어 C++ 개발자들이 쉽게 적응할 수 있으며, C++환경에 기반하기 때문에 파이썬 기반의 프레임워크에 비해 메모리 및 성능 최적화에도 유리하다. 따라서, 프레임워크 자체를 자원이 제한된 환경에 맞도록 수정하기에도 용이하다. 일관성 높은 코드와 API로 구성되어 가독성과 확장성이 우수하며, 한국어 문서를 제공해 국내 개발자들이 쉽게 접근할 수 있다. WICWIU는 Apache 2.0 라이선스를 적용해 어떠한 연구 목적 및 상용 목적으로도 자유롭게 활용할 수 있다.   \n",
      "24                                                                                                      전 세계적으로 온라인 상거래 시장 규모가 성장함에 따라 국제 및 국내 기업의 상표권이 침해되는 사례가 빈번하게 발생하고 있다. 다양한 연구 및 보고서에 따르면, 해외 기업 또는 개인이 국내 기업의 상표권을 침해한 사례와, 국내 기업 간 발생하는 상표권 분쟁 사례가 증가하고 있는 것으로 나타나고 있으며, 특허청의 보고서에 따르면 기업의 규모가 작을수록 상표보호를 위한 사전 예방활동을 수행하지 않는다고 응답한 비율이 높은 것으로 나타났다. 이러한 문제는 선등록 상표에 대한 사전조사 또는 자사의 상표보호를위해 소요되는 인력과 비용이 원인인 것으로 판단된다.한편, 국내에서 선등록상표에 대한 사전조사를 위해 상용되는 서비스를 살펴보면 상표 이미지를 활용한검색 서비스를 제공하고 있지 않은 상황이다. 이로 인해 국내 대다수의 기업은 자사의 상표 보호 및 선등록 상표에 대한 사전조사 수행 시 방대한 양의 선등록된 상표를 수작업으로 조사해야하는 문제가 발생한다.따라서 본 연구에서는 기업의 상표권 보호 및 선등록 상표에 대한 사전조사 수행 시 투입되는 인력 및비용절감과, 국내외에서 발생하고 있는 상표권 침해 문제를 해결하기 위해 합성곱 신경망 기법을 활용한지능형 유사 상표 검색 모델을 개발하고자 한다. 지적 재산권 전문가가 선정한 테스트 데이터를 활용하여지능형 유사 상표 검색 모델의 정확도를 측정한 결과 ResNet V1 101의 성능이 가장 높게 나타났다. 해당결과를 통해 이미지 분류 알고리즘이 단순한 사물 인식 분야뿐만 아니라 이미지 검색 분야에서도 높은 성능을 나타낸다는 것을 실증적으로 입증했으며, 본 연구는 실제 상표 이미지 데이터를 활용했다는 측면에서실제 산업 환경에서 활용성이 높을 것으로 사료된다.   \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                         본 논문에서는 심층신경망을 이용한 시간 영역 음향 이벤트 검출 알고리즘을 제시한다. 본 시스템에서는 주파수 영역으로 변환되지 않은 시간 영역의 음향 데이터를 심층신경망의 입력으로 사용한다. 전반적인 구조는 CRNN 구조를 사용하였으며, GLU, ResNet, Squeeze- and-excitation 블럭을 적용하였다. 그리고 여러 계층에서 추출된 특징을 함께 고려하는 구조를 제안하였다. 또한 본 연구에서는 강한 라벨이 있는 훈련 데이터를 확보하는 것이 현실적으로 어렵다는 전제 아래에서 약한 라벨이 있는 훈련 데이터 약간 그리고 다수의 라벨이 없는 훈련 데이터를 활용하여 훈련을 수행하였다. 적은 수의 훈련 데이터를 효과적으로 사용하기 위해 타임 스트레칭, 피치 변화, 동적 영역 압축, 블럭 혼합 등의 데이터 증강 방법을 적용하였다. 라벨이 없는 데이터에는 의사 라벨을 붙여 부족한 훈련 데이터를 보완하였다. 본 논문에서 제안한 신경망과 데이터 증강 방법을 사용하는 경우, 종래의 방식으로 CRNN 구조의 신경망을 훈련하여 사용하는 경우보다, 음향 이벤트 검출 성능이 약 6 % (f-score 기준)가 개선되었다.   \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None   \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           최근 자율운항선박에 대한 관심이 증가하고 있으며, 바다를 항해하는 자율운항선박은 유인선과 같이 국제해상충돌방지규칙을 준수해야한다. 따라서 본 논문에서는 자율운항선박이 국제해상충돌예방규칙을 준수하기 위해서 필요한 선박 범주 및 합성곱 신경망 기반의 선박 분류 기술을 제안하였다. 먼저 국제해상충돌예방규칙을 분석하여 자율운항선박이 구별해야 되는 14개의 선박 범주를 정의하였다. 또한 본 논문에서 정의된 선박 범주에 맞도록 인터넷 영상검색 및 기존 데이터 셋 정제를 통하여 40,300장 규모의 선박 범주 분류 데이터 셋을 구축하였다. 마지막으로 최신 합성곱 신경망 모델을 구축된 선박 범주 분류 데이터 셋에 적용하여 선박 범주 분류 성능을 분석하였다. 실험결과 전이학습을 통하여 학습된 Inception-ResNet v2 모델은 14개 선박 범주를 91%의 높은 정확도로 분류함을 확인하였다.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Deep learning approach based on convolution neural network (CNN)  has extensively studied in the field of computer vision. However, periocular feature extraction using CNN was not well studied because it is practically impossible to collect large volume of biometric data. This study uses the ResNet model which was trained with the ImageNet dataset. To overcome the problem of insufficient training data, we focused on the training of multi-layer perception (MLP) having simple structure rather than training the CNN having complex structure. It first extracts features using the pretrained ResNet model and reduces the feature dimension by principle component analysis (PCA), then trains a MLP classifier. Experimental results with the public periocular dataset UBIPr show that the proposed method is effective in person authentication using periocular region. Especially it has the advantage which can be directly applied for other biometric traits.  \n",
      "1                                                                                                         To explore an effective non-invasion medical imaging diagnostics approach for hepatocellular carcinoma (HCC), we propose a method based on adopting the multiple technologies with the multi-parametric data fusion, transfer learning, and multi-scale deep feature extraction. Firstly, to make full use of complementary and enhancing the contribution of different modalities viz. multi-parametric MRI images in the lesion diagnosis, we propose a data-level fusion strategy. Secondly, based on the fusion data as the input, the multi-scale residual neural network with SPP (Spatial Pyramid Pooling) is utilized for the discriminative feature representation learning. Thirdly, to mitigate the impact of the lack of training samples, we do the pre-training of the proposed multi-scale residual neural network model on the natural image dataset and the fine-tuning with the chosen multi-parametric MRI images as complementary data. The comparative experiment results on the dataset from the clinical cases show that our proposed approach by employing the multiple strategies achieves the highest accuracy of 0.847±0.023 in the classification problem on the HCC differentiation. In the problem of discriminating the HCC lesion from the non-tumor area, we achieve a good performance with accuracy, sensitivity, specificity and AUC (area under the ROC curve) being 0.981±0.002, 0.981±0.002, 0.991±0.007 and 0.999±0.0008, respectively.  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The biggest problem in the AI field, which has become a hot topic in recent years, is how to deal with the lack of training data. Since manual data construction takes a lot of time and efforts, it is non-trivial for an individual to easily build the necessary data. On the other hand, automatic data construction needs to handle data quality issue. In this paper, we introduce a method to automatically extract the data required to develop Korean speech command recognizer from the web and to automatically select the data that can be used for training data. In particular, we propose a modified ResNet model that shows modest performance for the automatically constructed Korean speech command data. We conducted an experiment to show the applicability of the command set of the health and daily life domain. In a series of experiments using only automatically constructed data, the accuracy of the health domain was 89.5% in ResNet15 and 82% in ResNet8 in the daily lives domain, respectively.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                        We devise a layer‐wise hint training method to improve the existing hint‐based knowledge distillation (KD) training approach, which is employed for knowledge transfer in a teacher‐student framework using a residual network (ResNet). To achieve this objective, the proposed method first iteratively trains the student ResNet and incrementally employs hint‐based information extracted from the pretrained teacher ResNet containing several hint and guided layers. Next, typical softening factor‐based KD training is performed using the previously estimated hint‐based information. We compare the recognition accuracy of the proposed approach with that of KD training without hints, hint‐based KD training, and ResNet‐based layer‐wise pretraining using reliable datasets, including CIFAR‐10, CIFAR‐100, and MNIST. When using the selected multiple hint‐based information items and their layer‐wise transfer in the proposed method, the trained student ResNet more accurately reflects the pretrained teacher ResNet's rich information than the baseline training methods, for all the benchmark datasets we consider in this study.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 In this paper, we compare and analyze the classification performance of deep learning algorithm Convolutional Neural Network(CNN) ac cording to ensemble generation and combining techniques. We used several CNN models(VGG16, VGG19, DenseNet121, DenseNet169, DenseNet201, ResNet18, ResNet34, ResNet50, ResNet101, ResNet152, GoogLeNet) to create 10 ensemble generation combinations and applied 6 combine techniques(average, weighted average, maximum, minimum, median, product) to the optimal combination. Experimental results, DenseNet169-VGG16-GoogLeNet combination in ensemble generation, and the product rule in ensemble combination showed the best performance. Based on this, it was concluded that ensemble in different models of high benchmarking scores is another way to get good results.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In this paper, we present the performance of a medical image classification model pretrained on natural images. In addition, another model is scratch trained from available medical magnetic resonance images in order to get a comparative analysis. We perform shallow tuning and fine-tuning of the pretrained model (AlexNet, GoogLeNet, and ResNet50) in a bunch of layers in order to find the impact of each section of layers in the classification result. We use 28 normal controls (NC) and 28 Alzheimer’s disease (AD) patients for classification, selecting 30 important slices from each patient. Once all the slices were collected, each model was trained, validated, and tested at a ratio of 6:2:2 on a random selection basis. The testing results are reported and analyzed so the final CNN model could be built with a minimal number of layers for optimal performance.  \n",
      "7                                                                                                                                                                                        In this study, various types of deep learning models that have been proposed recently are classified according to data input / output types and analyzed to find the deep learning model suitable for constructing a crack detection model. First the deep learning models are classified into image classification model, object segmentation model, object detection model, and instance segmentation model. ResNet-101, DeepLab V2, Faster R-CNN, and Mask R-CNN were selected as representative deep learning model of each type. For the comparison, ResNet-101 was implemented for all the types of deep learning model as a backbone network which serves as a main feature extractor. The four types of deep learning models were trained with 500 crack images taken from real concrete structures and collected from the Internet. The four types of deep learning models showed high accuracy above 94% during the training.Comparative evaluation was conducted using 40 images taken from real concrete structures. The performance of each type of deep learning model was measured using precision and recall. In the experimental result, Mask R-CNN, an instance segmentation deep learning model showed the highest precision and recall on crack detection.Qualitative analysis also shows that Mask R-CNN could detect crack shapes most similarly to the real crack shapes.  \n",
      "8                                                                                                                                                                                                                                                                                                                 In this paper, we explore the use of current deep learning methods, convolutional neural networks (CNNs) in the field of computer-aided diagnosis systems to classify several endoscopic colon diseases. Transfer learning by fine-tuning deep convolutional neural networks (CNNs) is applied due to the limited amount of data. For this, state-of-the-art CNN architectures, such as VGG16, VGG19, InceptionV3, ResNet50, Inception-ResNet-V2, DenseNet169 were used for training and validating the dataset. However, these existing architectures cannot extract more dense endoscopic image features and have problem on similar-looking images of different category. Therefore, we propose a novel integrated convolutional neural network to develop a more accurate and highly efficient method for endoscopic image classification, which uses the features of earlier layers in the classification process and increases the receptive field of view at the end layers in the network. We compare and evaluate our performance using performance metrics Accuracy (ACC), Recall, Precision and F1-score. In our experimental results, the proposed method outperforms the existing architectures, obtaining an accuracy of about 92.4% on the test dataset.  \n",
      "9                                                                                                                                                                                                                                                                                                                                            One of the serious factors for honeybee decline is due to the various attacks from Vespa hornets, indigenous and invaded. Population monitoring as well as the alerting systems is requested against the Vespa. Automated image recognition is the primary step for the unmanned autonomous monitoring system development. This study compared the recent deep convolutional neural network (DCNN) algorithms such as AlexNet, VGG19, GoogLeNet, and ResNet50 for the best model selection for classification of 3 Vespa species, V. mandarinia, V. crabro and V. velutina. To evaluate classification performance, accuracy was utilized after transfer learning on each DCNN. As a result, the ResNet50 showed the best in terms of accuracy after sufficient training of 100 epochs. If performance and speed are considered simultaneously, AlexNet could be the alternative. The real-time monitoring system for objects requires both localization and classification. And Vespa occurrence or population change would need rapid recognition for the objects. Therefore speedy image recognition based on the DCNN, which combines localization and classification for objects in an image, should be considered in the future works.  \n",
      "10                                                          The importance of high-resolution sea ice maps of the Arctic Ocean is increasing due to the possibility of pioneering North Pole Routes and the necessity of precise climate prediction models. In this study, sea ice classification algorithms for two deep learning models were examined using Sentinel- 1 A/B SAR data to generate high-resolution sea ice classification maps. Based on current ice charts, three classes (Open Water, First Year Ice, Multi Year Ice) of training data sets were generated by Arctic sea ice and remote sensing experts. Ten sea ice classification algorithms were generated by combing two deep learning models (i.e. Simple CNN and Resnet50) and five cases of input bands including incident angles and thermal noise corrected HV bands. For the ten algorithms, analyses were performed by comparing classification results with ground truth points. A confusion matrix and Cohen’s kappa coefficient were produced for the case that showed best result. Furthermore, the classification result with the Maximum Likelihood Classifier that has been traditionally employed to classify sea ice. In conclusion, the Convolutional Neural Network case, which has two convolution layers and two max pooling layers, with HV and incident angle input bands shows classification accuracy of 96.66%, and Cohen’s kappa coefficient of 0.9499. All deep learning cases shows better classification accuracy than the classification result of the Maximum Likelihood Classifier.  \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          With the recent introduction of a virtualized platform with virtualization technology, the benefits of increased resource utilization through partitioning of a single hardware resource and scalability through migration provide a reliable response rate for services. Traditional single hardware server-based monitoring application services have had problems with using more resources than needed or lack of resources compared to the user's request, resulting in slower response times. To address this, this paper implemented a monitoring application with a CNN model based on ResNet50 that enables fire and smoke prediction for images through open source virtualization platforms, OpenBaton, OpenStack and Docker. In this paper, we studied how to ensure a stable response rate through comparing the response speed of the existing single hardware server with the proposed system.  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Due to the development of endoscopic equipment, there has been much progress in the early detection and treatment of gastrointestinal diseases. Therefore, many endoscopic images have been provided, and studies on endoscopic image classification have been increased actively. In this paper, we aim to classify gastric ulcer, a disease frequently found in endoscopy.Gastric ulcers can lead to complications if not properly treated early on, so it is most important to diagnose the lesions early.Our study used the ResNet-50 in-depth learning model. A model was created using a total of 1,525 images, and data enhancement was applied for efficient learning. The classification performance of the proposed model is 0.9016 with accuracy and 0.83 with ROC curve.  \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   None  \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 In this paper, we compares the performance of the gredient descent optimizers of the Faster Region-based Convolutional Neural Network (R-CNN) model for the chromosome object detection in digital images composed of human metaphase chromosomes. In faster R-CNN, the gradient descent optimizer is used to minimize the objective function of the region proposal network (RPN) module and the classification score and bounding box regression blocks. The gradient descent optimizer. Through performance comparisons among these four gradient descent optimizers in our experiments, we found that the Adamax optimizer could achieve the mean average precision (mAP) of about 52% when considering faster R-CNN with a base network, VGG16. In case of faster R-CNN with a base network, ResNet50, the Adadelta optimizer could achieve the mAP of about 58%.  \n",
      "15                                                                                                                                                                                                                                                                                            As AI technology is recently introduced into various fields, it is being applied to the fashion field. This paper proposes a system for recommending cody clothes suitable for a user's selected clothes. The proposed system consists of user app, cody recommendation module, and server interworking of each module and managing database data. Cody recommendation system classifies clothing images into 80 categories composed of feature combinations, selects multiple representative reference images for each category, and selects 3 full body cordy images for each representative reference image. Cody images of the representative reference image were determined by analyzing the user's preference using Google survey app. The proposed algorithm classifies categories the clothing image selected by the user into a category, recognizes the most similar image among the classification category reference images, and transmits the linked cody images to the user's app. The proposed system uses the ResNet-50 model to categorize the input image and measures similarity using ORB and HOG features to select a reference image in the category. We test the proposed algorithm in the Android app, and the result shows that the recommended system runs well.  \n",
      "16                                                                                                                                                                                                                                                                                            As AI technology is recently introduced into various fields, it is being applied to the fashion field. This paper proposes a system for recommending cody clothes suitable for a user's selected clothes. The proposed system consists of user app, cody recommendation module, and server interworking of each module and managing database data. Cody recommendation system classifies clothing images into 80 categories composed of feature combinations, selects multiple representative reference images for each category, and selects 3 full body cordy images for each representative reference image. Cody images of the representative reference image were determined by analyzing the user's preference using Google survey app. The proposed algorithm classifies categories the clothing image selected by the user into a category, recognizes the most similar image among the classification category reference images, and transmits the linked cody images to the user's app. The proposed system uses the ResNet-50 model to categorize the input image and measures similarity using ORB and HOG features to select a reference image in the category. We test the proposed algorithm in the Android app, and the result shows that the recommended system runs well.  \n",
      "17                                                                                                                                                                                                                                                                                            As AI technology is recently introduced into various fields, it is being applied to the fashion field. This paper proposes a system for recommending cody clothes suitable for a user's selected clothes. The proposed system consists of user app, cody recommendation module, and server interworking of each module and managing database data. Cody recommendation system classifies clothing images into 80 categories composed of feature combinations, selects multiple representative reference images for each category, and selects 3 full body cordy images for each representative reference image. Cody images of the representative reference image were determined by analyzing the user's preference using Google survey app. The proposed algorithm classifies categories the clothing image selected by the user into a category, recognizes the most similar image among the classification category reference images, and transmits the linked cody images to the user's app. The proposed system uses the ResNet-50 model to categorize the input image and measures similarity using ORB and HOG features to select a reference image in the category. We test the proposed algorithm in the Android app, and the result shows that the recommended system runs well.  \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                Deep learning has become increasingly popular in both academic and industrial areas nowadays. Various domains including pattern recognition, Computer vision have witnessed the great power of deep neural networks. However, current studies on deep learning mainly focus on quality data sets with balanced class labels, while training on bad and imbalanced data set have been providing great challenges for classification tasks. We propose in this paper a method of data analysis-based data reduction techniques for selecting good and diversity data samples from a large dataset for a deep learning model. Furthermore, data sampling techniques could be applied to decrease the large size of raw data by retrieving its useful knowledge as representatives. Therefore, instead of dealing with large size of raw data, we can use some data reduction techniques to sample data without losing important information. We group PCB characters in classes and train deep learning on the ResNet56 v2 and SENet model in order to improve the classification performance of optical character recognition (OCR) character classifier.  \n",
      "19                                                                                                                                                                                                                                                                                    Over the past decade, researchers were able to solve complex medical problems as well as acquire deeper understanding of entire issue due to the availability of machine learning techniques, particularly predictive algorithms and automatic recognition of patterns in medical imaging. In this study, a technique called transfer learning has been utilized to classify Magnetic Resonance (MR) images by a pre-trained Convolutional Neural Network (CNN). Rather than training an entire model from scratch, transfer learning approach uses the CNN model by fine-tuning them, to classify MR images into Alzheimer’s disease (AD), mild cognitive impairment (MCI) and normal control (NC). The performance of this method has been evaluated over Alzheimer’s Disease Neuroimaging (ADNI) dataset by changing the learning rate of the model. Moreover, in this study, in order to demonstrate the transfer learning approach we utilize different pre-trained deep learning models such as GoogLeNet, VGG-16, AlexNet and ResNet-18, and compare their efficiency to classify AD. The overall classification accuracy resulted by GoogLeNet for training and testing was 99.84% and 98.25% respectively, which was exceptionally more than other models training and testing accuracies.  \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           To understand how a Convolutional Neural Network (CNN) model captures the features of a pattern to determine which class it belongs to, in this paper, we use Gradient-weighted Class Activation Mapping (Grad-CAM) to visualize and analyze how well a CNN model behave on the CNU weeds dataset. We apply this technique to Resnet model and figure out which features this model captures to determine a specific class, what makes the model get a correct/wrong classification, and how those wrong label images can cause a negative effect to a CNN model during the training process. In the experiment, Grad-CAM highlights the important regions of weeds, depending on the patterns learned by Resnet, such as the lobe and limb on 미국가막사리, or the entire leaf surface on 단풍잎돼지풀. Besides, Grad-CAM points out a CNN model can localize the object even though it is trained only for the classification problem.  \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this paper, we present an approach for detecting slide transitions in lecture videos by introducing the spatio-temporal residual networks. Given a lecture video which records the digital slides, the speaker, and the audience by multiple cameras, our goal is to find keyframes where slide content changes. Since temporal dependency among video frames is important for detecting slide changes, 3D Convolutional Networks has been regarded as an efficient approach to learn the spatio-temporal features in videos. However, 3D ConvNet will cost much training time and need lots of memory. Hence, we utilize ResNet to ease the training of network, which is easy to optimize. Consequently, we present a novel ConvNet architecture based on 3D ConvNet and ResNet for slide transition detection in lecture videos. Experimental results show that the proposed novel ConvNet architecture achieves the better accuracy than other slide progression detection approaches.  \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          A Pneumothorax is an abnormal collection of air in the space between lung and chest wall and is diagnosed using chest X-ray. Preprocessings, such as the eliminaton of missing value and masking, were performed. Performances of U-Net, Mask R-CNN, Resnet, EfficientNet and Xception for diagnosing pneumothorax are compared. U-Net uses semantic segmentation and Mask R-CNN uses instance segmentation. Semantic segmentation uses a class as the unit of segmentation. Therefore, objects in the same class are denoted using the same color on the prediction mask. On the other hand, instance segmentation uses object as the unit of segmentation. Therefore, objects in the same class are denoted using the different color on the prediction mask, if they belong to the different objects. EfficientNet got the best result. It can be efficiently used because it performs well enough to assist for physicians to diagnose pneumothorax using the chest X-ray  \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In this paper, we introduce WICWIU, the first open source deep learning framework among Korean universities. WICWIU provides a variety of operators and modules together with a network structure that can represent an arbitrary general computational graph. The WICWIU features are sufficient to compose widely used deep learning models such as Inception, ResNet, and DenseNet.WICWIU also supports GPU-based massive parallel computing which significantly accelerates the training of neural networks. It is also easily accessible for C++ developers because the whole API is provided in C++. WICWIU has an advantage over Python-based frameworks in memory and performance optimization based on the C++ environment. This eases the customizability of WICWIU for environments with limited resources. WICWIU is readable and extensible because it is composed of C++ codes coupled with consistent APIs. With Korean documentation, it is particularly suitable for Korean developers. WICWIU applies the Apache 2.0 license which is available for any research or commercial purposes for free.  \n",
      "24  Recently, many companies improving their management performance by building a powerful brand value which is recognized for trademark rights. However, as growing up the size of online commerce market, the infringement of trademark rights is increasing. According to various studies and reports, cases of foreign and domestic companies infringing on their trademark rights are increased. As the manpower and the cost required for the protection of trademark are enormous, small and medium enterprises(SMEs) could not conduct preliminary investigations to protect their trademark rights.Besides, due to the trademark image search service does not exist, many domestic companies have a problem that investigating huge amounts of trademarks manually when conducting preliminary investigations to protect their rights of trademark.Therefore, we develop an intelligent similar trademark search model to reduce the manpower and cost for preliminary investigation. To measure the performance of the model which is developed in this study, test data selected by intellectual property experts was used, and the performance of ResNet V1 101 was the highest. The significance of this study is as follows. The experimental results empirically demonstrate that the image classification algorithm shows high performance not only object recognition but also image retrieval. Since the model that developed in this study was learned through actual trademark image data, it is expected that it can be applied in the real industrial environment.  \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   None  \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   None  \n",
      "27                                                                                                                                                                                                                            BACKGROUND/OBJECTIVES: The aim of this study was to develop Korean food image detection and recognition model for use in mobile devices for accurate estimation of dietary intake.MATERIALS/METHODS: We collected food images by taking pictures or by searching web images and built an image dataset for use in training a complex recognition model for Korean food. Augmentation techniques were performed in order to increase the dataset size. The dataset for training contained more than 92,000 images categorized into 23 groups of Korean food.All images were down-sampled to a fixed resolution of 150 × 150 and then randomly divided into training and testing groups at a ratio of 3:1, resulting in 69,000 training images and 23,000 test images. We used a Deep Convolutional Neural Network (DCNN) for the complex recognition model and compared the results with those of other networks: AlexNet, GoogLeNet, Very Deep Convolutional Neural Network, VGG and ResNet, for large-scale image recognition.RESULTS: Our complex food recognition model, K-foodNet, had higher test accuracy (91.3%) and faster recognition time (0.4 ms) than those of the other networks.CONCLUSION: The results showed that K-foodNet achieved better performance in detecting and recognizing Korean food compared to other state-of-the-art models.  \n",
      "28                                                                                                                                                                                                                 BACKGROUND/OBJECTIVES: The aim of this study was to develop Korean food image detection and recognition model for use in mobile devices for accurate estimation of dietary intake. MATERIALS/METHODS: We collected food images by taking pictures or by searching web images and built an image dataset for use in training a complex recognition model for Korean food. Augmentation techniques were performed in order to increase the dataset size. The dataset for training contained more than 92,000 images categorized into 23 groups of Korean food. All images were down-sampled to a fixed resolution of $150{\\times}150$ and then randomly divided into training and testing groups at a ratio of 3:1, resulting in 69,000 training images and 23,000 test images. We used a Deep Convolutional Neural Network (DCNN) for the complex recognition model and compared the results with those of other networks: AlexNet, GoogLeNet, Very Deep Convolutional Neural Network, VGG and ResNet, for large-scale image recognition. RESULTS: Our complex food recognition model, K-foodNet, had higher test accuracy (91.3%) and faster recognition time (0.4 ms) than those of the other networks. CONCLUSION: The results showed that K-foodNet achieved better performance in detecting and recognizing Korean food compared to other state-of-the-art models.  \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Recently, IT companies such as Google, NVIDIA, and NAVER have been also developing autonomous vehicle platform technologies. In particular, sensors for object detection in surrounding environments have been improved in recognition rates by applying multi-sensor systems using camera, LiDAR, and radar. With the increasing importance of recognition technology, 3D information-based recognition technologies have been actively advanced as a commercial product of 3D-LiDAR. In this paper, a candidate group of point-clouds from 3D-LiDAR is extracted using Euclidean clustering in order to reduce the processing time delay in RPN (Region Proposal Network), which is one of the basic schemes for existing object detection. Then, it proposes types of input slicing, based on the extracted candidates. In addition, the accuracy and the processing time using four CNN networks (Basic CNN, ResNet, VGG16, and MobileNet) are compared over not only the private data (CVLab dataset) obtained in actual road environment but also the publicly open KITTI dataset.  \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The interest in autonomous ships for marine industries has increased significantly over the past few years and autonomous ships also must follow maritime laws in the same way as regular ships operated by crews. Therefore, in this paper, we propose the vessel taxonomy for COLREGs compliance of autonomous ships and evaluate the performance of the vessel classification method using CNNs. First, we define the vessel taxonomy for complying with maritime laws by analyzing the COLREGs. And then, we build our dataset separated manually by the vessel taxonomy. For the dataset, 40,300 images are collected by image search on websites and refining the publicly available dataset. Finally, the state-of-the-art CNN model is applied to evaluate the recognition rate of our dataset. The experimental results show that the Inception-ResNet v2 model which is trained by transfer learning effectively classifies the ships with a high accuracy of 91%.  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                                        keyword  count\n",
      "2                                           CNN      8\n",
      "11                                Deep Learning      5\n",
      "7                                 deep learning      4\n",
      "71                             User Preferences      3\n",
      "70                          Cody Recommendation      3\n",
      "69                                      Fashion      3\n",
      "68                                deep-learning      3\n",
      "12                                       ResNet      3\n",
      "38                Convolutional Neural Networks      3\n",
      "42                                Deep learning      2\n",
      "121                          dietary assessment      2\n",
      "34                             object detection      2\n",
      "118                            Food recognition      2\n",
      "119   deep convolutional neural networks (DCNN)      2\n",
      "120                               mobile device      2\n",
      "59                                          딥러닝      2\n",
      "50                                    OpenStack      2\n",
      "51                                       Docker      2\n",
      "52                                         IaaS      2\n",
      "20                                       합성곱신경망      2\n",
      "91                                        프레임워크      1\n",
      "93                                       WICWIU      1\n",
      "92                                         오픈소스      1\n",
      "89                                     Xecption      1\n",
      "94                              neural networks      1\n",
      "95                                    framework      1\n",
      "96                                  open source      1\n",
      "90                                          신경망      1\n",
      "0                             Periocular Region      1\n",
      "88                                     ask-RCNN      1\n",
      "87                                      ResNetM      1\n",
      "86                                 EfficientNet      1\n",
      "85                                        U-Net      1\n",
      "84                                   3D ConvNet      1\n",
      "83                             slide transition      1\n",
      "97                 Convolutional Neural Network      1\n",
      "81                                       Resnet      1\n",
      "80                                visualization      1\n",
      "79                                     Grad-CAM      1\n",
      "78                           Transfer learning.      1\n",
      "77                                    MR images      1\n",
      "76                          Alzheimer’s disease      1\n",
      "82                                Lecture video      1\n",
      "101                                   상표 검색 시스템      1\n",
      "98                   Trademark Retrieval System      1\n",
      "99                    Image retrieval Algorithm      1\n",
      "130                       Vessel Classification      1\n",
      "129                            Autonomous Ships      1\n",
      "128                                     합성곱 신경망      1\n",
      "127                                  국제해상충돌예방규칙      1\n",
      "126                                       선박 분류      1\n",
      "125                                      자율운항선박      1\n",
      "124                                  clustering      1\n",
      "123                                    3d-LiDAR      1\n",
      "122                          autonomous vehicle      1\n",
      "117                             pseudo-labeling      1\n",
      "116                           Data augmentation      1\n",
      "115                                   ResGLU-SE      1\n",
      "114             Time-domain based DNN structure      1\n",
      "113                 Sound Event Detection (SED)      1\n",
      "112                          Object recognition      1\n",
      "111  Region based convolutional neural networks      1\n",
      "110                            Road maintenance      1\n",
      "109                         Deep neural network      1\n",
      "108                         Road surface damage      1\n",
      "107                                       객체 인식      1\n",
      "106                                   영역 기반 합성곱      1\n",
      "105                                        유지보수      1\n",
      "104                                      심층 신경망      1\n",
      "103                                    도로 노면 파손      1\n",
      "102                                 이미지 검색 알고리즘      1\n",
      "74                               Data reduction      1\n",
      "100                                        딥 러닝      1\n",
      "75                                    Sampling.      1\n",
      "66                               K-ear database      1\n",
      "73                Optical character recognition      1\n",
      "33                         image classification      1\n",
      "31                                          CAD      1\n",
      "30                                     ResNet50      1\n",
      "29                                    GoogLeNet      1\n",
      "28                                      AlexNet      1\n",
      "27                                  Medical MRI      1\n",
      "26                  Ensemble Learning Algorithm      1\n",
      "25                              Computer Vision      1\n",
      "24                    teacher‐student framework      1\n",
      "23                            residual networks      1\n",
      "22                     layer‐wise hint training      1\n",
      "21                           knowledge transfer      1\n",
      "19                                          레스넷      1\n",
      "18                                    자동 데이터 구축      1\n",
      "17                                         음성인식      1\n",
      "16                                   한국어 명령어 인식      1\n",
      "15                  Automatic Data Construction      1\n",
      "14                           Speech Recognition      1\n",
      "13                        Korean Speech Command      1\n",
      "10                               Access Control      1\n",
      "9                              Face Recognition      1\n",
      "8      hepatocellular carcinoma differentiation      1\n",
      "6                             transfer learning      1\n",
      "5                                   data fusion      1\n",
      "4                          Multi-parametric MRI      1\n",
      "3                                           MLP      1\n",
      "32                              crack detection      1\n",
      "35                        semantic segmentation      1\n",
      "72                               PCB inspection      1\n",
      "36                        instance segmentation      1\n",
      "67                                     Ensemble      1\n",
      "1                                Authentication      1\n",
      "65                Convolutional neural networks      1\n",
      "64                              Ear recognition      1\n",
      "63                       Artificial Intelligent      1\n",
      "62                             Endoscopic Image      1\n",
      "61                                Gastric Ulcer      1\n",
      "60                                         인공지능      1\n",
      "58                                       내시경 영상      1\n",
      "57                                          위궤양      1\n",
      "56                                         모니터링      1\n",
      "55                                     클라우드 컴퓨팅      1\n",
      "54                                      가상화 플랫폼      1\n",
      "53                                   Monitoring      1\n",
      "49                              Cloud Computing      1\n",
      "48                         Virtualized Platform      1\n",
      "47                               classification      1\n",
      "46                                thermal noise      1\n",
      "45                                      sea ice      1\n",
      "44                               Sentinel-1 A/B      1\n",
      "43                               Classification      1\n",
      "41            Deep convolutional neural network      1\n",
      "40                                Vespa hornets      1\n",
      "39                            Transfer Learning      1\n",
      "37                 Colon Disease Classification      1\n",
      "131                                     COLREGs      1\n",
      "데이터가 저장되었습니다: resnet_2019_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2019_academic_riss.csv\n",
      "SSD-Mobilenet과 ResNet을 이용한 모바일 기기용 자동차 번호판 인식시스템 2020 ['Vehicle License Plate Recognition', 'Deep Learning', 'SSD-Mobilenet', 'ResNet', '자동차 번호판 인식 시스템', '딥러닝', 'SSD-Mobilenet', 'ResNet'] 본 논문은 고성능의 서버 없이 안드로이드 스마트폰 단독으로 동작할 수 있도록 경량화 딥러닝 모델을 사용하여 구현한 자동차 번호판 인식 시스템을 제안한다. 자동차 번호판 인식시스템은 [번호판검출]-[문자영역 분할]-[문자인식]으로 3단계의 과정으로 구성되며, 번호판검출은 SSD-Mobilenet, 문자영역 분할은 ResNet에 localization을 추가하여 사용하였고 문자인식은 ResNet을 이용하여 구현하였다. 테스트한 기기는 삼성 갤럭시 S7, LG Q9이며 정확도는 약 85.3%, 실행속도는 약 1.1초가 소요된다. This paper proposes a vehicle license plate recognition system using light weight deep learning models without high-end server. The proposed license plate recognition system consists of 3 steps: [license plate detection]-[character area segmentation]-[character recognition]. SSD-Mobilenet was used for license plate detection, ResNet with localization was used for character area segmentation, ResNet was used for character recognition. Experiemnts using Samsung Galaxy S7 and LG Q9, accuracy showed 85.3% accuracy and around 1.1 second running time.\n",
      "ResNet 알고리즘을 이용한 가로수 객체의 폐색영역 검출 및 해결 2020 ['3D Spatial Information', 'Occlusion Area', 'Street Tree', 'Deep Learning', 'ResNet Algorithm'] 국토를 효율적으로 관리하고 도시문제를 과학적으로 해결하기 위해 최근 스마트시티, 디지털트윈 등 3차원 공간정보 관련 기술이 급격하게 발전하고 있다. 이러한 3차원 공간정보 구축은 주로 영상정보를 이용하여 객체를 3차원 입체화하고 실감형 영상인 텍스처링 영상을 추출하여 객체벽면에 영상을 부여하는 방식으로 수행된다. 하지만 객체 주변의 다양한 요인으로 인해 텍스처링 영상에서는 필연적으로 폐색영역이 발생한다. 이에 본 연구에서는 최근 기술인 딥러닝 기술 중에서 ResNet 알고리즘을 이용하여 건물 폐색을 유발하는 가로수에 대한 데이터셋을 만들고 이에 대한 해결방안을 제시하고자 한다. 연구결과 ResNet 알고리즘의 공간정보 적용 가능성을 판단하고 이를 적용한 레이블링 생성 SW 개발하여 실제 가로수를 대상으로 데이터셋을 구축하였다. 구축된 데이터셋을 텍스처링 영상에 적용하여 정확도와 재현율로 검출능력을 분석하였다. 분석결과를 위해 딥러닝 분야에서 많이 사용되고 있는 정밀도와 재현율을 이용한 F값을 적용하였으며 가로수 단일 객체가 포함된 건물의 측면부 영상과 경사 영상에 대해서는 높은 F값을 도출하여 우수한 성과를 확인하였으나, 같은 해상도를 가진 건물 전면부 영상에서는 그림자 등의 요인으로 F값이 낮음을 확인하였다. The technologies of 3D spatial information, such as Smart City and Digital Twins, are developing rapidly for managing land and solving urban problems scientifically. In this construction of 3D spatial information, an object using aerial photo images is built as a digital DB. Realistically, the task of extracting a texturing image, which is an actual image of the object wall, and attaching an image to the object wall are important. On the other hand, occluded areas occur in the texturing image. In this study, the ResNet algorithm in deep learning technologies was tested to solve these problems. A dataset was constructed, and the street tree was detected using the ResNet algorithm. The ability of the ResNet algorithm to detect the street tree was dependent on the brightness of the image. The ResNet algorithm can detect the street tree in an image with side and inclination angles.\n",
      "Crack detection based on ResNet with spatial attention 2020 ['crack detection', 'attention mechanism', 'deep convolution neural network'] None Deep Convolution neural network (DCNN) has been widely used in the healthy maintenance of civil infrastructure. Using DCNN to improve crack detection performance has attracted many researchers’ attention. In this paper, a light-weight spatial attention network module is proposed to strengthen the representation capability of ResNet and improve the crack detection performance. It utilizes attention mechanism to strengthen the interested objects in global receptive field of ResNet convolution layers. Global average spatial information over all channels are used to construct an attention scalar. The scalar is combined with adaptive weighted sigmoid function to activate the output of each channel’s feature maps. Salient objects in feature maps are refined by the attention scalar. The proposed spatial attention module is stacked in ResNet50 to detect crack. Experiments results show that the proposed module can got significant performance improvement in crack detection.\n",
      "Classroom Roll-Call System Based on ResNet Networks 2020 ['Face Recognition', 'Game', 'ResNet Networks'] None A convolution neural networks (CNNs) has demonstrated outstanding performance compared to otheralgorithms in the field of face recognition. Regarding the over-fitting problem of CNN, researchers haveproposed a residual network to ease the training for recognition accuracy improvement. In this study, a novelface recognition model based on game theory for call-over in the classroom was proposed. In the proposedscheme, an image with multiple faces was used as input, and the residual network identified each face with aconfidence score to form a list of student identities. Face tracking of the same identity or low confidence weredetermined to be the optimisation objective, with the game participants set formed from the student identitylist. Game theory optimises the authentication strategy according to the confidence value and identity set toimprove recognition accuracy. We observed that there exists an optimal mapping relation between face andidentity to avoid multiple faces associated with one identity in the proposed scheme and that the proposedgame-based scheme can reduce the error rate, as compared to the existing schemes with deeper neural network.\n",
      "An Experimental Comparison of CNN-based Deep Learning Algorithms for Recognition of Beauty-related Skin Disease 2020 ['Deep Learning', 'CNN', 'Beauty-related Skin Disease Recognition', 'Image Recognition', 'Algorithm Comparison', 'Experimental Comparison', '딥러닝', '피부미용 질환 인식', '이미지 인식', '알고리즘 비교', '실험적 비교'] 본 논문에서는 딥러닝 지도학습 알고리즘을 사용한 학습 모델을 대상으로 미용 관련 피부질환 인식의 효과성을 실험적으로 비교한다. 최근 딥러닝 기술을 산업, 교육, 의료 등 다양한 분야에 적용하고 있으며, 의료 분야에서는 중요 피부질환 중 하나인 피부암 식별의 수준을 전문가 수준으로 높인 성과를 보이고 있다. 그러나 아직 피부미용과 관련된 질환에 적용한 사례가 다양하지 못하다. 따라서 딥러닝 기반 이미지 분류에 활용도가 높은 CNN 알고리즘을 비롯하여 ResNet, SE-ResNet을 적용하여 실험적으로 정확도를 비교함으로써 미용 관련 피부질환을 판단하는 효과성을 평가한다. 각 알고리즘을 적용한 학습 모델을 실험한 결과에서 CNN의 경우 평균 71.5%, ResNet은 평균 90.6%, SE-ResNet은 평균 95.3%의 정확도를 보였다. 특히 학습 깊이를 다르게하여 비교한 결과 50개의 계층 구조를 갖는 SE-ResNet-50 모델이 평균 96.2%의 정확도로 미용 관련 피부질환 식별을 위해 가장 효과적인 결과를 보였다. 본 논문의 목적은 피부 미용과 관련된 질환의 판별을 고려하여 효과적인 딥러닝 알고리즘의 학습과 방법을 연구하기 위한 것으로 이를 통해 미용 관련 피부질환 개선을 위한 서비스 개발로 확장할 수 있을 것이다. In this paper, we empirically compare the effectiveness of training models to recognize beauty-related skin disease using supervised deep learning algorithms. Recently, deep learning algorithms are being actively applied for various fields such as industry, education, and medical. For instance, in the medical field, the ability to diagnose cutaneous cancer using deep learning based artificial intelligence has improved to the experts level. However, there are still insufficient cases applied to disease related to skin beauty. This study experimentally compares the effectiveness of identifying beauty-related skin disease by applying deep learning algorithms, considering CNN, ResNet, and SE-ResNet. The experimental results using these training models show that the accuracy of CNN is 71.5% on average, ResNet is 90.6% on average, and SE-ResNet is 95.3% on average. In particular, the SE-ResNet-50 model, which is a SE-ResNet algorithm with 50 hierarchical structures, showed the most effective result for identifying beauty-related skin diseases with an average accuracy of 96.2%. The purpose of this paper is to study effective training and methods of deep learning algorithms in consideration of the identification for beauty-related skin disease. Thus, it will be able to contribute to the development of services used to treat and easy the skin disease.\n",
      "다양한 합성곱 신경망 방식을 이용한 모바일 기기를 위한 시작 단어 검출의 성능 비교 2020 ['성능 비교', '시작 단어 검출', '합성곱 신경망', '인공지능 비서', 'Performance comparison', 'Wake-up-word detection', 'Convolutional neural network', 'Artificial Intelligence (AI) assistant'] 음성인식 기능을 제공하는 인공지능 비서들은 정확도가 뛰어난 클라우드 기반의 음성인식을 통해 동작한다.클라우드 기반의 음성인식에서 시작 단어 인식은 대기 중인 기기를 활성화하는 데 중요한 역할을 한다. 본 논문에서는공개 데이터셋인 구글의 Speech Commands 데이터셋을 사용하여 스펙트로그램 및 멜-주파수 캡스트럼 계수 특징을입력으로 하여 모바일 기기에 대응한 저 연산 시작 단어 검출을 위한 합성곱 신경망의 성능을 비교한다. 본 논문에서사용한 합성곱 신경망은 다층 퍼셉트론, 일반적인 합성곱 신경망, VGG16, VGG19, ResNet50, ResNet101, ResNet152, MobileNet이며, MobileNet의 성능을 유지하면서 모델 크기를 1/25로 줄인 네트워크도 제안한다. Artificial intelligence assistants that provide speech recognition operate through cloud-based voice recognition with high accuracy. In cloud-based speech recognition, Wake-Up-Word (WUW) detection plays an important role in activating devices on standby. In this paper, we compare the performance of Convolutional Neural Network (CNN)-based WUW detection models for mobile devices by using Google's speech commands dataset, using the spectrogram and mel-frequency cepstral coefficient features as inputs. The CNN models used in this paper are multi-layer perceptron, general convolutional neural network, VGG16, VGG19, ResNet50, ResNet101, ResNet152, MobileNet. We also propose network that reduces the model size to 1/25 while maintaining the performance of MobileNet is also proposed.\n",
      "조류 울음소리를 이용한 조류 분류 딥러닝 시스템 개발 2020 ['Deep learning', 'Classification', 'Spectrogram', 'AI', 'Convolutional Neural Networks', 'ResNet', 'AlexNet'] None The activity and distribution of wild birds are biological indicators to evaluate biodiversity. In order to identify bird habitats, collecting and classifying sounds should have to do. Using the bird sound can make easier to distinguish location or type of wild birds. Recently, attempts to analyze bioacoustic data have been risen using the machine learning. We are going to classify the bird songs using deep learning. The bird songs convert into the spectrogram images. Spectrogram images are used for the input of convolutional neural network. In generally the bird song data set for classification contains a lot of noise. Even obtaining the data including noise is difficult. The data is about 200 bird sounds of 20 species. Based on transfer learning, ResNet34, ResNet50 and AlexNet of Convolutional Neural Network are used as the experiment. The experiment parameter is learning rate and epochs. As a result, the ResNet34 shows the highest accuracy of 99.7% and an average of 93% in the test. Therefore, In this paper, we are going to develop the deep learning system that classifies 20 kinds of bird song using ResNet34. By using this system, it can be helpful various activities such as the prevention of avian influenza.\n",
      "변형된 잔차블록을 적용한 CNN 2020 ['CNN', 'Residual Learning', 'Bottleneck', 'ResNet'] None This paper proposes an image classification algorithm that transforms the number of convolution layers in the residual block of ResNet, CNN's representative method. The proposed method modified the structure of 34/50 layer of ResNet structure. First, we analyzed the performance of small and many convolution layers for the structure consisting of only shortcut and 3 × 3 convolution layers for 34 and 50 layers.And then the performance was analyzed in the case of small and many cases of convolutional layers for the bottleneck structure of 50 layers. By applying the results, the best classification method in the residual block was applied to construct a 34-layer simple structure and a 50-layer bottleneck image classification model. To evaluate the performance of the proposed image classification model, the results were analyzed by applying to the cifar10 dataset. The proposed 34-layer simple structure and 50-layer bottleneck showed improved performance over the ResNet-110 and Densnet-40 models\n",
      "Weather Recognition Based on 3C-CNN 2020 ['Weather recognition', 'deep learning', 'ResNet50', '3C-CNN', 'WeatherDataset-6'] None Human activities are often affected by weather conditions. Automatic weather recognition is meaningful to traffic alerting, driving assistance, and intelligent traffic. With the boost of deep learning and AI, deep convolutional neural networks (CNN) are utilized to identify weather situations. In this paper, a three-channel convolutional neural network (3C-CNN) model is proposed on the basis of ResNet50.The model extracts global weather features from the whole image through the ResNet50 branch, and extracts the sky and ground features from the top and bottom regions by two CNN5 branches. Then the global features and the local features are merged by the Concat function. Finally, the weather image is classified by Softmax classifier and the identification result is output. In addition, a medium-scale dataset containing 6,185 outdoor weather images named WeatherDataset-6 is established. 3C-CNN is used to train and test both on the Two-class Weather Images and WeatherDataset-6. The experimental results show that 3C-CNN achieves best on both datasets, with the average recognition accuracy up to 94.35% and 95.81% respectively, which is superior to other classic convolutional neural networks such as AlexNet, VGG16, and ResNet50. It is prospected that our method can also work well for images taken at night with further improvement.\n",
      "탄성파 층서 구분을 위한 합성곱 신경망 기법 비교 연구 2020 ['머신 러닝', '합성곱신경망', '탄성파층서구분', '인코더-디코더 모델', '네덜란드 F3 block', 'Machine learning', 'Convolutional neural network', 'Seismic sequence identification', 'Encoder–decoder model', 'Netherlands F3 block'] 머신 러닝 기술은 탄성파탐사 분야로 그 적용 범위를 확장하고 있다. 탄성파 해석에서 중요한 탄성파 층서 구분에 머신 러닝의 적용 가능성을 알아보았다. 이미지 분야에 탁월한 결과를 보여온 합성곱 신경망 기법 중 4가지 모델을 네덜란드 F3 block에 적용시켰다. 4가지 모델은 ResNet34 모델, 인코더-디코더 형태를 가지는 U-Net, Residual U-Net, FD U-Net이다. 예측된 이미지의 정성적 분석 수행 후 정량적 분석을 위해 pixel accuracy, mean class accuracy, mean intersection over union, frequency weighted IU의 수식을 활용하였다. 본 연구의 분석 결과 ResNet34의 정확도 결과가 가장 낮았고, 인코더-디코더 형태를 가지는 모델들이 높은 정확도를 보여주었다. 그리고 계산에 필요한 파라미터수와 학습시간을 고려 할 때 U-Net이 가장 효율적임을 확인할 수 있었다. Application of the machine learning technique is expanding to the field of seismic exploration. For the purpose of feasibility assessment, we applied the machine learning technique to seismic sequence identification, which is important in seismic interpretation.From among the convolutional neural network techniques used in image analysis, we applied four models to seismic data obtained in the F3 block, offshore Netherlands, which have yielded remarkable results. One of the four models was ResNet34. The others were encoder– decoder models: U-Net, Residual U-Net, and FD U-Net. We first performed a qualitative analysis of the predicted images and then conducted quantitative analysis using pixel accuracy, mean class accuracy, mean intersection over union, and frequency weighted IU equations. The numerical results showed that ResNet34 had the lowest accuracy and that the encoder–decoder type models had higher accuracy. Considering the number of parameters required for calculation and the learning time, we confirmed that U-Net is the most efficient model.\n",
      "사전 학습된 네트워크 모델을 이용한 심전도 신호 기반 개인 식별 성능 분석 2020 ['electrocardiogram signal', 'deep learning', 'convolutional neural network', 'personal identification'] None The existing personal identification method has a problem that is vulnerable to various crimes, and researches using biosignals, which are internal characteristics of the body, are being conducted to compensate for this. Among them, ECG signals are unique to each person according to the size and location of the heart, which makes them suitable for personal identification, and many studies are being conducted in conjunction with deep learning. In this paper, we analyze the performance of personal identification according to the pre-trained network model using 2-D ECG images. The pre-trained network model for training ECG data sets uses 11 networks of Inception and ResNet. The training data of the network uses 2-D image data using one period of the ECG signal, and the experiment is performed by changing the number of learning. Inception-ResNet-V2 is the highest in the Inception network with 96.18% performance, ResNet-V2-152 is the highest in the ResNet network with 99.12% performance.\n",
      "상처와 주름이 있는 지문 판별에 효율적인 심층 학습 비교연구 2020 ['딥러닝', '지문', '생체정보', '2D 합성 곱 신경망', '상처 지문 판별', '주름 지문 판별', 'Deep learning', 'Biometric information', '2D Convolutional Neural Network', 'discriminating of scar fingerprint', 'discriminating of wrinkle fingerprint'] 인간의 특성과 관련된 측정 항목을 나타내는 생체정보는 도난이나 분실의 염려가 없으므로 높은 신뢰성을 가진 보안 기술로서큰 주목을 받고 있다. 이러한 생체정보 중 지문은 본인 인증, 신원 파악 등의 분야에 주로 사용된다. 신원을 파악할 때 지문 이미지에인증을 수행하기 어려운 상처, 주름, 습기 등의 문제가 있을 경우, 지문 전문가가 전처리단계를 통해 직접 지문에 어떠한 문제가 있는지 파악하고 문제에 맞는 영상처리 알고리즘을 적용해 문제를 해결한다. 이때 지문에 상처와 주름이 있는 지문 영상을 판별해주는인공지능 소프트웨어를 구현하면 손쉽게 상처나 주름의 여부를 확인할 수 있고, 알맞은 알고리즘을 선정해 쉽게 지문 이미지를 개선할 수 있다. 본 연구에서는 이러한 인공지능 소프트웨어의 개발을 위해 캄보디아 왕립대학교의 학생 1,010명, Sokoto 오픈 데이터셋 600명, 국내 학생 98명의 모든 손가락 지문을 취득해 총 17,080개의 지문 데이터베이스를 구축했다. 구축한 데이터베이스에서 상처나 주름이 있는 경우를 판별하기 위해 기준을 확립하고 전문가의 검증을 거쳐 데이터 어노테이션을 진행했다. 트레이닝 데이터셋과테스트 데이터셋은 캄보디아의 데이터, Sokoto 데이터로 구성하였으며 비율을 8:2로 설정했다. 그리고 국내 학생 98명의 데이터를검증 데이터 셋으로 설정했다, 구성된 데이터셋을 사용해 Classic CNN, AlexNet, VGG-16, Resnet50, Yolo v3 등의 다섯 가지 CNN 기반아키텍처를 구현해 학습을 진행했으며 지문의 상처와 주름 판독에서 가장 좋은 성능을 보이는 모델을 찾는 연구를 수행했다. 다섯가지 아키텍처 중 지문 영상에서 상처와 주름 여부를 가장 잘 판별할 수 있는 아키텍처는 ResNet50으로 검증 결과 81.51%로 가장좋은 성능을 보였다. Biometric information indicating measurement items related to human characteristics has attracted great attention as security technology with high reliability since there is no fear of theft or loss. Among these biometric information, fingerprints are mainly used in fields such as identity verification and identification. If there is a problem such as a wound, wrinkle, or moisture that is difficult to authenticate to the fingerprint image when identifying the identity, the fingerprint expert can identify the problem with the fingerprint directly through the preprocessing step, and apply the image processing algorithm appropriate to the problem. Solve the problem. In this case, by implementing artificial intelligence software that distinguishes fingerprint images with cuts and wrinkles on the fingerprint, it is easy to check whether there are cuts or wrinkles, and by selecting an appropriate algorithm, the fingerprint image can be easily improved. In this study, we developed a total of 17,080 fingerprint databases by acquiring all finger prints of 1,010 students from the Royal University of Cambodia, 600 Sokoto open data sets, and 98 Korean students. In order to determine if there are any injuries or wrinkles in the built database, criteria were established, and the data were validated by experts. The training and test datasets consisted of Cambodian data and Sokoto data, and the ratio was set to 8: 2. The data of 98 Korean students were set up as a validation data set. Using the constructed data set, five CNN-based architectures such as Classic CNN, AlexNet, VGG-16, Resnet50, and Yolo v3 were implemented. A study was conducted to find the model that performed best on the readings. Among the five architectures, ResNet50 showed the best performance with 81.51%.\n",
      "콘크리트 균열 탐지를 위한 딥 러닝 기반 CNN 모델 비교 2020 ['균열 탐지', 'ILSVRC', '딥 러닝', 'CNN', '전이 학습', 'Crack Detection', 'Deep Learning', 'Transfer Learning'] None The purpose of this study is to compare the models of Deep Learning-based Convolution Neural Network(CNN) for concrete crack detection. The comparison models are AlexNet, GoogLeNet, VGG16, VGG19, ResNet-18, ResNet-50, ResNet-101, and SqueezeNet which won ImageNet Large Scale Visual Recognition Challenge(ILSVRC). To train, validate and test these models, we constructed 3000 training data and 12000 validation data with 256×256 pixel resolution consisting of cracked and non-cracked images, and constructed 5 test data with 4160×3120 pixel resolution consisting of concrete images with crack. In order to increase the efficiency of the training, transfer learning was performed by taking the weight from the pre-trained network supported by MATLAB. From the trained network, the validation data is classified into crack image and non-crack image, yielding True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN), and 6 performance indicators, False Negative Rate (FNR), False Positive Rate (FPR), Error Rate, Recall, Precision, Accuracy were calculated. The test image was scanned twice with a sliding window of 256×256 pixel resolution to classify the cracks, resulting in a crack map. From the comparison of the performance indicators and the crack map, it was concluded that VGG16 and VGG19 were the most suitable for detecting concrete cracks.\n",
      "딥러닝 알고리즘을 이용한 토마토에서 발생하는 여러가지 병해충의 탐지와 식별에 대한 웹응용 플렛폼의 구축 2020 ['Agricultural Tomato Images', 'Plant Diseases and Pests', 'Deep Learning Algorithm', 'Faster R-CNN', 'Convolution Neural Network', 'Web Application Platform'] None Purpose: purpose of this study was to propose the web application platform which can be to detect and discriminate various diseases and pest of tomato plant based on the large amount of disease image data observed in the facility or the open field.Methods: The deep learning algorithms uesed at the web applivation platform are consisted as the combining form of Faster R-CNN with the pre-trained convolution neural network (CNN) models such as SSD_mobilenet v1, Inception v2, Resnet50 and Resnet101 models. To evaluate the superiority of the newly proposed web application platform, we collected 850 images of four diseases such as Bacterial cankers, Late blight, Leaf miners, and Powdery mildew that occur the most frequent in tomato plants. Of these, 750 were used to learn the algorithm, and the remaining 100 images were used to evaluate the algorithm.Results: From the experiments, the deep learning algorithm combining Faster R-CNN with SSD_mobilnet v1, Inception v2, Resnet50, and Restnet101 showed detection accuracy of 31.0%, 87.7%, 84.4%, and 90.8% respectively. Finally, we constructed a web application platform that can detect and discriminate various tomato deseases using best deep learning algorithm. If farmers uploaded image captured by their digital cameras such as smart phone camera or DSLR (Digital Single Lens Reflex) camera, then they can receive an information for detection, identification and disease control about captured tomato disease through the proposed web application platform.Conclusion: Incheon Port needs to act actively paying\n",
      "내시경의 위암과 위궤양 영상을 이용한 합성곱 신경망 기반의 자동 분류 모델 2020 ['Gastroscopy', 'Classification', 'ResNet-50', 'Gastric ulcer', 'Gastric cancer'] None Although benign gastric ulcers do not develop into gastric cancer, they are similar to early gastric cancer and difficult to distinguish. This may lead to misconsider early gastric cancer as gastric ulcer while diagnosing. Since gastric cancer does not have any special symptoms until discovered, it is important to detect gastric ulcers by early gastroscopy to prevent the gastric cancer. Therefore, we developed a Convolution Neural Network (CNN) model that can be helpful for endoscopy. 3,015 images of gastroscopy of patients undergoing endoscopy at Gachon University Gil Hospital were used in this study. Using ResNet-50, three models were developed to classify normal and gastric ulcers, normal and gastric cancer, and gastric ulcer and gastric cancer. We applied the data augmentation technique to increase the number of training data and examined the effect on accuracy by varying the multiples. The accuracy of each model with the highest performance are as follows. The accuracy of normal and gastric ulcer classification model was 95.11% when the data were increased 15 times, the accuracy of normal and gastric cancer classification model was 98.28% when 15 times increased likewise, and 5 times increased data in gastric ulcer and gastric cancer classification model yielded 87.89%. We will collect additional specific shape of gastric ulcer and cancer data and will apply various image processing techniques for visual enhancement. Models that classify normal and lesion, which showed relatively high accuracy, will be re-learned through optimal parameter search.\n",
      "딥러닝을 활용한 실내 식물 이미지 분류 및 식물 정보 제공 웹 어플리케이션 2020 ['Indoor plant information', 'Deep learning', 'Transfer learning', 'ResNet', 'Fast.ai'] None Plants have good effects such as air purification and landscaping, but they have special ingredients to protect themselves. Ingredients made to protect the plant itself can harm people or animals. There are also accidents that are mistaken for other plants with similar plant features. We have implemented a program that can identify plants and display information about each plant. Using deep learning to classify images, we created a web application that predicts plant names and displays information that matches the predicted plants. We created a data set of 61 indoor plants using Google Image Search. The positive effects and negative toxicity of indoor plants are summarized in the database. Deep learning is implemented using fast.ai, a Pytorch-based framework. Through data Augmentation, we increased the number of images to learn. Indoor plant image data were trained using ResNet50, a pretrained model using various images. The accuracy of the model was about 97.5%, which predicted most plants accurately. The web application was implemented using flask, a Python-based web framework. Using the implemented image classification deep learning model, the plant name is predicted and the information corresponding to the predicted plant name is displayed on the web page. The web application can be optimized for mobile devices and used conveniently.\n",
      "Masked cross self-attentive encoding based speaker embedding for speaker verification 2020 ['Speaker verification', '화자검증', 'Masked cross self-attentive encoding', 'Speaker embedding', 'ResNet', '마스킹된 교차 자기주의 인코딩', '화자 임베딩', '잔차 네트워크'] None Constructing speaker embeddings in speaker verification is an important issue. In general, a self-attention mechanism has been applied for speaker embedding encoding. Previous studies focused on training the self-attention in a high-level layer, such as the last pooling layer. In this case, the effect of low-level layers is not well represented in the speaker embedding encoding. In this study, we propose Masked Cross Self-Attentive Encoding (MCSAE) using ResNet. It focuses on training the features of both high-level and low-level layers.Based on multi-layer aggregation, the output features of each residual layer are used for the MCSAE. In the MCSAE, the interdependence of each input features is trained by cross self-attention module. A random masking regularization module is also applied to prevent overfitting problem. The MCSAE enhances the weight of frames representing the speaker information. Then, the output features are concatenated and encoded in the speaker embedding. Therefore, a more informative speaker embedding is encoded by using the MCSAE. The experimental results showed an equal error rate of 2.63 % using the VoxCeleb1 evaluation dataset. It improved performance compared with the previous self-attentive encoding and state-of-the-art methods.\n",
      "딥러닝 기반 소나무 재선충 피해목 탐색 2020 ['소나무 재선충', '드론', 'RGB 정사영상', '딥러닝 분류기', 'pine wilt disease', 'unmanned aviation vehicle', 'RGB ortho-image', 'deep learning-based classifier', 'heat map'] 소나무 재선충은 한국과 일본, 중국을 포함한 동아시아 지역의 소나무산림에 막대한 피해를 주는 원인이며, 피해목의 조기 발견과 제거는 재선충 확산을 막는 효과적인 방법이다. 본 논문에서는 드론으로 촬영되고 처리된 RGB 정사영상을 딥러닝 분류에 의한 재선충 피해목 탐색방법을 제안한다. 제안된 방법은 학습영상 데이터가 많지 않다는 가정아래 ResNet18을 백본으로 하는 패치기반의 분류기를 구성하고 RGB 정사영상을 분류하고 그 결과를 heatmap 형태로 만든다. 제작된 정사영상의 heat map는 재선충 피해목의 분포를 알아내고 확산해가는 모습을 관찰할 수 있게 하며, 재선충 피해목 지역의 RGB 분포 특징을 추출해낼 수도 있다. 본 연구의 패치기반 분류기 성능은 94.7%의 정확도를 나타내었다. Pine wilt disease is one of the reasons that results in huge damage on pine trees in east Asia including Korea, Japan, and China, and early finding and removing the diseased trees is an efficient way to prevent the forest from wide spreading. This paper proposes a searching method of the damaged pine trees from wilt disease in ortho-images corrected from RGB images, which are captured by unmanned aviation vehicles. The proposed method constructs patch-based classifier using ResNet18 backbone network, classifies the RGB ortho-image patches, and make the results as a heat map. The heat map can be used to find the distribution of diseased pine trees, to show the trend of spreading disease, and to extract the RGB distribution of the diseased areas in the image. The classifier in the work shows 94.7% of accuracy.\n",
      "딥러닝 기반 지하공동구 화재 탐지 모델 개발 : 학습데이터 보강 및 편향 최적화 2020 ['Underground Utility Facility', 'Fire Detection', 'Deep Learning', 'Convolutional Neural Network', 'Bias Training'] 화재는 높은 비정형성으로 인해 딥러닝 모델을 이용한 영상인식 분야에서도 좋은 성능을 내기가 어려운 대상 중 하나이다. 특히 지하공동구 내 화재는 딥러닝 모델의 학습을 위한 화재 데이터 확보가 어렵고 열약한 영상 조건 및 화재로 오인할 수 있는 객체가 많아 화재 검출이 어렵고 성능이 낮다. 이러한 이유로 본 연구는 딥러닝 기반의 지하공동구 내 화재 탐지 모델을 제안하고, 제안된 모델의 성능을 평가하였다. 기존 합성곱 인공신경망에 GoogleNet의 Inception block과 ResNet의 skip connection을 조합하여 어두운 환경에서 발생되는 화재 탐지를 위한 모델 구조를 제안하였으며, 제안된 모델을 효과적으로 학습시키기 위한 방법도 함께 제시하였다. 제안된 방법의 효과를 평가하기 위해 학습 후 모델을 지하공동구 및 유사환경 조건의 화재 문제와 화재로 오인할 수 있는 객체를 포함한 이미지에 적용해 결과를 분석하였다. 또한 기존 딥러닝 기반 화재 탐지 모델의 정밀도, 검출률 지표와 비교함으로써 모델의 화재 탐지성능을 정량적으로 평가하였다. 제안된 모델의 결과는 어두운 환경에서 발생되는 화재 문제에 대해 높은 정밀도와 검출률을 나타내었으며, 유사 화재 객체에 대해 낮은 오탐 및 미탐 성능을 가지고 있음을 보여주었다. Fire is difficult to achieve good performance in image detection using deep learning because of its high irregularity. In particular, there is little data on fire detection in underground utility facilities, which have poor light conditions and many objects similar to fire. These make fire detection challenging and cause low performance of deep learning models. Therefore, this study proposed a fire detection model using deep learning and estimated the performance of the model. The proposed model was designed using a combination of a basic convolutional neural network, Inception block of GoogleNet, and Skip connection of ResNet to optimize the deep learning model for fire detection under underground utility facilities. In addition, a training technique for the model was proposed. To examine the effectiveness of the method, the trained model was applied to fire images, which included fire and non-fire (which can be misunderstood as a fire) objects under the underground facilities or similar conditions, and results were analyzed. Metrics, such as precision and recall from deep learning models of other studies, were compared with those of the proposed model to estimate the model performance qualitatively. The results showed that the proposed model has high precision and recall for fire detection under low light intensity and both low erroneous and missing detection capabilities for things similar to fire.\n",
      "템플릿 재사용을 통한 패러미터 효율적 신경망 네트워크 2020 ['Neural Network', 'Parameter Sharing', 'Layer Reuse', 'Parameter Efficiency', '신경망', '패러미터 공유', '레이어 재사용', '패러미터 효율'] 최근 심층 신경망 (Deep Neural Networks, DNNs)는 모바일 및 임베디드 디바이스에 인간과 유사한 수준의 인공지능을 제공해 많은 응용에서 혁명을 가져왔다. 하지만, 이러한 DNN의 높은 추론 정확도는 큰 연산량을 요구하며, 따라서 기존의 사용되던 모델을 압축하거나 리소스가 제한적인 디바이스를 위해 작은 풋프린트를 가진 새로운 DNN 구조를 만드는 방법으로 DNN의 연산 오버헤드를 줄이기 위한 많은 노력들이 있어왔다. 이들 중 최근 작은 메모리 풋프린트를 갖는 모델 설계에서 주목받는 기법중 하나는 레이어 간에 패러미터를 공유하는 것이다. 하지만, 기존의 패러미터 공유 기법들은 ResNet과 같이 패러미터에 중복(redundancy)이 높은 것으로 알려진 깊은 심층 신경망에 적용되어왔다. 본 논문은 ShuffleNetV2와 같이 이미 패러미터 사용에 효율적인 구조를 갖는 소형 신경망에 적용할 수 있는 패러미터 공유 방법을 제안한다. 본 논문에서 제안하는 방법은 작은 크기의 템플릿과 레이어에 고유한 작은 패러미터를 결합하여 가중치를 생성한다. ImageNet과 CIFAR-100 데이터셋에 대한 우리의 실험 결과는 ShuffleNetV2의 패러미터를 15%-35% 감소시키면서도 기존의 패러미터 공유 방법과 pruning 방법에 대비 작은 정확도 감소만이 발생한다. 또한 우리는 제안된 방법이 최근의 임베디드 디바이스상에서 응답속도 및 에너지 소모량 측면에서 효율적임을 보여준다. Recently, deep neural networks (DNNs) have brought revolutions to many mobile and embedded devices by providing human-level machine intelligence for various applications. However, high inference accuracy of such DNNs comes at high computational costs, and, hence, there have been significant efforts to reduce computational overheads of DNNs either by compressing off-the-shelf models or by designing a new small footprint DNN architecture tailored to resource constrained devices. One notable recent paradigm in designing small footprint DNN models is sharing parameters in several layers. However, in previous approaches, the parameter-sharing techniques have been applied to large deep networks, such as ResNet, that are known to have high redundancy. In this paper, we propose a parameter-sharing method for already parameter-efficient small networks such as ShuffleNetV2. In our approach, small templates are combined with small layer-specific parameters to generate weights. Our experiment results on ImageNet and CIFAR100 datasets show that our approach can reduce the size of parameters by 15%-35% of ShuffleNetV2 while achieving smaller drops in accuracies compared to previous parameter-sharing and pruning approaches. We further show that the proposed approach is efficient in terms of latency and energy consumption on modern embedded devices.\n",
      "심층신경망의 더블 프루닝 기법의 적용 및 성능 분석에 관한 연구 2020 ['Model Compression', 'Model Light Weight', 'Deep Learning', 'Pruning', 'Network-Slimming', '모델압축', '모델 경량화', '딥러닝', '프루닝', '네트워크 간소화'] 최근 인공지능 딥러닝 분야는 컴퓨팅 자원의 높은 연산량과 가격문제로 인해 상용화에 어려움이 존재했다. 본 논문은 더블 프루닝 기법을 적용하여 심층신경망 모델들과 다수의 데이터셋에서의 성능을 평가하고자 한다. 더블 프루닝은 기본의 네트워크 간소화(Network-Slimming)과 파라미터 프루닝(Parameter-Pruning)을 결합한다. 이는 기존의 학습에 중요하지 않는 매개변수를 절감하여 학습 정확도를 저해하지 않고 속도를 향상시킬 수 있다는 장점이 있다. 다양한 데이터셋 학습 이후에 프루닝 비율을 증가시켜, 모델의 사이즈를 감소시켰다. NetScore 성능 분석 결과 MobileNet-V3가 가장 성능이 높게 나타났다. 프루닝 이후의 성능은 Cifar 10 데이터셋에서 깊이 우선 합성곱 신경망으로 구성된 MobileNet-V3이 가장 성능이 높았고, 전통적인 합성곱 신경망으로 이루어진 VGGNet, ResNet또한 높은 폭으로 성능이 증가함을 확인하였다. Recently, the artificial intelligence deep learning field has been hard to commercialize due to the high computing power and the price problem of computing resources. In this paper, we apply a double pruning techniques to evaluate the performance of the in-depth neural network and various datasets. Double pruning combines basic Network-slimming and Parameter-prunning. Our proposed technique has the advantage of reducing the parameters that are not important to the existing learning and improving the speed without compromising the learning accuracy. After training various datasets, the pruning ratio was increased to reduce the size of the model.We confirmed that MobileNet-V3 showed the highest performance as a result of NetScore performance analysis. We confirmed that the performance after pruning was the highest in MobileNet-V3 consisting of depthwise seperable convolution neural networks in the Cifar 10 dataset, and VGGNet and ResNet in traditional convolutional neural networks also increased significantly.\n",
      "HS 코드 분류를 위한 CNN 기반의 추천 모델 개발 2020 ['HS'] None The current tariff return system requires tax officials to calculate tax amount by themselves and pay the tax amount on their own responsibility. In other words, in principle, the duty and responsibility of reporting payment system are imposed only on the taxee who is required to calculate and pay the tax accurately. In case the tax payment system fails to fulfill the duty and responsibility, the additional tax is imposed on the taxee by collecting the tax shortfall and imposing the tax deduction on For this reason, item classifications, together with tariff assessments, are the most difficult and could pose a significant risk to entities if they are misclassified. For this reason, import reports are consigned to customs officials, who are customs experts, while paying a substantial fee. The purpose of this study is to classify HS items to be reported upon import declaration and to indicate HS codes to be recorded on import declaration. HS items were classified using the attached image in the case of item classification based on the case of the classification of items by the Korea Customs Service for classification of HS items. For image classification, CNN was used as a deep learning algorithm commonly used for image recognition and Vgg16, Vgg19, ResNet50 and Inception-V3 models were used among CNN models. To improve classification accuracy, two datasets were created. Dataset1 selected five types with the most HS code images, and Dataset2 was tested by dividing them into five types with 87 Chapter, the most among HS code 2 units. The classification accuracy was highest when HS item classification was performed by learning with dual database2, the corresponding model was Inception-V3, and the ResNet50 had the lowest classification accuracy. The study identified the possibility of HS item classification based on the first item image registered in the item classification determination case, and the second point of this study is that HS item classification, which has not been attempted before, was attempted through the CNN model.\n",
      "수피 특징 추출을 위한 상용 DCNN 모델의 비교와 다층 퍼셉트론을 이용한 수종 인식 2020 ['Tree Species Identification', 'ResNet50', 'DCNN', 'MLP'] None None\n",
      "YOLO 네트워크를 활용한 전이학습 기반 객체 탐지 알고리즘 2020 ['Object Detection', 'Transfer Learning', 'Deep Learning', 'Image Processing'] None To guarantee AI model's prominent recognition rate and recognition precision, obtaining the large number of data is essential. In this paper, we propose transfer learning-based object detection algorithm for maintaining outstanding performance even when the volume of training data is small. Also, we proposed a tranfer learning network combining Resnet-50 and YOLO(You Only Look Once) network. The transfer learning network uses the Leeds Sports Pose dataset to train the network that detects the person who occupies the largest part of each images. Simulation results yield to detection rate as 84% and detection precision as 97%.\n",
      "손목 관절 단순 방사선 영상에서 딥 러닝을 이용한 전후방 및 측면 영상 분류와 요골 영역 분할 2020 ['Distal radius fractures', 'Deep learning', 'Classification', 'Segmentation', 'X-rays'] None The purpose of this study was to present the models for classifying the wrist X-ray images by types and for segmenting the radius automatically in each image using deep learning and to verify the learned models. The data were a total of 904 wrist X-rays with the distal radius fracture, consisting of 472 anteroposterior (AP) and 432 lateral images. The learning model was the ResNet50 model for AP/lateral image classification, and the U-Net model for segmentation of the radius. In the model for AP/lateral image classification, 100.0% was showed in precision, recall, and F1 score and area under curve (AUC) was 1.0. The model for segmentation of the radius showed an accuracy of 99.46%, a sensitivity of 89.68%, a specificity of 99.72%, and a Dice similarity coefficient of 90.05% in AP images and an accuracy of 99.37%, a sensitivity of 88.65%, a specificity of 99.69%, and a Dice similarity coefficient of 86.05% in lateral images. The model for AP/lateral classification and the segmentation model of the radius learned through deep learning showed favorable performances to expect clinical application.\n",
      "핵의학 감마카메라 정도관리의 딥러닝 적용 2020 ['핵의학', '정도관리', '인공지능', '콘볼루션 신경망', '딥러닝', 'Nuclear medicine', 'Quality Control', 'AI', 'CNN', 'Deep Learning'] None In the field of nuclear medicine, errors are sometimes generated because the assessment of the uniformity of gamma cameras relies on the naked eye of the evaluator. To minimize these errors, we created an artificial intelligence model based on CNN algorithm and wanted to assess its usefulness. We produced 20,000 normal images and partial cold region images using Python, and conducted artificial intelligence training with Resnet18 models. The training results showed that accuracy, specificity and sensitivity were 95.01%, 92.30%, and 97.73%, respectively. According to the results of the evaluation of the confusion matrix of artificial intelligence and expert groups, artificial intelligence was accuracy, specificity and sensitivity of 94.00%, 91.50%, and 96.80%, respectively, and expert groups was accuracy, specificity and sensitivity of 69.00%, 64.00%, and 74.00%, respectively. The results showed that artificial intelligence was better than expert groups. In addition, by checking together with the radiological technologist and AI, errors that may occur during the quality control process can be reduced, providing a better examination environment for patients, providing convenience to radiologists, and improving work efficiency.\n",
      "코로나바이러스 감염증19 데이터베이스에 기반을 둔 인공신경망 모델의 특성 평가 2020 ['알렉스넷', '흉부 방사선검사', '코로나바이러스감염증19', '심층학습', '인공신경망', 'AlexNet', 'Chest Radiography(CXR)', 'COVID-19', 'Deep-learning', 'Neural network'] None Coronavirus disease(COVID-19) is highly infectious disease that directly affects the lungs. To observe the clinical findings from these lungs, the Chest Radiography(CXR) can be used in a fast manner. However, the diagnostic performance via CXR needs to be improved, since the identifying these findings are highly time-consuming and prone to human error. Therefore, Artificial Intelligence(AI) based tool may be useful to aid the diagnosis of COVID-19 via CXR. In this study, we explored various Deep learning(DL) approach to classify COVID-19, other viral pneumonia and normal. For the original dataset and lung-segmented dataset, the pre-trained AlexNet, SqueezeNet, ResNet18, DenseNet201 were transfer- trained and validated for 3 class - COVID-19, viral pneumonia, normal. In the results, AlexNet showed the highest mean accuracy of 99.15±2.69% and fastest training time of 1.61±0.56 min among 4 pre-trained neural networks. In this study, we demonstrated the performance of 4 pre-trained neural networks in COVID-19 diagnosis with CXR images. Further, we plotted the class activation map(CAM) of each network and demonstrated that the lung-segmentation pre-processing improve the performance of COVID-19 classifier with CXR images by excluding background features.\n",
      "손글씨 인식 학습 모델 기반 수식 연산에 대한 연구 2020 ['딥러닝', '인식', '계산', 'Deep-Learning', 'Recognition', 'Calculation'] 정보화 기술의 발달로 인해 문서 작성이나 브라우저 검색기능 같은 작업들이 모두 키보드 타이핑만으로 가능하게 되었다. 하지만 수학 계산식 같은 경우에는 키보드 타이핑으로 작성하기가 어려운 것을 알 수 있다. 계산식 같은 경우는 키보드가 아닌 아날로그 식으로 본인이 직접 수기하는 것이 시간도 절약되고 타이핑하는 것보다 오히려 편하다는 것을 알 수 있다. 그러기에 수학 계산식을 검색해서 답을 찾으려 할 때 사용자들은 불편함을 느끼게 된다.본 논문은 컴퓨터가 학습한 딥 러닝 모델로 수기로 작성된 수학 수식을 텍스트 형태로 바꿀 때, 더 정확하게 인식하는 모델의 종류가 무엇인지 제공한다. 숫자와 사칙 연산 기호뿐만 아니라 사용자가 정의한 연산자로도 정확한 인식이 가능한지를 확인한다. 이를 위해 DenseNet, ResNet과 같은 CNN 모델 이용하여 실험을 수행하였고, 실험을 통하여 그 중 가장 적합한 모델을 찾아내었다. None\n",
      "결절성 폐암 검출을 위한 상용 및 맞춤형 CNN의 성능 비교 2020 ['Pulmonary Nodule', 'Computer Aided Detection', 'Deep Neural Network', 'Convolutional Neural Networ'] None Screening with low-dose spiral computed tomography (LDCT) has been shown to reduce lung cancer mortality by about 20% when compared to standard chest radiography. One of the problems arising from screening programs is that large amounts of CT image data must be interpreted by radiologists. To solve this problem, automated detection of pulmonary nodules is necessary; however, this is a challenging task because of the high number of false positive results. Here we demonstrate detection of pulmonary nodules using six off-the-shelf convolutional neural network (CNN) models after modification of the input/output layers and end-to-end training based on publicly databases for comparative evaluation. We used the well-known CNN models, LeNet-5, VGG-16, GoogLeNet Inception V3, ResNet-152, DensNet201, and NASNet. Most of the CNN models provided superior results to those of obtained using customized CNN models. It is more desirable to modify the proven off-the-shelf network model than to customize the network model to detect the pulmonary nodules.\n",
      "Potato Detection and Segmentation Based on Mask R-CNN 2020 ['Deep learning . Mask R-CNN . Potato detection . Potato segmentation'] None Purpose Potatoes are similar in color and size to soil and its clods. They are mostly irregular in the shape as well. Therefore, it is not easy to distinguish potatoes from the soil surface background only with machine vision. This study applied Mask R-CNN, one of the object recognition technologies using deep learning to detect potatoes. The size of object in pixel was obtained on individual potato, and they will be used to predict the yield of potatoes.Methods In order to collect the images needed for deep learning, potato images at the time of harvesting were obtained from potato farms. Annotation was entered for each irregular potato shape, where approximately 4500 potatoes were used. Resnet-101 was selected as the backbone of Mask R-CNN with a feature pyramid network. Transfer training was applied to shorten training time and limit the number of images needed to train the model. The classification performance evaluation was conducted to verify the trained model. The size of potato in pixel was obtained from the output image through the potato detection model by the segmentation algorithm using MATLAB.Results The total number of training for the potato detection model was 12,000, and the training loss of Mask R-CNN was less than 0.1%. The potato detection results from 69 randomly selected test images showed that the average detection precision was 90.8%, recall 93.0%, and F1 score 91.9%.Conclusions Potato detection model with Mask R-CNN can detect irregularly shaped potatoes on similar color soil surface. The size of the detected potato region can be extracted as well.\n",
      "기본 특징추출신경망에 따른 YOLO의 말벌인식 성능 평가 2020 ['Deep CNN', 'Vespa monitoring system', 'YOLOv2', 'Feature extraction layer', 'VGG19', 'Transfer learning'] None Real-time monitoring system for Vespa is necessary to reduce the damage to beekeeping farmers. In this paper, we compare and analyze the performance of the YOLO-based deep learning algorithms suitable for developing real-time automatic recognition and classification systems for Apis mellifera and five types of vespas such as V. velutina nigrithorax, V. mandarinia, V. ducalis, V. similima and V. crabro. YOLO has shown the best quality for real-time object detection due to its fast speed by replacing iterative object detection with once regression for an input image. However, the YOLO utilizes the conventional DCNN (deep convolutional neural network) algorithm to extract features from images thus, detection performance of the YOLO depends on the DCNN utilized. Therefore, we evaluate the object detection performance by changing the feature extraction layers of YOLOv2 with AlexNet, VGG19, GoogLeNet, and ResNet50 to find the best combination DCNN model for YOLO on the six bees above. The comparison results are as follows. In terms of the speed of detection and classification, the model in which YOLOv2 is combined with the AlexNet as a feature extraction layer showed the highest detection speed per second. This model can detect and classify 5 vespas and a bee from an average of 438 images per second, but has a relatively low accuracy of 71.7%. In terms of accuracy, the model that combines the feature extraction layer of VGG19 with YOLOv2 generated the highest accuracy, 83.2%. In addition, it can process an average of 135 images per second, enabling real-time processing of surveillance videos for wasps and bees. Therefore, we verified that the combined YOLO model with the VGG19 can be the best real-time monitoring system for the vespa detection.\n",
      "임베디드 보드에서의 CNN 모델 압축 및 성능 검증 2020 ['CNN', 'Neural Network Compression', 'Pruning', 'Matrix Decomposition', 'Embedded Board'] None Recently, deep neural networks such as CNN are showing excellent performance in various fields such as image classification, object recognition, visual quality enhancement, etc. However, as the model size and computational complexity of deep learning models for most applications increases, it is hard to apply neural networks to IoT and mobile environments. Therefore, neural network compression algorithms for reducing the model size while keeping the performance have been being studied. In this paper, we apply few compression methods to CNN models and evaluate their performances in the embedded environment. For evaluate the performance, the classification performance and inference time of the original CNN models and the compressed CNN models on the image inputted by the camera are evaluated in the embedded board equipped with QCS605, which is a customized AI chip. In this paper, a few CNN models of MobileNetV2, ResNet50, and VGG-16 are compressed by applying the methods of pruning and matrix decomposition. The experimental results show that the compressed models give not only the model size reduction of 1.3~11.2 times at a classification performance loss of less than 2% compared to the original model, but also the inference time reduction of 1.2~2.21 times, and the memory reduction of 1.2~3.8 times in the embedded board.\n",
      "다시점 영상 집합을 활용한 선체 블록 분류를 위한 CNN 모델 성능 비교 연구 2020 ['Multi-view image set(다시점 영상 집합)', 'Convolutional Neural Network(CNN', '합성곱신경망)', 'Ship hull block(선체 블록)', 'Classification(분류)', 'Data augmentation(데이터 확장)', 'Transfer learning(전이학습)'] None It is important to identify the location of ship hull blocks with exact block identification number when scheduling the shipbuilding process. The wrong information on the location and identification number of some hull block can cause low productivity by spending time to find where the exact hull block is. In order to solve this problem, it is necessary to equip the system to track the location of the blocks and to identify the identification numbers of the blocks automatically. There were a lot of researches of location tracking system for the hull blocks on the stockyard. However there has been no research to identify the hull blocks on the stockyard. This study compares the performance of 5 Convolutional Neural Network (CNN) models with multi-view image set on the classification of the hull blocks to identify the blocks on the stockyard. The CNN models are open algorithms of ImageNet Large-Scale Visual Recognition Competition (ILSVRC). Four scaled hull block models are used to acquire the images of ship hull blocks. Learning and transfer learning of the CNN models with original training data and augmented data of the original training data were done. 20 tests and predictions in consideration of five CNN models and four cases of training conditions are performed. In order to compare the classification performance of the CNN models, accuracy and average F1-Score from confusion matrix are adopted as the performance measures. As a result of the comparison, Resnet-152v2 model shows the highest accuracy and average F1-Score with full block prediction image set and with cropped block prediction image set.\n",
      "Empirical Comparison of Deep Learning Networks on Backbone Method of Human Pose Estimation 2020 ['Deep learning', 'human pose estimation', 'CNN', 'VGG', 'Resnet'] None Accurate estimation of human pose relies on backbone method in which its role is to extract feature map. Up to dated, the method of backbone feature extraction is conducted by the plain convolutional neural networks named by CNN and the residual neural networks named by Resnet, both of which have various architectures and performances. The CNN family network such as VGG which is well-known as a multiple stacked hidden layers architecture of deep learning methods, is base and simple while Resnet which is a bottleneck layers architecture yields fewer parameters and outperform. They have achieved inspired results as a backbone network in human pose estimation. However, they were used then followed by different pose estimation networks named by pose parsing module. Therefore, in this paper, we present a comparison between the plain CNN family network (VGG) and bottleneck network (Resnet) as a backbone method in the same pose parsing module. We investigate their performances such as number of parameters, loss score, precision and recall. We experiment them in the bottom-up method of human pose estimation system by adapted the pose parsing module of openpose. Our experimental results show that the backbone method using VGG network outperforms the Resent network with fewer parameter, lower loss score and higher accuracy of precision and recall.\n",
      "A Feasibility Study on Application of a Deep Convolutional Neural Network for Automatic Rock Type Classification 2020 ['Rock type classification', 'Deep learning', 'ResNet', 'Rock sample image dataset', '암종 분류', '딥러닝', '레즈넷', '암석 샘플 이미지 데이터셋'] 암종 분류은 현장의 지질학적 또는 지반공학적 특성 파악을 위해 요구되는 매우 기본적인 행위이나 암석의 성인, 지역, 지질학적 이력 특성에 따라 동일 암종이라 하여도 매우 다양한 형태와 색 조성을 보이므로 깊은 지질학적 학식과 경험 없이는 쉬운 일은 아니다. 또한, 다른 여러 분야의 분류 작업에서 딥러닝 영상처리 기법들이 성공적으로 적용되고 있으며, 지질학적 분류나 평가 분야에서도 딥러닝 기법의 적용에 대한 관심이 증대되고 있다. 따라서, 본 연구에서는 동일 암종임에도 다양한 형태와 색을 갖게 되는 실제 상황을 감안하여, 정확한 자동 암종 분류를 위한 딥러닝 기법의 적용 가능성에 대해 검토하였다. 이러한 기법은 향후에 현장 암종분류 작업을 수행하는 현장 기술자들을 지원할 수 있는 효과적인 툴로 활용 가능할 것이다. 본 연구에서 사용된 딥러닝 알고리즘은 매우 깊은 네트워크 구조로 객체 인식과 분류를 할 수 있는 것으로 잘 알려진 ’ResNet’ 계열의 딥러닝 알고리즘을 사용하였다. 적용된 딥러닝에서는 10개의 암종에 대한 다양한 암석 이미지들을 학습시켰으며, 학습 시키지 않은 암석 이미지들에 대하여 84% 수준 이상의 암종 분류 정확도를 보였다. 본 결과로 부터 다양한 성인과 지질학적 이력을 갖는 다양한 형태와 색의 암석들도 지질 전문가 수준으로 분류해 낼 수 있는 것으로 파악되었다. 나아가 다양한 지역과 현장에서 수집된 암석의 이미지와 지질학자들의 분류 결과가 학습데이터로 지속적으로 누적이 되어 재학습에 반영된다면 암종분류 성능은 자동으로 향상될 것이다. Rock classification is fundamental discipline of exploring geological and geotechnical features in a site, which, however, may not be easy works because of high diversity of rock shape and color according to its origin, geological history and so on. With the great success of convolutional neural networks (CNN) in many different image-based classification tasks, there has been increasing interest in taking advantage of CNN to classify geological material. In this study, a feasibility of the deep CNN is investigated for automatically and accurately identifying rock types, focusing on the condition of various shapes and colors even in the same rock type. It can be further developed to a mobile application for assisting geologist in classifying rocks in fieldwork. The structure of CNN model used in this study is based on a deep residual neural network (ResNet), which is an ultra-deep CNN using in object detection and classification. The proposed CNN was trained on 10 typical rock types with an overall accuracy of 84% on the test set. The result demonstrates that the proposed approach is not only able to classify rock type using images, but also represents an improvement as taking highly diverse rock image dataset as input.\n",
      "딥러닝을 이용한 컨베이어 시스템의 배출구 막힘 상태 판단 기술에 관한 연구 2020 ['컨베이어 시스템', '막힘 판단', '딥러닝', 'CNN', 'VGGNet', 'ResNet', 'DenseNet', 'NASNet', 'Conveyor Systems', 'Blockage Determination', 'Deep Learning', 'Convolutional Neural Network', 'Visual Geometry Group Network', 'Residual Network', 'Dense Network', 'Neural Architecture Search Network'] 본 연구는 컨베이어 시스템에서 딥러닝을 이용한 배출구 막힘 판단 기술에 대하여 제안한다.제안 방법은 산업 현장의 CCTV에서 수집한 영상을 이용하여 배출구 막힘 판단을 위한 다양한CNN 모델들을 학습시키고, 성능이 가장 좋은 모델을 사용하여 실제 공정에 적용하는 것을 목적으로 한다. CNN 모델로는 잘 알려진 VGGNet, ResNet, DenseNet, 그리고 NASNet을 사용하였으며, 모델 학습과 성능 테스트를 위하여 CCTV에서 수집한 18,000장의 영상을 이용하였다. 다양한 모델에 대한 실험 결과, VGGNet은 99.89%의 정확도와 29.05ms의 처리 시간으로 가장 좋은 성능을 보였으며, 이로부터 배출구 막힘 판단 문제에 VGGNet이 가장 적합함을 확인하였다. This study proposes a technique for the determination of outlet blockage using deep learning in a conveyor system. The proposed method aims to apply the best model to the actual process, where we train various CNN models for the determination of outlet blockage using images collected by CCTV in an industrial scene.We used the well-known CNN model such as VGGNet, ResNet, DenseNet and NASNet, and used 18,000 images collected by CCTV for model training and performance evaluation. As a experiment result with various models, VGGNet showed the best performance with 99.03% accuracy and 29.05ms processing time, and we confirmed that VGGNet is suitable for the determination of outlet blockage.\n",
      "블록 계층별 재학습을 이용한 다중 힌트정보 기반 지식전이 학습 2020 ['Multiple hint information', 'Block-wise retraining', 'Knowledge transfer', 'Deep learning', 'Residual network'] None In this paper, we propose a stage-wise knowledge transfer method that uses block-wise retraining to transfer the useful knowledge of a pre-trained residual network (ResNet) in a teacher-student framework (TSF). First, multiple hint information transfer and block-wise supervised retraining of the information was alternatively performed between teacher and student ResNet models. Next, Softened output information-based knowledge transfer was additionally considered in the TSF. The results experimentally showed that the proposed method using multiple hint-based bottom-up knowledge transfer coupled with incremental block-wise retraining provided the improved student ResNet with higher accuracy than existing KD and hint-based knowledge transfer methods considered in this study.\n",
      "Research on Crack Segmentation Method of Hydro-Junction Project Based on Target Detection Network 2020 ['Hydro-junction project', 'Faster-RCNN', 'Inception Resnet V2', 'Data augmentation', 'K-means'] None The defect detection is an important task for maintaining the hydro-junction project. A two-stage crack defect segmentation method based on target detection network is proposed to solve the problem of severe brightness imbalance and large noise in dam surface images. In the first stage, to improve the ability to locate crack areas, Inception Resnet V2 is used as feature extraction network to help Faster-RCNN extract more effective deep features, and the brightness, contrast of image is randomly adjusted before training. In the second segmentation stage, the crack areas are segmented at pixel-level using K-means. The experimental results on the self-made crack image dataset show that the location accuracy (AP) of the crack areas can be improved by 1.9%, reaching 96.8%, compared with other segmentation networks that do not locate crack areas, the intersection over union for segmentation of cracks (Iou) of the final segmentation results is at least 9.4% higher, reaching 52.7%. This method can provide effective technical support for inspection work of hydro-junction project.\n",
      "머신러닝을 사용한 탄성파 자료 보간법 기술 연구 동향 분석 2020 ['탄성파 자료 보간', '머신러닝', '서포트 벡터 머신', '유넷', '잔차넷', '생성적 적대 신경망', 'seismic data interpolation', 'machine learning', 'support vector machine', 'U-Net', 'ResNet', 'GAN'] 탄성파 탐사를 수행할 때 경제적, 환경적 제약 또는 탐사 장비의 문제 등에 의해 탄성파 자료의 일부가 규칙적또는 불규칙적으로 손실되는 경우가 발생하게 된다. 이러한 자료 손실은 탄성파 자료 처리와 해석 결과에 부정적인 영향을 주기 때문에 사라진 탄성파 자료를 복원할 필요가 있다. 탄성파 자료 복원을 위해 재탐사 또는 추가적인 탐사를 진행하는 경우 시간적, 경제적 비용이 발생하기 때문에, 많은 연구자들이 사라진 탄성파 자료를 정확히 복원하기 위한 보간 기법 연구를 진행해왔다. 최근에는 머신러닝 기술 발달에 따라 머신러닝 기법을 활용한 연구들이 진행되고 있고, 다양한 머신러닝 기술들 중에서도 서포트 벡터 회귀, 오토인코더, 유넷, 잔차넷, 생성적 적대 신경망 등의 알고리즘을 활용한 탄성파 자료의 보간 연구가 활발하게 진행되고 있다. 이 논문에서는 이러한 연구들을 조사하고 분석하여 복잡한 신경망 모델뿐 아니라 상대적으로 구조가 간단한 서포트 벡터 회귀 모델을 통해서도 뛰어난 보간 결과를 얻을 수 있다는 것을 확인했다. 추후 머신러닝 기법들을 사용하는 탄성파 자료 보간 연구들에서 오픈소스로 공개된 실제 자료를 이용하며 데이터증식, 전이학습, 기존 기법을 이용한 규제 등의 기술을 활용하면 탄성파 자료 보간 성능을 향상시킬 수 있을 것으로 기대된다. We acquire seismic data with regularly or irregularly missing traces, due to economic, environmental, and mechanical problems. Since these missing data adversely affect the results of seismic data processing and analysis, we need to reconstruct the missing data before subsequent processing. However, there are economic and temporal burdens to conducting further exploration and reconstructing missing parts. Many researchers have been studying interpolation methods to accurately reconstruct missing data. Recently, various machine learning technologies such as support vector regression, autoencoder, U-Net, ResNet, and generative adversarial network (GAN) have been applied in seismic data interpolation. In this study, by reviewing these studies, we found that not only neural network models, but also support vector regression models that have relatively simple structures can interpolate missing parts of seismic data effectively.We expect that future research can improve the interpolation performance of these machine learning models by using open-source field data, data augmentation, transfer learning, and regularization based on conventional interpolation technologies.\n",
      "ONNX기반 스파이킹 심층 신경망 변환 도구 2020 ['Deep neural network', 'ONNX', 'Spiking neural network'] 스파이킹 신경망은 기존 신경망과 다른 메커니즘으로 동작한다. 기존 신경망은 신경망을 구성하는 뉴런으로 들어 오는 입력 값에 대해 생물학적 메커니즘을 고려하지 않은 활성화 함수를 거쳐 다음 뉴런으로 출력 값을 전달한다. 뿐만 아니라 VGGNet, ResNet, SSD, YOLO와 같은 심층 구조를 사용한 좋은 성과들이 있었다. 반면 스파이킹 신경망은 기존 활성화함수 보다 실제 뉴런의 생물학적 메커니즘과 유사하게 동작하는 방식이지만 스파이킹 뉴런을 사용한 심층 구조에 대한 연구는 기존 뉴런을 사용한 심층 신경망과 비교해 활발히 진행되지 않았다. 본 논문은 기존 뉴런으로 만들어 진 심층 신경망 모델을 변환 툴에 로드하여 기존 뉴런을 스파이킹 뉴런으로 대체하여 스파이킹 심층 신경망으로 변환하 는 방법에 대해 제안한다. The spiking neural network operates in a different mechanism than the existing neural network. The existing neural network transfers the output value to the next neuron via an activation function that does not take into account the biological mechanism for the input value to the neuron that makes up the neural network. In addition, there have been good results using deep structures such as VGGNet, ResNet, SSD and YOLO. spiking neural networks, on the other hand, operate more like the biological mechanism of real neurons than the existing activation function, but studies of deep structures using spiking neurons have not been actively conducted compared to in-depth neural networks using conventional neurons. This paper proposes the method of loading an deep neural network model made from existing neurons into a conversion tool and converting it into a spiking deep neural network through the method of replacing an existing neuron with a spiking neuron.\n",
      "Development of A Uniform And Casual Clothing Recognition System For Patient Care In Nursing Hospitals 2020 ['Nursing Hospital', 'Patient Uniform', 'Casual Clothing', 'Clothing Recognition', '요양병원', '환자복', '평상복', '의복 인식'] 본 연구의 목적은 요양병원에서 발생할 수 있는 노인안전사고 발생률을 감소시키는 것이다. 즉, 위험지역으로 접근하는 인물이 노인(환자복) 그룹인지 실무자(평상복) 그룹인지를 CCTV에 나타나는 의복을 기준으로 구별하는 것이다. Web Crawling기법과 요양병원으로부터 지원을 받아 기초데이터를 수집하였다. 이후 Image Generator와 Labeling으로 모델 학습 데이터를 만들었다. CCTV의 제한된 성능 때문에 높은 정확도와 속도를 모두 갖춘 모델을 만드는 것은 어려웠다. 그러므로 정확성이 상대적으로 우수한 ResNet 모델, 속도에서 상대적으로 우수한 YOLO3 모델을 각각 구현했다. 그리고 요양병원이 자신의 실정에 맞는 모델을 고를 수 있게 하고자 했다. 연구 결과 환자복과 평상복을 적절한 정확도로 구별할 수 있는 모델을 구현하였다. 따라서 실제 사용처에서 노인들이 위험구역에 접근하지 못하도록 하여 요양병원 안전사고 감소에 이바지 할 것으로 평가된다. The purpose of this paper is to reduce the ratio of the patient accidents that may occur in nursing hospitals. In other words, it determines whether the person approaching the dangerous area is a elderly (patient uniform) group or a practitioner(Casual Clothing) group, based on the clothing displayed by CCTV. We collected the basic learning data from web crawling techniques and nursing hospitals. Then model training data was created with Image Generator and Labeling program. Due to the limited performance of CCTV, it is difficult to create a good model with both high accuracy and speed. Therefore, we implemented the ResNet model with relatively excellent accuracy and the YOLO3 model with relatively excellent speed. Then we wanted to allow nursing hospitals to choose a model that they wanted. As a result of the study, we implemented a model that can distinguish patient and casual clothes with appropriate accuracy. Therefore, it is believed that it will contribute to the reduction of safety accidents in nursing hospitals by preventing the elderly from accessing the danger zone.\n",
      "Enhancement of Tongue Segmentation by Using Data Augmentation 2020 ['Data augmentation', 'Deep Learning', 'Tongue segmentation', 'Transfer learning'] 많은 양의 데이터는 딥 러닝 모델의 견고성을 향상시키고 과적합 문제를 방지할 수 있게 해준다. 자동 혀 분할에서, 혀 영상 데이터 세트를 실제로 수집하고 라벨링하는 데에는 많은 어려움이 수반되므로 많은 양의 혀 영상 데이터를 사용하기 쉽지 않다. 데이터 증강은 새로운 데이터를 수집하지 않고 레이블 보존 변환을 사용하여 학습 데이터 세트를 확장하고 학습 데이터의 다양성을 증가시킬 수 있다. 이 논문에서는 이미지 자르기, 회전, 뒤집기, 색상 변환과 같은 7 가지 데이터 증강 방법을 사용하여 확장된 혀 영상 학습 데이터 세트를 생성하였다. 데이터 증강 방법의 성능을 확인하기 위하여 InceptionV3, EfficientNet, ResNet, DenseNet 등과 같은 전이 학습 모델을 사용하였다. 실험 결과 데이터 증강 방법을 적용함으로써 혀 분할의 정확도를 5~20% 향상시켰으며 기하학적 변환이 색상 변환보다 더 많은 성능 향상을 가져올 수 있음을 보여주었다. 또한 기하학적 변환 및 색상 변환을 임의로 선형 조합한 방법이 다른 데이터 증강 방법보다 우수한 분할 성능을 제공하여 InveptionV3 모델을 사용한 경우에 94.98 %의 정확도를 보였다. A large volume of data will improve the robustness of deep learning models and avoid overfitting problems. In automatic tongue segmentation, the availability of annotated tongue images is often limited because of the difficulty of collecting and labeling the tongue image datasets in reality. Data augmentation can expand the training dataset and increase the diversity of training data by using label-preserving transformations without collecting new data. In this paper, augmented tongue image datasets were developed using seven augmentation techniques such as image cropping, rotation, flipping，color transformations. Performance of the data augmentation techniques were studied using state-of-the-art transfer learning models, for instance, InceptionV3, EfficientNet, ResNet, DenseNet and etc. Our results show that geometric transformations can lead to more performance gains than color transformations and the segmentation accuracy can be increased by 5% to 20% compared with no augmentation. Furthermore, a random linear combination of geometric and color transformations augmentation dataset gives the superior segmentation performance than all other datasets and results in a better accuracy of 94.98% with InceptionV3 models.\n",
      "인공신경망 모델 압축을 위한 적응적 양자화 기반 지식 증류 기법 2020 ['Adaptive quantization', 'Knowledge distillation', 'Model compression'] None None\n",
      "딥러닝 기반 교량 구성요소 자동 분류 2020 ['BIM', '교량 구성요소 분류', '딥러닝', 'CNN', 'BIM', 'Bridge component classification', 'Deep Learning', 'CNN'] 최근 BIM (Building Information Modeling)이 건설 산업계에서 폭넓게 활용되고 있다. 하지만 과거에 시공이 된 구조물에 경우 대부분 BIM이 구축되어 있지 않다. BIM이 구축되지 않은 구조물의 경우, 카메라로부터 얻은 2D 이미지에 SfM (Structure from Motion) 기법을 활용하면 3D 모델의 점군 데이터(Point cloud)를 생성하고 BIM을 구축할 수 있다. 하지만 이렇게 생성된 점군 데이터는 의미론적 정보가 포함되어 있지 않기 때문에, 수작업으로 구조물의 어떤 요소인지 분류해 주어야 한다. 따라서 본 연구에서는 구조물 구성요소를 분류하는 과정을 자동화하기 위하여 딥러닝을 적용하였다. 딥러닝 네트워크 구축에는 CNN (Convolutional Neural Network) 구조의 Inception-ResNet-v2를 사용하였고, 전이학습을 통하여 교량 구조물의 구성요소를 학습하였다. 개발된 시스템을 검증하기 위하여 수집한 데이터를 이용하여 구성요소를 분류한 결과, 교량의 구성요소를 96.13 %의 정확도로 분류할 수 있었다. Recently, BIM (Building Information Modeling) are widely being utilized in Construction industry. However, most structures that have been constructed in the past do not have BIM. For structures without BIM, the use of SfM (Structure from Motion) techniques in the 2D image obtained from the camera allows the generation of 3D model point cloud data and BIM to be established. However, since these generated point cloud data do not contain semantic information, it is necessary to manually classify what elements of the structure. Therefore, in this study, deep learning was applied to automate the process of classifying structural components. In the establishment of deep learning network, Inception-ResNet-v2 of CNN (Convolutional Neural Network) structure was used, and the components of bridge structure were learned through transfer learning. As a result of classifying components using the data collected to verify the developed system, the components of the bridge were classified with an accuracy of 96.13 %.\n",
      "심층 신경망 기반의 앙상블 방식을 이용한 토마토 작물의 질병 식별 2020 ['Crop Disease Classification', 'Ensemble Approach', 'Deep Neural Network'] None None\n",
      "PET-CT 영상 알츠하이머 분류에서 유전 알고리즘 이용한 심층학습 모델 최적화 2020 ['Alzheimer’s Disease Classification', 'Genetic Algorithm', 'Deep Learning', 'ResNet'] None None\n",
      "자율주행을 위한 딥러닝 기반의 차선 검출 방법에 관한 연구 2020 ['Deep learning', 'Faster R-CNN', 'Machine learning', 'Support vector machine', 'Unmanned vehicle'] None This study used the Deep Learning models used in previous studies, we selected the basic model. The selected model was selected as ZFNet among ZFNet, Googlenet and ResNet, and the object was detected using a ZFNet based FRCNN. In order to reduce the detection error rate of FRCNN, location of four types of objects detected inside the image was designed by SVM classifier and location-based filtering was applied. As simulation results, it showed similar performance to the lane marking classification method with conventional 경계 detection, with an average accuracy of about 88.8%. In addition, studies using the Linear-parabolic Model showed a processing speed of 165.65ms with a minimum resolution of 600 × 800, but in this study, the resolution was treated at about 33ms with an input resolution image of 1280 × 960, so it was possible to classify lane marking at a faster rate than the previous study by CNN-based End to End method.\n",
      "Experiment on Intermediate Feature Coding for Object Detection and Segmentation 2020 ['Deep learning', 'intermediate features', 'video coding for machine', 'object detection', 'object segmentation'] None With the recent development of deep learning, most computer vision-related tasks are being solved with deep learning-based network technologies such as CNN and RNN. Computer vision tasks such as object detection or object segmentation use intermediate features extracted from the same backbone such as Resnet or FPN for training and inference for object detection and segmentation. In this paper, an experiment was conducted to find out the compression efficiency and the effect of encoding on task inference performance when the features extracted in the intermediate stage of CNN are encoded. The feature map that combines the features of 256 channels into one image and the original image were encoded in HEVC to compare and analyze the inference performance for object detection and segmentation. Since the intermediate feature map encodes the five levels of feature maps (P2 to P6), the image size and resolution are increased compared to the original image. However, when the degree of compression is weakened, the use of feature maps yields similar or better inference results to the inference performance of the original image.\n",
      "딥러닝 기반 가상공간에서의 손 제스처 인식 2020 ['딥러닝', '가상 공간', 'CNN', '손 제스쳐 인식', '사용자 인터페이스', 'Deep Learning', 'Virtual Space', 'Hand Gesture Recognition', 'User Interface'] None In this paper, we define static gestures and dynamic gestures to be used as a user interface in a virtual space, and propose a method to extract features using deep learning models and to recognize hand gestures input through RGB camera in order to improve the price and recognition speed of the existing virtual / augmented reality interface device. Through various deep learning models, we learned the data in various ways and extracted the features to recognize hand gestures. Deep learning models used are Faster-RCNN, ResNet, U-Net, and 3D-CNN. Since we recognize hand gestures in the virtual space and use them as user interfaces, we want to contribute to using virtual / augmented reality through high recognition rates and fast recognition speeds without the help of specific sensors or wearable devices.\n",
      "딥러닝 표정 인식을 활용한 실시간 온라인 강의 이해도 분석 2020 ['Degree of Understanding', 'Real-time Analysis', 'Face Detection', 'Facial Expression Recognition', 'Deep Learning'] None Due to the spread of COVID-19, the online lecture has become more prevalent. However, it was found that a lot of students and professors are experiencing lack of communication. This study is therefore designed to improve interactive communication between professors and students in real-time online lectures. To do so, we explore deep learning approaches for automatic recognition of students' facial expressions and classification of their understanding into 3 classes (Understand / Neutral / Not Understand). We use 'BlazeFace' model for face detection and 'ResNet-GRU' model for facial expression recognition (FER). We name this entire process 'Degree of Understanding (DoU)' algorithm. DoU algorithm can analyze a multitude of students collectively and present the result in visualized statistics. To our knowledge, this study has great significance in that this is the first study offers the statistics of understanding in lectures using FER. As a result, the algorithm achieved rapid speed of 0.098sec/frame with high accuracy of 94.3% in CPU environment, demonstrating the potential to be applied to real-time online lectures. DoU Algorithm can be extended to various fields where facial expressions play important roles in communications such as interactions with hearing impaired people.\n",
      "정비 자료 디지털 변환을 위한 영상 인식 알고리듬 : CNN and FCN 2020 ['Tabular Maintenance Data(정비 자료표)', 'Digitization(디지털화)', 'CNN(합성곱 신경망)', 'FCN(완전 연결망)'] None Tabulated data has been widely used to facilitate systematic and intuitive management. In particular, tabular images that contain a few simple symbols are useful for maintaining mechanical systems. Several companies have accumulated tabular images as their property. Although these images are valuable as they can be used to solve difficult problems using data-based methods, such as deep learning, they still remain unavailable because it is expensive to digitize them. For these reasons, we propose a model comprised of a convolutional neural network (CNN) and fully convolutional network (FCN) to digitize tabular images. We used some ResNet components as they are well-suited to the characteristics of tabular image data. A training set for each model was constructed by writing symbols in blank tables and then augmenting them. As a result, the trained CNN and FCN models exhibited 99.2 % and 97.7 % accuracy in 4.75 s and 0.132 s of inference time, respectively.\n",
      "임베디드 연산을 위한 잡음에서 음성추출 U-Net 설계 2020 ['Speech enhancement', 'Noise reduction', 'Deep noise suppression', 'Deep neural network', 'wav-U-Net'] None In this paper, we propose wav-U-Net to improve speech enhancement in heavy noisy environments, and it has implemented three principal techniques. First, as input data, we use 128 modified Mel-scale filter banks which can reduce computational burden instead of 512 frequency bins. Mel-scale aims to mimic the non-linear human ear perception of sound by being more discriminative at lower frequencies and less discriminative at higher frequencies. Therefore, Mel-scale is the suitable feature considering both performance and computing power because our proposed network focuses on speech signals. Second, we add a simple ResNet as pre-processing that helps our proposed network make estimated speech signals clear and suppress high-frequency noises. Finally, the proposed U-Net model shows significant performance regardless of the kinds of noise. Especially, despite using a single channel, we confirmed that it can well deal with non-stationary noises whose frequency properties are dynamically changed, and it is possible to estimate speech signals from noisy speech signals even in extremely noisy environments where noises are much lauder than speech (less than SNR 0dB).The performance on our proposed wav-U-Net was improved by about 200% on SDR and 460% on NSDR compared to the conventional Jansson’s wav-U-Net. Also, it was confirmed that the processing time of out wav-U-Net with 128 modified Mel-scale filter banks was about 2.7 times faster than the common wav-U-Net with 512 frequency bins as input values.\n",
      "딥러닝 기반 치과 의료영상 판독에 대한 문헌 분석 2020 ['Dentistry', 'Dental disease', 'Artificial intelligence', 'Convolutional neural network', 'Object detection', 'Segmentation'] None This study analyzes the papers, which studied to find the most adequate CNN based algorithms for segmentation, object detection in dentistry. According to our purpose, we created several keywords like “Dental+Object Detection+Neural+Network.” We searched articles in ‘PubMed’, ‘IEEE’, using created 34 keywords. We found 458 papers and excluded under a study-purpose provision. So This paper had categorized those 23 papers by 11 of segmentation of tooth structure with dental filling and FDI numbering, 12 of detecting dental caries, periodontitis, or multiple lesions. To compare the performance of models, we organized the results by DICE/IoU index and accuracy, precision, recall, etc.. Various dataset was used for analyzing. The most common dataset was dental panoramic image, then periapical, CBCT, NILT, and intra-oral image. The algorithms were used according to the purpose. For example, VGG16, 19 was used for object detection algorithms were used according to the purpose. For example, VGG16, 19 was used for object detection, U-Net, and Mask R-CNN used for segmentation by study purpose.For segmentation of teeth, Zhimming Cui(2019), used Mask R-CNN, and the accuracy was 0.9755. Vranck(2020) used ResNet for molar detection(IoU 0.9, precision 0.94, 0.93). To label the tooth numbering according to FDI rule, Tuzoff(2019) and Chen(2019), used Faster R-CNN, VGG16, and Faster R-CNN with DNN. Tuzoff’s index was slightly better than Chen’s. Casalegno(2019) investigated the detection of dental caries by using VGG16. The result was IoU 0.727. To find periodontitis, used VGG16 also, by Prajapaty(2017). And the accuracy was 0.8846. Using the Mask R-CNN, Jader(2018) could separate instances of multiple lesions, accuracy was 0.8846.\n",
      "Automatic detection of periodontal compromised teeth in digital panoramic radiographs using faster regional convolutional neural networks 2020 ['Alveolar Bone Loss', 'Panoramic Radiography', 'Artificial Intelligence', 'Deep Learning'] None Purpose: Periodontal disease causes tooth loss and is associated with cardiovascular diseases, diabetes, and rheumatoid arthritis. The present study proposes using a deep learning-based object detection method to identify periodontally compromised teeth on digital panoramic radiographs. A faster regional convolutional neural network (faster R-CNN) which is a state-of-the-art deep detection network, was adapted from the natural image domain using a small annotated clinical data- set.Materials and Methods: In total, 100 digital panoramic radiographs of periodontally compromised patients were retrospectively collected from our hospital’s information system and augmented. The periodontally compromised teeth found in each image were annotated by experts in periodontology to obtain the ground truth. The Keras library, which is written in Python, was used to train and test the model on a single NVidia 1080Ti GPU. The faster R-CNN model used a pretrained ResNet architecture.Results: The average precision rate of 0.81 demonstrated that there was a significant region of overlap between the predicted regions and the ground truth. The average recall rate of 0.80 showed that the periodontally compromised teeth regions generated by the detection method excluded healthiest teeth areas. In addition, the model achieved a sensitivity of 0.84, a specificity of 0.88 and an F-measure of 0.81.Conclusion: The faster R-CNN trained on a limited amount of labeled imaging data performed satisfactorily in detecting periodontally compromised teeth. The application of a faster R-CNN to assist in the detection of periodontally compromised teeth may reduce diagnostic effort by saving assessment time and allowing automated screening documentation.\n",
      "정규화 기법을 통한 안면 인식 알고리즘 성능 향상에 관한 연구 2020 ['Computer Vision', 'Deep Learning', 'Face Recognition', 'Safety Management'] None Through the combination of computer vision technology and artificial intelligence, facial recognition technology is drawing attention as a new means of personal authentication in the era of the fourth industry. Facial recognition technology uses imaging equipment to photograph a person\"s face and extract characteristic data. The extracted data are matched against the facial features of the stored database. Facial recognition technology is a contactless technology compared to other biometric recognition technologies, which is used in various fields due to its high hygiene, convenience and security, and in particular, safety accidents in workplaces are closely related to life, and various studies related to workplace safety management using intelligent video information are being conducted in the manufacturing industry. In this paper, a study is conducted on the development of facial recognition algorithm using deep learning to control worker access in hazardous areas. The accuracy of the recognition of the proposed facial recognition algorithm (object detection algorithm (SSD) and object recognition algorithm (ResNet)) is closely related to the safety of the operator. Therefore, the goal is to analyze the relationship between various normalization techniques (Min-Max Scaler, MaxAbs Scaler, Standard Scaler) and the recognition rate of the proposed facial recognition algorithm to propose a high-accuracy facial recognition algorithm. In the future, we will conduct research on safety issues in the manufacturing industry based on facial recognition and image recognition technologies.\n",
      "Automatic detection of periodontal compromised teeth in digital panoramic radiographs using faster regional convolutional neural networks 2020 ['Alveolar Bone Loss', 'Panoramic Radiography', 'Artificial Intelligence', 'Deep Learning'] None Purpose: Periodontal disease causes tooth loss and is associated with cardiovascular diseases, diabetes, and rheumatoid arthritis. The present study proposes using a deep learning-based object detection method to identify periodontally compromised teeth on digital panoramic radiographs. A faster regional convolutional neural network (faster R-CNN) which is a state-of-the-art deep detection network, was adapted from the natural image domain using a small annotated clinical data- set. Materials and Methods: In total, 100 digital panoramic radiographs of periodontally compromised patients were retrospectively collected from our hospital's information system and augmented. The periodontally compromised teeth found in each image were annotated by experts in periodontology to obtain the ground truth. The Keras library, which is written in Python, was used to train and test the model on a single NVidia 1080Ti GPU. The faster R-CNN model used a pretrained ResNet architecture. Results: The average precision rate of 0.81 demonstrated that there was a significant region of overlap between the predicted regions and the ground truth. The average recall rate of 0.80 showed that the periodontally compromised teeth regions generated by the detection method excluded healthiest teeth areas. In addition, the model achieved a sensitivity of 0.84, a specificity of 0.88 and an F-measure of 0.81. Conclusion: The faster R-CNN trained on a limited amount of labeled imaging data performed satisfactorily in detecting periodontally compromised teeth. The application of a faster R-CNN to assist in the detection of periodontally compromised teeth may reduce diagnostic effort by saving assessment time and allowing automated screening documentation.\n",
      "Effective Hand Gesture Recognition by Key Frame Selection and 3D Neural Network 2020 ['hand gesture recognition', 'dynamic hand gesture', 'key frame extraction', 'action recognition'] None This paper presents an approach for dynamic hand gesture recognition by using algorithm based on 3D Convolutional Neural Network (3D_CNN), which is later extended to 3D Residual Networks (3D_ResNet), and the neural network based key frame selection. Typically, 3D deep neural network is used to classify gestures from the input of image frames, randomly sampled from a video data. In this work, to improve the classification performance, we employ key frames which represent the overall video, as the input of the classification network. The key frames are extracted by SegNet instead of conventional clustering algorithms for video summarization (VSUMM) which require heavy computation. By using a deep neural network, key frame selection can be performed in a real-time system. Experiments are conducted using 3D convolutional kernels such as 3D_CNN, Inflated 3D_CNN (I3D) and 3D_ResNet for gesture classification. Our algorithm achieved up to 97.8% of classification accuracy on the Cambridge gesture dataset. The experimental results show that the proposed approach is efficient and outperforms existing methods.\n",
      "2020년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                                                                        title  \\\n",
      "0                                                                                            SSD-Mobilenet과 ResNet을 이용한 모바일 기기용 자동차 번호판 인식시스템   \n",
      "1                                                                                                       ResNet 알고리즘을 이용한 가로수 객체의 폐색영역 검출 및 해결   \n",
      "2                                                                                      Crack detection based on ResNet with spatial attention   \n",
      "3                                                                                         Classroom Roll-Call System Based on ResNet Networks   \n",
      "4                             An Experimental Comparison of CNN-based Deep Learning Algorithms for Recognition of Beauty-related Skin Disease   \n",
      "5                                                                                              다양한 합성곱 신경망 방식을 이용한 모바일 기기를 위한 시작 단어 검출의 성능 비교   \n",
      "6                                                                                                               조류 울음소리를 이용한 조류 분류 딥러닝 시스템 개발   \n",
      "7                                                                                                                           변형된 잔차블록을 적용한 CNN   \n",
      "8                                                                                                         Weather Recognition Based on 3C-CNN   \n",
      "9                                                                                                              탄성파 층서 구분을 위한 합성곱 신경망 기법 비교 연구   \n",
      "10                                                                                                  사전 학습된 네트워크 모델을 이용한 심전도 신호 기반 개인 식별 성능 분석   \n",
      "11                                                                                                          상처와 주름이 있는 지문 판별에 효율적인 심층 학습 비교연구   \n",
      "12                                                                                                           콘크리트 균열 탐지를 위한 딥 러닝 기반 CNN 모델 비교   \n",
      "13                                                                                  딥러닝 알고리즘을 이용한 토마토에서 발생하는 여러가지 병해충의 탐지와 식별에 대한 웹응용 플렛폼의 구축   \n",
      "14                                                                                                  내시경의 위암과 위궤양 영상을 이용한 합성곱 신경망 기반의 자동 분류 모델   \n",
      "15                                                                                                  딥러닝을 활용한 실내 식물 이미지 분류 및 식물 정보 제공 웹 어플리케이션   \n",
      "16                                                      Masked cross self-attentive encoding based speaker embedding for speaker verification   \n",
      "17                                                                                                                      딥러닝 기반 소나무 재선충 피해목 탐색   \n",
      "18                                                                                               딥러닝 기반 지하공동구 화재 탐지 모델 개발 : 학습데이터 보강 및 편향 최적화   \n",
      "19                                                                                                              템플릿 재사용을 통한 패러미터 효율적 신경망 네트워크   \n",
      "20                                                                                                        심층신경망의 더블 프루닝 기법의 적용 및 성능 분석에 관한 연구   \n",
      "21                                                                                                              HS 코드 분류를 위한 CNN 기반의 추천 모델 개발   \n",
      "22                                                                                            수피 특징 추출을 위한 상용 DCNN 모델의 비교와 다층 퍼셉트론을 이용한 수종 인식   \n",
      "23                                                                                                          YOLO 네트워크를 활용한 전이학습 기반 객체 탐지 알고리즘   \n",
      "24                                                                                       손목 관절 단순 방사선 영상에서 딥 러닝을 이용한 전후방 및 측면 영상 분류와 요골 영역 분할   \n",
      "25                                                                                                                     핵의학 감마카메라 정도관리의 딥러닝 적용   \n",
      "26                                                                                                코로나바이러스 감염증19 데이터베이스에 기반을 둔 인공신경망 모델의 특성 평가   \n",
      "27                                                                                                               손글씨 인식 학습 모델 기반 수식 연산에 대한 연구   \n",
      "28                                                                                                          결절성 폐암 검출을 위한 상용 및 맞춤형 CNN의 성능 비교   \n",
      "29                                                                                      Potato Detection and Segmentation Based on Mask R-CNN   \n",
      "30                                                                                                            기본 특징추출신경망에 따른 YOLO의 말벌인식 성능 평가   \n",
      "31                                                                                                               임베디드 보드에서의 CNN 모델 압축 및 성능 검증   \n",
      "32                                                                                                다시점 영상 집합을 활용한 선체 블록 분류를 위한 CNN 모델 성능 비교 연구   \n",
      "33                                                 Empirical Comparison of Deep Learning Networks on Backbone Method of Human Pose Estimation   \n",
      "34                           A Feasibility Study on Application of a Deep Convolutional Neural Network for Automatic Rock Type Classification   \n",
      "35                                                                                                  딥러닝을 이용한 컨베이어 시스템의 배출구 막힘 상태 판단 기술에 관한 연구   \n",
      "36                                                                                                         블록 계층별 재학습을 이용한 다중 힌트정보 기반 지식전이 학습   \n",
      "37                                          Research on Crack Segmentation Method of Hydro-Junction Project Based on Target Detection Network   \n",
      "38                                                                                                           머신러닝을 사용한 탄성파 자료 보간법 기술 연구 동향 분석   \n",
      "39                                                                                                                   ONNX기반 스파이킹 심층 신경망 변환 도구   \n",
      "40                                      Development of A Uniform And Casual Clothing Recognition System For Patient Care In Nursing Hospitals   \n",
      "41                                                                              Enhancement of Tongue Segmentation by Using Data Augmentation   \n",
      "42                                                                                                        인공신경망 모델 압축을 위한 적응적 양자화 기반 지식 증류 기법   \n",
      "43                                                                                                                       딥러닝 기반 교량 구성요소 자동 분류   \n",
      "44                                                                                                       심층 신경망 기반의 앙상블 방식을 이용한 토마토 작물의 질병 식별   \n",
      "45                                                                                               PET-CT 영상 알츠하이머 분류에서 유전 알고리즘 이용한 심층학습 모델 최적화   \n",
      "46                                                                                                           자율주행을 위한 딥러닝 기반의 차선 검출 방법에 관한 연구   \n",
      "47                                                            Experiment on Intermediate Feature Coding for Object Detection and Segmentation   \n",
      "48                                                                                                                    딥러닝 기반 가상공간에서의 손 제스처 인식   \n",
      "49                                                                                                           딥러닝 표정 인식을 활용한 실시간 온라인 강의 이해도 분석   \n",
      "50                                                                                                  정비 자료 디지털 변환을 위한 영상 인식 알고리듬 : CNN and FCN   \n",
      "51                                                                                                             임베디드 연산을 위한 잡음에서 음성추출 U-Net 설계   \n",
      "52                                                                                                                딥러닝 기반 치과 의료영상 판독에 대한 문헌 분석   \n",
      "53  Automatic detection of periodontal compromised teeth in digital panoramic radiographs using faster regional convolutional neural networks   \n",
      "54                                                                                                         정규화 기법을 통한 안면 인식 알고리즘 성능 향상에 관한 연구   \n",
      "55  Automatic detection of periodontal compromised teeth in digital panoramic radiographs using faster regional convolutional neural networks   \n",
      "56                                                            Effective Hand Gesture Recognition by Key Frame Selection and 3D Neural Network   \n",
      "\n",
      "    date  \\\n",
      "0   2020   \n",
      "1   2020   \n",
      "2   2020   \n",
      "3   2020   \n",
      "4   2020   \n",
      "5   2020   \n",
      "6   2020   \n",
      "7   2020   \n",
      "8   2020   \n",
      "9   2020   \n",
      "10  2020   \n",
      "11  2020   \n",
      "12  2020   \n",
      "13  2020   \n",
      "14  2020   \n",
      "15  2020   \n",
      "16  2020   \n",
      "17  2020   \n",
      "18  2020   \n",
      "19  2020   \n",
      "20  2020   \n",
      "21  2020   \n",
      "22  2020   \n",
      "23  2020   \n",
      "24  2020   \n",
      "25  2020   \n",
      "26  2020   \n",
      "27  2020   \n",
      "28  2020   \n",
      "29  2020   \n",
      "30  2020   \n",
      "31  2020   \n",
      "32  2020   \n",
      "33  2020   \n",
      "34  2020   \n",
      "35  2020   \n",
      "36  2020   \n",
      "37  2020   \n",
      "38  2020   \n",
      "39  2020   \n",
      "40  2020   \n",
      "41  2020   \n",
      "42  2020   \n",
      "43  2020   \n",
      "44  2020   \n",
      "45  2020   \n",
      "46  2020   \n",
      "47  2020   \n",
      "48  2020   \n",
      "49  2020   \n",
      "50  2020   \n",
      "51  2020   \n",
      "52  2020   \n",
      "53  2020   \n",
      "54  2020   \n",
      "55  2020   \n",
      "56  2020   \n",
      "\n",
      "                                                                                                                                                                                                                                                    keywords  \\\n",
      "0                                                                                                                                      [Vehicle License Plate Recognition, Deep Learning, SSD-Mobilenet, ResNet, 자동차 번호판 인식 시스템, 딥러닝, SSD-Mobilenet, ResNet]   \n",
      "1                                                                                                                                                                     [3D Spatial Information, Occlusion Area, Street Tree, Deep Learning, ResNet Algorithm]   \n",
      "2                                                                                                                                                                                    [crack detection, attention mechanism, deep convolution neural network]   \n",
      "3                                                                                                                                                                                                                  [Face Recognition, Game, ResNet Networks]   \n",
      "4                                                                                  [Deep Learning, CNN, Beauty-related Skin Disease Recognition, Image Recognition, Algorithm Comparison, Experimental Comparison, 딥러닝, 피부미용 질환 인식, 이미지 인식, 알고리즘 비교, 실험적 비교]   \n",
      "5                                                                                                  [성능 비교, 시작 단어 검출, 합성곱 신경망, 인공지능 비서, Performance comparison, Wake-up-word detection, Convolutional neural network, Artificial Intelligence (AI) assistant]   \n",
      "6                                                                                                                                                           [Deep learning, Classification, Spectrogram, AI, Convolutional Neural Networks, ResNet, AlexNet]   \n",
      "7                                                                                                                                                                                                               [CNN, Residual Learning, Bottleneck, ResNet]   \n",
      "8                                                                                                                                                                                   [Weather recognition, deep learning, ResNet50, 3C-CNN, WeatherDataset-6]   \n",
      "9                                                                          [머신 러닝, 합성곱신경망, 탄성파층서구분, 인코더-디코더 모델, 네덜란드 F3 block, Machine learning, Convolutional neural network, Seismic sequence identification, Encoder–decoder model, Netherlands F3 block]   \n",
      "10                                                                                                                                                          [electrocardiogram signal, deep learning, convolutional neural network, personal identification]   \n",
      "11                                                        [딥러닝, 지문, 생체정보, 2D 합성 곱 신경망, 상처 지문 판별, 주름 지문 판별, Deep learning, Biometric information, 2D Convolutional Neural Network, discriminating of scar fingerprint, discriminating of wrinkle fingerprint]   \n",
      "12                                                                                                                                                                      [균열 탐지, ILSVRC, 딥 러닝, CNN, 전이 학습, Crack Detection, Deep Learning, Transfer Learning]   \n",
      "13                                                                                                       [Agricultural Tomato Images, Plant Diseases and Pests, Deep Learning Algorithm, Faster R-CNN, Convolution Neural Network, Web Application Platform]   \n",
      "14                                                                                                                                                                                   [Gastroscopy, Classification, ResNet-50, Gastric ulcer, Gastric cancer]   \n",
      "15                                                                                                                                                                             [Indoor plant information, Deep learning, Transfer learning, ResNet, Fast.ai]   \n",
      "16                                                                                                                          [Speaker verification, 화자검증, Masked cross self-attentive encoding, Speaker embedding, ResNet, 마스킹된 교차 자기주의 인코딩, 화자 임베딩, 잔차 네트워크]   \n",
      "17                                                                                                                 [소나무 재선충, 드론, RGB 정사영상, 딥러닝 분류기, pine wilt disease, unmanned aviation vehicle, RGB ortho-image, deep learning-based classifier, heat map]   \n",
      "18                                                                                                                                                [Underground Utility Facility, Fire Detection, Deep Learning, Convolutional Neural Network, Bias Training]   \n",
      "19                                                                                                                                                    [Neural Network, Parameter Sharing, Layer Reuse, Parameter Efficiency, 신경망, 패러미터 공유, 레이어 재사용, 패러미터 효율]   \n",
      "20                                                                                                                                       [Model Compression, Model Light Weight, Deep Learning, Pruning, Network-Slimming, 모델압축, 모델 경량화, 딥러닝, 프루닝, 네트워크 간소화]   \n",
      "21                                                                                                                                                                                                                                                      [HS]   \n",
      "22                                                                                                                                                                                                        [Tree Species Identification, ResNet50, DCNN, MLP]   \n",
      "23                                                                                                                                                                                    [Object Detection, Transfer Learning, Deep Learning, Image Processing]   \n",
      "24                                                                                                                                                                            [Distal radius fractures, Deep learning, Classification, Segmentation, X-rays]   \n",
      "25                                                                                                                                                               [핵의학, 정도관리, 인공지능, 콘볼루션 신경망, 딥러닝, Nuclear medicine, Quality Control, AI, CNN, Deep Learning]   \n",
      "26                                                                                                                                     [알렉스넷, 흉부 방사선검사, 코로나바이러스감염증19, 심층학습, 인공신경망, AlexNet, Chest Radiography(CXR), COVID-19, Deep-learning, Neural network]   \n",
      "27                                                                                                                                                                                                    [딥러닝, 인식, 계산, Deep-Learning, Recognition, Calculation]   \n",
      "28                                                                                                                                                            [Pulmonary Nodule, Computer Aided Detection, Deep Neural Network, Convolutional Neural Networ]   \n",
      "29                                                                                                                                                                                     [Deep learning . Mask R-CNN . Potato detection . Potato segmentation]   \n",
      "30                                                                                                                                                           [Deep CNN, Vespa monitoring system, YOLOv2, Feature extraction layer, VGG19, Transfer learning]   \n",
      "31                                                                                                                                                                          [CNN, Neural Network Compression, Pruning, Matrix Decomposition, Embedded Board]   \n",
      "32                                                                              [Multi-view image set(다시점 영상 집합), Convolutional Neural Network(CNN, 합성곱신경망), Ship hull block(선체 블록), Classification(분류), Data augmentation(데이터 확장), Transfer learning(전이학습)]   \n",
      "33                                                                                                                                                                                                  [Deep learning, human pose estimation, CNN, VGG, Resnet]   \n",
      "34                                                                                                                                             [Rock type classification, Deep learning, ResNet, Rock sample image dataset, 암종 분류, 딥러닝, 레즈넷, 암석 샘플 이미지 데이터셋]   \n",
      "35  [컨베이어 시스템, 막힘 판단, 딥러닝, CNN, VGGNet, ResNet, DenseNet, NASNet, Conveyor Systems, Blockage Determination, Deep Learning, Convolutional Neural Network, Visual Geometry Group Network, Residual Network, Dense Network, Neural Architecture Search Network]   \n",
      "36                                                                                                                                                   [Multiple hint information, Block-wise retraining, Knowledge transfer, Deep learning, Residual network]   \n",
      "37                                                                                                                                                                    [Hydro-junction project, Faster-RCNN, Inception Resnet V2, Data augmentation, K-means]   \n",
      "38                                                                                                               [탄성파 자료 보간, 머신러닝, 서포트 벡터 머신, 유넷, 잔차넷, 생성적 적대 신경망, seismic data interpolation, machine learning, support vector machine, U-Net, ResNet, GAN]   \n",
      "39                                                                                                                                                                                                       [Deep neural network, ONNX, Spiking neural network]   \n",
      "40                                                                                                                                                         [Nursing Hospital, Patient Uniform, Casual Clothing, Clothing Recognition, 요양병원, 환자복, 평상복, 의복 인식]   \n",
      "41                                                                                                                                                                                [Data augmentation, Deep Learning, Tongue segmentation, Transfer learning]   \n",
      "42                                                                                                                                                                                        [Adaptive quantization, Knowledge distillation, Model compression]   \n",
      "43                                                                                                                                                                     [BIM, 교량 구성요소 분류, 딥러닝, CNN, BIM, Bridge component classification, Deep Learning, CNN]   \n",
      "44                                                                                                                                                                                     [Crop Disease Classification, Ensemble Approach, Deep Neural Network]   \n",
      "45                                                                                                                                                                            [Alzheimer’s Disease Classification, Genetic Algorithm, Deep Learning, ResNet]   \n",
      "46                                                                                                                                                                 [Deep learning, Faster R-CNN, Machine learning, Support vector machine, Unmanned vehicle]   \n",
      "47                                                                                                                                                   [Deep learning, intermediate features, video coding for machine, object detection, object segmentation]   \n",
      "48                                                                                                                                            [딥러닝, 가상 공간, CNN, 손 제스쳐 인식, 사용자 인터페이스, Deep Learning, Virtual Space, Hand Gesture Recognition, User Interface]   \n",
      "49                                                                                                                                               [Degree of Understanding, Real-time Analysis, Face Detection, Facial Expression Recognition, Deep Learning]   \n",
      "50                                                                                                                                                                         [Tabular Maintenance Data(정비 자료표), Digitization(디지털화), CNN(합성곱 신경망), FCN(완전 연결망)]   \n",
      "51                                                                                                                                                             [Speech enhancement, Noise reduction, Deep noise suppression, Deep neural network, wav-U-Net]   \n",
      "52                                                                                                                                        [Dentistry, Dental disease, Artificial intelligence, Convolutional neural network, Object detection, Segmentation]   \n",
      "53                                                                                                                                                                       [Alveolar Bone Loss, Panoramic Radiography, Artificial Intelligence, Deep Learning]   \n",
      "54                                                                                                                                                                                     [Computer Vision, Deep Learning, Face Recognition, Safety Management]   \n",
      "55                                                                                                                                                                       [Alveolar Bone Loss, Panoramic Radiography, Artificial Intelligence, Deep Learning]   \n",
      "56                                                                                                                                                                [hand gesture recognition, dynamic hand gesture, key frame extraction, action recognition]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             abstract  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             본 논문은 고성능의 서버 없이 안드로이드 스마트폰 단독으로 동작할 수 있도록 경량화 딥러닝 모델을 사용하여 구현한 자동차 번호판 인식 시스템을 제안한다. 자동차 번호판 인식시스템은 [번호판검출]-[문자영역 분할]-[문자인식]으로 3단계의 과정으로 구성되며, 번호판검출은 SSD-Mobilenet, 문자영역 분할은 ResNet에 localization을 추가하여 사용하였고 문자인식은 ResNet을 이용하여 구현하였다. 테스트한 기기는 삼성 갤럭시 S7, LG Q9이며 정확도는 약 85.3%, 실행속도는 약 1.1초가 소요된다.   \n",
      "1                                                                                                                                                                                                                                                                              국토를 효율적으로 관리하고 도시문제를 과학적으로 해결하기 위해 최근 스마트시티, 디지털트윈 등 3차원 공간정보 관련 기술이 급격하게 발전하고 있다. 이러한 3차원 공간정보 구축은 주로 영상정보를 이용하여 객체를 3차원 입체화하고 실감형 영상인 텍스처링 영상을 추출하여 객체벽면에 영상을 부여하는 방식으로 수행된다. 하지만 객체 주변의 다양한 요인으로 인해 텍스처링 영상에서는 필연적으로 폐색영역이 발생한다. 이에 본 연구에서는 최근 기술인 딥러닝 기술 중에서 ResNet 알고리즘을 이용하여 건물 폐색을 유발하는 가로수에 대한 데이터셋을 만들고 이에 대한 해결방안을 제시하고자 한다. 연구결과 ResNet 알고리즘의 공간정보 적용 가능성을 판단하고 이를 적용한 레이블링 생성 SW 개발하여 실제 가로수를 대상으로 데이터셋을 구축하였다. 구축된 데이터셋을 텍스처링 영상에 적용하여 정확도와 재현율로 검출능력을 분석하였다. 분석결과를 위해 딥러닝 분야에서 많이 사용되고 있는 정밀도와 재현율을 이용한 F값을 적용하였으며 가로수 단일 객체가 포함된 건물의 측면부 영상과 경사 영상에 대해서는 높은 F값을 도출하여 우수한 성과를 확인하였으나, 같은 해상도를 가진 건물 전면부 영상에서는 그림자 등의 요인으로 F값이 낮음을 확인하였다.   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                None   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                None   \n",
      "4                                                                                                                                                                                                                                                                       본 논문에서는 딥러닝 지도학습 알고리즘을 사용한 학습 모델을 대상으로 미용 관련 피부질환 인식의 효과성을 실험적으로 비교한다. 최근 딥러닝 기술을 산업, 교육, 의료 등 다양한 분야에 적용하고 있으며, 의료 분야에서는 중요 피부질환 중 하나인 피부암 식별의 수준을 전문가 수준으로 높인 성과를 보이고 있다. 그러나 아직 피부미용과 관련된 질환에 적용한 사례가 다양하지 못하다. 따라서 딥러닝 기반 이미지 분류에 활용도가 높은 CNN 알고리즘을 비롯하여 ResNet, SE-ResNet을 적용하여 실험적으로 정확도를 비교함으로써 미용 관련 피부질환을 판단하는 효과성을 평가한다. 각 알고리즘을 적용한 학습 모델을 실험한 결과에서 CNN의 경우 평균 71.5%, ResNet은 평균 90.6%, SE-ResNet은 평균 95.3%의 정확도를 보였다. 특히 학습 깊이를 다르게하여 비교한 결과 50개의 계층 구조를 갖는 SE-ResNet-50 모델이 평균 96.2%의 정확도로 미용 관련 피부질환 식별을 위해 가장 효과적인 결과를 보였다. 본 논문의 목적은 피부 미용과 관련된 질환의 판별을 고려하여 효과적인 딥러닝 알고리즘의 학습과 방법을 연구하기 위한 것으로 이를 통해 미용 관련 피부질환 개선을 위한 서비스 개발로 확장할 수 있을 것이다.   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             음성인식 기능을 제공하는 인공지능 비서들은 정확도가 뛰어난 클라우드 기반의 음성인식을 통해 동작한다.클라우드 기반의 음성인식에서 시작 단어 인식은 대기 중인 기기를 활성화하는 데 중요한 역할을 한다. 본 논문에서는공개 데이터셋인 구글의 Speech Commands 데이터셋을 사용하여 스펙트로그램 및 멜-주파수 캡스트럼 계수 특징을입력으로 하여 모바일 기기에 대응한 저 연산 시작 단어 검출을 위한 합성곱 신경망의 성능을 비교한다. 본 논문에서사용한 합성곱 신경망은 다층 퍼셉트론, 일반적인 합성곱 신경망, VGG16, VGG19, ResNet50, ResNet101, ResNet152, MobileNet이며, MobileNet의 성능을 유지하면서 모델 크기를 1/25로 줄인 네트워크도 제안한다.   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                None   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                None   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                None   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                머신 러닝 기술은 탄성파탐사 분야로 그 적용 범위를 확장하고 있다. 탄성파 해석에서 중요한 탄성파 층서 구분에 머신 러닝의 적용 가능성을 알아보았다. 이미지 분야에 탁월한 결과를 보여온 합성곱 신경망 기법 중 4가지 모델을 네덜란드 F3 block에 적용시켰다. 4가지 모델은 ResNet34 모델, 인코더-디코더 형태를 가지는 U-Net, Residual U-Net, FD U-Net이다. 예측된 이미지의 정성적 분석 수행 후 정량적 분석을 위해 pixel accuracy, mean class accuracy, mean intersection over union, frequency weighted IU의 수식을 활용하였다. 본 연구의 분석 결과 ResNet34의 정확도 결과가 가장 낮았고, 인코더-디코더 형태를 가지는 모델들이 높은 정확도를 보여주었다. 그리고 계산에 필요한 파라미터수와 학습시간을 고려 할 때 U-Net이 가장 효율적임을 확인할 수 있었다.   \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "11  인간의 특성과 관련된 측정 항목을 나타내는 생체정보는 도난이나 분실의 염려가 없으므로 높은 신뢰성을 가진 보안 기술로서큰 주목을 받고 있다. 이러한 생체정보 중 지문은 본인 인증, 신원 파악 등의 분야에 주로 사용된다. 신원을 파악할 때 지문 이미지에인증을 수행하기 어려운 상처, 주름, 습기 등의 문제가 있을 경우, 지문 전문가가 전처리단계를 통해 직접 지문에 어떠한 문제가 있는지 파악하고 문제에 맞는 영상처리 알고리즘을 적용해 문제를 해결한다. 이때 지문에 상처와 주름이 있는 지문 영상을 판별해주는인공지능 소프트웨어를 구현하면 손쉽게 상처나 주름의 여부를 확인할 수 있고, 알맞은 알고리즘을 선정해 쉽게 지문 이미지를 개선할 수 있다. 본 연구에서는 이러한 인공지능 소프트웨어의 개발을 위해 캄보디아 왕립대학교의 학생 1,010명, Sokoto 오픈 데이터셋 600명, 국내 학생 98명의 모든 손가락 지문을 취득해 총 17,080개의 지문 데이터베이스를 구축했다. 구축한 데이터베이스에서 상처나 주름이 있는 경우를 판별하기 위해 기준을 확립하고 전문가의 검증을 거쳐 데이터 어노테이션을 진행했다. 트레이닝 데이터셋과테스트 데이터셋은 캄보디아의 데이터, Sokoto 데이터로 구성하였으며 비율을 8:2로 설정했다. 그리고 국내 학생 98명의 데이터를검증 데이터 셋으로 설정했다, 구성된 데이터셋을 사용해 Classic CNN, AlexNet, VGG-16, Resnet50, Yolo v3 등의 다섯 가지 CNN 기반아키텍처를 구현해 학습을 진행했으며 지문의 상처와 주름 판독에서 가장 좋은 성능을 보이는 모델을 찾는 연구를 수행했다. 다섯가지 아키텍처 중 지문 영상에서 상처와 주름 여부를 가장 잘 판별할 수 있는 아키텍처는 ResNet50으로 검증 결과 81.51%로 가장좋은 성능을 보였다.   \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         소나무 재선충은 한국과 일본, 중국을 포함한 동아시아 지역의 소나무산림에 막대한 피해를 주는 원인이며, 피해목의 조기 발견과 제거는 재선충 확산을 막는 효과적인 방법이다. 본 논문에서는 드론으로 촬영되고 처리된 RGB 정사영상을 딥러닝 분류에 의한 재선충 피해목 탐색방법을 제안한다. 제안된 방법은 학습영상 데이터가 많지 않다는 가정아래 ResNet18을 백본으로 하는 패치기반의 분류기를 구성하고 RGB 정사영상을 분류하고 그 결과를 heatmap 형태로 만든다. 제작된 정사영상의 heat map는 재선충 피해목의 분포를 알아내고 확산해가는 모습을 관찰할 수 있게 하며, 재선충 피해목 지역의 RGB 분포 특징을 추출해낼 수도 있다. 본 연구의 패치기반 분류기 성능은 94.7%의 정확도를 나타내었다.   \n",
      "18                                                                                                                                                                                                                                                                      화재는 높은 비정형성으로 인해 딥러닝 모델을 이용한 영상인식 분야에서도 좋은 성능을 내기가 어려운 대상 중 하나이다. 특히 지하공동구 내 화재는 딥러닝 모델의 학습을 위한 화재 데이터 확보가 어렵고 열약한 영상 조건 및 화재로 오인할 수 있는 객체가 많아 화재 검출이 어렵고 성능이 낮다. 이러한 이유로 본 연구는 딥러닝 기반의 지하공동구 내 화재 탐지 모델을 제안하고, 제안된 모델의 성능을 평가하였다. 기존 합성곱 인공신경망에 GoogleNet의 Inception block과 ResNet의 skip connection을 조합하여 어두운 환경에서 발생되는 화재 탐지를 위한 모델 구조를 제안하였으며, 제안된 모델을 효과적으로 학습시키기 위한 방법도 함께 제시하였다. 제안된 방법의 효과를 평가하기 위해 학습 후 모델을 지하공동구 및 유사환경 조건의 화재 문제와 화재로 오인할 수 있는 객체를 포함한 이미지에 적용해 결과를 분석하였다. 또한 기존 딥러닝 기반 화재 탐지 모델의 정밀도, 검출률 지표와 비교함으로써 모델의 화재 탐지성능을 정량적으로 평가하였다. 제안된 모델의 결과는 어두운 환경에서 발생되는 화재 문제에 대해 높은 정밀도와 검출률을 나타내었으며, 유사 화재 객체에 대해 낮은 오탐 및 미탐 성능을 가지고 있음을 보여주었다.   \n",
      "19                                                                                                                                                                         최근 심층 신경망 (Deep Neural Networks, DNNs)는 모바일 및 임베디드 디바이스에 인간과 유사한 수준의 인공지능을 제공해 많은 응용에서 혁명을 가져왔다. 하지만, 이러한 DNN의 높은 추론 정확도는 큰 연산량을 요구하며, 따라서 기존의 사용되던 모델을 압축하거나 리소스가 제한적인 디바이스를 위해 작은 풋프린트를 가진 새로운 DNN 구조를 만드는 방법으로 DNN의 연산 오버헤드를 줄이기 위한 많은 노력들이 있어왔다. 이들 중 최근 작은 메모리 풋프린트를 갖는 모델 설계에서 주목받는 기법중 하나는 레이어 간에 패러미터를 공유하는 것이다. 하지만, 기존의 패러미터 공유 기법들은 ResNet과 같이 패러미터에 중복(redundancy)이 높은 것으로 알려진 깊은 심층 신경망에 적용되어왔다. 본 논문은 ShuffleNetV2와 같이 이미 패러미터 사용에 효율적인 구조를 갖는 소형 신경망에 적용할 수 있는 패러미터 공유 방법을 제안한다. 본 논문에서 제안하는 방법은 작은 크기의 템플릿과 레이어에 고유한 작은 패러미터를 결합하여 가중치를 생성한다. ImageNet과 CIFAR-100 데이터셋에 대한 우리의 실험 결과는 ShuffleNetV2의 패러미터를 15%-35% 감소시키면서도 기존의 패러미터 공유 방법과 pruning 방법에 대비 작은 정확도 감소만이 발생한다. 또한 우리는 제안된 방법이 최근의 임베디드 디바이스상에서 응답속도 및 에너지 소모량 측면에서 효율적임을 보여준다.   \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                            최근 인공지능 딥러닝 분야는 컴퓨팅 자원의 높은 연산량과 가격문제로 인해 상용화에 어려움이 존재했다. 본 논문은 더블 프루닝 기법을 적용하여 심층신경망 모델들과 다수의 데이터셋에서의 성능을 평가하고자 한다. 더블 프루닝은 기본의 네트워크 간소화(Network-Slimming)과 파라미터 프루닝(Parameter-Pruning)을 결합한다. 이는 기존의 학습에 중요하지 않는 매개변수를 절감하여 학습 정확도를 저해하지 않고 속도를 향상시킬 수 있다는 장점이 있다. 다양한 데이터셋 학습 이후에 프루닝 비율을 증가시켜, 모델의 사이즈를 감소시켰다. NetScore 성능 분석 결과 MobileNet-V3가 가장 성능이 높게 나타났다. 프루닝 이후의 성능은 Cifar 10 데이터셋에서 깊이 우선 합성곱 신경망으로 구성된 MobileNet-V3이 가장 성능이 높았고, 전통적인 합성곱 신경망으로 이루어진 VGGNet, ResNet또한 높은 폭으로 성능이 증가함을 확인하였다.   \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                      정보화 기술의 발달로 인해 문서 작성이나 브라우저 검색기능 같은 작업들이 모두 키보드 타이핑만으로 가능하게 되었다. 하지만 수학 계산식 같은 경우에는 키보드 타이핑으로 작성하기가 어려운 것을 알 수 있다. 계산식 같은 경우는 키보드가 아닌 아날로그 식으로 본인이 직접 수기하는 것이 시간도 절약되고 타이핑하는 것보다 오히려 편하다는 것을 알 수 있다. 그러기에 수학 계산식을 검색해서 답을 찾으려 할 때 사용자들은 불편함을 느끼게 된다.본 논문은 컴퓨터가 학습한 딥 러닝 모델로 수기로 작성된 수학 수식을 텍스트 형태로 바꿀 때, 더 정확하게 인식하는 모델의 종류가 무엇인지 제공한다. 숫자와 사칙 연산 기호뿐만 아니라 사용자가 정의한 연산자로도 정확한 인식이 가능한지를 확인한다. 이를 위해 DenseNet, ResNet과 같은 CNN 모델 이용하여 실험을 수행하였고, 실험을 통하여 그 중 가장 적합한 모델을 찾아내었다.   \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "34                                                                                                                                   암종 분류은 현장의 지질학적 또는 지반공학적 특성 파악을 위해 요구되는 매우 기본적인 행위이나 암석의 성인, 지역, 지질학적 이력 특성에 따라 동일 암종이라 하여도 매우 다양한 형태와 색 조성을 보이므로 깊은 지질학적 학식과 경험 없이는 쉬운 일은 아니다. 또한, 다른 여러 분야의 분류 작업에서 딥러닝 영상처리 기법들이 성공적으로 적용되고 있으며, 지질학적 분류나 평가 분야에서도 딥러닝 기법의 적용에 대한 관심이 증대되고 있다. 따라서, 본 연구에서는 동일 암종임에도 다양한 형태와 색을 갖게 되는 실제 상황을 감안하여, 정확한 자동 암종 분류를 위한 딥러닝 기법의 적용 가능성에 대해 검토하였다. 이러한 기법은 향후에 현장 암종분류 작업을 수행하는 현장 기술자들을 지원할 수 있는 효과적인 툴로 활용 가능할 것이다. 본 연구에서 사용된 딥러닝 알고리즘은 매우 깊은 네트워크 구조로 객체 인식과 분류를 할 수 있는 것으로 잘 알려진 ’ResNet’ 계열의 딥러닝 알고리즘을 사용하였다. 적용된 딥러닝에서는 10개의 암종에 대한 다양한 암석 이미지들을 학습시켰으며, 학습 시키지 않은 암석 이미지들에 대하여 84% 수준 이상의 암종 분류 정확도를 보였다. 본 결과로 부터 다양한 성인과 지질학적 이력을 갖는 다양한 형태와 색의 암석들도 지질 전문가 수준으로 분류해 낼 수 있는 것으로 파악되었다. 나아가 다양한 지역과 현장에서 수집된 암석의 이미지와 지질학자들의 분류 결과가 학습데이터로 지속적으로 누적이 되어 재학습에 반영된다면 암종분류 성능은 자동으로 향상될 것이다.   \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    본 연구는 컨베이어 시스템에서 딥러닝을 이용한 배출구 막힘 판단 기술에 대하여 제안한다.제안 방법은 산업 현장의 CCTV에서 수집한 영상을 이용하여 배출구 막힘 판단을 위한 다양한CNN 모델들을 학습시키고, 성능이 가장 좋은 모델을 사용하여 실제 공정에 적용하는 것을 목적으로 한다. CNN 모델로는 잘 알려진 VGGNet, ResNet, DenseNet, 그리고 NASNet을 사용하였으며, 모델 학습과 성능 테스트를 위하여 CCTV에서 수집한 18,000장의 영상을 이용하였다. 다양한 모델에 대한 실험 결과, VGGNet은 99.89%의 정확도와 29.05ms의 처리 시간으로 가장 좋은 성능을 보였으며, 이로부터 배출구 막힘 판단 문제에 VGGNet이 가장 적합함을 확인하였다.   \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "38                                                                                                                                                                                                                                                             탄성파 탐사를 수행할 때 경제적, 환경적 제약 또는 탐사 장비의 문제 등에 의해 탄성파 자료의 일부가 규칙적또는 불규칙적으로 손실되는 경우가 발생하게 된다. 이러한 자료 손실은 탄성파 자료 처리와 해석 결과에 부정적인 영향을 주기 때문에 사라진 탄성파 자료를 복원할 필요가 있다. 탄성파 자료 복원을 위해 재탐사 또는 추가적인 탐사를 진행하는 경우 시간적, 경제적 비용이 발생하기 때문에, 많은 연구자들이 사라진 탄성파 자료를 정확히 복원하기 위한 보간 기법 연구를 진행해왔다. 최근에는 머신러닝 기술 발달에 따라 머신러닝 기법을 활용한 연구들이 진행되고 있고, 다양한 머신러닝 기술들 중에서도 서포트 벡터 회귀, 오토인코더, 유넷, 잔차넷, 생성적 적대 신경망 등의 알고리즘을 활용한 탄성파 자료의 보간 연구가 활발하게 진행되고 있다. 이 논문에서는 이러한 연구들을 조사하고 분석하여 복잡한 신경망 모델뿐 아니라 상대적으로 구조가 간단한 서포트 벡터 회귀 모델을 통해서도 뛰어난 보간 결과를 얻을 수 있다는 것을 확인했다. 추후 머신러닝 기법들을 사용하는 탄성파 자료 보간 연구들에서 오픈소스로 공개된 실제 자료를 이용하며 데이터증식, 전이학습, 기존 기법을 이용한 규제 등의 기술을 활용하면 탄성파 자료 보간 성능을 향상시킬 수 있을 것으로 기대된다.   \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 스파이킹 신경망은 기존 신경망과 다른 메커니즘으로 동작한다. 기존 신경망은 신경망을 구성하는 뉴런으로 들어 오는 입력 값에 대해 생물학적 메커니즘을 고려하지 않은 활성화 함수를 거쳐 다음 뉴런으로 출력 값을 전달한다. 뿐만 아니라 VGGNet, ResNet, SSD, YOLO와 같은 심층 구조를 사용한 좋은 성과들이 있었다. 반면 스파이킹 신경망은 기존 활성화함수 보다 실제 뉴런의 생물학적 메커니즘과 유사하게 동작하는 방식이지만 스파이킹 뉴런을 사용한 심층 구조에 대한 연구는 기존 뉴런을 사용한 심층 신경망과 비교해 활발히 진행되지 않았다. 본 논문은 기존 뉴런으로 만들어 진 심층 신경망 모델을 변환 툴에 로드하여 기존 뉴런을 스파이킹 뉴런으로 대체하여 스파이킹 심층 신경망으로 변환하 는 방법에 대해 제안한다.   \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                          본 연구의 목적은 요양병원에서 발생할 수 있는 노인안전사고 발생률을 감소시키는 것이다. 즉, 위험지역으로 접근하는 인물이 노인(환자복) 그룹인지 실무자(평상복) 그룹인지를 CCTV에 나타나는 의복을 기준으로 구별하는 것이다. Web Crawling기법과 요양병원으로부터 지원을 받아 기초데이터를 수집하였다. 이후 Image Generator와 Labeling으로 모델 학습 데이터를 만들었다. CCTV의 제한된 성능 때문에 높은 정확도와 속도를 모두 갖춘 모델을 만드는 것은 어려웠다. 그러므로 정확성이 상대적으로 우수한 ResNet 모델, 속도에서 상대적으로 우수한 YOLO3 모델을 각각 구현했다. 그리고 요양병원이 자신의 실정에 맞는 모델을 고를 수 있게 하고자 했다. 연구 결과 환자복과 평상복을 적절한 정확도로 구별할 수 있는 모델을 구현하였다. 따라서 실제 사용처에서 노인들이 위험구역에 접근하지 못하도록 하여 요양병원 안전사고 감소에 이바지 할 것으로 평가된다.   \n",
      "41                                                                                                                                                                                                                                                                                                               많은 양의 데이터는 딥 러닝 모델의 견고성을 향상시키고 과적합 문제를 방지할 수 있게 해준다. 자동 혀 분할에서, 혀 영상 데이터 세트를 실제로 수집하고 라벨링하는 데에는 많은 어려움이 수반되므로 많은 양의 혀 영상 데이터를 사용하기 쉽지 않다. 데이터 증강은 새로운 데이터를 수집하지 않고 레이블 보존 변환을 사용하여 학습 데이터 세트를 확장하고 학습 데이터의 다양성을 증가시킬 수 있다. 이 논문에서는 이미지 자르기, 회전, 뒤집기, 색상 변환과 같은 7 가지 데이터 증강 방법을 사용하여 확장된 혀 영상 학습 데이터 세트를 생성하였다. 데이터 증강 방법의 성능을 확인하기 위하여 InceptionV3, EfficientNet, ResNet, DenseNet 등과 같은 전이 학습 모델을 사용하였다. 실험 결과 데이터 증강 방법을 적용함으로써 혀 분할의 정확도를 5~20% 향상시켰으며 기하학적 변환이 색상 변환보다 더 많은 성능 향상을 가져올 수 있음을 보여주었다. 또한 기하학적 변환 및 색상 변환을 임의로 선형 조합한 방법이 다른 데이터 증강 방법보다 우수한 분할 성능을 제공하여 InveptionV3 모델을 사용한 경우에 94.98 %의 정확도를 보였다.   \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "43                                                                                                                                                                                                                                                                                                                                                            최근 BIM (Building Information Modeling)이 건설 산업계에서 폭넓게 활용되고 있다. 하지만 과거에 시공이 된 구조물에 경우 대부분 BIM이 구축되어 있지 않다. BIM이 구축되지 않은 구조물의 경우, 카메라로부터 얻은 2D 이미지에 SfM (Structure from Motion) 기법을 활용하면 3D 모델의 점군 데이터(Point cloud)를 생성하고 BIM을 구축할 수 있다. 하지만 이렇게 생성된 점군 데이터는 의미론적 정보가 포함되어 있지 않기 때문에, 수작업으로 구조물의 어떤 요소인지 분류해 주어야 한다. 따라서 본 연구에서는 구조물 구성요소를 분류하는 과정을 자동화하기 위하여 딥러닝을 적용하였다. 딥러닝 네트워크 구축에는 CNN (Convolutional Neural Network) 구조의 Inception-ResNet-v2를 사용하였고, 전이학습을 통하여 교량 구조물의 구성요소를 학습하였다. 개발된 시스템을 검증하기 위하여 수집한 데이터를 이용하여 구성요소를 분류한 결과, 교량의 구성요소를 96.13 %의 정확도로 분류할 수 있었다.   \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This paper proposes a vehicle license plate recognition system using light weight deep learning models without high-end server. The proposed license plate recognition system consists of 3 steps: [license plate detection]-[character area segmentation]-[character recognition]. SSD-Mobilenet was used for license plate detection, ResNet with localization was used for character area segmentation, ResNet was used for character recognition. Experiemnts using Samsung Galaxy S7 and LG Q9, accuracy showed 85.3% accuracy and around 1.1 second running time.  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The technologies of 3D spatial information, such as Smart City and Digital Twins, are developing rapidly for managing land and solving urban problems scientifically. In this construction of 3D spatial information, an object using aerial photo images is built as a digital DB. Realistically, the task of extracting a texturing image, which is an actual image of the object wall, and attaching an image to the object wall are important. On the other hand, occluded areas occur in the texturing image. In this study, the ResNet algorithm in deep learning technologies was tested to solve these problems. A dataset was constructed, and the street tree was detected using the ResNet algorithm. The ability of the ResNet algorithm to detect the street tree was dependent on the brightness of the image. The ResNet algorithm can detect the street tree in an image with side and inclination angles.  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Deep Convolution neural network (DCNN) has been widely used in the healthy maintenance of civil infrastructure. Using DCNN to improve crack detection performance has attracted many researchers’ attention. In this paper, a light-weight spatial attention network module is proposed to strengthen the representation capability of ResNet and improve the crack detection performance. It utilizes attention mechanism to strengthen the interested objects in global receptive field of ResNet convolution layers. Global average spatial information over all channels are used to construct an attention scalar. The scalar is combined with adaptive weighted sigmoid function to activate the output of each channel’s feature maps. Salient objects in feature maps are refined by the attention scalar. The proposed spatial attention module is stacked in ResNet50 to detect crack. Experiments results show that the proposed module can got significant performance improvement in crack detection.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A convolution neural networks (CNNs) has demonstrated outstanding performance compared to otheralgorithms in the field of face recognition. Regarding the over-fitting problem of CNN, researchers haveproposed a residual network to ease the training for recognition accuracy improvement. In this study, a novelface recognition model based on game theory for call-over in the classroom was proposed. In the proposedscheme, an image with multiple faces was used as input, and the residual network identified each face with aconfidence score to form a list of student identities. Face tracking of the same identity or low confidence weredetermined to be the optimisation objective, with the game participants set formed from the student identitylist. Game theory optimises the authentication strategy according to the confidence value and identity set toimprove recognition accuracy. We observed that there exists an optimal mapping relation between face andidentity to avoid multiple faces associated with one identity in the proposed scheme and that the proposedgame-based scheme can reduce the error rate, as compared to the existing schemes with deeper neural network.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In this paper, we empirically compare the effectiveness of training models to recognize beauty-related skin disease using supervised deep learning algorithms. Recently, deep learning algorithms are being actively applied for various fields such as industry, education, and medical. For instance, in the medical field, the ability to diagnose cutaneous cancer using deep learning based artificial intelligence has improved to the experts level. However, there are still insufficient cases applied to disease related to skin beauty. This study experimentally compares the effectiveness of identifying beauty-related skin disease by applying deep learning algorithms, considering CNN, ResNet, and SE-ResNet. The experimental results using these training models show that the accuracy of CNN is 71.5% on average, ResNet is 90.6% on average, and SE-ResNet is 95.3% on average. In particular, the SE-ResNet-50 model, which is a SE-ResNet algorithm with 50 hierarchical structures, showed the most effective result for identifying beauty-related skin diseases with an average accuracy of 96.2%. The purpose of this paper is to study effective training and methods of deep learning algorithms in consideration of the identification for beauty-related skin disease. Thus, it will be able to contribute to the development of services used to treat and easy the skin disease.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Artificial intelligence assistants that provide speech recognition operate through cloud-based voice recognition with high accuracy. In cloud-based speech recognition, Wake-Up-Word (WUW) detection plays an important role in activating devices on standby. In this paper, we compare the performance of Convolutional Neural Network (CNN)-based WUW detection models for mobile devices by using Google's speech commands dataset, using the spectrogram and mel-frequency cepstral coefficient features as inputs. The CNN models used in this paper are multi-layer perceptron, general convolutional neural network, VGG16, VGG19, ResNet50, ResNet101, ResNet152, MobileNet. We also propose network that reduces the model size to 1/25 while maintaining the performance of MobileNet is also proposed.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The activity and distribution of wild birds are biological indicators to evaluate biodiversity. In order to identify bird habitats, collecting and classifying sounds should have to do. Using the bird sound can make easier to distinguish location or type of wild birds. Recently, attempts to analyze bioacoustic data have been risen using the machine learning. We are going to classify the bird songs using deep learning. The bird songs convert into the spectrogram images. Spectrogram images are used for the input of convolutional neural network. In generally the bird song data set for classification contains a lot of noise. Even obtaining the data including noise is difficult. The data is about 200 bird sounds of 20 species. Based on transfer learning, ResNet34, ResNet50 and AlexNet of Convolutional Neural Network are used as the experiment. The experiment parameter is learning rate and epochs. As a result, the ResNet34 shows the highest accuracy of 99.7% and an average of 93% in the test. Therefore, In this paper, we are going to develop the deep learning system that classifies 20 kinds of bird song using ResNet34. By using this system, it can be helpful various activities such as the prevention of avian influenza.  \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     This paper proposes an image classification algorithm that transforms the number of convolution layers in the residual block of ResNet, CNN's representative method. The proposed method modified the structure of 34/50 layer of ResNet structure. First, we analyzed the performance of small and many convolution layers for the structure consisting of only shortcut and 3 × 3 convolution layers for 34 and 50 layers.And then the performance was analyzed in the case of small and many cases of convolutional layers for the bottleneck structure of 50 layers. By applying the results, the best classification method in the residual block was applied to construct a 34-layer simple structure and a 50-layer bottleneck image classification model. To evaluate the performance of the proposed image classification model, the results were analyzed by applying to the cifar10 dataset. The proposed 34-layer simple structure and 50-layer bottleneck showed improved performance over the ResNet-110 and Densnet-40 models  \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Human activities are often affected by weather conditions. Automatic weather recognition is meaningful to traffic alerting, driving assistance, and intelligent traffic. With the boost of deep learning and AI, deep convolutional neural networks (CNN) are utilized to identify weather situations. In this paper, a three-channel convolutional neural network (3C-CNN) model is proposed on the basis of ResNet50.The model extracts global weather features from the whole image through the ResNet50 branch, and extracts the sky and ground features from the top and bottom regions by two CNN5 branches. Then the global features and the local features are merged by the Concat function. Finally, the weather image is classified by Softmax classifier and the identification result is output. In addition, a medium-scale dataset containing 6,185 outdoor weather images named WeatherDataset-6 is established. 3C-CNN is used to train and test both on the Two-class Weather Images and WeatherDataset-6. The experimental results show that 3C-CNN achieves best on both datasets, with the average recognition accuracy up to 94.35% and 95.81% respectively, which is superior to other classic convolutional neural networks such as AlexNet, VGG16, and ResNet50. It is prospected that our method can also work well for images taken at night with further improvement.  \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Application of the machine learning technique is expanding to the field of seismic exploration. For the purpose of feasibility assessment, we applied the machine learning technique to seismic sequence identification, which is important in seismic interpretation.From among the convolutional neural network techniques used in image analysis, we applied four models to seismic data obtained in the F3 block, offshore Netherlands, which have yielded remarkable results. One of the four models was ResNet34. The others were encoder– decoder models: U-Net, Residual U-Net, and FD U-Net. We first performed a qualitative analysis of the predicted images and then conducted quantitative analysis using pixel accuracy, mean class accuracy, mean intersection over union, and frequency weighted IU equations. The numerical results showed that ResNet34 had the lowest accuracy and that the encoder–decoder type models had higher accuracy. Considering the number of parameters required for calculation and the learning time, we confirmed that U-Net is the most efficient model.  \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The existing personal identification method has a problem that is vulnerable to various crimes, and researches using biosignals, which are internal characteristics of the body, are being conducted to compensate for this. Among them, ECG signals are unique to each person according to the size and location of the heart, which makes them suitable for personal identification, and many studies are being conducted in conjunction with deep learning. In this paper, we analyze the performance of personal identification according to the pre-trained network model using 2-D ECG images. The pre-trained network model for training ECG data sets uses 11 networks of Inception and ResNet. The training data of the network uses 2-D image data using one period of the ECG signal, and the experiment is performed by changing the number of learning. Inception-ResNet-V2 is the highest in the Inception network with 96.18% performance, ResNet-V2-152 is the highest in the ResNet network with 99.12% performance.  \n",
      "11                                                                                                                                                                                                                                                                                   Biometric information indicating measurement items related to human characteristics has attracted great attention as security technology with high reliability since there is no fear of theft or loss. Among these biometric information, fingerprints are mainly used in fields such as identity verification and identification. If there is a problem such as a wound, wrinkle, or moisture that is difficult to authenticate to the fingerprint image when identifying the identity, the fingerprint expert can identify the problem with the fingerprint directly through the preprocessing step, and apply the image processing algorithm appropriate to the problem. Solve the problem. In this case, by implementing artificial intelligence software that distinguishes fingerprint images with cuts and wrinkles on the fingerprint, it is easy to check whether there are cuts or wrinkles, and by selecting an appropriate algorithm, the fingerprint image can be easily improved. In this study, we developed a total of 17,080 fingerprint databases by acquiring all finger prints of 1,010 students from the Royal University of Cambodia, 600 Sokoto open data sets, and 98 Korean students. In order to determine if there are any injuries or wrinkles in the built database, criteria were established, and the data were validated by experts. The training and test datasets consisted of Cambodian data and Sokoto data, and the ratio was set to 8: 2. The data of 98 Korean students were set up as a validation data set. Using the constructed data set, five CNN-based architectures such as Classic CNN, AlexNet, VGG-16, Resnet50, and Yolo v3 were implemented. A study was conducted to find the model that performed best on the readings. Among the five architectures, ResNet50 showed the best performance with 81.51%.  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The purpose of this study is to compare the models of Deep Learning-based Convolution Neural Network(CNN) for concrete crack detection. The comparison models are AlexNet, GoogLeNet, VGG16, VGG19, ResNet-18, ResNet-50, ResNet-101, and SqueezeNet which won ImageNet Large Scale Visual Recognition Challenge(ILSVRC). To train, validate and test these models, we constructed 3000 training data and 12000 validation data with 256×256 pixel resolution consisting of cracked and non-cracked images, and constructed 5 test data with 4160×3120 pixel resolution consisting of concrete images with crack. In order to increase the efficiency of the training, transfer learning was performed by taking the weight from the pre-trained network supported by MATLAB. From the trained network, the validation data is classified into crack image and non-crack image, yielding True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN), and 6 performance indicators, False Negative Rate (FNR), False Positive Rate (FPR), Error Rate, Recall, Precision, Accuracy were calculated. The test image was scanned twice with a sliding window of 256×256 pixel resolution to classify the cracks, resulting in a crack map. From the comparison of the performance indicators and the crack map, it was concluded that VGG16 and VGG19 were the most suitable for detecting concrete cracks.  \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Purpose: purpose of this study was to propose the web application platform which can be to detect and discriminate various diseases and pest of tomato plant based on the large amount of disease image data observed in the facility or the open field.Methods: The deep learning algorithms uesed at the web applivation platform are consisted as the combining form of Faster R-CNN with the pre-trained convolution neural network (CNN) models such as SSD_mobilenet v1, Inception v2, Resnet50 and Resnet101 models. To evaluate the superiority of the newly proposed web application platform, we collected 850 images of four diseases such as Bacterial cankers, Late blight, Leaf miners, and Powdery mildew that occur the most frequent in tomato plants. Of these, 750 were used to learn the algorithm, and the remaining 100 images were used to evaluate the algorithm.Results: From the experiments, the deep learning algorithm combining Faster R-CNN with SSD_mobilnet v1, Inception v2, Resnet50, and Restnet101 showed detection accuracy of 31.0%, 87.7%, 84.4%, and 90.8% respectively. Finally, we constructed a web application platform that can detect and discriminate various tomato deseases using best deep learning algorithm. If farmers uploaded image captured by their digital cameras such as smart phone camera or DSLR (Digital Single Lens Reflex) camera, then they can receive an information for detection, identification and disease control about captured tomato disease through the proposed web application platform.Conclusion: Incheon Port needs to act actively paying  \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Although benign gastric ulcers do not develop into gastric cancer, they are similar to early gastric cancer and difficult to distinguish. This may lead to misconsider early gastric cancer as gastric ulcer while diagnosing. Since gastric cancer does not have any special symptoms until discovered, it is important to detect gastric ulcers by early gastroscopy to prevent the gastric cancer. Therefore, we developed a Convolution Neural Network (CNN) model that can be helpful for endoscopy. 3,015 images of gastroscopy of patients undergoing endoscopy at Gachon University Gil Hospital were used in this study. Using ResNet-50, three models were developed to classify normal and gastric ulcers, normal and gastric cancer, and gastric ulcer and gastric cancer. We applied the data augmentation technique to increase the number of training data and examined the effect on accuracy by varying the multiples. The accuracy of each model with the highest performance are as follows. The accuracy of normal and gastric ulcer classification model was 95.11% when the data were increased 15 times, the accuracy of normal and gastric cancer classification model was 98.28% when 15 times increased likewise, and 5 times increased data in gastric ulcer and gastric cancer classification model yielded 87.89%. We will collect additional specific shape of gastric ulcer and cancer data and will apply various image processing techniques for visual enhancement. Models that classify normal and lesion, which showed relatively high accuracy, will be re-learned through optimal parameter search.  \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Plants have good effects such as air purification and landscaping, but they have special ingredients to protect themselves. Ingredients made to protect the plant itself can harm people or animals. There are also accidents that are mistaken for other plants with similar plant features. We have implemented a program that can identify plants and display information about each plant. Using deep learning to classify images, we created a web application that predicts plant names and displays information that matches the predicted plants. We created a data set of 61 indoor plants using Google Image Search. The positive effects and negative toxicity of indoor plants are summarized in the database. Deep learning is implemented using fast.ai, a Pytorch-based framework. Through data Augmentation, we increased the number of images to learn. Indoor plant image data were trained using ResNet50, a pretrained model using various images. The accuracy of the model was about 97.5%, which predicted most plants accurately. The web application was implemented using flask, a Python-based web framework. Using the implemented image classification deep learning model, the plant name is predicted and the information corresponding to the predicted plant name is displayed on the web page. The web application can be optimized for mobile devices and used conveniently.  \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Constructing speaker embeddings in speaker verification is an important issue. In general, a self-attention mechanism has been applied for speaker embedding encoding. Previous studies focused on training the self-attention in a high-level layer, such as the last pooling layer. In this case, the effect of low-level layers is not well represented in the speaker embedding encoding. In this study, we propose Masked Cross Self-Attentive Encoding (MCSAE) using ResNet. It focuses on training the features of both high-level and low-level layers.Based on multi-layer aggregation, the output features of each residual layer are used for the MCSAE. In the MCSAE, the interdependence of each input features is trained by cross self-attention module. A random masking regularization module is also applied to prevent overfitting problem. The MCSAE enhances the weight of frames representing the speaker information. Then, the output features are concatenated and encoded in the speaker embedding. Therefore, a more informative speaker embedding is encoded by using the MCSAE. The experimental results showed an equal error rate of 2.63 % using the VoxCeleb1 evaluation dataset. It improved performance compared with the previous self-attentive encoding and state-of-the-art methods.  \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Pine wilt disease is one of the reasons that results in huge damage on pine trees in east Asia including Korea, Japan, and China, and early finding and removing the diseased trees is an efficient way to prevent the forest from wide spreading. This paper proposes a searching method of the damaged pine trees from wilt disease in ortho-images corrected from RGB images, which are captured by unmanned aviation vehicles. The proposed method constructs patch-based classifier using ResNet18 backbone network, classifies the RGB ortho-image patches, and make the results as a heat map. The heat map can be used to find the distribution of diseased pine trees, to show the trend of spreading disease, and to extract the RGB distribution of the diseased areas in the image. The classifier in the work shows 94.7% of accuracy.  \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Fire is difficult to achieve good performance in image detection using deep learning because of its high irregularity. In particular, there is little data on fire detection in underground utility facilities, which have poor light conditions and many objects similar to fire. These make fire detection challenging and cause low performance of deep learning models. Therefore, this study proposed a fire detection model using deep learning and estimated the performance of the model. The proposed model was designed using a combination of a basic convolutional neural network, Inception block of GoogleNet, and Skip connection of ResNet to optimize the deep learning model for fire detection under underground utility facilities. In addition, a training technique for the model was proposed. To examine the effectiveness of the method, the trained model was applied to fire images, which included fire and non-fire (which can be misunderstood as a fire) objects under the underground facilities or similar conditions, and results were analyzed. Metrics, such as precision and recall from deep learning models of other studies, were compared with those of the proposed model to estimate the model performance qualitatively. The results showed that the proposed model has high precision and recall for fire detection under low light intensity and both low erroneous and missing detection capabilities for things similar to fire.  \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Recently, deep neural networks (DNNs) have brought revolutions to many mobile and embedded devices by providing human-level machine intelligence for various applications. However, high inference accuracy of such DNNs comes at high computational costs, and, hence, there have been significant efforts to reduce computational overheads of DNNs either by compressing off-the-shelf models or by designing a new small footprint DNN architecture tailored to resource constrained devices. One notable recent paradigm in designing small footprint DNN models is sharing parameters in several layers. However, in previous approaches, the parameter-sharing techniques have been applied to large deep networks, such as ResNet, that are known to have high redundancy. In this paper, we propose a parameter-sharing method for already parameter-efficient small networks such as ShuffleNetV2. In our approach, small templates are combined with small layer-specific parameters to generate weights. Our experiment results on ImageNet and CIFAR100 datasets show that our approach can reduce the size of parameters by 15%-35% of ShuffleNetV2 while achieving smaller drops in accuracies compared to previous parameter-sharing and pruning approaches. We further show that the proposed approach is efficient in terms of latency and energy consumption on modern embedded devices.  \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Recently, the artificial intelligence deep learning field has been hard to commercialize due to the high computing power and the price problem of computing resources. In this paper, we apply a double pruning techniques to evaluate the performance of the in-depth neural network and various datasets. Double pruning combines basic Network-slimming and Parameter-prunning. Our proposed technique has the advantage of reducing the parameters that are not important to the existing learning and improving the speed without compromising the learning accuracy. After training various datasets, the pruning ratio was increased to reduce the size of the model.We confirmed that MobileNet-V3 showed the highest performance as a result of NetScore performance analysis. We confirmed that the performance after pruning was the highest in MobileNet-V3 consisting of depthwise seperable convolution neural networks in the Cifar 10 dataset, and VGGNet and ResNet in traditional convolutional neural networks also increased significantly.  \n",
      "21  The current tariff return system requires tax officials to calculate tax amount by themselves and pay the tax amount on their own responsibility. In other words, in principle, the duty and responsibility of reporting payment system are imposed only on the taxee who is required to calculate and pay the tax accurately. In case the tax payment system fails to fulfill the duty and responsibility, the additional tax is imposed on the taxee by collecting the tax shortfall and imposing the tax deduction on For this reason, item classifications, together with tariff assessments, are the most difficult and could pose a significant risk to entities if they are misclassified. For this reason, import reports are consigned to customs officials, who are customs experts, while paying a substantial fee. The purpose of this study is to classify HS items to be reported upon import declaration and to indicate HS codes to be recorded on import declaration. HS items were classified using the attached image in the case of item classification based on the case of the classification of items by the Korea Customs Service for classification of HS items. For image classification, CNN was used as a deep learning algorithm commonly used for image recognition and Vgg16, Vgg19, ResNet50 and Inception-V3 models were used among CNN models. To improve classification accuracy, two datasets were created. Dataset1 selected five types with the most HS code images, and Dataset2 was tested by dividing them into five types with 87 Chapter, the most among HS code 2 units. The classification accuracy was highest when HS item classification was performed by learning with dual database2, the corresponding model was Inception-V3, and the ResNet50 had the lowest classification accuracy. The study identified the possibility of HS item classification based on the first item image registered in the item classification determination case, and the second point of this study is that HS item classification, which has not been attempted before, was attempted through the CNN model.  \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            None  \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        To guarantee AI model's prominent recognition rate and recognition precision, obtaining the large number of data is essential. In this paper, we propose transfer learning-based object detection algorithm for maintaining outstanding performance even when the volume of training data is small. Also, we proposed a tranfer learning network combining Resnet-50 and YOLO(You Only Look Once) network. The transfer learning network uses the Leeds Sports Pose dataset to train the network that detects the person who occupies the largest part of each images. Simulation results yield to detection rate as 84% and detection precision as 97%.  \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The purpose of this study was to present the models for classifying the wrist X-ray images by types and for segmenting the radius automatically in each image using deep learning and to verify the learned models. The data were a total of 904 wrist X-rays with the distal radius fracture, consisting of 472 anteroposterior (AP) and 432 lateral images. The learning model was the ResNet50 model for AP/lateral image classification, and the U-Net model for segmentation of the radius. In the model for AP/lateral image classification, 100.0% was showed in precision, recall, and F1 score and area under curve (AUC) was 1.0. The model for segmentation of the radius showed an accuracy of 99.46%, a sensitivity of 89.68%, a specificity of 99.72%, and a Dice similarity coefficient of 90.05% in AP images and an accuracy of 99.37%, a sensitivity of 88.65%, a specificity of 99.69%, and a Dice similarity coefficient of 86.05% in lateral images. The model for AP/lateral classification and the segmentation model of the radius learned through deep learning showed favorable performances to expect clinical application.  \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the field of nuclear medicine, errors are sometimes generated because the assessment of the uniformity of gamma cameras relies on the naked eye of the evaluator. To minimize these errors, we created an artificial intelligence model based on CNN algorithm and wanted to assess its usefulness. We produced 20,000 normal images and partial cold region images using Python, and conducted artificial intelligence training with Resnet18 models. The training results showed that accuracy, specificity and sensitivity were 95.01%, 92.30%, and 97.73%, respectively. According to the results of the evaluation of the confusion matrix of artificial intelligence and expert groups, artificial intelligence was accuracy, specificity and sensitivity of 94.00%, 91.50%, and 96.80%, respectively, and expert groups was accuracy, specificity and sensitivity of 69.00%, 64.00%, and 74.00%, respectively. The results showed that artificial intelligence was better than expert groups. In addition, by checking together with the radiological technologist and AI, errors that may occur during the quality control process can be reduced, providing a better examination environment for patients, providing convenience to radiologists, and improving work efficiency.  \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Coronavirus disease(COVID-19) is highly infectious disease that directly affects the lungs. To observe the clinical findings from these lungs, the Chest Radiography(CXR) can be used in a fast manner. However, the diagnostic performance via CXR needs to be improved, since the identifying these findings are highly time-consuming and prone to human error. Therefore, Artificial Intelligence(AI) based tool may be useful to aid the diagnosis of COVID-19 via CXR. In this study, we explored various Deep learning(DL) approach to classify COVID-19, other viral pneumonia and normal. For the original dataset and lung-segmented dataset, the pre-trained AlexNet, SqueezeNet, ResNet18, DenseNet201 were transfer- trained and validated for 3 class - COVID-19, viral pneumonia, normal. In the results, AlexNet showed the highest mean accuracy of 99.15±2.69% and fastest training time of 1.61±0.56 min among 4 pre-trained neural networks. In this study, we demonstrated the performance of 4 pre-trained neural networks in COVID-19 diagnosis with CXR images. Further, we plotted the class activation map(CAM) of each network and demonstrated that the lung-segmentation pre-processing improve the performance of COVID-19 classifier with CXR images by excluding background features.  \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            None  \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Screening with low-dose spiral computed tomography (LDCT) has been shown to reduce lung cancer mortality by about 20% when compared to standard chest radiography. One of the problems arising from screening programs is that large amounts of CT image data must be interpreted by radiologists. To solve this problem, automated detection of pulmonary nodules is necessary; however, this is a challenging task because of the high number of false positive results. Here we demonstrate detection of pulmonary nodules using six off-the-shelf convolutional neural network (CNN) models after modification of the input/output layers and end-to-end training based on publicly databases for comparative evaluation. We used the well-known CNN models, LeNet-5, VGG-16, GoogLeNet Inception V3, ResNet-152, DensNet201, and NASNet. Most of the CNN models provided superior results to those of obtained using customized CNN models. It is more desirable to modify the proven off-the-shelf network model than to customize the network model to detect the pulmonary nodules.  \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Purpose Potatoes are similar in color and size to soil and its clods. They are mostly irregular in the shape as well. Therefore, it is not easy to distinguish potatoes from the soil surface background only with machine vision. This study applied Mask R-CNN, one of the object recognition technologies using deep learning to detect potatoes. The size of object in pixel was obtained on individual potato, and they will be used to predict the yield of potatoes.Methods In order to collect the images needed for deep learning, potato images at the time of harvesting were obtained from potato farms. Annotation was entered for each irregular potato shape, where approximately 4500 potatoes were used. Resnet-101 was selected as the backbone of Mask R-CNN with a feature pyramid network. Transfer training was applied to shorten training time and limit the number of images needed to train the model. The classification performance evaluation was conducted to verify the trained model. The size of potato in pixel was obtained from the output image through the potato detection model by the segmentation algorithm using MATLAB.Results The total number of training for the potato detection model was 12,000, and the training loss of Mask R-CNN was less than 0.1%. The potato detection results from 69 randomly selected test images showed that the average detection precision was 90.8%, recall 93.0%, and F1 score 91.9%.Conclusions Potato detection model with Mask R-CNN can detect irregularly shaped potatoes on similar color soil surface. The size of the detected potato region can be extracted as well.  \n",
      "30                                                                                                                                                                                                                                                                                                        Real-time monitoring system for Vespa is necessary to reduce the damage to beekeeping farmers. In this paper, we compare and analyze the performance of the YOLO-based deep learning algorithms suitable for developing real-time automatic recognition and classification systems for Apis mellifera and five types of vespas such as V. velutina nigrithorax, V. mandarinia, V. ducalis, V. similima and V. crabro. YOLO has shown the best quality for real-time object detection due to its fast speed by replacing iterative object detection with once regression for an input image. However, the YOLO utilizes the conventional DCNN (deep convolutional neural network) algorithm to extract features from images thus, detection performance of the YOLO depends on the DCNN utilized. Therefore, we evaluate the object detection performance by changing the feature extraction layers of YOLOv2 with AlexNet, VGG19, GoogLeNet, and ResNet50 to find the best combination DCNN model for YOLO on the six bees above. The comparison results are as follows. In terms of the speed of detection and classification, the model in which YOLOv2 is combined with the AlexNet as a feature extraction layer showed the highest detection speed per second. This model can detect and classify 5 vespas and a bee from an average of 438 images per second, but has a relatively low accuracy of 71.7%. In terms of accuracy, the model that combines the feature extraction layer of VGG19 with YOLOv2 generated the highest accuracy, 83.2%. In addition, it can process an average of 135 images per second, enabling real-time processing of surveillance videos for wasps and bees. Therefore, we verified that the combined YOLO model with the VGG19 can be the best real-time monitoring system for the vespa detection.  \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Recently, deep neural networks such as CNN are showing excellent performance in various fields such as image classification, object recognition, visual quality enhancement, etc. However, as the model size and computational complexity of deep learning models for most applications increases, it is hard to apply neural networks to IoT and mobile environments. Therefore, neural network compression algorithms for reducing the model size while keeping the performance have been being studied. In this paper, we apply few compression methods to CNN models and evaluate their performances in the embedded environment. For evaluate the performance, the classification performance and inference time of the original CNN models and the compressed CNN models on the image inputted by the camera are evaluated in the embedded board equipped with QCS605, which is a customized AI chip. In this paper, a few CNN models of MobileNetV2, ResNet50, and VGG-16 are compressed by applying the methods of pruning and matrix decomposition. The experimental results show that the compressed models give not only the model size reduction of 1.3~11.2 times at a classification performance loss of less than 2% compared to the original model, but also the inference time reduction of 1.2~2.21 times, and the memory reduction of 1.2~3.8 times in the embedded board.  \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                    It is important to identify the location of ship hull blocks with exact block identification number when scheduling the shipbuilding process. The wrong information on the location and identification number of some hull block can cause low productivity by spending time to find where the exact hull block is. In order to solve this problem, it is necessary to equip the system to track the location of the blocks and to identify the identification numbers of the blocks automatically. There were a lot of researches of location tracking system for the hull blocks on the stockyard. However there has been no research to identify the hull blocks on the stockyard. This study compares the performance of 5 Convolutional Neural Network (CNN) models with multi-view image set on the classification of the hull blocks to identify the blocks on the stockyard. The CNN models are open algorithms of ImageNet Large-Scale Visual Recognition Competition (ILSVRC). Four scaled hull block models are used to acquire the images of ship hull blocks. Learning and transfer learning of the CNN models with original training data and augmented data of the original training data were done. 20 tests and predictions in consideration of five CNN models and four cases of training conditions are performed. In order to compare the classification performance of the CNN models, accuracy and average F1-Score from confusion matrix are adopted as the performance measures. As a result of the comparison, Resnet-152v2 model shows the highest accuracy and average F1-Score with full block prediction image set and with cropped block prediction image set.  \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Accurate estimation of human pose relies on backbone method in which its role is to extract feature map. Up to dated, the method of backbone feature extraction is conducted by the plain convolutional neural networks named by CNN and the residual neural networks named by Resnet, both of which have various architectures and performances. The CNN family network such as VGG which is well-known as a multiple stacked hidden layers architecture of deep learning methods, is base and simple while Resnet which is a bottleneck layers architecture yields fewer parameters and outperform. They have achieved inspired results as a backbone network in human pose estimation. However, they were used then followed by different pose estimation networks named by pose parsing module. Therefore, in this paper, we present a comparison between the plain CNN family network (VGG) and bottleneck network (Resnet) as a backbone method in the same pose parsing module. We investigate their performances such as number of parameters, loss score, precision and recall. We experiment them in the bottom-up method of human pose estimation system by adapted the pose parsing module of openpose. Our experimental results show that the backbone method using VGG network outperforms the Resent network with fewer parameter, lower loss score and higher accuracy of precision and recall.  \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Rock classification is fundamental discipline of exploring geological and geotechnical features in a site, which, however, may not be easy works because of high diversity of rock shape and color according to its origin, geological history and so on. With the great success of convolutional neural networks (CNN) in many different image-based classification tasks, there has been increasing interest in taking advantage of CNN to classify geological material. In this study, a feasibility of the deep CNN is investigated for automatically and accurately identifying rock types, focusing on the condition of various shapes and colors even in the same rock type. It can be further developed to a mobile application for assisting geologist in classifying rocks in fieldwork. The structure of CNN model used in this study is based on a deep residual neural network (ResNet), which is an ultra-deep CNN using in object detection and classification. The proposed CNN was trained on 10 typical rock types with an overall accuracy of 84% on the test set. The result demonstrates that the proposed approach is not only able to classify rock type using images, but also represents an improvement as taking highly diverse rock image dataset as input.  \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This study proposes a technique for the determination of outlet blockage using deep learning in a conveyor system. The proposed method aims to apply the best model to the actual process, where we train various CNN models for the determination of outlet blockage using images collected by CCTV in an industrial scene.We used the well-known CNN model such as VGGNet, ResNet, DenseNet and NASNet, and used 18,000 images collected by CCTV for model training and performance evaluation. As a experiment result with various models, VGGNet showed the best performance with 99.03% accuracy and 29.05ms processing time, and we confirmed that VGGNet is suitable for the determination of outlet blockage.  \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this paper, we propose a stage-wise knowledge transfer method that uses block-wise retraining to transfer the useful knowledge of a pre-trained residual network (ResNet) in a teacher-student framework (TSF). First, multiple hint information transfer and block-wise supervised retraining of the information was alternatively performed between teacher and student ResNet models. Next, Softened output information-based knowledge transfer was additionally considered in the TSF. The results experimentally showed that the proposed method using multiple hint-based bottom-up knowledge transfer coupled with incremental block-wise retraining provided the improved student ResNet with higher accuracy than existing KD and hint-based knowledge transfer methods considered in this study.  \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The defect detection is an important task for maintaining the hydro-junction project. A two-stage crack defect segmentation method based on target detection network is proposed to solve the problem of severe brightness imbalance and large noise in dam surface images. In the first stage, to improve the ability to locate crack areas, Inception Resnet V2 is used as feature extraction network to help Faster-RCNN extract more effective deep features, and the brightness, contrast of image is randomly adjusted before training. In the second segmentation stage, the crack areas are segmented at pixel-level using K-means. The experimental results on the self-made crack image dataset show that the location accuracy (AP) of the crack areas can be improved by 1.9%, reaching 96.8%, compared with other segmentation networks that do not locate crack areas, the intersection over union for segmentation of cracks (Iou) of the final segmentation results is at least 9.4% higher, reaching 52.7%. This method can provide effective technical support for inspection work of hydro-junction project.  \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We acquire seismic data with regularly or irregularly missing traces, due to economic, environmental, and mechanical problems. Since these missing data adversely affect the results of seismic data processing and analysis, we need to reconstruct the missing data before subsequent processing. However, there are economic and temporal burdens to conducting further exploration and reconstructing missing parts. Many researchers have been studying interpolation methods to accurately reconstruct missing data. Recently, various machine learning technologies such as support vector regression, autoencoder, U-Net, ResNet, and generative adversarial network (GAN) have been applied in seismic data interpolation. In this study, by reviewing these studies, we found that not only neural network models, but also support vector regression models that have relatively simple structures can interpolate missing parts of seismic data effectively.We expect that future research can improve the interpolation performance of these machine learning models by using open-source field data, data augmentation, transfer learning, and regularization based on conventional interpolation technologies.  \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The spiking neural network operates in a different mechanism than the existing neural network. The existing neural network transfers the output value to the next neuron via an activation function that does not take into account the biological mechanism for the input value to the neuron that makes up the neural network. In addition, there have been good results using deep structures such as VGGNet, ResNet, SSD and YOLO. spiking neural networks, on the other hand, operate more like the biological mechanism of real neurons than the existing activation function, but studies of deep structures using spiking neurons have not been actively conducted compared to in-depth neural networks using conventional neurons. This paper proposes the method of loading an deep neural network model made from existing neurons into a conversion tool and converting it into a spiking deep neural network through the method of replacing an existing neuron with a spiking neuron.  \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The purpose of this paper is to reduce the ratio of the patient accidents that may occur in nursing hospitals. In other words, it determines whether the person approaching the dangerous area is a elderly (patient uniform) group or a practitioner(Casual Clothing) group, based on the clothing displayed by CCTV. We collected the basic learning data from web crawling techniques and nursing hospitals. Then model training data was created with Image Generator and Labeling program. Due to the limited performance of CCTV, it is difficult to create a good model with both high accuracy and speed. Therefore, we implemented the ResNet model with relatively excellent accuracy and the YOLO3 model with relatively excellent speed. Then we wanted to allow nursing hospitals to choose a model that they wanted. As a result of the study, we implemented a model that can distinguish patient and casual clothes with appropriate accuracy. Therefore, it is believed that it will contribute to the reduction of safety accidents in nursing hospitals by preventing the elderly from accessing the danger zone.  \n",
      "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            A large volume of data will improve the robustness of deep learning models and avoid overfitting problems. In automatic tongue segmentation, the availability of annotated tongue images is often limited because of the difficulty of collecting and labeling the tongue image datasets in reality. Data augmentation can expand the training dataset and increase the diversity of training data by using label-preserving transformations without collecting new data. In this paper, augmented tongue image datasets were developed using seven augmentation techniques such as image cropping, rotation, flipping，color transformations. Performance of the data augmentation techniques were studied using state-of-the-art transfer learning models, for instance, InceptionV3, EfficientNet, ResNet, DenseNet and etc. Our results show that geometric transformations can lead to more performance gains than color transformations and the segmentation accuracy can be increased by 5% to 20% compared with no augmentation. Furthermore, a random linear combination of geometric and color transformations augmentation dataset gives the superior segmentation performance than all other datasets and results in a better accuracy of 94.98% with InceptionV3 models.  \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            None  \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Recently, BIM (Building Information Modeling) are widely being utilized in Construction industry. However, most structures that have been constructed in the past do not have BIM. For structures without BIM, the use of SfM (Structure from Motion) techniques in the 2D image obtained from the camera allows the generation of 3D model point cloud data and BIM to be established. However, since these generated point cloud data do not contain semantic information, it is necessary to manually classify what elements of the structure. Therefore, in this study, deep learning was applied to automate the process of classifying structural components. In the establishment of deep learning network, Inception-ResNet-v2 of CNN (Convolutional Neural Network) structure was used, and the components of bridge structure were learned through transfer learning. As a result of classifying components using the data collected to verify the developed system, the components of the bridge were classified with an accuracy of 96.13 %.  \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            None  \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            None  \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This study used the Deep Learning models used in previous studies, we selected the basic model. The selected model was selected as ZFNet among ZFNet, Googlenet and ResNet, and the object was detected using a ZFNet based FRCNN. In order to reduce the detection error rate of FRCNN, location of four types of objects detected inside the image was designed by SVM classifier and location-based filtering was applied. As simulation results, it showed similar performance to the lane marking classification method with conventional 경계 detection, with an average accuracy of about 88.8%. In addition, studies using the Linear-parabolic Model showed a processing speed of 165.65ms with a minimum resolution of 600 × 800, but in this study, the resolution was treated at about 33ms with an input resolution image of 1280 × 960, so it was possible to classify lane marking at a faster rate than the previous study by CNN-based End to End method.  \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 With the recent development of deep learning, most computer vision-related tasks are being solved with deep learning-based network technologies such as CNN and RNN. Computer vision tasks such as object detection or object segmentation use intermediate features extracted from the same backbone such as Resnet or FPN for training and inference for object detection and segmentation. In this paper, an experiment was conducted to find out the compression efficiency and the effect of encoding on task inference performance when the features extracted in the intermediate stage of CNN are encoded. The feature map that combines the features of 256 channels into one image and the original image were encoded in HEVC to compare and analyze the inference performance for object detection and segmentation. Since the intermediate feature map encodes the five levels of feature maps (P2 to P6), the image size and resolution are increased compared to the original image. However, when the degree of compression is weakened, the use of feature maps yields similar or better inference results to the inference performance of the original image.  \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In this paper, we define static gestures and dynamic gestures to be used as a user interface in a virtual space, and propose a method to extract features using deep learning models and to recognize hand gestures input through RGB camera in order to improve the price and recognition speed of the existing virtual / augmented reality interface device. Through various deep learning models, we learned the data in various ways and extracted the features to recognize hand gestures. Deep learning models used are Faster-RCNN, ResNet, U-Net, and 3D-CNN. Since we recognize hand gestures in the virtual space and use them as user interfaces, we want to contribute to using virtual / augmented reality through high recognition rates and fast recognition speeds without the help of specific sensors or wearable devices.  \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Due to the spread of COVID-19, the online lecture has become more prevalent. However, it was found that a lot of students and professors are experiencing lack of communication. This study is therefore designed to improve interactive communication between professors and students in real-time online lectures. To do so, we explore deep learning approaches for automatic recognition of students' facial expressions and classification of their understanding into 3 classes (Understand / Neutral / Not Understand). We use 'BlazeFace' model for face detection and 'ResNet-GRU' model for facial expression recognition (FER). We name this entire process 'Degree of Understanding (DoU)' algorithm. DoU algorithm can analyze a multitude of students collectively and present the result in visualized statistics. To our knowledge, this study has great significance in that this is the first study offers the statistics of understanding in lectures using FER. As a result, the algorithm achieved rapid speed of 0.098sec/frame with high accuracy of 94.3% in CPU environment, demonstrating the potential to be applied to real-time online lectures. DoU Algorithm can be extended to various fields where facial expressions play important roles in communications such as interactions with hearing impaired people.  \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Tabulated data has been widely used to facilitate systematic and intuitive management. In particular, tabular images that contain a few simple symbols are useful for maintaining mechanical systems. Several companies have accumulated tabular images as their property. Although these images are valuable as they can be used to solve difficult problems using data-based methods, such as deep learning, they still remain unavailable because it is expensive to digitize them. For these reasons, we propose a model comprised of a convolutional neural network (CNN) and fully convolutional network (FCN) to digitize tabular images. We used some ResNet components as they are well-suited to the characteristics of tabular image data. A training set for each model was constructed by writing symbols in blank tables and then augmenting them. As a result, the trained CNN and FCN models exhibited 99.2 % and 97.7 % accuracy in 4.75 s and 0.132 s of inference time, respectively.  \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In this paper, we propose wav-U-Net to improve speech enhancement in heavy noisy environments, and it has implemented three principal techniques. First, as input data, we use 128 modified Mel-scale filter banks which can reduce computational burden instead of 512 frequency bins. Mel-scale aims to mimic the non-linear human ear perception of sound by being more discriminative at lower frequencies and less discriminative at higher frequencies. Therefore, Mel-scale is the suitable feature considering both performance and computing power because our proposed network focuses on speech signals. Second, we add a simple ResNet as pre-processing that helps our proposed network make estimated speech signals clear and suppress high-frequency noises. Finally, the proposed U-Net model shows significant performance regardless of the kinds of noise. Especially, despite using a single channel, we confirmed that it can well deal with non-stationary noises whose frequency properties are dynamically changed, and it is possible to estimate speech signals from noisy speech signals even in extremely noisy environments where noises are much lauder than speech (less than SNR 0dB).The performance on our proposed wav-U-Net was improved by about 200% on SDR and 460% on NSDR compared to the conventional Jansson’s wav-U-Net. Also, it was confirmed that the processing time of out wav-U-Net with 128 modified Mel-scale filter banks was about 2.7 times faster than the common wav-U-Net with 512 frequency bins as input values.  \n",
      "52                                                                                                                                                                                                                                                                                                                                 This study analyzes the papers, which studied to find the most adequate CNN based algorithms for segmentation, object detection in dentistry. According to our purpose, we created several keywords like “Dental+Object Detection+Neural+Network.” We searched articles in ‘PubMed’, ‘IEEE’, using created 34 keywords. We found 458 papers and excluded under a study-purpose provision. So This paper had categorized those 23 papers by 11 of segmentation of tooth structure with dental filling and FDI numbering, 12 of detecting dental caries, periodontitis, or multiple lesions. To compare the performance of models, we organized the results by DICE/IoU index and accuracy, precision, recall, etc.. Various dataset was used for analyzing. The most common dataset was dental panoramic image, then periapical, CBCT, NILT, and intra-oral image. The algorithms were used according to the purpose. For example, VGG16, 19 was used for object detection algorithms were used according to the purpose. For example, VGG16, 19 was used for object detection, U-Net, and Mask R-CNN used for segmentation by study purpose.For segmentation of teeth, Zhimming Cui(2019), used Mask R-CNN, and the accuracy was 0.9755. Vranck(2020) used ResNet for molar detection(IoU 0.9, precision 0.94, 0.93). To label the tooth numbering according to FDI rule, Tuzoff(2019) and Chen(2019), used Faster R-CNN, VGG16, and Faster R-CNN with DNN. Tuzoff’s index was slightly better than Chen’s. Casalegno(2019) investigated the detection of dental caries by using VGG16. The result was IoU 0.727. To find periodontitis, used VGG16 also, by Prajapaty(2017). And the accuracy was 0.8846. Using the Mask R-CNN, Jader(2018) could separate instances of multiple lesions, accuracy was 0.8846.  \n",
      "53                                                                                                                                                                                                                                                                                                                                Purpose: Periodontal disease causes tooth loss and is associated with cardiovascular diseases, diabetes, and rheumatoid arthritis. The present study proposes using a deep learning-based object detection method to identify periodontally compromised teeth on digital panoramic radiographs. A faster regional convolutional neural network (faster R-CNN) which is a state-of-the-art deep detection network, was adapted from the natural image domain using a small annotated clinical data- set.Materials and Methods: In total, 100 digital panoramic radiographs of periodontally compromised patients were retrospectively collected from our hospital’s information system and augmented. The periodontally compromised teeth found in each image were annotated by experts in periodontology to obtain the ground truth. The Keras library, which is written in Python, was used to train and test the model on a single NVidia 1080Ti GPU. The faster R-CNN model used a pretrained ResNet architecture.Results: The average precision rate of 0.81 demonstrated that there was a significant region of overlap between the predicted regions and the ground truth. The average recall rate of 0.80 showed that the periodontally compromised teeth regions generated by the detection method excluded healthiest teeth areas. In addition, the model achieved a sensitivity of 0.84, a specificity of 0.88 and an F-measure of 0.81.Conclusion: The faster R-CNN trained on a limited amount of labeled imaging data performed satisfactorily in detecting periodontally compromised teeth. The application of a faster R-CNN to assist in the detection of periodontally compromised teeth may reduce diagnostic effort by saving assessment time and allowing automated screening documentation.  \n",
      "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Through the combination of computer vision technology and artificial intelligence, facial recognition technology is drawing attention as a new means of personal authentication in the era of the fourth industry. Facial recognition technology uses imaging equipment to photograph a person\"s face and extract characteristic data. The extracted data are matched against the facial features of the stored database. Facial recognition technology is a contactless technology compared to other biometric recognition technologies, which is used in various fields due to its high hygiene, convenience and security, and in particular, safety accidents in workplaces are closely related to life, and various studies related to workplace safety management using intelligent video information are being conducted in the manufacturing industry. In this paper, a study is conducted on the development of facial recognition algorithm using deep learning to control worker access in hazardous areas. The accuracy of the recognition of the proposed facial recognition algorithm (object detection algorithm (SSD) and object recognition algorithm (ResNet)) is closely related to the safety of the operator. Therefore, the goal is to analyze the relationship between various normalization techniques (Min-Max Scaler, MaxAbs Scaler, Standard Scaler) and the recognition rate of the proposed facial recognition algorithm to propose a high-accuracy facial recognition algorithm. In the future, we will conduct research on safety issues in the manufacturing industry based on facial recognition and image recognition technologies.  \n",
      "55                                                                                                                                                                                                                                                                                                                             Purpose: Periodontal disease causes tooth loss and is associated with cardiovascular diseases, diabetes, and rheumatoid arthritis. The present study proposes using a deep learning-based object detection method to identify periodontally compromised teeth on digital panoramic radiographs. A faster regional convolutional neural network (faster R-CNN) which is a state-of-the-art deep detection network, was adapted from the natural image domain using a small annotated clinical data- set. Materials and Methods: In total, 100 digital panoramic radiographs of periodontally compromised patients were retrospectively collected from our hospital's information system and augmented. The periodontally compromised teeth found in each image were annotated by experts in periodontology to obtain the ground truth. The Keras library, which is written in Python, was used to train and test the model on a single NVidia 1080Ti GPU. The faster R-CNN model used a pretrained ResNet architecture. Results: The average precision rate of 0.81 demonstrated that there was a significant region of overlap between the predicted regions and the ground truth. The average recall rate of 0.80 showed that the periodontally compromised teeth regions generated by the detection method excluded healthiest teeth areas. In addition, the model achieved a sensitivity of 0.84, a specificity of 0.88 and an F-measure of 0.81. Conclusion: The faster R-CNN trained on a limited amount of labeled imaging data performed satisfactorily in detecting periodontally compromised teeth. The application of a faster R-CNN to assist in the detection of periodontally compromised teeth may reduce diagnostic effort by saving assessment time and allowing automated screening documentation.  \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    This paper presents an approach for dynamic hand gesture recognition by using algorithm based on 3D Convolutional Neural Network (3D_CNN), which is later extended to 3D Residual Networks (3D_ResNet), and the neural network based key frame selection. Typically, 3D deep neural network is used to classify gestures from the input of image frames, randomly sampled from a video data. In this work, to improve the classification performance, we employ key frames which represent the overall video, as the input of the classification network. The key frames are extracted by SegNet instead of conventional clustering algorithms for video summarization (VSUMM) which require heavy computation. By using a deep neural network, key frame selection can be performed in a real-time system. Experiments are conducted using 3D convolutional kernels such as 3D_CNN, Inflated 3D_CNN (I3D) and 3D_ResNet for gesture classification. Our algorithm achieved up to 97.8% of classification accuracy on the Cambridge gesture dataset. The experimental results show that the proposed approach is efficient and outperforms existing methods.  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                                                                 keyword  \\\n",
      "1                                                          Deep Learning   \n",
      "16                                                                   CNN   \n",
      "3                                                                 ResNet   \n",
      "5                                                                    딥러닝   \n",
      "33                                                         Deep learning   \n",
      "84                                                     Transfer learning   \n",
      "31                                          Convolutional neural network   \n",
      "34                                                        Classification   \n",
      "36                                                                    AI   \n",
      "129                                                         Segmentation   \n",
      "72                                                     Transfer Learning   \n",
      "38                                                               AlexNet   \n",
      "225                                                                  BIM   \n",
      "42                                                         deep learning   \n",
      "43                                                              ResNet50   \n",
      "104                                         Convolutional Neural Network   \n",
      "116                                                              Pruning   \n",
      "51                                                      Machine learning   \n",
      "210                                                  Deep neural network   \n",
      "197                                                    Data augmentation   \n",
      "2                                                          SSD-Mobilenet   \n",
      "76                                                          Faster R-CNN   \n",
      "262                                              Artificial Intelligence   \n",
      "261                                                Panoramic Radiography   \n",
      "13                                                      Face Recognition   \n",
      "260                                                   Alveolar Bone Loss   \n",
      "153                                                  Deep Neural Network   \n",
      "190                                            Multiple hint information   \n",
      "182                                                             DenseNet   \n",
      "178                                                       암석 샘플 이미지 데이터셋   \n",
      "179                                                             컨베이어 시스템   \n",
      "180                                                                막힘 판단   \n",
      "181                                                               VGGNet   \n",
      "195                                                          Faster-RCNN   \n",
      "200                                                                 머신러닝   \n",
      "199                                                            탄성파 자료 보간   \n",
      "194                                               Hydro-junction project   \n",
      "198                                                              K-means   \n",
      "193                                                     Residual network   \n",
      "189                                   Neural Architecture Search Network   \n",
      "183                                                               NASNet   \n",
      "184                                                     Conveyor Systems   \n",
      "196                                                  Inception Resnet V2   \n",
      "192                                                   Knowledge transfer   \n",
      "185                                               Blockage Determination   \n",
      "177                                                                  레즈넷   \n",
      "186                                        Visual Geometry Group Network   \n",
      "187                                                     Residual Network   \n",
      "188                                                        Dense Network   \n",
      "191                                                Block-wise retraining   \n",
      "170                                              Transfer learning(전이학습)   \n",
      "176                                                                암종 분류   \n",
      "175                                            Rock sample image dataset   \n",
      "147                                                                   계산   \n",
      "148                                                        Deep-Learning   \n",
      "149                                                          Recognition   \n",
      "150                                                          Calculation   \n",
      "151                                                     Pulmonary Nodule   \n",
      "152                                             Computer Aided Detection   \n",
      "154                                          Convolutional Neural Networ   \n",
      "155  Deep learning . Mask R-CNN . Potato detection . Potato segmentation   \n",
      "156                                                             Deep CNN   \n",
      "157                                              Vespa monitoring system   \n",
      "158                                                               YOLOv2   \n",
      "159                                             Feature extraction layer   \n",
      "160                                                                VGG19   \n",
      "161                                           Neural Network Compression   \n",
      "162                                                 Matrix Decomposition   \n",
      "163                                                       Embedded Board   \n",
      "164                                      Multi-view image set(다시점 영상 집합)   \n",
      "165                                     Convolutional Neural Network(CNN   \n",
      "166                                                              합성곱신경망)   \n",
      "167                                               Ship hull block(선체 블록)   \n",
      "168                                                   Classification(분류)   \n",
      "169                                            Data augmentation(데이터 확장)   \n",
      "202                                                                   유넷   \n",
      "171                                                human pose estimation   \n",
      "172                                                                  VGG   \n",
      "173                                                               Resnet   \n",
      "174                                             Rock type classification   \n",
      "201                                                            서포트 벡터 머신   \n",
      "0                                      Vehicle License Plate Recognition   \n",
      "203                                                                  잔차넷   \n",
      "250                                                         CNN(합성곱 신경망)   \n",
      "237                                                  object segmentation   \n",
      "238                                                                가상 공간   \n",
      "239                                                             손 제스쳐 인식   \n",
      "240                                                            사용자 인터페이스   \n",
      "241                                                        Virtual Space   \n",
      "242                                             Hand Gesture Recognition   \n",
      "243                                                       User Interface   \n",
      "244                                              Degree of Understanding   \n",
      "245                                                   Real-time Analysis   \n",
      "246                                                       Face Detection   \n",
      "247                                        Facial Expression Recognition   \n",
      "248                                     Tabular Maintenance Data(정비 자료표)   \n",
      "249                                                   Digitization(디지털화)   \n",
      "251                                                          FCN(완전 연결망)   \n",
      "204                                                           생성적 적대 신경망   \n",
      "252                                                   Speech enhancement   \n",
      "253                                                      Noise reduction   \n",
      "254                                               Deep noise suppression   \n",
      "255                                                            wav-U-Net   \n",
      "256                                                            Dentistry   \n",
      "257                                                       Dental disease   \n",
      "258                                              Artificial intelligence   \n",
      "259                                                     Object detection   \n",
      "263                                                      Computer Vision   \n",
      "264                                                    Safety Management   \n",
      "265                                             hand gesture recognition   \n",
      "266                                                 dynamic hand gesture   \n",
      "267                                                 key frame extraction   \n",
      "236                                                     object detection   \n",
      "235                                             video coding for machine   \n",
      "234                                                intermediate features   \n",
      "233                                                     Unmanned vehicle   \n",
      "205                                           seismic data interpolation   \n",
      "206                                                     machine learning   \n",
      "207                                               support vector machine   \n",
      "208                                                                U-Net   \n",
      "209                                                                  GAN   \n",
      "145                                                       Neural network   \n",
      "211                                                                 ONNX   \n",
      "212                                               Spiking neural network   \n",
      "213                                                     Nursing Hospital   \n",
      "214                                                      Patient Uniform   \n",
      "215                                                      Casual Clothing   \n",
      "216                                                 Clothing Recognition   \n",
      "217                                                                 요양병원   \n",
      "218                                                                  환자복   \n",
      "219                                                                  평상복   \n",
      "220                                                                의복 인식   \n",
      "221                                                  Tongue segmentation   \n",
      "222                                                Adaptive quantization   \n",
      "223                                               Knowledge distillation   \n",
      "224                                                    Model compression   \n",
      "226                                                           교량 구성요소 분류   \n",
      "227                                      Bridge component classification   \n",
      "228                                          Crop Disease Classification   \n",
      "229                                                    Ensemble Approach   \n",
      "230                                   Alzheimer’s Disease Classification   \n",
      "231                                                    Genetic Algorithm   \n",
      "232                                               Support vector machine   \n",
      "146                                                                   인식   \n",
      "134                                                             콘볼루션 신경망   \n",
      "144                                                        Deep-learning   \n",
      "59                                                                  생체정보   \n",
      "45                                                      WeatherDataset-6   \n",
      "46                                                                 머신 러닝   \n",
      "47                                                                합성곱신경망   \n",
      "48                                                               탄성파층서구분   \n",
      "49                                                            인코더-디코더 모델   \n",
      "50                                                         네덜란드 F3 block   \n",
      "52                                       Seismic sequence identification   \n",
      "53                                                 Encoder–decoder model   \n",
      "54                                                  Netherlands F3 block   \n",
      "55                                              electrocardiogram signal   \n",
      "56                                          convolutional neural network   \n",
      "57                                               personal identification   \n",
      "58                                                                    지문   \n",
      "60                                                           2D 합성 곱 신경망   \n",
      "143                                                             COVID-19   \n",
      "61                                                              상처 지문 판별   \n",
      "62                                                              주름 지문 판별   \n",
      "63                                                 Biometric information   \n",
      "64                                       2D Convolutional Neural Network   \n",
      "65                                    discriminating of scar fingerprint   \n",
      "66                                 discriminating of wrinkle fingerprint   \n",
      "67                                                                 균열 탐지   \n",
      "68                                                                ILSVRC   \n",
      "69                                                                  딥 러닝   \n",
      "70                                                                 전이 학습   \n",
      "71                                                       Crack Detection   \n",
      "73                                            Agricultural Tomato Images   \n",
      "74                                              Plant Diseases and Pests   \n",
      "44                                                                3C-CNN   \n",
      "41                                                   Weather recognition   \n",
      "40                                                            Bottleneck   \n",
      "39                                                     Residual Learning   \n",
      "4                                                         자동차 번호판 인식 시스템   \n",
      "6                                                 3D Spatial Information   \n",
      "7                                                         Occlusion Area   \n",
      "8                                                            Street Tree   \n",
      "9                                                       ResNet Algorithm   \n",
      "10                                                       crack detection   \n",
      "11                                                   attention mechanism   \n",
      "12                                       deep convolution neural network   \n",
      "14                                                                  Game   \n",
      "15                                                       ResNet Networks   \n",
      "17                               Beauty-related Skin Disease Recognition   \n",
      "18                                                     Image Recognition   \n",
      "19                                                  Algorithm Comparison   \n",
      "20                                               Experimental Comparison   \n",
      "21                                                            피부미용 질환 인식   \n",
      "22                                                                이미지 인식   \n",
      "23                                                               알고리즘 비교   \n",
      "24                                                                실험적 비교   \n",
      "25                                                                 성능 비교   \n",
      "26                                                              시작 단어 검출   \n",
      "27                                                               합성곱 신경망   \n",
      "28                                                               인공지능 비서   \n",
      "29                                                Performance comparison   \n",
      "30                                                Wake-up-word detection   \n",
      "32                                Artificial Intelligence (AI) assistant   \n",
      "35                                                           Spectrogram   \n",
      "37                                         Convolutional Neural Networks   \n",
      "75                                               Deep Learning Algorithm   \n",
      "77                                            Convolution Neural Network   \n",
      "78                                              Web Application Platform   \n",
      "111                                                              패러미터 공유   \n",
      "113                                                              패러미터 효율   \n",
      "114                                                    Model Compression   \n",
      "115                                                   Model Light Weight   \n",
      "117                                                     Network-Slimming   \n",
      "118                                                                 모델압축   \n",
      "119                                                               모델 경량화   \n",
      "120                                                                  프루닝   \n",
      "121                                                             네트워크 간소화   \n",
      "122                                                                   HS   \n",
      "123                                          Tree Species Identification   \n",
      "124                                                                 DCNN   \n",
      "125                                                                  MLP   \n",
      "126                                                     Object Detection   \n",
      "127                                                     Image Processing   \n",
      "128                                              Distal radius fractures   \n",
      "130                                                               X-rays   \n",
      "131                                                                  핵의학   \n",
      "132                                                                 정도관리   \n",
      "133                                                                 인공지능   \n",
      "135                                                     Nuclear medicine   \n",
      "136                                                      Quality Control   \n",
      "137                                                                 알렉스넷   \n",
      "138                                                             흉부 방사선검사   \n",
      "139                                                         코로나바이러스감염증19   \n",
      "140                                                                 심층학습   \n",
      "141                                                                인공신경망   \n",
      "142                                               Chest Radiography(CXR)   \n",
      "112                                                              레이어 재사용   \n",
      "110                                                                  신경망   \n",
      "79                                                           Gastroscopy   \n",
      "109                                                 Parameter Efficiency   \n",
      "80                                                             ResNet-50   \n",
      "81                                                         Gastric ulcer   \n",
      "82                                                        Gastric cancer   \n",
      "83                                              Indoor plant information   \n",
      "85                                                               Fast.ai   \n",
      "86                                                  Speaker verification   \n",
      "87                                                                  화자검증   \n",
      "88                                  Masked cross self-attentive encoding   \n",
      "89                                                     Speaker embedding   \n",
      "90                                                      마스킹된 교차 자기주의 인코딩   \n",
      "91                                                                화자 임베딩   \n",
      "92                                                               잔차 네트워크   \n",
      "93                                                               소나무 재선충   \n",
      "94                                                                    드론   \n",
      "95                                                              RGB 정사영상   \n",
      "96                                                               딥러닝 분류기   \n",
      "97                                                     pine wilt disease   \n",
      "98                                             unmanned aviation vehicle   \n",
      "99                                                       RGB ortho-image   \n",
      "100                                       deep learning-based classifier   \n",
      "101                                                             heat map   \n",
      "102                                         Underground Utility Facility   \n",
      "103                                                       Fire Detection   \n",
      "105                                                        Bias Training   \n",
      "106                                                       Neural Network   \n",
      "107                                                    Parameter Sharing   \n",
      "108                                                          Layer Reuse   \n",
      "268                                                   action recognition   \n",
      "\n",
      "     count  \n",
      "1       17  \n",
      "16      10  \n",
      "3       10  \n",
      "5       10  \n",
      "33       9  \n",
      "84       3  \n",
      "31       3  \n",
      "34       3  \n",
      "36       2  \n",
      "129      2  \n",
      "72       2  \n",
      "38       2  \n",
      "225      2  \n",
      "42       2  \n",
      "43       2  \n",
      "104      2  \n",
      "116      2  \n",
      "51       2  \n",
      "210      2  \n",
      "197      2  \n",
      "2        2  \n",
      "76       2  \n",
      "262      2  \n",
      "261      2  \n",
      "13       2  \n",
      "260      2  \n",
      "153      2  \n",
      "190      1  \n",
      "182      1  \n",
      "178      1  \n",
      "179      1  \n",
      "180      1  \n",
      "181      1  \n",
      "195      1  \n",
      "200      1  \n",
      "199      1  \n",
      "194      1  \n",
      "198      1  \n",
      "193      1  \n",
      "189      1  \n",
      "183      1  \n",
      "184      1  \n",
      "196      1  \n",
      "192      1  \n",
      "185      1  \n",
      "177      1  \n",
      "186      1  \n",
      "187      1  \n",
      "188      1  \n",
      "191      1  \n",
      "170      1  \n",
      "176      1  \n",
      "175      1  \n",
      "147      1  \n",
      "148      1  \n",
      "149      1  \n",
      "150      1  \n",
      "151      1  \n",
      "152      1  \n",
      "154      1  \n",
      "155      1  \n",
      "156      1  \n",
      "157      1  \n",
      "158      1  \n",
      "159      1  \n",
      "160      1  \n",
      "161      1  \n",
      "162      1  \n",
      "163      1  \n",
      "164      1  \n",
      "165      1  \n",
      "166      1  \n",
      "167      1  \n",
      "168      1  \n",
      "169      1  \n",
      "202      1  \n",
      "171      1  \n",
      "172      1  \n",
      "173      1  \n",
      "174      1  \n",
      "201      1  \n",
      "0        1  \n",
      "203      1  \n",
      "250      1  \n",
      "237      1  \n",
      "238      1  \n",
      "239      1  \n",
      "240      1  \n",
      "241      1  \n",
      "242      1  \n",
      "243      1  \n",
      "244      1  \n",
      "245      1  \n",
      "246      1  \n",
      "247      1  \n",
      "248      1  \n",
      "249      1  \n",
      "251      1  \n",
      "204      1  \n",
      "252      1  \n",
      "253      1  \n",
      "254      1  \n",
      "255      1  \n",
      "256      1  \n",
      "257      1  \n",
      "258      1  \n",
      "259      1  \n",
      "263      1  \n",
      "264      1  \n",
      "265      1  \n",
      "266      1  \n",
      "267      1  \n",
      "236      1  \n",
      "235      1  \n",
      "234      1  \n",
      "233      1  \n",
      "205      1  \n",
      "206      1  \n",
      "207      1  \n",
      "208      1  \n",
      "209      1  \n",
      "145      1  \n",
      "211      1  \n",
      "212      1  \n",
      "213      1  \n",
      "214      1  \n",
      "215      1  \n",
      "216      1  \n",
      "217      1  \n",
      "218      1  \n",
      "219      1  \n",
      "220      1  \n",
      "221      1  \n",
      "222      1  \n",
      "223      1  \n",
      "224      1  \n",
      "226      1  \n",
      "227      1  \n",
      "228      1  \n",
      "229      1  \n",
      "230      1  \n",
      "231      1  \n",
      "232      1  \n",
      "146      1  \n",
      "134      1  \n",
      "144      1  \n",
      "59       1  \n",
      "45       1  \n",
      "46       1  \n",
      "47       1  \n",
      "48       1  \n",
      "49       1  \n",
      "50       1  \n",
      "52       1  \n",
      "53       1  \n",
      "54       1  \n",
      "55       1  \n",
      "56       1  \n",
      "57       1  \n",
      "58       1  \n",
      "60       1  \n",
      "143      1  \n",
      "61       1  \n",
      "62       1  \n",
      "63       1  \n",
      "64       1  \n",
      "65       1  \n",
      "66       1  \n",
      "67       1  \n",
      "68       1  \n",
      "69       1  \n",
      "70       1  \n",
      "71       1  \n",
      "73       1  \n",
      "74       1  \n",
      "44       1  \n",
      "41       1  \n",
      "40       1  \n",
      "39       1  \n",
      "4        1  \n",
      "6        1  \n",
      "7        1  \n",
      "8        1  \n",
      "9        1  \n",
      "10       1  \n",
      "11       1  \n",
      "12       1  \n",
      "14       1  \n",
      "15       1  \n",
      "17       1  \n",
      "18       1  \n",
      "19       1  \n",
      "20       1  \n",
      "21       1  \n",
      "22       1  \n",
      "23       1  \n",
      "24       1  \n",
      "25       1  \n",
      "26       1  \n",
      "27       1  \n",
      "28       1  \n",
      "29       1  \n",
      "30       1  \n",
      "32       1  \n",
      "35       1  \n",
      "37       1  \n",
      "75       1  \n",
      "77       1  \n",
      "78       1  \n",
      "111      1  \n",
      "113      1  \n",
      "114      1  \n",
      "115      1  \n",
      "117      1  \n",
      "118      1  \n",
      "119      1  \n",
      "120      1  \n",
      "121      1  \n",
      "122      1  \n",
      "123      1  \n",
      "124      1  \n",
      "125      1  \n",
      "126      1  \n",
      "127      1  \n",
      "128      1  \n",
      "130      1  \n",
      "131      1  \n",
      "132      1  \n",
      "133      1  \n",
      "135      1  \n",
      "136      1  \n",
      "137      1  \n",
      "138      1  \n",
      "139      1  \n",
      "140      1  \n",
      "141      1  \n",
      "142      1  \n",
      "112      1  \n",
      "110      1  \n",
      "79       1  \n",
      "109      1  \n",
      "80       1  \n",
      "81       1  \n",
      "82       1  \n",
      "83       1  \n",
      "85       1  \n",
      "86       1  \n",
      "87       1  \n",
      "88       1  \n",
      "89       1  \n",
      "90       1  \n",
      "91       1  \n",
      "92       1  \n",
      "93       1  \n",
      "94       1  \n",
      "95       1  \n",
      "96       1  \n",
      "97       1  \n",
      "98       1  \n",
      "99       1  \n",
      "100      1  \n",
      "101      1  \n",
      "102      1  \n",
      "103      1  \n",
      "105      1  \n",
      "106      1  \n",
      "107      1  \n",
      "108      1  \n",
      "268      1  \n",
      "데이터가 저장되었습니다: resnet_2020_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2020_academic_riss.csv\n",
      "ResNet-Variational AutoEncoder기반 변종 악성코드 패밀리 분류 연구 2021 ['변종 악성코드', '악성코드 분류', '변이 오토인코더', '전이학습', '앙상블 학습', 'Variant Malware', 'Malware Classification', 'Variational AutoEncoder', 'Tranfer Learning', 'Ensemble Learning'] 전통적으로 대부분의 악성코드는 도메인 전문가에 의해 추출된 특징 정보를 활용하여 분석되었다. 하지만 이러한 특징 기반의 분석방식은 분석가의 역량에 의존적이며 기존의 악성코드를 변형한 변종 악성코드를 탐지하는 데 한계를 가지고 있다. 본 연구에서는 도메인 전문가의 개입 없이도 변종 악성코드의 패밀리를 분류할 수 있는 ResNet-Variational AutoEncder 기반 변종 악성코드 분류 방법을 제안한다. Variational AutoEncoder 네트워크는 입력값으로 제공되는 훈련 데이터의 학습 과정에서 데이터의 특징을 잘 이해하며 정규 분포 내에서 새로운 데이터를 생성하는 특징을 가지고 있다. 본 연구에서는 Variational AutoEncoder의 학습 과정에서 잠재 변수를 추출을 통해 악성코드의 중요 특징을 추출할 수 있었다. 또한 훈련 데이터의 특징을 더욱 잘 학습하고 학습의 효율성을 높이기 위해 전이 학습을 수행했다. ImageNet Dataset으로 사전학습된 ResNet-152 모델의 학습 파라미터를 Encoder Network의 학습 파라미터로 전이했다. 전이학습을 수행한 ResNet-Variational AutoEncoder의 경우 기존 Variational AutoEncoder에 비해 높은 성능을 보였으며 학습의 효율성을 제공하였다. 한편 변종 악성코드 분류를 위한 방법으로는 앙상블 모델인 Stacking Classifier가 사용되었다. ResNet-VAE 모델의 Encoder Network로 추출한 변종 악성코드 특징 데이터를 바탕으로 Stacking Classifier를 학습한 결과 98.66%의 Accuracy와 98.68의 F1-Score를 얻을 수 있었다. Traditionally, most malicious codes have been analyzed using feature information extracted by domain experts. However, this feature-based analysis method depends on the analyst's capabilities and has limitations in detecting variant malicious codes that have modified existing malicious codes. In this study, we propose a ResNet-Variational AutoEncder-based variant malware classification method that can classify a family of variant malware without domain expert intervention. The Variational AutoEncoder network has the characteristics of creating new data within a normal distribution and understanding the characteristics of the data well in the learning process of training data provided as input values. In this study, important features of malicious code could be extracted by extracting latent variables in the learning process of Variational AutoEncoder. In addition, transfer learning was performed to better learn the characteristics of the training data and increase the efficiency of learning. The learning parameters of the ResNet-152 model pre-trained with the ImageNet Dataset were transferred to the learning parameters of the Encoder Network. The ResNet-Variational AutoEncoder that performed transfer learning showed higher performance than the existing Variational AutoEncoder and provided learning efficiency. Meanwhile, an ensemble model, Stacking Classifier, was used as a method for classifying variant malicious codes. As a result of learning the Stacking Classifier based on the characteristic data of the variant malware extracted by the Encoder Network of the ResNet-VAE model, an accuracy of 98.66% and an F1-Score of 98.68 were obtained.\n",
      "복소수 ResNet 네트워크 기반의 SAR 영상 물체 인식 알고리즘 2021 None None Unlike optical equipment, SAR(Synthetic Aperture Radar) has the advantage of obtaining images in all weather, and object detection in SAR images is an important issue. Generally, deep learning-based object detection was mainly performed in real-valued network using only amplitude of SAR image. Since the SAR image is complex data consist of amplitude and phase data, a complex-valued network is required. In this paper, a complex-valued ResNet network is proposed. SAR image object detection was performed by combining the ROI transformer detector specialized for aerial image detection and the proposed complex-valued ResNet. It was confirmed that higher accuracy was obtained in complex-valued network than in existing real-valued network.\n",
      "ResNet 정확도 향상을 위한 깊이별 Residual Connection 비율 조절 방법 제안 2021 ['ResNet', 'Residual Connection', 'Residual Learning', 'Variational Scaling', 'Degradation'] ResNet은 residual learning으로 학습 최적화를 용이하게 만들어 gradient vanishing과 상관없이 신경망이 깊어질 때 성능이 하락하는 degradation 문제를 해결한다. 하지만 기존 ResNet에서는 모든 residual block에 동일한 비율로 residual connection을 적용하여 residual learning의 최적화 효과를 극대화하지 못하는 한계가 있다. 따라서 본 논문에서는 residual learning의 최적화 효과를 극대화하기 위해 깊이 별 residual connection 비율을 달리하여 ResNet 정확도를 향상시키는 variational scaling 방법을 제안한다. 성능을 검증하기 위해 두가지 다른 데이터셋인 CIFAR-10과 CIFAR-100에서 다른 깊이를 갖는 ResNet-32, ResNet-56를 사용해 실험을 수행한다. 실험 결과 모든 케이스에서 기존 ResNet과 비교하여 연산량 증가 없이 정확도 향상을 이룬다. ResNet resolves a degradation problem by residual learning which allows ease to the learning optimization. However, original ResNet has such a limitation that do not maximize the optimization of residual learning by applying residual connections with the constant scale. This paper suggests a variational scaling method that adjusts scales of residual connection by depth in order to maximize the optimization effect of residual learning hence to enhance the model accuracy. Experiments are conducted in two different datasets CIFAR-10 and CIFAR-100, and with 2 models with different depth, ResNet –32 and ResNet-56. As the result of experiments, the variational scaling method enhanced accuracy without the computational amount increased compared to the original ResNet in all cases.\n",
      "ResNet-합성곱 오토인코더 기반 신경망을 이용한 스펙트럼 데이터 압축 2021 ['Data Compression', 'PCA', 'Autoencoder', 'ResNet', 'Raman Spectrum'] 본 논문에서는 스펙트럼 저장 시 데이터용량을 줄이기 위해 합성곱 오토인코더(convolutional autoencoder) 구조에 ResNet(Residual Neural Network) 알고리즘을 적용한 스펙트럼 데이터 압축 신경망을 제안한다. 최근 분광법(spectroscopy)의 적용 분야가 넓어짐에 따라 스펙트럼 데이터베이스가 대용량화되어 효율적인 전송이 어렵고 많은 저장 공간을 필요로 한다. 이러한 대용량의 데이터베이스를 효율적으로 관리하기 위해 데이터 압축을 수행한다. 기존 데이터 압축에 주로 사용되는 PCA(Principal Component Analysis)는 주성분의 개수에 따라 압축률이 결정된다. 주성분 개수가 적을수록 압축률은 높아지지만 정보 손실이 보다 쉽게 발생하기 복원 시 원본 데이터와의 크게 오차가 발생한다. 이러한 한계점을 극복하기 위해 본 논문에서는 제안한 신경망인 CAER(Convolutional AutoEncoder+ResNet)을 통하여 데이터 압축을 수행하였다. 신경망 학습은 실제 스펙트럼 데이터를 묘사하여 생성한 모의실험 데이터를 통해 수행하였다. CAER 신경망의 성능 검증을 위해 라만 스펙트럼을 PCA와 신경망을 통하여 75%, 87.5%, 93.75%의 압축률로 압축과 복원을 수행한 후 각각의 결과를 비교 분석하였다. 원본과 복원 데이터의 오차 비교를 하였을 때 CAER 신경망은 PCA보다 평균 94.2%의 낮은 오차를 보인다. 이 결과를 통해 CAER 신경망이 스펙트럼 데이터 압축에 효과적으로 적용될 수 있음을 확인하였다. In this paper, we propose a spectrum compression neural network that applied the ResNet (Residual Neural Network) algorithm to the convolutional autoencoder structure to reduce data capacity requirement in storing the spectrum. Recently, as the field of application of spectroscopy widens, the spectrum database is becoming larger, making efficient transmission difficult and requiring large amount of storage. Therefore, data compression is performed to manage large amounts of data efficiently. In PCA (Principal Component Analysis), which is mainly used for data compression, the compression ratio is determined by the number of principal components. As the number of principal components decreases, the compression rate increases, but at the same time, it is easier for information loss to occur. Hence, errors occur between reconstruction and the raw spectrum. To overcome these limitations, we perform compression through the proposed CAER (Convolutional AutoEncoder+ResNet) network. The training of the network was performed through simulated data describing the real spectrum. To verify the performance of the CAER network, the Raman spectrum was compressed and reconstructed at compression rates of 75%, 87.5%, and 93.75% through the PCA and CAER networks. Comparing the errors between raw and reconstructed data, the CAER network shows an average error of 94.2% lower than that of the PCA. The results obtained confirm that the CAER network can be effectively applied to spectrum compression.\n",
      "ResNet과 Unet을 결합한 딥러닝 모델을 이용한 분광 신호에서 ROI 검출 2021 ['Peak Detection', 'Region of Interest', 'Deep Learning', 'CNN', 'Raman Spectroscopy'] 본 연구에서는 딥러닝 기술(deep learning technology)을 이용하여 분광 신호의 ROI(region of interest)를 찾는 방법을 제안한다. 제안한 방법은 모의실험 데이터로 학습된 딥러닝 모델을 이용하여 분광 신호의 ROI를 검출하는 방법이다. 분광 신호의 피크는 물질의 물리 화학적인 정보를 포함하고 있으므로 정확한 피크 검출은 분석 시스템의 성능에 영향을 미치는 중요한 과정이다. 지금까지 가장 많이 사용되는 방법은 진폭을 기반으로 피크 검출을 진행하는 것이다. 하지만 이런 방법들은 전처리 과정을 포함하거나 분광 신호에 따라 파라미터를 육안 검사로 선택하여 추정하므로 복잡하고 주관적이다. 이러한 문제점 개선을 위해 딥러닝 모델을 통해 분광 신호의 ROI 검출을 수행하였다. 제안한 방법은 전처리 과정이 없고 파라미터를 설정하지 않아도 되는 장점을 갖는다. 또한 검출한 ROI에 따라 분광 신호에 후처리(post-processing)를 수행하여 피크를 얻을 수 있다. 디폴트 손실 함수에 3만개 테스트 데이터를 적용하여 얻은 손실값을 통해 성능 평가를 수행하였다. 제안된 ResNet과 Unet을 결합한 딥러닝 모델은 일반적인 컨볼루션 신경망(CNN: Convolutional Neural Network), ResNet, 그리고 Unet에 비해 각각 76.5%, 69.8%, 5.9%의 성능 향상을 보였으며, 실제 라만 분광 신호의 ROI 검출에도 효과적으로 적용될 수 있음을 확인하였다. This study proposes a method to find the ROI (region of interest) of spectral signals using deep learning technology. The proposed method detects the ROI of spectral signals using a deep learning model trained with simulated data. Since the peak of the spectral signal contains physical and chemical information of the substance, accurate peak detection is an important process affecting the performance of the analyzed system. The widely used method for peak detection is the one based on the amplitude. However, this method is complex and subjective because it involves pre-processing or select and estimate parameters using visual inspection according to spectral signals. To overcome this problem, ROI detection of the spectral signal was performed through a deep learning model. The proposed method has the advantage of requiring no pre-processing and parameter setting. In addition, a peak may be obtained by performing post-processing of the spectral signal according to the detected ROI. Performance evaluation was performed through loss values obtained by applying 30,000 test data to the custom loss function. The proposed deep learning model combining ResNet and Unet showed performance improvements of 76.5%, 69.8%, and 5.9% compared to the general convolutional neural network (CNN), ResNet, and Unet, respectively. It was also confirmed that the proposed method could be effectively applied to measured spectral signals.\n",
      "Distribution Analysis of Feature Map and Gradients in Mobilenet and Resnet Model Layers using Glorot and He`s initialization 2021 ['가중치 초기화', 'Glorot 초기화', 'He 초기화', '컨볼루션 신경망 네트워크', '잡초 분류', 'Weights initialization', 'Glorot initialization', 'He initialization', 'Convolutional neural network', 'Weeds classification'] None Initializing the weights plays an essential role in a convolutional neural network model. This paper investigates how Glorot and Hes initialization methods behave in Mobilenet and Resnet models on the weeds classification problem. Experiments show that pointwise and depthwise convolution in Mobilenet reduces the variance of feature maps from earlier layers. Using the He’s method, shortcut connection in Resnet saturate values in logistic classify layer. The accuracy of Mobilenet and Resnet, using Glorots method, are 0.9568 and 0.9711, respectively. While using Hes method, we obtain 0.9471 using Mobilenet and 0.9645 using Resnet. Also, both models converge faster and better generalization using Glorots method than using Hes method.\n",
      "ResNet을 이용한 도로 네트워크 교통 데이터 예측 2021 ['합성곱 신경망', '잔차 학습', '전이 학습', '교통 속도 예측', 'Convolutional Neural Network', 'Residual Learning', 'ResNet', 'Transfer Learning', 'Traffic Speed Prediction'] None None\n",
      "SegNet과 ResNet을 조합한 딥러닝에 기반한 횡단보도 영역 검출 2021 ['Deep Learning', 'Semantic Segmentation', 'Zebra-crossing Detection', 'Neural Network', '딥러닝', '시맨틱 분할', '횡단보도 검출', '신경 네트워크'] None None\n",
      "운전자의 주의분산 연구동향 및 딥러닝 기반 동작 분류 모델 2021 ['Driver’s Behavior', 'Driver’s Distraction', 'Behavior Recognition', 'ResNet-101', 'CAM', '운전자의 동작', '운전자의 주의 분산', '동작 인식'] 본 논문에서는 운전자의 주의산만을 유발하는 운전자, 탑승자의 동작을 분석하고 핸드폰과 관련된 운전자의 행동 10가지를 인식하였다. 먼저 주의산만을 유발하는 동작을 환경 및 요인으로 분류하고 관련 최근 논문을 분석하였다. 분석된 논문을 기반으로 주의산만을 유발하는 주요 원인인 핸드폰과 관련된 10가지 운전자의 행동을 인식하였다. 약 10만 개의 이미지 데이터를 기반으로 실험을 진행하였다. SURF를 통해 특징을 추출하고 3가지 모델(CNN, ResNet-101, 개선된 ResNet-101)로 실험하였다. 개선된 ResNet-101 모델은 CNN보다 학습 오류와 검증 오류가 8.2배, 44.6배가량 줄어들었으며 평균적인 정밀도와 f1-score는 0.98로 높은 수준을 유지하였다. 또한 CAM(class activation maps)을 활용하여 딥러닝 모델이 운전자의 주의 분산 행동을 판단할 때, 핸드폰 객체와 위치를 결정적 원인으로 활용했는지 검토하였다. In this paper, we analyzed driver\"s and passenger\"s motions that cause driver\"s distraction, and recognized 10 driver\"s behaviors related to mobile phones. First, distraction-inducing behaviors were classified into environments and factors, and related recent papers were analyzed. Based on the analyzed papers, 10 driver\"s behaviors related to cell phones, which are the main causes of distraction, were recognized. The experiment was conducted based on about 100,000 image data. Features were extracted through SURF and tested with three models (CNN, ResNet-101, and improved ResNet-101). The improved ResNet-101 model reduced training and validation errors by 8.2 times and 44.6 times compared to CNN, and the average precision and f1-score were maintained at a high level of 0.98. In addition, using CAM (class activation maps), it was reviewed whether the deep learning model used the cell phone object and location as the decisive cause when judging the driver\"s distraction behavior.\n",
      "Identification of Indian butterflies using Deep Convolutional Neural Network 2021 ['Indian butterfly identification', 'ButterflyNet', 'Butterfly', 'classification CNN', 'Computer vision'] None The conventional butterfly identification method is based on their different morphological characters namely wing-venation, color, shape, patterns and through the dissection studies and molecular techniques which are tedious, expensive and highly time-consuming. To overcome the above aforesaid challenges, a new butterfly identification system using butterfly images has been designed to instantly identify the butterfly with high ac curacy. In this study, we construct a new butterfly dataset with 34,024 butterfly images belonging to 315 species from India. We propose and prove the effectiveness of new data augmentation techniques on our dataset. To identify butterflies using photographic images, we built eleven new Deep Convolutional Neural Network (DCNN) butterfly classifier models using eleven pre-trained architectures namely ResNet-18, ResNet-34, ResNet-50, ResNet-121, ResNet-152, Alex-Net, DenseNet-121, DenseNet-161, VGG-16, VGG-19 and SqueezeNet-v1.1. The different model’s classification results were compared and the proposed technique achieved a maximum top-1 accuracy(94.44%), top-3 accuracy(98.46%) and top-5 accuracy(99.09%) using ResNet-152 model, followed by DenseNet-161 model achieved the top-1 accuracy(94.31%), top-3 accuracy (98.07%) and top-5 accuracy (98.66%). The results suggest that models can be assertively used to identify butterflies in India.\n",
      "딥러닝과 의미론적 영상분할을 이용한 자동차 번호판의 숫자 및 문자영역 검출 2021 ['딥러닝', '합성곱 신경망(CNN)', '의미론적 분할', '자동차 번호판', '영상분할 및 인식', 'Deep Learning', 'Convolution Neural Network(CNN)', 'Semantic Segmentation', 'License Plate', 'Image Segmentation and Recognition'] 자동차 번호판 인식은 지능형 교통시스템에서 핵심적인 역할을 담당한다. 따라서 효율적으로 자동차 번호판의 숫자 및 문자영역을 검출하는 것은 매우 중요한 과정이다. 본 연구에서는 딥러닝과 의미론적 영상분할 알고리즘을 적용 하여 효과적으로 자동차 번호판의 번호영역을 검출하는 방법을 제안한다. 제안된 방법은 화소 투영과 같은 전처리과정 없이 번호판 영상에서 바로 숫자 및 문자영역을 검출하는 알고리즘이다. 번호판 영상은 도로 위에 설치된 고정 카메라로 부터 획득한 영상으로 날씨 및 조명변화 등을 모두 포함한 다양한 실제 상황에서 촬영된 것을 사용하였다. 입력 영상은 색상변화를 줄이기 위해 정규화하고 실험에 사용된 딥러닝 신경망 모델은 Vgg16, Vgg19, ResNet18 및 ResNet50이 다. 제안방법의 성능을 검토하기 위해 번호판 영상 500장으로 실험하였다. 학습을 위해 300장을 할당하였으며 테스트용 으로 200장을 사용하였다. 컴퓨터모의 실험결과 ResNet50을 사용할 때 가장 우수하였으며 95.77% 정확도를 얻었다. License plate recognition plays a key role in intelligent transportation systems. Therefore, it is a very important process to efficiently detect the number and character areas. In this paper, we propose a method to effectively detect license plate number area by applying deep learning and semantic image segmentation algorithm. The proposed method is an algorithm that detects number and text areas directly from the license plate without preprocessing such as pixel projection. The license plate image was acquired from a fixed camera installed on the road, and was used in various real situations taking into account both weather and lighting changes. The input images was normalized to reduce the color change, and the deep learning neural networks used in the experiment were Vgg16, Vgg19, ResNet18, and ResNet50. To examine the performance of the proposed method, we experimented with 500 license plate images. 300 sheets were used for learning and 200 sheets were used for testing. As a result of computer simulation, it was the best when using ResNet50, and 95.77% accuracy was obtained.\n",
      "사과 품종 분류를 위한 CNN기반 모델링 및 분류 기법 연구 2021 ['스마트팜', '딥러닝', 'ResNet', '신경망', 'Smart Farm', 'Deep Learning', 'ResNet', 'Convolution Neural Network'] 농장주들이 과일을 분류하는 데까지의 시간소모를 줄이고 향후 과일의 등급 판정 기준을 정량화하기 위하여 우리나라의 대표적인 과일이며 다양한 품종을 가지고 있는 사과를 대상으로 신경망 기반 분류 자동화 시스템을 제안한다. 컨베이어 벨트를 통해 지나가는 객체를 카메라모듈로 촬영하여 이를 통신 및 연산을 수행하는 라즈베리파이 기반 시스템을 설계, 구현한다. 깊은 네트워크와 나머지(residual)를 학습하는 ResNet 기반 알고리즘이 구동되는 딥러닝 서버와는 SSH통신을 통해 이미지와 학습된 모델을 주고받는다. 향후 품종뿐만 아니라 등급까지 분류 자동화한다면 다양한 품종의 과일을 대상으로 한 출하 자동화 시스템으로의 적용이 가능하다. This paper propose a neural network-based classification automation system for apples, which are representative fruits of Korea and have a variety of varieties, in order to reduce the consumption of time for farmers to classify fruits and to quantify the criteria for grading the fruits. Raspberry Pi-based system that communicates and performs calculations by photographing objects passing through a conveyor belt with a camera module is designed and implemented. Then it receive the trained model using ResNet-based algorithm that runs on the deep-learning server. In the future, if classifying not only varieties but also grades is automated, it can be applied as a shipping automation system targeting various varieties of fruit.\n",
      "The Classification of EEG-based Wink Signals: A CWT-Transfer Learning Pipeline 2021 ['BCI', 'CWT', 'EEG', 'Transfer Learning', 'SVM'] None Brain–Computer Interface technology plays a vital role in facilitating post-stroke patients’ ability to carry out their daily activities of living. The extraction of features and the classification of electroencephalogram (EEG) signals are pertinent parts in enabling such a system. This research investigates the efficacy of Transfer Learning models namely ResNet50 V2, ResNet101 V2, and ResNet152 V2 in extracting features from CWT converted wink-based EEG signals, prior to its classification via a fine-tuned Support Vector Machine (SVM) classifier. It was shown that ResNet152 V2-SVM pipeline could achieve an excellent accuracy on all train, test and validation datasets.\n",
      "Application of convolutional neural networks for distal radio-ulnar fracture detection on plain radiographs in the emergency room 2021 ['Wrist', 'Fractures', 'bone', 'Deep learning', 'Neural networks', 'computer'] None Objective Recent studies have suggested that deep-learning models can satisfactorily assist in fracture diagnosis. We aimed to evaluate the performance of two of such models in wrist fracture detection. Methods We collected image data of patients who visited with wrist trauma at the emergency department. A dataset extracted from January 2018 to May 2020 was split into training (90%) and test (10%) datasets, and two types of convolutional neural networks (i.e., DenseNet-161 and ResNet-152) were trained to detect wrist fractures. Gradient-weighted class activation mapping was used to highlight the regions of radiograph scans that contributed to the decision of the model. Performance of the convolutional neural network models was evaluated using the area under the receiver operating characteristic curve. Results For model training, we used 4,551 radiographs from 798 patients and 4,443 radiographs from 1,481 patients with and without fractures, respectively. The remaining 10% (300 radiographs from 100 patients with fractures and 690 radiographs from 230 patients without fractures) was used as a test dataset. The sensitivity, specificity, positive predictive value, negative predictive value, and accuracy of DenseNet-161 and ResNet-152 in the test dataset were 90.3%, 90.3%, 80.3%, 95.6%, and 90.3% and 88.6%, 88.4%, 76.9%, 94.7%, and 88.5%, respectively. The area under the receiver operating characteristic curves of DenseNet-161 and ResNet-152 for wrist fracture detection were 0.962 and 0.947, respectively. Conclusion We demonstrated that DenseNet-161 and ResNet-152 models could help detect wrist fractures in the emergency room with satisfactory performance.\n",
      "장면 복잡도 기반 적응적 얼굴 마스크 탐지 모델 2021 ['인공지능', '딥러닝', '기계학습', '객체 감지', '마스크 감지', '코로나바이러스-19', 'Artificial intelligence', 'Machine learning', 'Object detection', 'Deep learning', 'Mask detection', 'COVID-19'] 코로나바이러스-19(COVID-19)의 대유행에 따라 전 세계 수많은 확진자가 발생하고 있으며 국민을 불안에 떨게 하고 있다. 바이러스 감염 확산을 방지하기 위해서는 마스크를 제대로 착용하는것이 필수적이지만 몇몇 사람들은 마스크를 쓰지 않거나 제대로 착용하지 않고 있다. 본 논문에서는 영상 이미지에서의 효율적인 마스크 감지 시스템을 제안한다. 제안 방법은 우선 입력 이미지의 모든 얼굴의 영역을 YOLOv5를 사용하여 감지하고 감지된 얼굴의 수에 따라 3가지의 장면복잡도(Simple, Moderate, Complex) 중 하나로 분류한다. 그 후 장면 복잡도에 따라 3가지ResNet(ResNet-18, 50, 101) 중 하나를 기반으로 한 Faster-RCNN을 사용하여 얼굴 부위를 감지하고마스크를 제대로 착용하였는지 식별한다. 공개 마스크 감지 데이터셋을 활용하여 실험한 결과 제안한 장면 복잡도 기반 적응적인 모델이 다른 모델에 비해 가장 성능이 뛰어남을 확인하였다. Coronavirus disease 2019 (COVID-19) has affected the world seriously. Every person is required for wearing a mask properly in a public area to prevent spreading the virus. However, many people are not wearing a mask properly. In this paper, we propose an efficient mask detection system. In our proposed system, we first detect the faces of input images using YOLOv5 and classify them as the one of three scene complexity classes (Simple, Moderate, and Complex) based on the number of detected faces. After that, the image is fed into the Faster-RCNN with the one of three ResNet (ResNet-18, 50, and 101) as backbone network depending on the scene complexity for detecting the face area and identifying whether the person is wearing the mask properly or not. We evaluated our proposed system using public mask detection datasets. The results show that our proposed system outperforms other models.\n",
      "User Interface Application for Cancer Classification using Histopathology Images 2021 ['Deep Learning', 'Histopathology images', 'ResNet-34', 'Digital Pathology', 'AI', 'CAD'] None User interface for cancer classification system is a software application with clinician's friendly tools and functions to diagnose cancer from pathology images. Pathology evolved from manual diagnosis to computer-aided diagnosis with the help of Artificial Intelligence tools and algorithms. In this paper, we explained each block of the project life cycle for the implementation of automated breast cancer classification software using AI and machine learning algorithms to classify normal and invasive breast histology images. The system was designed to help the pathologists in an automatic and efficient diagnosis of breast cancer. To design the classification model, Hematoxylin and Eosin (H&E) stained breast histology images were obtained from the ICIAR Breast Cancer challenge. These images are stain normalized to minimize the error that can occur during model training due to pathological stains. The normalized dataset was fed into the ResNet-34 for the classification of normal and invasive breast cancer images. ResNet-34 gave 94% accuracy, 93% F Score, 95% of model Recall, and 91% precision.\n",
      "텍스타일 디자인 분류 및 관심 영역 도출에 대한 연구 2021 ['텍스타일 디자인 분류', '관심 영역 도출', 'Textile design', 'VGG-16', 'ResNet-34', 'LIME', 'region of interest'] 디자인에 있어서 유사한 디자인들을 그룹핑하여 분류하는 것은 관리적인 측면에서 효율성을 높여주고 사용적인 측면에서는 편의성을 제공한다. 본 연구는 인공지능 알고리즘을 이용하여 텍스타일 디자인을 도트, 꽃무늬, 줄무늬, 그리고 기하학으로 4개의 카테고리로 분류하고자 하였다. 특히, 인공지능의 관점에서 분류의 근거가 되는 관심 영역을 찾아내고 설명할 수 있는 지를 탐색하였다. 총 4,536개의 디자인을 8:2의 비율로 무작위 추출하여 학습용 데이터 3,629개와 테스트용 데이터 907개로 구성하였다. 분류에 사용된 모델은 VGG-16과 ResNet-34로 두 모델의 꽃무늬 디자인에 대한 정밀도는 각각 0.79%, 0.89%이며, 재현율은 0.95%, 0.38%로 우수한 분류 성과를 보였다. LIME(Local Interpretable Model-agnostic Explanation) 기법을 이용하여 분석한 결과에 따르면, 기하학과 꽃무늬 디자인의 경우 도형과 꽃잎 부분이 분류의 근거가 되는 관심 영역으로 도출되었다. Grouping and classifying similar designs in design increase efficiency in terms of management and provide convenience in terms of use. Using artificial intelligence algorithms, this study attempted to classify textile designs into four categories: dots, flower patterns, stripes, and geometry. In particular, we explored whether it is possible to find and explain the regions of interest underlying classification from the perspective of artificial intelligence. We randomly extracted a total of 4,536 designs at a ratio of 8:2, comprising 3,629 for training and 907 for testing. The models used in the classification were VGG-16 and ResNet-34, both of which showed excellent classification performance with precision on flower pattern designs of 0.79%, 0.89% and recall of 0.95% and 0.38%. Analysis using the Local Interpretable Model-agnostic Explanation (LIME) technique has shown that geometry and flower-patterned designs derived shapes and petals from the region of interest on which classification was based.\n",
      "공연예술에서 광고포스터의 이미지 특성을 활용한 딥러닝 기반 관객예측 2021 ['공연예술', '흥행 예측', 'CNN', 'VGG-16', 'Inception-v3', 'ResNet50', 'Performing Arts', 'Box Office Prediction'] 공연예술 기관에서의 공연에 대한 흥행 예측은 공연예술 산업 및 기관에서 매우 흥미롭고도 중요한 문제이다. 이를 위해 출연진, 공연장소, 가격 등 정형화된 데이터를 활용한 전통적인 예측방법론, 데이터마이닝 방법론이 제시되어 왔다. 그런데 관객들은 공연안내 포스터에 의하여 관람 의도가 소구되는 경향이 있음에도 불구하고, 포스터 이미지 분석을 통한 흥행 예측은 거의 시도되지 않았다. 그러나 최근 이미지를 통해 판별하는 CNN 계열의 딥러닝 방법이 개발되면서 포스터 분석의 가능성이 열렸다. 이에 본 연구의 목적은 공연 관련 포스터 이미지를 통해 흥행을 예측할 수 있는 딥러닝 방법을 제안하는 것이다. 이를 위해 KOPIS 공연예술 통합전산망에 공개된 포스터 이미지를 학습데이터로 하여 Pure CNN, VGG-16, Inception-v3, ResNet50 등 딥러닝 알고리즘을 통해 예측을 수행하였다. 또한 공연 관련 정형데이터를 활용한 전통적 회귀분석 방법론과의 앙상블을 시도하였다. 그 결과 흥행 예측 정확도 85%를 상회하는 높은 판별 성과를 보였다. 본 연구는 공연예술 분야에서 이미지 정보를 활용하여 흥행을 예측하는 첫 시도이며 본 연구에서 제안한 방법은 연극 외에 영화, 기관 홍보, 기업 제품 광고 등 포스터 기반의 광고를 하는 영역으로도 적용이 가능할 것이다. The prediction of box office performance in performing arts institutions is an important issue in the performing arts industry and institutions. For this, traditional prediction methodology and data mining methodology using standardized data such as cast members, performance venues, and ticket prices have been proposed. However, although it is evident that audiences tend to seek out their intentions by the performance guide poster, few attempts were made to predict box office performance by analyzing poster images. Hence, the purpose of this study is to propose a deep learning application method that can predict box office success through performance-related poster images. Prediction was performed using deep learning algorithms such as Pure CNN, VGG-16, Inception-v3, and ResNet50 using poster images published on the KOPIS as learning data set. In addition, an ensemble with traditional regression analysis methodology was also attempted. As a result, it showed high discrimination performance exceeding 85% of box office prediction accuracy. This study is the first attempt to predict box office success using image data in the performing arts field, and the method proposed in this study can be applied to the areas of poster-based advertisements such as institutional promotions and corporate product advertisements.\n",
      "Automated detection of corrosion in used nuclear fuel dry storage canisters using residual neural networks 2021 ['Convolutional neural networks', 'Corrosion', 'Deep learning', 'Dry storage canisters', 'Feature detection', 'Residual neural networks'] None Nondestructive evaluation methods play an important role in ensuring component integrity and safety in many industries. Operator fatigue can play a critical role in the reliability of such methods. This is important for inspecting high value assets or assets with a high consequence of failure, such as aerospace and nuclear components. Recent advances in convolution neural networks can support and automate these inspection efforts. This paper proposes using residual neural networks (ResNets) for real-time detection of corrosion, including iron oxide discoloration, pitting and stress corrosion cracking, in dry storage stainless steel canisters housing used nuclear fuel. The proposed approach crops nuclear canister images into smaller tiles, trains a ResNet on these tiles, and classifies images as corroded or intact using the per-image count of tiles predicted as corroded by the ResNet. The results demonstrate that such a deep learning approach allows to detect the locus of corrosion via smaller tiles, and at the same time to infer with high accuracy whether an image comes from a corroded canister. Thereby, the proposed approach holds promise to automate and speed up nuclear fuel canister inspections, to minimize inspection costs, and to partially replace human-conducted onsite inspections, thus reducing radiation doses to personnel.\n",
      "심층 신경망 기반 객체 인식 기법의 유사 객체 분류 성능 분석 2021 ['Deep Neural Networks', 'ResNet', 'DenseNet', 'Smart Factory', 'Armored Fighting Vehicle'] None In this study, deep neural networks were applied to a similar object classification method, and the classification performance was analyzed. For the similar object classification performance analysis, ResNet50 and DenseNet169 models, which are known to show similar behaviors, were selected. To verify the performance of these deep neural networks, a bolt recognition for smart factories and an armored fighting vehicle recognition were performed. In addition, image preprocessing methods to improve the similar object classification performance were proposed. The experimental results confirmed that appropriate image preprocessing methods should be applied according to the type of similar object to be classified.\n",
      "평활화 알고리즘에 따른 자궁경부 분류 모델의 성능 비교 연구 2021 ['Cervical cancer', 'Histogram equalization', 'Classification', 'ResNet-50'] None We developed a model to classify the absence of cervical cancer using deep learning from the cervical image to which the histogram equalization algorithm was applied, and to compare the performance of each model. A total of 4259 images were used for this study, of which 1852 images were normal and 2407 were abnormal. And this paper applied Image Sharpening(IS), Histogram Equalization(HE), and Contrast Limited Adaptive Histogram Equalization(CLAHE) to the original image. Peak Signal-to-Noise Ratio(PSNR) and Structural Similarity index for Measuring image quality(SSIM) were used to assess the quality of images objectively. As a result of assessment, IS showed 81.75dB of PSNR and 0.96 of SSIM, showing the best image quality. CLAHE and HE showed the PSNR of 62.67dB and 62.60dB respectively, while SSIM of CLAHE was shown as 0.86, which is closer to 1 than HE of 0.75. Using ResNet-50 model with transfer learning, digital image-processed images are classified into normal and abnormal each. In conclusion, the classification accuracy of each model is as follows. 90.77% for IS, which shows the highest, 90.26% for CLAHE and 87.60% for HE. As this study shows, applying proper digital image processing which is for cervical images to Computer Aided Diagnosis(CAD) can help both screening and diagnosing.\n",
      "웹 검색을 이용한 새 품종 분류를 위한 전이학습에 대한 연구 2021 ['deep learning', 'transfer learning', 'bird breed recognition', 'web crawling', 'ResNet50', 'ImageNet'] None Recently, research on image recognition and object extraction has been actively carried out through deep learning. In addition, research on transfer learning, which is used for new neural network learning by maintaining or partially changing the neural network function of a deep learning model pre-trained in a specific field, is also being actively conducted. However, although a large number of datasets are required for image learning, it is difficult to obtain desired quantities of images. In this paper, a large number of images were collected through Google and bird-related professional website crawling for bird breed classification, and images that can be used for learning were selected through various preprocessing. Then, using the ResNet50 model pre-trained with ImageNet, fine-tuning transfer learning was performed to classify bird breeds. In addition, the trained model was tested using the CUB_200-2011 data set, which is a data set for classifying new breeds, and the reliability of the search image was obtained with an accuracy of 87.87%.\n",
      "딥러닝 경량화를 위한 구조, 가지치기, 지식증류 비교 2021 ['Deep learning', 'Structure Reduction', 'Pruning', 'Knowledge Distillation', 'CIFAR10/100', 'ResNet56/110'] None We compare three approaches of structure reduction, pruning, and knowledge distillation for lightning of a deep learning network. Structure reduction eliminates a set of layers of the model, but pruning deletes filters within a layer. Knowledge distillation effectively learns a small student model from a large teacher model using KL Divergence. Therefore, it has a similar effect of reduction of the model. The above three methods for lightning are rarely compared to each other in terms of performance. To compare these approaches for network reduction problem, we investigate the accuracy and flops of the methods on CIFAR10 and CIFAR100 data for ResNet models. A systematic analysis for the fundamental orientations and differences of each method is supplemented.\n",
      "딥러닝 기반의 소비자 데이터를 응용한 외식업체 추천 시스템 구현에 관한 연구 2021 ['추천 시스템', '인공지능', '딥러닝', '분류', '감성 분석', '외식업체', 'Recommendation System', 'Artificial Intelligence', 'Deep Learning', 'Classification', 'Sentiment Analysis', 'Restaurant'] 본 연구에서는 소비자 데이터를 딥러닝 기반의 분류(Classification) 모델을 학습 시켜 추천 알고리즘을 구현하였다. 이를 위하여 사용자 데이터를 이미지로 변환 시켜 분류 과제에서 보편적으로 사용되는 ResNet50을 사용하여 학습한 결과로서 유의미한 결과에 대하여 제시함 In this study, a recommendation algorithm was implemented by learning a deep learning-based classification model for consumer data. For this purpose, a meaningful result is presented as a result of learning using ResNet50, which is commonly used in classification tasks by converting user data into images.\n",
      "Implementation and Performance Evaluation of Farm Waste Image Classification System using CNN-based Transfer Learning Models 2021 ['Artificial intelligence', 'Transfer-learning', 'CNN', 'Image classification', 'Farm waste collection'] 영농 폐기물의 증가로 인해, 빠르고 효율적으로 수거할 수 있는 스마트 영농 폐기물 모니터링 시스템 개발이 필요하다. 본 논문에서는 영농 폐기물 분류 시스템을 제안하고 실제 지역 농촌에서 직접 수집한 영상을 이용하여 CNN 기반의 전이학습 모델들을 구현하고 비교하였다. 영농 폐기물 영상 분류에 적합한 모델과 학습 조건을 찾기 위해, 3가지의 학습 자료군 구성 조건 (2종 분류, 6종 분류, 6종 하위분류를 가진 2종 분류)을 달리하여 미세 조정된 6개의 사전 훈련 CNN 모델들의 검증 정확도를 비교하였다. 그 결과, ResNet-50 모델의 성능이 모든 학습 조건에서 평균 90.9%의 정확도로 가장 높았고, 폐기물 영상을 6종 분류했을 때보다 2종 분류로 했을 때의 검증 정확도가 10% 더 높았다. 특히, 학습 자료군 구성 방법 중 6종 하위분류를 가진 2종 분류했을 때의 검증 정확도는 2종 분류했을 때와 유사했다. 이를 통해 영농 폐기물은 한 종류만 모여 있지 않을뿐더러 다양한 폐기물들이 한데 섞여 있어서 영농 폐기물의 특정한 세부 종류로 분류하는 것보다 폐기물인지 아닌지를 이진 분류하는 것이 더 효과적임을 확인하였다. 나아가, 제안된 시스템의 동작을 확인하기 위해, 영농 환경 모니터링 서버와 영농 폐기물 영상 분류 서버 사이에 TCP / IP 기반의 통신 환경을 구축하고, 모의실험을 통해 구현한 영농 폐기물 영상 분류 시스템이 스마트 영농 폐기물 모니터링 시스템으로 사용될 가능성을 확인하였다. 본 연구의 결과는 정형화되지 않거나 여러 병변이 혼합된 의료 영상을 분류하는 경우에도 활용될 수 있을 것이다. Due to the increase of farm waste in many countries, there’s a need to develop a smart farm waste monitoring system that can collect it promptly and efficiently. In this paper, we proposed, compared the performance of a convolutional neural network (CNN) -based transfer learning models and implement a farm waste image classification system, which is crucial component for the monitoring system. To find an appropriate model and labelling methods for farm waste image classification, we compared each validation accuracy of six different pre-trained CNN methods with three types of labelling scheme, using the waste images taken directly from the farming area. As a result, the ResNet-50 model performed best with an accuracy of 90.9% on average. Also, when classified into 2 categories, the accuracy was about 10% higher than that of the 6 categories. Furthermore, when the image was classified into 2 main categories with 6 sub-categories, the validation accuracy was similar to that of the 2 categories. Through these results, it seemed to be more effective to classify with binary labels such as ‘trash’ and ‘non-trash’, rather than with multiple labels of specific categories because farm waste is generated not only by single type of waste but also by various types of mixed waste. And a TCP / IP based communication environment between farm environment monitoring server and farm waste image classification server has been implemented. Experimental results using the system implemented for a smart farm waste monitoring showed that the proposed system can be used for a smart farm waste collection system. Also, the result of this study could be applied to classify medical images of unstructured and/or mixed lesion.\n",
      "Five-Class Classification of Cervical Pap Smear Images: A Study of CNN-Error-Correcting SVM Models 2021 ['Cervix Uteri', 'Diagnosis', 'Nerve Net', 'Papanicolaou Test', 'Support Vector Network'] None Objectives: Different complex strategies of fusing handcrafted descriptors and features from convolutional neural network(CNN) models have been studied, mainly for two-class Papanicolaou (Pap) smear image classification. This paper explores asimplified system using combined binary coding for a five-class version of this problem. Methods: This system extracted featuresfrom transfer learning of AlexNet, VGG19, and ResNet50 networks before reducing this problem into multiple binarysub-problems using error-correcting coding. The learners were trained using the support vector machine (SVM) method. The outputs of these classifiers were combined and compared to the true class codes for the final prediction. Results: Despitethe superior performance of VGG19-SVM, with mean ± standard deviation accuracy and sensitivity of 80.68% ± 2.00% and80.86% ± 0.45%, respectively, this model required a long training time. There were also false-negative cases using both theVGGNet-SVM and ResNet-SVM models. AlexNet-SVM was more efficient in terms of running speed and prediction consistency. Our findings also showed good diagnostic ability, with an area under the curve of approximately 0.95. Further investigationalso showed good agreement between our research outcomes and that of the state-of-the-art methods, with specificityranging from 93% to 100%. Conclusions: We believe that the AlexNet-SVM model can be conveniently applied for clinicaluse. Further research could include the implementation of an optimization algorithm for hyperparameter tuning, as well asan appropriate selection of experimental design to improve the efficiency of Pap smear image classification.\n",
      "영상 콘텐츠의 오디오 분석을 통한 메타데이터 자동 생성 방법 2021 ['Audio', 'AI', 'Metadata', 'Recommendation System', 'Voice Recognition'] 영상 콘텐츠를 사용자에게 추천하기 위해서는 메타데이터가 필수적인 요소로 자리 잡고 있다. 하지만 이러한 메타데이터는 영상 콘텐츠 제공자에 의해 수동적으로 생성되고 있다. 본 논문에서는 기존 수동으로 직접 메타데이터를 입력하는 방식에서 자동으로 메타데이터를 생성하는 방법을 연구하였다. 기존 연구에서 감정 태그를 추출하는 방법에 추가로 영화 오디오를 통한 장르와 제작국가에 대한 메타데이터 자동 생성 방법에 대해 연구를 진행하였다. 전이학습 모델인 ResNet34 인공 신경망 모델을 이용하여 오디오의 스펙트로그램으로부터 장르를 추출하고, 영화 속 화자의 음성을 음성인식을 통해 언어를 감지하였다. 이를 통해 메타데이터를 생성 인공지능을 통해 자동 생성 가능성을 확인할 수 있었다. A meatadata has become an essential element in order to recommend video content to users. However, it is passively generated by video content providers. In the paper, a method for automatically generating metadata was studied in the existing manual metadata input method. In addition to the method of extracting emotion tags in the previous study, a study was conducted on a method for automatically generating metadata for genre and country of production through movie audio. The genre was extracted from the audio spectrogram using the ResNet34 artificial neural network model, a transfer learning model, and the language of the speaker in the movie was detected through speech recognition. Through this, it was possible to confirm the possibility of automatically generating metadata through artificial intelligence.\n",
      "Detection of surface roughness of mechanical drawings with deep learning 2021 ['· Mechanical drawings · Image analysis · Object detection · Image recognition · Deep learning models · Evaluation metrics'] None Engineering drawing inspection is important to CAD modeling of mechanical parts. Traditional inspection methods mainly rely on manual analysis by using the CAD software, which requires expert knowledge and massive time. In view of simplifying the analysis for non-experts and improving detection efficiency and accuracy, this study proposes a generic approach combining object detection and image recognition methods to identify surface roughness of mechanical drawings. For both the object detection and image recognition methods, deep learning models with different backbone networks are trained and tested independently.Experimental results show that a combination of Faster-RCNN with ResNet101 as backbone network, and SSD with ResNet50 as backbone network achieves the best performance under our evaluation metrics.\n",
      "다양한 CNN 가속기에서 아키텍처에 따른 면적, 에너지, 성능 분석 2021 ['CNN 가속기', '아키텍처', '메모리 대역폭', '뉴럴 네트워크', 'CNN accelerators', 'architecture', 'memory bandwidth', 'neural networks'] Convolution Neural Network 가속기는 AI시대에 중요한 요소 중 하나로 떠오르게 되었다. CNN 가속기의 내부 구성을 몇 가지로 나눈다면 연산을 위한 Multiplier-Accumulator (MAC unit), 데이터 저장을 위한 SRAM, 데이터 이동을 위한 메모리 인터페이스 그리고 제어 로직으로 구분할 수 있다. 다양한 CNN 가속기들의 경우, 각기 다른 공정과 동작 주파수를 기준으로 제안되었으며, 또한 아키텍처 형태에 따라 내부 MAC unit의 수와 SRAM의 크기가 매우 큰 차이를 갖는 형태로 구성되어있다. 각 가속기들의 기본 사양으로 면적, 에너지, 성능을 비교하였을 때는 공정이나 동작 주파수 등 여러 조건들에 의해서 아키텍처에 따른 정량적인 비교가 용이하지 않게 된다. 따라서, 본 논문에서는 다양한 CNN 가속기에서 여러 조건들을 동일하게 재구성하였을 때, ResNet-50 추론 동작 시에 요구되는 면적, 에너지, 성능을 비교하여 아키텍처의 특징과 경향성을 분석하였다. The Convolution Neural Network accelerator has emerged as an important element in the AI era. The primary components of a CNN accelerator include the Multiplier-Accumulator (MAC unit) for calculation, SRAM for data storage, memory interface for data movement, and control logic. Different CNN accelerators have been designed based on different assumptions regarding the process technologies and operating frequencies. In addition, the number of internal MAC units and the size of SRAM vary substantially between different types of architectures. These factors make it difficult to design a fair comparison of the area, energy, and performance of different CNN accelerators. In this paper, we attempt to compare the area, energy, and performance of different CNN accelerator architectures by constructing them all with the same fabrication process and operating frequency while making inferences using the ResNet-50 network.\n",
      "비디오 인코더를 통한 딥러닝 모델의 정수 가중치 압축 2021 ['Deep Learning Model Parameter Quantization', 'Weight compression', 'Lightweight model'] None Recently, various lightweight methods for using Convolutional Neural Network(CNN) models in mobile devices have emerged. Weight quantization, which lowers bit precision of weights, is a lightweight method that enables a model to be used through integer calculation in a mobile environment where GPU acceleration is unable. Weight quantization has already been used in various models as a lightweight method to reduce computational complexity and model size with a small loss of accuracy. Considering the size of memory and computing speed as well as the storage size of the device and the limited network environment, this paper proposes a method of compressing integer weights after quantization using a video codec as a method. To verify the performance of the proposed method, experiments were conducted on VGG16, Resnet50, and Resnet18 models trained with ImageNet and Places365 datasets. As a result, loss of accuracy less than 2% and high compression efficiency were achieved in various models. In addition, as a result of comparison with similar compression methods, it was verified that the compression efficiency was more than doubled.\n",
      "이미지 감성분류를 위한 CNN과 K-means RGB Cluster 이-단계 학습 방안 2021 ['이미지 감성분류', '색감', '이-단계 학습', 'Sentiment Analysis of Image', 'Sense of Color', 'CNN', 'Two-stage learning'] None The biggest reason for using a deep learning model in image classification is that it is possible to consider the relationship between each region by extracting each regions features from the overall information of the image. However, the CNN model may not be suitable for emotional image data without the images regional features. To solve the difficulty of classifying emotion images, many researchers each year propose a CNN-based architecture suitable for emotion images. Studies on the relationship between color and human emotion were also conducted, and results were derived that different emotions are induced according to color. In studies using deep learning, there have been studies that apply color information to image subtraction classification. The case where the images color information is additionally used than the case where the classification model is trained with only the image improves the accuracy of classifying image emotions.  This study proposes two ways to increase the accuracy by incorporating the result value after the model classifies an images emotion. Both methods improve accuracy by modifying the result value based on statistics using the color of the picture. When performing the test by finding the two-color combinations most distributed for all training data, the two-color combinations most distributed for each test data image were found. The result values were corrected according to the color combination distribution. This method weights the result value obtained after the model classifies an images emotion by creating an expression based on the log function and the exponential function.  Emotion6, classified into six emotions, and Artphoto classified into eight categories were used for the image data. Densenet169, Mnasnet, Resnet101, Resnet152, and Vgg19 architectures were used for the CNN model, and the performance evaluation was compared before and after applying the two-stage learning to the CNN model.  Inspired by color psychology, which deals with the relationship between colors and emotions, when creating a model that classifies an images sentiment, we studied how to improve accuracy by modifying the result values based on color. Sixteen colors were used: red, orange, yellow, green, blue, indigo, purple, turquoise, pink, magenta, brown, gray, silver, gold, white, and black. It has meaning. Using Scikit-learns Clustering, the seven colors that are primarily distributed in the image are checked. Then, the RGB coordinate values of the colors from the image are compared with the RGB coordinate values of the 16 colors presented in the above data. That is, it was converted to the closest color. Suppose three or more color combinations are selected. In that case, too many color combinations occur, resulting in a problem in which the distribution is scattered, so a situation fewer influences the result value. Therefore, to solve this problem, two-color combinations were found and weighted to the model. Before training, the most distributed color combinations were found for all training data images. The distribution of color combinations for each class was stored in a Python dictionary format to be used during testing. During the test, the two-color combinations that are most distributed for each test data image are found. After that, we checked how the color combinations were distributed in the training data and corrected the result. We devised several equations to weight the result value from the model based on the extracted color as described above.  The data set was randomly divided by 80:20, and the model was verified using 20% of the data as a test set. After splitting the remaining 80% of the data into five divisions to perform 5-fold cross-validation, the model was trained five times using different verification datasets. Finally, the performance was checked using the test dataset that was previously separated. Adam was used as the activation function, and the learning rate\n",
      "전단 융합 기반 멀티모달 심층학습을 이용한 손동작 분류 2021 ['Hand Gesture Classification', 'Deep Learning', 'EMG', 'Multimodal Learning', 'Ninapro DB'] None In this paper, we propose a new hand gesture classification strategy using early fusion based multimodal deep learning. The structure and parameters of the state-of-the-art deep learning models such as ResNet152, DenseNet201, EfficientNetB0 for the source task of image classification are reused in the target task of hand gesture classification using surface electromyograph(EMG) and finger\"s kinematic data. The time-domain EMG and kinematic signals are normalized and then transformed into combined 2-D images for the early-fusion network. The experimental results support the superiority of the proposed method in terms of classification accuracy. The transfer learning model with the EfficientNetB0 shows the 93.94% accuracy for 40 gestures of 40 participants in the Ninapro DB2.\n",
      "RVC 정규화와 전이학습을 이용한 손동작 인식 2021 ['Transfer Learning', 'Reference Voluntary Contraction', 'Hand Gesture Recognition', 'EMG', 'Ninapro DB'] None In this paper, we propose a new hand gesture recognition strategy using network-based transfer learning(TL) and reference voluntary contraction(RVC) normalization. The structure and parameters of the state-of-the-art deep learning models such as VGG19, ResNet152 and DenseNet121 for source task of image classification are reused in the target task of hand gesture recognition based on surface electromyography(EMG) signals. To mitigate the difficulty in handling the subject-dependent EMG signals, the RVC normalization is adopted in the signal pre-processing. The time-domain EMG signals are transformed into 2-D images for TL networks. The experimental results verify the validity of the proposed method in terms of recognition accuracy. The TL using VGG19, RVC normalization and gray image transformation shows 99.78% accuracy for the data from 15 participants performing 20 different gestures.\n",
      "Former Unmanned Surface Vehicle Detection Based on Improved Convolutional Neural Network 2021 ['Object detection', 'Accuracy', 'Tracking system', 'Monocular camera'] None This paper proposes an approach to the real-time implementation of a convolutional neural network (CNN)-based object detector for a former Unmanned Surface Vehicle (USV). The original network VGG-16 of the Single Shot MultiBox Detector (SSD) is first replaced with ResNet-18, as the basic feature extraction network. The classifying network is then redesigned by reducing half of the convolutional kernel numbers, where kernel sizes of 1×1 and 3×3 are mainly used. Simultaneously, a monocular camera installed on the tracking system, is used to calculate the distance and azimuth of the former USV. The experimental results show that the proposed method has advantages of higher accuracy and lower computational complexity, compared with other existing approaches. Therefore, the proposed approach can be efficiently used on real-time tracking systems.\n",
      "Apple Detection Algorithm based on an Improved SSD 2021 ['RFB', 'Attention Model', 'SSD', 'Apple detection', 'Objection detection', 'CNN'] None Under natural conditions, Apple detection has the problems of occlusion and small object detection difficulties. This paper proposes an improved model based on SSD. The SSD backbone network VGG16 is replaced with the ResNet50 network model, and the receptive field structure RFB structure is introduced. The RFB model amplifies the feature information of small objects and improves the detection accuracy of small objects. Combined with the attention mechanism (SE) to filter out the information that needs to be retained, the semantic information of the detection objectis enhanced. An improved SSD algorithm is trained on the VOC2007 data set. Compared with SSD, the improved algorithm has increased the accuracy of occlusion and small object detection by 3.4% and 3.9%. The algorithm has improved the false detection rate and missed detection rate. The improved algorithm proposed in this paper has higher efficiency.\n",
      "Comparison of Pre-processed Brain Tumor MR Images Using Deep Learning Detection Algorithms 2021 ['Brain Tumor', 'RetinaNet', 'Deep Learning', 'Histogram Equalization'] None Detecting brain tumors of different sizes is a challenging task. This study aimed to identify brain tumors using detection algorithms. Most studies in this area use segmentation; however, we utilized detection owing to its advantages. Data were obtained from 64 patients and 11,200 MR images. The deep learning model used was RetinaNet, which is based on ResNet152. The model learned three different types of pre-processing images: normal, general histogram equalization, and contrast-limited adaptive histogram equalization (CLAHE). The three types of images were compared to determine the pre-processing technique that exhibits the best performance in the deep learning algorithms. During pre-processing, we converted the MR images from DICOM to JPG format. Additionally, we regulated the window level and width. The model compared the pre-processed images to determine which images showed adequate performance; CLAHE showed the best performance, with a sensitivity of 81.79%. The RetinaNet model for detecting brain tumors through deep learning algorithms demonstrated satisfactory performance in finding lesions. In future, we plan to develop a new model for improving the detection performance using well-processed data. This study lays the groundwork for future detection technologies that can help doctors find lesions more easily in clinical tasks.\n",
      "Rice Fungal Diseases Recognition Using Modern Computer Vision Techniques 2021 ['Convolutional neural networks', 'Machine learning', 'Computer vision', 'Rice', 'Fungal diseases'] None In the article, the authors study the possibility of detecting some fungal diseases of rice using visual computing and machine learning techniques. Leaf blast and brown spot diseases are considered. Modern computer vision methods based on convolutional neural networks are used to identify a particular disease on an image. The authors compare the four most successful and compact convolutional neural network architectures: GoogleNet, ResNet-18, SqueezeNet-1.0, and DenseNet-121. The authors show that in the dataset used for the analysis, the disease can be detected with an accuracy of at least 95%. Testing the algorithm on real data not used in training showed an accuracy of up to 95.6%. This is a good indicator of the reliability and stability of the obtained solution even to a change in the data distribution. Data not used in training showed an accuracy of up to 95.6%. This is a good indicator of the reliability and stability of the obtained solution even to a change in the data distribution.\n",
      "MAT-AGCA: Multi Augmentation Technique on small dataset for Balinese character recognition using Convolutional Neural Network 2021 ['Balinese character', 'Lontar manuscript', 'Data augmentation', 'Adaptive Gaussian Thresholding', 'Convolutional Autoencoder'] None The lontar manuscript is an ancient Balinese cultural heritage written using Balinese characters on palm leaves. The recognition of Balinese characters in lontar is challenging because it has noise and limited data availability. To solve these problems, data augmentation is needed to increase the variety and amount of data to improve recognition performance. In this study, we collected Balinese character images from 50 lontar manuscript writers. We proposed MAT-AGCA that combines Adaptive Gaussian Thresholding and Convolutional Autoencoder for data augmentation. Based on experiments using InceptionResnetV2, DenseNet169, ResNet152V2, VGG19, and MobileNetV2, our proposed method achieved the best performance with 96.29% accuracy.\n",
      "Deep Convolutional Neural Network를 적용한 피하 종괴의 초음파적 진단: 실험적 연구 2021 ['Deep learning', 'Epidermal cyst', 'Lipoma', 'Ultrasonography'] None Background: Ultrasonography is an effective noninvasive imaging modality for the diagnosis of subcutaneous masses. To date, few studies have reported skin ultrasonography using deep convolutional neural networks (DCNNs). We investigated the accuracy of DCNNs for the diagnosis of epidermal cysts, lipomas, and other subcutaneous masses.Objective: The purpose of this study was to evaluate whether DCNNs could diagnose subcutaneous masses with ultrasonographic images at level of competence comparable to dermatologists.Methods: We created a dataset of 1,361 skin ultrasonography images obtained from 202 patients diagnosed with epidermal cysts, lipomas, and other subcutaneous masses, to train the DCNNs using ResNet18. Performance was compared with another set of 93 ultrasonographic images (24 epidermal cysts, 25 lipomas, and 44 other subcutaneous masses) from open-access articles.Results: The DCNNs yielded 87.10% classification accuracy and 86.10% F1-scores. The area under the curve, sensitivity, and specificity were 0.92 (95% confidence interval [CI] 0.86∼0.98), 75.00%, and 98.55% for epidermal cysts; 0.93 (95% CI 0.88∼0.98), 80.00%, and 94.12% for lipomas; and 0.97 (95% CI 0.93∼1.00), 97.73%, and 85.71% for other subcutaneous masses, respectively. Analysis using gradient-weighted class activation mapping revealed that the DCNNs could detect specific ultrasonographic findings of epidermal cysts and lipomas.Conclusion: We propose that DCNNs combined with ultrasonography may aid in the diagnosis of subcutaneous masses in outpatient settings. (Korean J Dermatol 2021;59(7):513∼520)\n",
      "합성곱 신경망의 Channel Attention 모듈 및 제한적인 각도 다양성 조건에서의 SAR 표적영상 식별로의 적용 2021 None None In the field of automatic target recognition(ATR) with synthetic aperture radar(SAR) imagery, it is usually impractical to obtain SAR target images covering a full range of aspect views. When the database consists of SAR target images with limited angular diversity, it can lead to performance degradation of the SAR-ATR system. To address this problem, this paper proposes a deep learning-based method where channel attention modules(CAMs) are inserted to a convolutional neural network(CNN). Motivated by the idea of the squeeze-and-excitation(SE) network, the CAM is considered to help improve recognition performance by selectively emphasizing discriminative features and suppressing ones with less information. After testing various CAM types included in the ResNet18-type base network, the SE CAM and its modified forms are applied to SAR target recognition using MSTAR dataset with different reduction ratios in order to validate recognition performance improvement under the limited angular diversity condition.\n",
      "고성능 CNN 기반 정밀 요검사 판별 기법 2021 ['CNN', 'Urinalysis', 'Image Discrimination'] 요검사는 물리적 성상 검사, 화학적 검사, 현미경 검사 세 가지가 있다. 이 중에서 화학적 요검사는 일반인이 쉽게 접근하는 방법으로 요검사지의 화학반응을 눈으로 표준비색표와 비교하거나 휴대용 요검사기를 별도로 구매하여 검사를 진행한다. 현재는 스마트폰의 보급이 대중화되어 스마트폰을 활용한 요검사 서비스 연구가 높아지고 있다. 요검사 스크리닝 애플리케이션은 스마트폰을 활용한 요검사 서비스 중 하나이다. 그러나 요검사 스크리닝 애플리케이션으로 촬영한 요검사 패드 RGB 값은 조명영향으로 인해 큰 편차가 발생한다. 요검사 패드 RGB 값의 편차는 요검사 판별의 정확도를 떨어뜨린다. 따라서 본 논문에서는 스마트폰 기반 요검사 스크리닝 애플리케이션으로 촬영한 요검사지를 검사 항목별 요검사 패드로 분류한 후 CNN을 통해 요검사 패드 이미지 판별의 정확도를 높인다. 요검사지는 다양한 배경에서 촬영하여 CNN 이미지를 생성하였으며 ResNet-50 CNN 모델을 사용하여 요검사 판별을 분석하였다. None\n",
      "컨볼루션 신경망 모델을 이용한 분류에서 입력 영상의 종류가 정확도에 미치는 영향 2021 ['X-ray', 'Convolutional neural network', 'Classification', 'Deep learning'] None The purpose of this study is to classify TIFF images, PNG images, and JPEG images using deep learning, and to compare the accuracy by verifying the classification performance. The TIFF, PNG, and JPEG images converted from chest X-ray DICOM images were applied to five deep neural network models performed in image recognition and classification to compare classification performance. The data consisted of a total of 4,000 X-ray images, which were converted from DICOM images into 16-bit TIFF images and 8-bit PNG and JPEG images. The learning models are CNN models - VGG16, ResNet50, InceptionV3, DenseNet121, and EfficientNetB0. The accuracy of the five convolutional neural network models of TIFF images is 99.86%, 99.86%, 99.99%, 100%, and 99.89%. The accuracy of PNG images is 99.88%, 100%, 99.97%, 99.87%, and 100%. The accuracy of JPEG images is 100%, 100%, 99.96%, 99.89%, and 100%. Validation of classification performance using test data showed 100% in accuracy, precision, recall and F1 score. Our classification results show that when DICOM images are converted to TIFF, PNG, and JPEG images and learned through preprocessing, the learning works well in all formats. In medical imaging research using deep learning, the classification performance is not affected by converting DICOM images into any format.\n",
      "Novel Image Classification Method Based on Few-Shot Learning in Monkey Species 2021 ['Deep learning', 'Feature extraction', 'Few-shot learning', 'Image classification'] None This paper proposes a novel image classification method based on few-shot learning, which is mainly used to solve model overfitting and non-convergence in image classification tasks of small datasets and improve the accuracy of classification. This method uses model structure optimization to extend the basic convolutional neural network (CNN) model and extracts more image features by adding convolutional layers, thereby improving the classification accuracy. We incorporated certain measures to improve the performance of the model. First, we used general methods such as setting a lower learning rate and shuffling to promote the rapid convergence of the model. Second, we used the data expansion technology to preprocess small datasets to increase the number of training data sets and suppress over-fitting. We applied the model to 10 monkey species and achieved outstanding performances. Experiments indicated that our proposed method achieved an accuracy of 87.92%, which is 26.1% higher than that of the traditional CNN method and 1.1% higher than that of the deep convolutional neural network ResNet50.\n",
      "딥러닝 기반의 철강 표면의 결함 검출기 2021 ['Artificial intelligence', 'Convolutional neural network', 'Deep learning', 'Image classification', 'Metal surface defect', 'Surface defect detection (SDD)'] None Steel surface defect should be detected and repaired in steel industry. Therefore, automatic detection of the steel surface defects plays a vital role in the steel manufacturing process. For the defect detection, machine learning based classification methods have been widely used such as HAAR feature-based cascade classifiers and support vector machines (SVM). As deep learning methods have been popular, the neural network based surface defect detection has been recently introduced. As for the methods, many researchers, in general, adopt a trained neural network, which is mainly winner in the recent ILSVRC (ImageNet Large Scale Visual Recognition Challenge). Then, the weights and last layers are modified to be used for surface defect detection (SDD), which is called transfer learning. In the previous researches, ResNet152 (winner in ILSVRC 2015) was used and the resulting performances were F1=0.975 and F1=0.912 in two different studies, respectively. However, the neural network used in their research has very wide and deep. Therefore, huge memories to save the trained weights and many multiplier–accumulators (MAC) are necessary, which means expensive hardware systems are essential to predict surface defect on the steel surface. This paper suggests a small neural network dedicated to surface defect detection. The proposed network has only three convolution layers and two fully connected layers. From the experimental results, we obtained F1=0.931 and minimum AUC (area under the curve)=0.995.\n",
      "Wood Classification of Japanese Fagaceae using Partial Sample Area and Convolutional Neural Networks 2021 ['wood', 'microscopic image', 'sample selection', 'classification', 'convolutional neural network'] None Wood identification is regularly performed by observing the wood anatomy, such as colour, texture, fibre direction, and other characteristics. The manual process, however, could be time consuming, especially when identification work is required at high quantity. Considering this condition, a convolutional neural networks (CNN)-based program is applied to improve the image classification results. The research focuses on the algorithm accuracy and efficiency in dealing with the dataset limitations. For this, it is proposed to do the sample selection process or only take a small portion of the existing image. Still, it can be expected to represent the overall picture to maintain and improve the generalisation capabilities of the CNN method in the classification stages. The experiments yielded an incredible F1 score average up to 93.4% for medium sample area sizes (200 × 200 pixels) on each CNN architecture (VGG16, ResNet50, MobileNet, DenseNet121, and Xception based). Whereas DenseNet121-based architecture was found to be the best architecture in maintaining the generalisation of its model for each sample area size (100, 200, and 300 pixels). The experimental results showed that the proposed algorithm can be an accurate and reliable solution.\n",
      "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 2021 ['분류', '딥 러닝', '유사 이미지', '컨볼루셔널 뉴럴 네트워크', '혼동률', 'Classification', 'Deep Learning', 'Similar Image', 'CNN', 'Confusion Rate'] 딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다. Deep learning in computer vision has made accelerated improvement over a short period but large-scale learning data and computing power are still essential that required time-consuming trial and error tasks are involved to derive an optimal network model. In this study, we propose a similar image classification performance improvement method based on CR (Confusion Rate) that considers only the characteristics of the data itself regardless of network optimization or data reinforcement. The proposed method is a technique that improves the performance of the deep learning model by calculating the CRs for images in a dataset with similar characteristics and reflecting it in the weight of the Loss Function. Also, the CR-based recognition method is advantageous for image identification with high similarity because it enables image recognition in consideration of similarity between classes. As a result of applying the proposed method to the Resnet18 model, it showed a performance improvement of 0.22% in HanDB and 3.38% in Animal-10N. The proposed method is expected to be the basis for artificial intelligence research using noisy labeled data accompanying large-scale learning data.\n",
      "변형 Residual Convolutional Neural Network 모델을 이용한 고효율 심전도 데이터 분석 기법 2021 ['MIT-BIH arrhythmia 데이터베이스', 'ResNet', 'ResNeXt', 'Adabound', '주입기법'] None None\n",
      "깊은 합성곱 신경망 모델에 따른 유방 초음파 영상 분류 성능 비교 2021 ['Breast Ultrasound', 'Breast Cancer', 'Tumor', 'Classification', 'VGG', 'ResNet', 'InceptionNet', 'DenseNet', 'EfficientNet', 'Convolutional Neural Network'] None Breast ultrasound has been widely utilized for classifying tumors into benignancy and malignancy. The limitations of traditional breast ultrasound are the handcrafted features obtained by well-trained sonographers and subjective decision according to different individual experiences. Recently, CNN-based deep learning techniques have exhibited better performance in medical images. However, most research for deep learning in medical ultrasound adopts CNN models developed for natural images due to the lack of common standard and dataset. In this paper, we compare six DCNN models which exhibit good performance for natural images - VGGNet, ResNet, InceptionNet, DenseNet, and EfficientNet. Our classification results demonstrate that CNN models of relatively lower performance on natural images show better performance on gray-scale ultrasound images and further study of CNN models are needed focusing on the features of medical images.\n",
      "합성곱 신경망을 이용한 컨포멀 코팅 PCB에 발생한 문제성 기포 검출 알고리즘 2021 ['Problematic Bubble', 'Bubble Detection', 'Conformal Coating', 'CNN', 'ResNet'] None Conformal coating is a technology that protects PCB(Printed Circuit Board) and minimizes PCB failures. Since the defects in the coating are linked to failure of the PCB, the coating surface is examined for air bubbles to satisfy the successful conditions of the conformal coating. In this paper, we propose an algorithm for detecting problematic bubbles in high-risk groups by applying image signal processing. The algorithm consists of finding candidates for problematic bubbles and verifying candidates. Bubbles do not appear in visible light images, but can be visually distinguished from UV(Ultra Violet) light sources. In particular the center of the problematic bubble is dark in brightness and the border is high in brightness. In the paper, these brightness characteristics are called valley and mountain features, and the areas where both characteristics appear at the same time are candidates for problematic bubbles. However, it is necessary to verify candidates because there may be candidates who are not bubbles. In the candidate verification phase, we used convolutional neural network models, and ResNet performed best compared to other models. The algorithms presented in this paper showed the performance of precision 0.805, recall 0.763, and f1-score 0.767, and these results show sufficient potential for bubble test automation.\n",
      "전기화재 원인분석을 위한 용융흔 외형 판별 딥러닝 알고리즘 설계 2021 ['전기 화재', '단락흔', '열흔', 'CNN', 'Resnet 알고리즘', 'Electric fire', 'Arc beads', 'Molten mark', 'CNN', 'Resnet'] None None\n",
      "안면 연령 예측을 위한 CNN기반의 히트 맵을 이용한 랜드마크 선정 2021 None None The purpose of this study is to improve the performance of the artificial neural network system for facial image analysis through the image landmark selection technique. For landmark selection, a CNN-based multi-layer ResNet model for classification of facial image age is required. From the configured ResNet model, a heat map that detects the change of the output node according to the change of the input node is extracted. By combining a plurality of extracted heat maps, facial landmarks related to age classification prediction are created. The importance of each pixel location can be analyzed through facial landmarks. In addition, by removing the pixels with low weights, a significant amount of input data can be reduced.\n",
      "딥러닝을 이용한 마스크 착용 여부 검사 시스템 2021 None None Recently, due to COVID-19, studies have been popularly worked to apply neural network to mask wearing automatic detection system. For applying neural networks, the 1-stage detection or 2-stage detection methods are used, and if data are not sufficiently collected, the pretrained neural network models are studied by applying fine-tuning techniques. In this paper, the system is consisted of 2-stage detection method that contain MTCNN model for face recognition and ResNet model for mask detection. The mask detector was experimented by applying five ResNet models to improve accuracy and fps in various environments. Training data used 17,217 images that collected using web crawler, and for inference, we used 1,913 images and two one-minute videos respectively. The experiment showed a high accuracy of 96.39% for images and 92.98% for video, and the speed of inference for video was 10.78fps.\n",
      "심층 CNN 기반 구조를 이용한 토마토 작물 병해충 분류 모델 2021 ['Convolutional Neural Networks', 'Deep Learning', 'Transfer Learning', 'Fine Tuning', 'Plant Diseases Classification'] 토마토 작물은 병해충의 영향을 많이 받기 때문에 이를 예방하지 않으면 농업 경제에 막대한 손실을 초래할 수 있다. 따라서 토마토의 다양한 병해충의 진단을 빠르고 정확하게 진단하는 시스템이 요구된다. 본 논문에서는 ImageNet 데이터 셋 상에서 다양하게 사전 학습된 딥러닝 기반 CNN 모델을 적용하여 토마토의 9가지 병해충 및 정상인 경우의 클래스를 분류하는 시스템을 제안한다. PlantVillage 데이터 셋으로부터 발췌한 토마토 잎의 이미지 셋을 3가지 딥러닝 기반 CNN 구조를 갖는 ResNet, Xception, DenseNet의 입력으로 사용한다. 기본 CNN 모델 위에 톱-레벨 분류기를 추가하여 제안 모델을 구성하였으며, 훈련 데이터 셋에 대해 5-fold 교차검증 기법을 적용하여 학습시켰다. 3가지 제안 모델의 학습은 모두 기본 CNN 모델의 계층을 동결하여 학습시키는 전이 학습과 동결을 해제한 후 학습률을 매우 작은 수로 설정하여 학습시키는 미세 조정 학습 두 단계로 진행하였다. 모델 최적화 알고리즘으로는 SGD, RMSprop, Adam을 적용하였다. 실험 결과는 RMSprop 알고리즘이 적용된 DenseNet CNN 모델이 98.63%의 정확도로 가장 우수한 결과를 보였다. Tomato crops are highly affected by tomato diseases, and if not prevented, a disease can cause severe losses for the agricultural economy. Therefore, there is a need for a system that quickly and accurately diagnoses various tomato diseases. In this paper, we propose a system that classifies nine diseases as well as healthy tomato plants by applying various pretrained deep learning-based CNN models trained on an ImageNet dataset. The tomato leaf image dataset obtained from PlantVillage is provided as input to ResNet, Xception, and DenseNet, which have deep learning-based CNN architectures. The proposed models were constructed by adding a top-level classifier to the basic CNN model, and they were trained by applying a 5-fold cross-validation strategy. All three of the proposed models were trained in two stages: transfer learning (which freezes the layers of the basic CNN model and then trains only the top-level classifiers), and fine-tuned learning (which sets the learning rate to a very small number and trains after unfreezing basic CNN layers). SGD, RMSprop, and Adam were applied as optimization algorithms. The experimental results show that the DenseNet CNN model to which the RMSprop algorithm was applied output the best results, with 98.63% accuracy.\n",
      "전이학습기반 앙상블 딥러닝을 이용한 COVID-19 환자 영상 분류 2021 ['딥러닝', '스태킹 앙상블', '전이학습', 'X-ray/CT 영상', 'COVID-19', 'deep learning', 'stacking ensemble', 'transfer learning', 'X-ray/CT image'] COVID-19 팬데믹으로 인한 피해는 공중 보건적 측면 뿐 만 아니라 정치, 경제, 사회, 문화 전반에 심각한 영향을 미치고 있다. 현재까지 COVID-19 표준 진단검사인 RT-PCR 검사는 검체의 종류, 검체 채취 방법 및 보관에 따라 검사 결과가 달라질 수 있고 코로나바이러스 (SARS-CoV-2) 감염 후 검사 시점에도 영향을 받는다. 본 논문은 전이학습 (transfer learning) 기반 앙상블 딥러닝을 사용하여 COVID-19 환자 X-ray/CT 영상을 분류하고자 한다. 여기서 사용된 전이학습은 CNN (convolutional neural network) 기반인 AlexNet, ResNet, Inception V3, DenseNet 모형이다. 본 연구에서 제안한 스태킹 앙상블 (stacking ensemble) 모형은 세 단계에 걸쳐 이루어진다. 첫 번째 단계에서는 기본모형 (base model)로서 여러 전이학습 모형을 이용하여 예측된 결과들을 얻고, 두 번째 단계에서는 concatenate layer를 통해 이들 결과들을 결합한 다음, 세 번째 단계에서는 메타모형(meta model), 여기서는 DNN (deep neural network) 모형을 적용하여 최종 분류한다. 본 논문에서 제안된 앙상블 모형의 성능평가를 위해 3가지 실제 COVID-19 환자의 X-ray/CT 영상데이터셋을 고려하였으며 여러 가지 성능평가 지표를 가지고 기존의 전이학습 모형과 앙상블 모형과 비교 분석하였다. 성능실험결과, 전반적으로 제안된 앙상블 모형이 기존의 전이학습 모형과 앙상블 모형보다 우수함을 보였다. The damage caused by the COVID-19 pandemic has a serious impact not only on public health but also on politics, economy, society, and culture as a whole. To date, the RT-PCR test, a COVID-19 standard diagnostic test, may vary depending on the type of sample, sample collection method, and storage, and is also affected by the time of the test after infection with COVID-19. This paper attempts to classify COVID-19 patients with X-ray/CT images using transfer learning-based ensemble deep learning. The transfer learning used here is the AlexNet, ResNet, Inception V3, and DenseNet models based on the convolutional neural network (CNN). The stacking ensemble model proposed in this study takes place over three stages. In the first step, predicted results are obtained using several transfer learning models, in the second step, they are combined through a concatenate layer, and in the third step, a deep neural network (DNN) model is applied and finally classified. For the performance evaluation of the ensemble model proposed in this paper, three actual COVID-19 X-ray/CT image datasets were considered, and various performance evaluation indicators were compared and analyzed with the transfer learning model and the existing ensemble model. As a result of the performance experiment, the overall proposed ensemble model was superior to the transfer learning model and the existing ensemble model.\n",
      "영상처리와 딥러닝 네트워크를 결합한 자동차 번호판 인식시스템 2021 ['자동차 번호판 인식', '딥러닝', '영상처리 결합', '임베디드', '경량화', 'License plate recognition', 'Deep learning', 'Image processing combination', 'Embedded', 'Lightening'] 자동차 번호판인식 시스템은 기존에는 영상처리만을 이용한 방식으로 매우 빠른 실시간 처리가 가능하나 다양한 번호판에는 적용하기 어렵다는 한계가 있었고, 딥러닝을 이용하는 경우 다양성과 정확성이 좋아지나 고성능의 그래픽카드가 필요하고 처리하는 데 시간이 매우 오래 걸리는 문제점이 있었다. 본 논문은 각 방식의 장점을 살려 그래픽카드가 없는 일반 사무용 PC에서도 실시간 처리가 가능하며 높은 정확성을 가진 자동차 번호판인식 시스템을 제안하며 더 나아가 사무용 PC가 아닌 임베디드 환경에서도 사용할 수 있도록 경량화한 시스템을 제안한다. 제안하는 시스템은 기존의 번호판인식 시스템과 동일하게 [번호판검출]-[문자영역 분할]-[문자인식]의 3단계 과정을 거치며 각 과정에는 딥러닝 모델로서는 SSD-MobileNet, ResNet 네트워크를 사용하였고, 영상처리 기법으로는 Edge를 검출한 후 수직, 수평으로 전파하면서 관심 영역을 찾는 CLNF 알고리즘을 사용하였다. 제안하는 시스템으로 지하주차장 및 톨게이트 등의 장소에서 얻은 4,389장의 이미지로 테스트하였을 때 충분히 레이어가 깊은 경우 98.2% 정확성을 보여 주었고, 레이어가 얕아질수록 영상처리 결합 여부에 따른 정확성 차이가 커짐을 확인할 수 있었다. In the previous vehicle license plate recognition(LPR) systems, image processing method is able to process very fast in real time, but there is a limitation that it is difficult to apply to various license plates. Deep learning enables the variety and accuracy of LPR to be good, but ir reauires high-performance graphic cards and very long time processing time. In order to get around the advantages and disadvantages of both approaches, this paper proposes a light-weight vehicle license plate recognition system that can be processed in real time even using an embedded board or a general office PC without graphic cards. The proposed system consists of the three steps [license plate detection]-[character segmentation]-[character recognition] in the same with the conventional license plate recognition system. For each step, SSD-MobileNet and ResNet networks were used as deep learning models. As an image processing technique, the CLNF algorithm was used to detect an edge and to propagate vertically and horizontally for finding an ROI(region of interest). In the experiments for testing of the proposed system using 4,389 images obtained at places such as underground parking lots and toll gates, the accuracy was 98.2%. As the layer became shallower, the accuracy difference according to the image processing combination was bigger.\n",
      "마이크로스코프 이미지의 딥러닝 기반 이상검출 2021 ['딥러닝', '마이크로스코프', '이상검출', '흡연', '혓바닥', 'abnormal detection', 'deep learning', 'microscope', 'tongue surface', 'smoke'] 흡연자 중에서 담배가 인체에 유해하다는 사실을 모르는 사람은 없을 것임에도 불구하고, 정작 금연 성공률은 높지 않다. 금연을 위한 의지를 지속적으로 굳건하게 다지기 위하여 병원에서 실시하는 건강검진과 PET(positron emission tomography) 이미지를 통한 암 검사의 결과가 도움이 되지만, 일상생활 중에 간단히 실시할 수 있는 방법이 아니다. 본 연구에서는 일상생활 중에 관찰 가능한 흡연자의 신체 부위를 딥러닝 기반 마이크로스코프 이미지 측정 및 분석을 통하여 흡연자와 비흡연자의 차이를 검출할 수 있는 비침습적 방법을 제안하였다. 우선, 관찰 부위를 흡연시 직접적인 접촉을 하는 혓바닥 표면으로 설정하였다. 다음으로, 마이크로스코프로 혓바닥 표면(410배 확대)을 흡연자 10명과 비흡연자 10명의 실험 참가자를 통하여 데이터 셋(총 1,000장)을 구축하여 그 중 80%를 딥러닝 모델의 학습에 사용하였고, 나머지 20%는 예측에 사용하였다. 딥러닝 모델을 스케일링하는 방법(width scaling, depth scaling, resolution scaling) 중 한 가지 방법만 적용하는 VGG, ResNet, DenseNet과 세 가지를 모두 적용하여 스케일링하는 EfficientNet의 성능을 비교하여 모세혈관 이미지 처리에 EfficientNet의 우수성을 확인해 볼 수 있었다. The success rate of the no-smoking campaign has been low, although everybody knows that cigarettes are harmful to the human health. The results of both regular health and cancer checks in the hospital are useful for strengthening the human intention for quitting the smoking, however, those methods are difficult to use in daily life because of the use of large-scaled particular devices such as PET(positron emission tomography). Thus, this study proposed a non-invasive method that detects the difference between smokers and non-smokers through deep-learning-based analysis. At first, observing parts were decided to the tongue surface. Then, a data set(total 1,000) was made through the experiment to measure the tongue surface(410 times magnification) with the participants of 10 smokers and 10 non-smokers. The 80% ratio of data set was used for the train, and the left 20% was for the prediction. As a result, it was found that the classification through EfficientNet with the compound scaling including three scaling methods of width scaling, depth scaling and resolution scaling was much better than other models including VGG, ResNet, and DenseNet with the only one scaling.\n",
      "심전도 신호 분류를 위한 1D CNN 모델 구성 요소의 최적화 2021 ['Deep learning', 'Electrocardiogram', 'CNN', 'ResNet', 'Arrhythmia Detection', '딥러닝', '심전도', '부정맥 검출'] 본 논문에서는 딥러닝 모델을 이용하여 모바일 기기의 심전도 신호 측정 데이터를 분류한다. 비정상 심장박동을 높은 정확도로 분류하기 위해 딥러닝 모델의 구성 요소 세 가지를 선정하고 요소의 조건 변화에 따른 분류 정확도를 비교한다. 심전도 신호 데이터의 특징을 스스로 추출할 수 있는 CNN 모델을 적용하고 모델을 구성하는 모델의 깊이, 최적화 방법, 활성화 함수의 조건을 변경하여 총 48개의 조합의 성능을 비교한다. 가장 높은 정확도를 보이는 조건의 조합을 도출한 결과 컨볼루션 레이어 19개, 최적화 방법 SGD, 활성화 함수 Mish를 적용하였을 때 정확도 97.88%로 모든 조합 중 가장 높은 분류 정확도를 얻었다. 이 실험에서 CNN을 활용한 1-채널 심전도 신호의 특징 추출과 비정상 박동 검출의 적합성을 확인하였다. In this paper, we classify ECG signal data for mobile devices using deep learning models. To classify abnormal heartbeats with high accuracy, three factors of the deep learning model are selected, and the classification accuracy is compared according to the changes in the conditions of the factors. We apply a CNN model that can self-extract features of ECG data and compare the performance of a total of 48 combinations by combining conditions of the depth of model, optimization method, and activation functions that compose the model. Deriving the combination of conditions with the highest accuracy, we obtained the highest classification accuracy of 97.88% when we applied 19 convolutional layers, an optimization method SGD, and an activation function Mish. In this experiment, we confirmed the suitability of feature extraction and abnormal beat detection of 1-channel ECG signals using CNN.\n",
      "Generating 3D texture models of vessel pipes using 2D texture transferred by object recognition 2021 ['augmented reality', 'CycleGAN', 'ResNet', 'normalization', 'texture', '3D model'] None Research and development of smart vessels has progressed significantly in recent years, and ships have become high-value technology-intensive resources. These ships entail high production costs and long-life cycles. Thus, modernized technical design, professional training, and aggressive maintenance are important factors in the efficient management of ships. With the continuing digital revolution, the industrial shipbuilding applicability of augmented reality (AR) and virtual reality (VR) technologies as well as related 3D system modeling and processes has increased. However, resolving the differences between AR/VR and real-world models remains burdensome. This problem is particularly evident when mapping various texture characteristics to virtual objects. To mitigate the burden and improve the performance of such technologies, it is necessary to directly define various texture characteristics or to express them using expensive equipment. The use of deep-learning-based CycleGAN, however, has gained attention as a method of learning and automatically mapping real-object textures. Thus, we seek to use CycleGAN to improve the immersive capacities of AR/VR models and to reduce production costs for shipbuilding. However, when applying CycleGAN’s textures to pipe structures, the performance is insufficient for direct application to industrial piping networks. Therefore, this study investigates an improved CycleGAN algorithm that can be specifically applied to the shipbuilding industry by combining a modified object-recognition algorithm with a double normalization method. Thus, we demonstrate that basic knowledge on the production of AR industrial pipe models can be applied to virtual models through machine learning to deliver low-cost and high-quality textures. Our results provide an on-ramp for future CycleGAN studies related to the shipbuilding industry.\n",
      "딥러닝 기반의 PCB 부품 문자인식을 위한 코어 셋 구성 2021 ['Deep Learning', 'Coreset', 'PCB Inspection', 'OCR', 'ResNet'] None None\n",
      "자궁경부 영상에서의 라디오믹스 기반 판독 불가 영상 분류 알고리즘 연구 2021 ['Cervical cancer', 'Radiomics', 'Laplacian variance', 'Euclidean distance', 'ResNet-50'] None Recently, artificial intelligence for diagnosis system of obstetric diseases have been actively studied. Artificial intelligence diagnostic assist systems, which support medical diagnosis benefits of efficiency and accuracy, may experience problems of poor learning accuracy and reliability when inappropriate images are the model's input data. For this reason, before learning, We proposed an algorithm to exclude unread cervical imaging. 2,000 images of read cervical imaging and 257 images of unread cervical imaging were used for this study. Experiments were conducted based on the statistical method Radiomics to extract feature values of the entire images for classification of unread images from the entire images and to obtain a range of read threshold values. The degree to which brightness, blur, and cervical regions were photographed adequately in the image was determined as classification indicators. We compared the classification performance by learning read cervical imaging classified by the algorithm proposed in this paper and unread cervical imaging for deep learning classification model. We evaluate the classification accuracy for unread Cervical imaging of the algorithm by comparing the performance. Images for the algorithm showed higher accuracy of 91.6% on average. It is expected that the algorithm proposed in this paper will improve reliability by effectively excluding unread cervical imaging and ultimately reducing errors in artificial intelligence diagnosis.\n",
      "데이터 분석을 통한 지역별 고령친화도 시각화 2021 ['고령친화도시', '인구고령화', '지표', 'GIS', 'Smart Farm', 'Deep Learning', 'ResNet', 'Convolution Neural Network'] 전 세계적인 인구고령화 현상이 사회적으로 문제가 되고 있다. 우리나라 역시 초고령화 사회에 급속도로 진입하면서 사회적 생활공간의 실질적 변화가 필요하게 되었다. 이에 본 연구에서는 WHO가 제시한 ‘고령친화도시’ 개념과 가이드라인을 중심으로 군집분석과 EDA를 활용해 국내 고령친화도 현황을 분석하였다. 더불어 경기도의 이천시와 안성시를 중심으로 관련 데이터를 비교·분석 후 GIS 기반 데이터 시각화를 수행하였다. 분석 결과 B등급인 이천시에 비교하여 안성시의 고령친화도는 인구, 면적 등의 조건이 비슷함에도 불구하고 D등급으로 측정되어 매우 낮은 것으로 나타났다. 본 연구의 결과를 통해 정책적으로는 전국 단위의 고령친화도를 파악하여 효과적인 복지 정책 가이드라인 제시와 더불어, 한정된 정부예산에서 정비가 시급한 부분에 대한 우선적 고려가 가능할 것으로 기대된다. The population aging around the world is becoming a social problem. As Korea also entered a super-aged society rapidly, a substantial change in social living space became necessary. Therefore, this study analyzes the current status of elderly affinity in Korea using cluster analysis and EDA, focusing on the WHO's concept of ‘aged-friendly city’ and eight area guidelines. In addition, GIS-based data visualization carries after comparing and analyzing related data, centering on Icheon and Anseong in Gyeonggi province. As a result of the analysis, compared to Icheon, which is grade B, the elderly affinity of Anseong was measured as grade D despite similar conditions such as population and area. Through the results of this study, it is expected that it will be possible to identify the relative Age-Friendliness the country in terms of policy, present effective welfare policy guidelines, and prioritize areas where maintenance is urgent in the limited government budget\n",
      "LDAM 손실 함수를 활용한 클래스 불균형 상황에서의 옷차림 T.P.O 추론 모델 학습 2021 ['융합', '패션', 'T.P.O', '다중 레이블', '클래스 불균형', '딥러닝', 'Convergence', 'Fashion', 'T.P.O', 'Multi-label problem', 'Class imbalance problem', 'Deep learning'] 의복을 착용하는데 있어 목적 상황에 부합하는 옷차림을 구성하는 것은 중요하다. 따라서 인공지능 기반의 다양 한 패션 추천 시스템에서 의복 착용의 T.P.O(Time, Place, Occasion)를 고려하고 있다. 하지만 옷차림으로부터 직접 T.P.O를 추론하는 연구는 많지 않은데, 이는 문제 특성 상 다중 레이블 및 클래스 불균형 문제가 발생하여 모델 학습을 어렵게 하기 때문이다. 이에 본 연구에서는 label-distribution-aware margin(LDAM) loss를 도입하여 옷차림의 T.P.O를 추론할 수 있는 모델을 제안한다. 모델의 학습 및 평가를 위한 데이터셋은 패션 쇼핑몰로부터 수집되었고 이를 바탕으로 성능을 측정한 결과, 제안 모델은 비교 모델 대비 모든 T.P.O 클래스에서 균형잡힌 성능을 보여주는 것을 확인할 수 있었다. When a person wears clothing, it is important to configure an outfit appropriate to the intended occasion. Therefore, T.P.O(Time, Place, Occasion) of the outfit is considered in various fashion recommendation systems based on artificial intelligence. However, there are few studies that directly infer the T.P.O from outfit images, as the nature of the problem causes multi-label and class imbalance problems, which makes model training challenging. Therefore, in this study, we propose a model that can infer the T.P.O of outfit images by employing a label-distribution-aware margin(LDAM) loss function. Datasets for the model training and evaluation were collected from fashion shopping malls. As a result of measuring performance, it was confirmed that the proposed model showed balanced performance in all T.P.O classes compared to baselines.\n",
      "Deep Convolutional Neural Network Architectures for Tonal Frequency Identification in a Lofargram 2021 ['Convolutional neural networks', 'lofar analysis', 'sonar analysis', 'underwater recognition.'] None Advances in convolutional neural networks (CNNs) have driven the development of computer vision. Recent CNN architectures, such as those with skip residual connections (ResNets) or densely connected architectures (DenseNets), have facilitated backpropagation and improved the performance of feature extraction and classification. Detecting objects in underwater environments by analyzing sound navigation and ranging (sonar) signals is considered an important process that should be automated. Several previous approaches have addressed this challenge; however, there has been no in-depth study of CNN architectures that effectively analyze sonar grams. In this paper, we have presented the identification of tonal frequencies in lofargrams using recent CNN architectures. Our study includes 175 CNN models that are derived from five different CNN architectures and 35 different input patch sizes. The study results showed that the accuracy of the best model was as high as 96.2% for precision and 99.5% for recall, with an inference time of 0.184 s.\n",
      "Cerebral hemorrhage detection and localization with medical imaging for cerebrovascular disease diagnosis and treatment using explainable deep learning 2021 ['Cerebral hemorrhage prediction · Cerebrovascular disease · Explainable artificial intelligence'] None Cerebral hemorrhages require rapid diagnosis and intensive treatment. This study aimed to detect cerebral hemorrhages and their locations in images using a deep learning model applying explainable deep learning. Normal brain images with no hemorrhages and images with subarachnoid, intraventricular, subdural, epidural, and intraparenchymal hemorrhages according to computed tomography (CT) (n = 200) were analyzed. A ResNet deep learning model, including image processing, was utilized. The visual explanation from a heatmap was made at the hemorrhage location using a gradient-class activation map (Grad-CAM). To evaluate the performance of the deep learning system, the accuracy, sensitivity, and specificity were determined. A hemorrhage prediction system for images of normal brains and brains with subarachnoid, intraventricular, subdural, epidural, and intraparenchymal hemorrhages was built. The Grad-CAM representation indicated the location of the hemorrhages in these images. In the prediction results, accurate predictions of the hemorrhage areas were made and visualizations of the corresponding locations overlapped in the images within (− 4, 1) pixel difference. The evaluation of the system performance showed an accuracy of 0.81 with a sensitivity of 0.67 and specificity of 0.86. These results constitue a proof of concept for the use of explainable artificial intelligence (XAI) to detect cerebral hemorrhages and visualize their locations in medical images, which will allow rapid diagnosis and treatment.\n",
      "자동화 균열 탐지 시스템을 위한 딥러닝 모델에 관한 연구 2021 ['Surface Inspection', 'Crack Detection', 'Computer Vision', 'Deep Learning', '표면 검사', '균열 탐지', '컴퓨터 비전', '딥러닝'] None Cracks affect the robustness of infrastructures such as buildings, bridge, pavement, and pipelines. This paper presents an automatedcrack detection system which detect cracks in diverse surfaces. We first constructed the combined crack dataset, consists of multiplecrack datasets in diverse domains presented in prior studies. Then, state-of-the-art deep learning models in computer vision tasks includingVGG, ResNet, WideResNet, ResNeXt, DenseNet, and EfficientNet, were used to validate the performance of crack detection. We dividedthe combined dataset into train (80%) and test set (20%) to evaluate the employed models. DenseNet121 showed the highest accuracyat 96.20% with relatively low number of parameters compared to other models. Based on the validation procedures of the advanced deeplearning models in crack detection task, we shed light on the cost-effective automated crack detection system which can be appliedto different surfaces and structures with low computing resources.\n",
      "Convolutional Neural Network 기반 EBG 구조 설계를 통한 고속 PCB 노이즈 저감 2021 ['Simultaneous Switching Noise', 'Electromagnetic Band Gap', 'Machine Learning', 'Convolutional Neural Network'] 기술이 빠르게 발전하여 디지털 시스템의 동작 주파수는 수 GHz 대역까지 증가했다. 이로 인하여 Simultaneous Switching Noise 문제가 증가했고, 이를 줄이기 위해 Electromagnetic Band Gap(EBG) 구조가 많이 연구된다. EBG 구조 설계에서 중요한 과정 중 하나는 노이즈를 저감하는 Stopband 대역을 예측하는 것이다. 기존에 3차원 전자장 시뮬레이션 프로그램을 이용하는 방법과 Floquet 이론 기반의 수식을 이용하는 방법이 있으나, 한계점이 존재한다. 본 논문에서는 Convolutional Neural Network(CNN)을 이용하여 EBG 구조의 Stopband 대역을 예측하는 새로운 방법을 제안한다. 또한 기본 CNN 구조, GoogLeNet, ResNet, DenseNet과 같은 CNN Architecture 모델을 활용하여 어떤 CNN 구조가 Stopband 대역 예측에 높은 성능을 보이는지 분석한다. 900개의 EBG 구조 모델에 대해서 학습시킨 후 CNN 구조의 mean absolute error를 비교한 결과, DenseNet이 가장 우수한 성능을 보임을 확인하였다. With rapid advances in technology, the operating frequencies of digital systems have increased to several GHz bands. This has led to an increase in simultaneous switching noise(SSN). To reduce SSN, electromagnetic bandgap(EBG) structures have been intensively studied. One of the critical steps in the design of an EBG structure is to predict the stopband that reduces SSN. Existing methods include using a 3D electromagnetic field simulation program or equations based on the Floquet theory. However, these have limitations. In this study, we verified a new method for predicting the stopband using a convolutional neural network(CNN). Specifically, a CNN architectural model was used to compare structures that perform well in predicting the stopband. It was also used to confirm that the DenseNet showed high performance.\n",
      "합성곱 신경망을 이용한 프로펠러 캐비테이션 침식 위험도 연구 2021 ['Convolutional Neural Network(CNN', '합성곱 신경망)', 'Deep learning(딥러닝)', 'Propeller(프로펠러)', 'Cavitation(캐비테이션)', 'Erosion(침식)'] None Cavitation erosion is one of the major factors causing damage by lowering the structural strength of the marine propeller and the risk of it has been qualitatively evaluated by each institution with their own criteria based on the experiences. In this study, in order to quantitatively evaluate the risk of cavitation erosion on the propeller, we implement a deep learning algorithm based on a convolutional neural network. We train and verify it using the model tests results, including cavitation characteristics of various ship types. Here, we adopt the validated well-known networks such as VGG, GoogLeNet, and ResNet, and the results are compared with the expert’s qualitative prediction results to confirm the feasibility of the prediction algorithm using a convolutional neural network.\n",
      "고성능 CNN 기반 지정맥 인증 시스템 구현 2021 ['AI', 'Biometric authentication', 'Finger vein recognizer'] None Biometric technology using finger veins is receiving a lot of attention due to its high security, convenience and accuracy. And the recent development of deep learning technology has improved the processing speed and accuracy for authentication. However, the training data is a subset of real data not in a certain order or method and the results are not constant. so the amount of data and the complexity of the artificial neural network must be considered. In this paper, the deep learning model of Inception-Resnet-v2 was used to improve the high accuracy of the finger vein recognizer and the performance of the authentication system, We compared and analyzed the performance of the deep learning model of DenseNet-201. The simulations used data from MMCBNU_6000 of Jeonbuk National University and finger vein images taken directly. There is no preprocessing for the image in the finger vein authentication system, and the results are checked through EER.\n",
      "자율 운항 선박을 위한 딥 러닝 기반 선박 이미지 분류 방법 2021 ['Image Classification', 'Object Detection', 'Convolutional Neural Network', 'Deep Learning', 'Heatmap', 'Autonomous Ship'] None In the last few years, researches on autonomous ships have attracted attention. One of the essential techniques required for autonomous ships is the awareness of surroundings, including detection and classification of objects. Although researches on computer vision regarding the classification of ship images are still making progress, it is challenging to encounter a lack of enough database that was adequately labeled for ship classification. In this study, data obtained from Singapore Maritime Dataset (SMD) and public datasets such as MARVEL, FleetMon, and VesselFinder were labeled and integrated into a unified dataset for further study for ship classification. The ship image dataset was classified into seven classes, including bulk carrier, container ship, cruise ship, naval surface ship, tanker, tug boat, and buoy. Subsequently, Convolutional Neural Networks (CNNs) based on GoogleNet, VGG16, and ResNet were implemented for ship image classification, and a comparative test was done. As a result, the CNNs which were trained with the unified dataset showed high accuracy. The classification results were analyzed by the heatmap visualization with Grad-CAM, which indicates critical features best activating each class of ships, and further discussion was made.\n",
      "딥러닝을 이용한 핸드크림의 마찰 시계열 데이터 분류 2021 ['Time Series Classification', 'Deep Learning', 'Tribology', 'Cosmetics'] None The sensory stimulation of a cosmetic product has been deemed to be an ancillary aspect until a decade ago. That point of view has drastically changed on different levels in just a decade. Nowadays cosmetic formulators should unavoidably meet the needs of consumers who want sensory satisfaction, although they do not have much time for new product development. The selection of new products from candidate products largely depend on the panel of human sensory experts. As new product development cycle time decreases, the formulators wanted to find systematic tools that are required to filter candidate products into a short list. Traditional statistical analysis on most physical property tests for the products including tribology tests and rheology tests, do not give any sound foundation for filtering candidate products. In this paper, we suggest a deep learning-based analysis method to identify hand cream products by raw electric signals from tribological sliding test. We compare the result of the deep learning-based method using raw data as input with the results of several machine learning-based analysis methods using manually extracted features as input. Among them, ResNet that is a deep learning model proved to be the best method to identify hand cream used in the test. According to our search in the scientific reported papers, this is the first attempt for predicting test cosmetic product with only raw time-series friction data without any manual feature extraction. Automatic product identification capability without manually extracted features can be used to narrow down the list of the newly developed candidate products.\n",
      "전산화 단층 촬영(Computed tomography, CT) 이미지에 대한 EfficientNet 기반 두개내출혈 진단 및 가시화 모델 개발 2021 ['Deep-learning', 'EfficientNet', 'Intracranial hemorrhage', 'Computed tomography images'] None Intracranial hemorrhage (ICH) refers to acute bleeding inside the intracranial vault. Not only does this devastating disease record a very high mortality rate, but it can also cause serious chronic impairment of sensory, motor, and cognitive functions. Therefore, a prompt and professional diagnosis of the disease is highly critical. Noninvasive brain imaging data are essential for clinicians to efficiently diagnose the locus of brain lesion, volume of bleeding, and subsequent cortical damage, and to take clinical interventions. In particular, computed tomography (CT) images are used most often for the diagnosis of ICH. In order to diagnose ICH through CT images, not only medical specialists with a sufficient number of diagnosis experiences are required, but even when this condition is met, there are many cases where bleeding cannot be successfully detected due to factors such as low signal ratio and artifacts of the image itself. In addition, discrepancies between interpretations or even misinterpretations might exist causing critical clinical consequences. To resolve these clinical problems, we developed a diagnostic model predicting intracranial bleeding and its subtypes (intraparenchymal, intraventricular, subarachnoid, subdural, and epidural) by applying deep learning algorithms to CT images. We also constructed a visualization tool highlighting important regions in a CT image for predicting ICH. Specifically, 1) 27,758 CT brain images from RSNA were pre-processed to minimize the computational load. 2) Three different CNN-based models (ResNet, EfficientNet-B2, and EfficientNet-B7) were trained based on a training image data set. 3) Diagnosis performance of each of the three models was evaluated based on an independent test image data set: As a result of the model comparison, EfficientNet-B7's performance (classification accuracy = 91%) was a way greater than the other models. 4) Finally, based on the result of EfficientNet-B7, we visualized the lesions of internal bleeding using the Grad-CAM. Our research suggests that artificial intelligence-based diagnostic systems can help diagnose and treat brain diseases resolving various problems in clinical situations.\n",
      "다중 레이블 분류를 활용한 안면 피부 질환 인식에 관한 연구 2021 ['Deep Learning', 'Multi-Label Classification', 'Skin Diseases', '딥 러닝', '다중 레이블 분류', '피부 질환'] None Recently, as people's interest in facial skin beauty has increased, research on skin disease recognition for facial skin beauty is being conducted by using deep learning. These studies recognized a variety of skin diseases, including acne. Existing studies can recognize only the single skin diseases, but skin diseases that occur on the face can enact in a more diverse and complex manner. Therefore, in this paper, complex skin diseases such as acne, blackheads, freckles, age spots, normal skin, and whiteheads are identified using the Inception-ResNet V2 deep learning mode with multi-label classification. The accuracy was 98.8%, hamming loss was 0.003, and precision, recall, F1-Score achieved 96.6% or more for each single class.\n",
      "2021년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                                                                                      title  \\\n",
      "0                                                                                                        ResNet-Variational AutoEncoder기반 변종 악성코드 패밀리 분류 연구   \n",
      "1                                                                                                                     복소수 ResNet 네트워크 기반의 SAR 영상 물체 인식 알고리즘   \n",
      "2                                                                                                     ResNet 정확도 향상을 위한 깊이별 Residual Connection 비율 조절 방법 제안   \n",
      "3                                                                                                                  ResNet-합성곱 오토인코더 기반 신경망을 이용한 스펙트럼 데이터 압축   \n",
      "4                                                                                                              ResNet과 Unet을 결합한 딥러닝 모델을 이용한 분광 신호에서 ROI 검출   \n",
      "5                              Distribution Analysis of Feature Map and Gradients in Mobilenet and Resnet Model Layers using Glorot and He`s initialization   \n",
      "6                                                                                                                             ResNet을 이용한 도로 네트워크 교통 데이터 예측   \n",
      "7                                                                                                                   SegNet과 ResNet을 조합한 딥러닝에 기반한 횡단보도 영역 검출   \n",
      "8                                                                                                                          운전자의 주의분산 연구동향 및 딥러닝 기반 동작 분류 모델   \n",
      "9                                                                              Identification of Indian butterflies using Deep Convolutional Neural Network   \n",
      "10                                                                                                                딥러닝과 의미론적 영상분할을 이용한 자동차 번호판의 숫자 및 문자영역 검출   \n",
      "11                                                                                                                        사과 품종 분류를 위한 CNN기반 모델링 및 분류 기법 연구   \n",
      "12                                                                           The Classification of EEG-based Wink Signals: A CWT-Transfer Learning Pipeline   \n",
      "13                        Application of convolutional neural networks for distal radio-ulnar fracture detection on plain radiographs in the emergency room   \n",
      "14                                                                                                                               장면 복잡도 기반 적응적 얼굴 마스크 탐지 모델   \n",
      "15                                                                         User Interface Application for Cancer Classification using Histopathology Images   \n",
      "16                                                                                                                            텍스타일 디자인 분류 및 관심 영역 도출에 대한 연구   \n",
      "17                                                                                                                    공연예술에서 광고포스터의 이미지 특성을 활용한 딥러닝 기반 관객예측   \n",
      "18                                               Automated detection of corrosion in used nuclear fuel dry storage canisters using residual neural networks   \n",
      "19                                                                                                                       심층 신경망 기반 객체 인식 기법의 유사 객체 분류 성능 분석   \n",
      "20                                                                                                                        평활화 알고리즘에 따른 자궁경부 분류 모델의 성능 비교 연구   \n",
      "21                                                                                                                        웹 검색을 이용한 새 품종 분류를 위한 전이학습에 대한 연구   \n",
      "22                                                                                                                            딥러닝 경량화를 위한 구조, 가지치기, 지식증류 비교   \n",
      "23                                                                                                               딥러닝 기반의 소비자 데이터를 응용한 외식업체 추천 시스템 구현에 관한 연구   \n",
      "24                             Implementation and Performance Evaluation of Farm Waste Image Classification System using CNN-based Transfer Learning Models   \n",
      "25                                                       Five-Class Classification of Cervical Pap Smear Images: A Study of CNN-Error-Correcting SVM Models   \n",
      "26                                                                                                                        영상 콘텐츠의 오디오 분석을 통한 메타데이터 자동 생성 방법   \n",
      "27                                                                                 Detection of surface roughness of mechanical drawings with deep learning   \n",
      "28                                                                                                                    다양한 CNN 가속기에서 아키텍처에 따른 면적, 에너지, 성능 분석   \n",
      "29                                                                                                                            비디오 인코더를 통한 딥러닝 모델의 정수 가중치 압축   \n",
      "30                                                                                                         이미지 감성분류를 위한 CNN과 K-means RGB Cluster 이-단계 학습 방안   \n",
      "31                                                                                                                           전단 융합 기반 멀티모달 심층학습을 이용한 손동작 분류   \n",
      "32                                                                                                                                RVC 정규화와 전이학습을 이용한 손동작 인식   \n",
      "33                                                                 Former Unmanned Surface Vehicle Detection Based on Improved Convolutional Neural Network   \n",
      "34                                                                                                       Apple Detection Algorithm based on an Improved SSD   \n",
      "35                                                               Comparison of Pre-processed Brain Tumor MR Images Using Deep Learning Detection Algorithms   \n",
      "36                                                                                 Rice Fungal Diseases Recognition Using Modern Computer Vision Techniques   \n",
      "37                            MAT-AGCA: Multi Augmentation Technique on small dataset for Balinese character recognition using Convolutional Neural Network   \n",
      "38                                                                                            Deep Convolutional Neural Network를 적용한 피하 종괴의 초음파적 진단: 실험적 연구   \n",
      "39                                                                                       합성곱 신경망의 Channel Attention 모듈 및 제한적인 각도 다양성 조건에서의 SAR 표적영상 식별로의 적용   \n",
      "40                                                                                                                                  고성능 CNN 기반 정밀 요검사 판별 기법   \n",
      "41                                                                                                             컨볼루션 신경망 모델을 이용한 분류에서 입력 영상의 종류가 정확도에 미치는 영향   \n",
      "42                                                                           Novel Image Classification Method Based on Few-Shot Learning in Monkey Species   \n",
      "43                                                                                                                                    딥러닝 기반의 철강 표면의 결함 검출기   \n",
      "44                                                     Wood Classification of Japanese Fagaceae using Partial Sample Area and Convolutional Neural Networks   \n",
      "45                                                                                                                           유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구   \n",
      "46                                                                                       변형 Residual Convolutional Neural Network 모델을 이용한 고효율 심전도 데이터 분석 기법   \n",
      "47                                                                                                                     깊은 합성곱 신경망 모델에 따른 유방 초음파 영상 분류 성능 비교   \n",
      "48                                                                                                              합성곱 신경망을 이용한 컨포멀 코팅 PCB에 발생한 문제성 기포 검출 알고리즘   \n",
      "49                                                                                                                      전기화재 원인분석을 위한 용융흔 외형 판별 딥러닝 알고리즘 설계   \n",
      "50                                                                                                                    안면 연령 예측을 위한 CNN기반의 히트 맵을 이용한 랜드마크 선정   \n",
      "51                                                                                                                                딥러닝을 이용한 마스크 착용 여부 검사 시스템   \n",
      "52                                                                                                                       심층 CNN 기반 구조를 이용한 토마토 작물 병해충 분류 모델   \n",
      "53                                                                                                                    전이학습기반 앙상블 딥러닝을 이용한 COVID-19 환자 영상 분류   \n",
      "54                                                                                                                        영상처리와 딥러닝 네트워크를 결합한 자동차 번호판 인식시스템   \n",
      "55                                                                                                                                 마이크로스코프 이미지의 딥러닝 기반 이상검출   \n",
      "56                                                                                                                       심전도 신호 분류를 위한 1D CNN 모델 구성 요소의 최적화   \n",
      "57                                                          Generating 3D texture models of vessel pipes using 2D texture transferred by object recognition   \n",
      "58                                                                                                                          딥러닝 기반의 PCB 부품 문자인식을 위한 코어 셋 구성   \n",
      "59                                                                                                                  자궁경부 영상에서의 라디오믹스 기반 판독 불가 영상 분류 알고리즘 연구   \n",
      "60                                                                                                                                 데이터 분석을 통한 지역별 고령친화도 시각화   \n",
      "61                                                                                                         LDAM 손실 함수를 활용한 클래스 불균형 상황에서의 옷차림 T.P.O 추론 모델 학습   \n",
      "62                                                        Deep Convolutional Neural Network Architectures for Tonal Frequency Identification in a Lofargram   \n",
      "63  Cerebral hemorrhage detection and localization with medical imaging for cerebrovascular disease diagnosis and treatment using explainable deep learning   \n",
      "64                                                                                                                          자동화 균열 탐지 시스템을 위한 딥러닝 모델에 관한 연구   \n",
      "65                                                                                              Convolutional Neural Network 기반 EBG 구조 설계를 통한 고속 PCB 노이즈 저감   \n",
      "66                                                                                                                        합성곱 신경망을 이용한 프로펠러 캐비테이션 침식 위험도 연구   \n",
      "67                                                                                                                                 고성능 CNN 기반 지정맥 인증 시스템 구현   \n",
      "68                                                                                                                        자율 운항 선박을 위한 딥 러닝 기반 선박 이미지 분류 방법   \n",
      "69                                                                                                                             딥러닝을 이용한 핸드크림의 마찰 시계열 데이터 분류   \n",
      "70                                                                          전산화 단층 촬영(Computed tomography, CT) 이미지에 대한 EfficientNet 기반 두개내출혈 진단 및 가시화 모델 개발   \n",
      "71                                                                                                                        다중 레이블 분류를 활용한 안면 피부 질환 인식에 관한 연구   \n",
      "\n",
      "    date  \\\n",
      "0   2021   \n",
      "1   2021   \n",
      "2   2021   \n",
      "3   2021   \n",
      "4   2021   \n",
      "5   2021   \n",
      "6   2021   \n",
      "7   2021   \n",
      "8   2021   \n",
      "9   2021   \n",
      "10  2021   \n",
      "11  2021   \n",
      "12  2021   \n",
      "13  2021   \n",
      "14  2021   \n",
      "15  2021   \n",
      "16  2021   \n",
      "17  2021   \n",
      "18  2021   \n",
      "19  2021   \n",
      "20  2021   \n",
      "21  2021   \n",
      "22  2021   \n",
      "23  2021   \n",
      "24  2021   \n",
      "25  2021   \n",
      "26  2021   \n",
      "27  2021   \n",
      "28  2021   \n",
      "29  2021   \n",
      "30  2021   \n",
      "31  2021   \n",
      "32  2021   \n",
      "33  2021   \n",
      "34  2021   \n",
      "35  2021   \n",
      "36  2021   \n",
      "37  2021   \n",
      "38  2021   \n",
      "39  2021   \n",
      "40  2021   \n",
      "41  2021   \n",
      "42  2021   \n",
      "43  2021   \n",
      "44  2021   \n",
      "45  2021   \n",
      "46  2021   \n",
      "47  2021   \n",
      "48  2021   \n",
      "49  2021   \n",
      "50  2021   \n",
      "51  2021   \n",
      "52  2021   \n",
      "53  2021   \n",
      "54  2021   \n",
      "55  2021   \n",
      "56  2021   \n",
      "57  2021   \n",
      "58  2021   \n",
      "59  2021   \n",
      "60  2021   \n",
      "61  2021   \n",
      "62  2021   \n",
      "63  2021   \n",
      "64  2021   \n",
      "65  2021   \n",
      "66  2021   \n",
      "67  2021   \n",
      "68  2021   \n",
      "69  2021   \n",
      "70  2021   \n",
      "71  2021   \n",
      "\n",
      "                                                                                                                                                                      keywords  \\\n",
      "0                            [변종 악성코드, 악성코드 분류, 변이 오토인코더, 전이학습, 앙상블 학습, Variant Malware, Malware Classification, Variational AutoEncoder, Tranfer Learning, Ensemble Learning]   \n",
      "1                                                                                                                                                                           []   \n",
      "2                                                                                           [ResNet, Residual Connection, Residual Learning, Variational Scaling, Degradation]   \n",
      "3                                                                                                                 [Data Compression, PCA, Autoencoder, ResNet, Raman Spectrum]   \n",
      "4                                                                                                 [Peak Detection, Region of Interest, Deep Learning, CNN, Raman Spectroscopy]   \n",
      "5    [가중치 초기화, Glorot 초기화, He 초기화, 컨볼루션 신경망 네트워크, 잡초 분류, Weights initialization, Glorot initialization, He initialization, Convolutional neural network, Weeds classification]   \n",
      "6                                      [합성곱 신경망, 잔차 학습, 전이 학습, 교통 속도 예측, Convolutional Neural Network, Residual Learning, ResNet, Transfer Learning, Traffic Speed Prediction]   \n",
      "7                                                              [Deep Learning, Semantic Segmentation, Zebra-crossing Detection, Neural Network, 딥러닝, 시맨틱 분할, 횡단보도 검출, 신경 네트워크]   \n",
      "8                                                                 [Driver’s Behavior, Driver’s Distraction, Behavior Recognition, ResNet-101, CAM, 운전자의 동작, 운전자의 주의 분산, 동작 인식]   \n",
      "9                                                                              [Indian butterfly identification, ButterflyNet, Butterfly, classification CNN, Computer vision]   \n",
      "10  [딥러닝, 합성곱 신경망(CNN), 의미론적 분할, 자동차 번호판, 영상분할 및 인식, Deep Learning, Convolution Neural Network(CNN), Semantic Segmentation, License Plate, Image Segmentation and Recognition]   \n",
      "11                                                                                     [스마트팜, 딥러닝, ResNet, 신경망, Smart Farm, Deep Learning, ResNet, Convolution Neural Network]   \n",
      "12                                                                                                                                     [BCI, CWT, EEG, Transfer Learning, SVM]   \n",
      "13                                                                                                          [Wrist, Fractures, bone, Deep learning, Neural networks, computer]   \n",
      "14                          [인공지능, 딥러닝, 기계학습, 객체 감지, 마스크 감지, 코로나바이러스-19, Artificial intelligence, Machine learning, Object detection, Deep learning, Mask detection, COVID-19]   \n",
      "15                                                                                               [Deep Learning, Histopathology images, ResNet-34, Digital Pathology, AI, CAD]   \n",
      "16                                                                                        [텍스타일 디자인 분류, 관심 영역 도출, Textile design, VGG-16, ResNet-34, LIME, region of interest]   \n",
      "17                                                                                  [공연예술, 흥행 예측, CNN, VGG-16, Inception-v3, ResNet50, Performing Arts, Box Office Prediction]   \n",
      "18                                               [Convolutional neural networks, Corrosion, Deep learning, Dry storage canisters, Feature detection, Residual neural networks]   \n",
      "19                                                                                           [Deep Neural Networks, ResNet, DenseNet, Smart Factory, Armored Fighting Vehicle]   \n",
      "20                                                                                                        [Cervical cancer, Histogram equalization, Classification, ResNet-50]   \n",
      "21                                                                                [deep learning, transfer learning, bird breed recognition, web crawling, ResNet50, ImageNet]   \n",
      "22                                                                            [Deep learning, Structure Reduction, Pruning, Knowledge Distillation, CIFAR10/100, ResNet56/110]   \n",
      "23                         [추천 시스템, 인공지능, 딥러닝, 분류, 감성 분석, 외식업체, Recommendation System, Artificial Intelligence, Deep Learning, Classification, Sentiment Analysis, Restaurant]   \n",
      "24                                                                              [Artificial intelligence, Transfer-learning, CNN, Image classification, Farm waste collection]   \n",
      "25                                                                                             [Cervix Uteri, Diagnosis, Nerve Net, Papanicolaou Test, Support Vector Network]   \n",
      "26                                                                                                             [Audio, AI, Metadata, Recommendation System, Voice Recognition]   \n",
      "27                                                 [· Mechanical drawings · Image analysis · Object detection · Image recognition · Deep learning models · Evaluation metrics]   \n",
      "28                                                                        [CNN 가속기, 아키텍처, 메모리 대역폭, 뉴럴 네트워크, CNN accelerators, architecture, memory bandwidth, neural networks]   \n",
      "29                                                                                         [Deep Learning Model Parameter Quantization, Weight compression, Lightweight model]   \n",
      "30                                                                               [이미지 감성분류, 색감, 이-단계 학습, Sentiment Analysis of Image, Sense of Color, CNN, Two-stage learning]   \n",
      "31                                                                                          [Hand Gesture Classification, Deep Learning, EMG, Multimodal Learning, Ninapro DB]   \n",
      "32                                                                             [Transfer Learning, Reference Voluntary Contraction, Hand Gesture Recognition, EMG, Ninapro DB]   \n",
      "33                                                                                                             [Object detection, Accuracy, Tracking system, Monocular camera]   \n",
      "34                                                                                                      [RFB, Attention Model, SSD, Apple detection, Objection detection, CNN]   \n",
      "35                                                                                                             [Brain Tumor, RetinaNet, Deep Learning, Histogram Equalization]   \n",
      "36                                                                                   [Convolutional neural networks, Machine learning, Computer vision, Rice, Fungal diseases]   \n",
      "37                                                       [Balinese character, Lontar manuscript, Data augmentation, Adaptive Gaussian Thresholding, Convolutional Autoencoder]   \n",
      "38                                                                                                                    [Deep learning, Epidermal cyst, Lipoma, Ultrasonography]   \n",
      "39                                                                                                                                                                          []   \n",
      "40                                                                                                                                     [CNN, Urinalysis, Image Discrimination]   \n",
      "41                                                                                                        [X-ray, Convolutional neural network, Classification, Deep learning]   \n",
      "42                                                                                                [Deep learning, Feature extraction, Few-shot learning, Image classification]   \n",
      "43                          [Artificial intelligence, Convolutional neural network, Deep learning, Image classification, Metal surface defect, Surface defect detection (SDD)]   \n",
      "44                                                                                   [wood, microscopic image, sample selection, classification, convolutional neural network]   \n",
      "45                                                                   [분류, 딥 러닝, 유사 이미지, 컨볼루셔널 뉴럴 네트워크, 혼동률, Classification, Deep Learning, Similar Image, CNN, Confusion Rate]   \n",
      "46                                                                                                                [MIT-BIH arrhythmia 데이터베이스, ResNet, ResNeXt, Adabound, 주입기법]   \n",
      "47                                  [Breast Ultrasound, Breast Cancer, Tumor, Classification, VGG, ResNet, InceptionNet, DenseNet, EfficientNet, Convolutional Neural Network]   \n",
      "48                                                                                                      [Problematic Bubble, Bubble Detection, Conformal Coating, CNN, ResNet]   \n",
      "49                                                                                      [전기 화재, 단락흔, 열흔, CNN, Resnet 알고리즘, Electric fire, Arc beads, Molten mark, CNN, Resnet]   \n",
      "50                                                                                                                                                                          []   \n",
      "51                                                                                                                                                                          []   \n",
      "52                                                               [Convolutional Neural Networks, Deep Learning, Transfer Learning, Fine Tuning, Plant Diseases Classification]   \n",
      "53                                                            [딥러닝, 스태킹 앙상블, 전이학습, X-ray/CT 영상, COVID-19, deep learning, stacking ensemble, transfer learning, X-ray/CT image]   \n",
      "54                                         [자동차 번호판 인식, 딥러닝, 영상처리 결합, 임베디드, 경량화, License plate recognition, Deep learning, Image processing combination, Embedded, Lightening]   \n",
      "55                                                                         [딥러닝, 마이크로스코프, 이상검출, 흡연, 혓바닥, abnormal detection, deep learning, microscope, tongue surface, smoke]   \n",
      "56                                                                                     [Deep learning, Electrocardiogram, CNN, ResNet, Arrhythmia Detection, 딥러닝, 심전도, 부정맥 검출]   \n",
      "57                                                                                                     [augmented reality, CycleGAN, ResNet, normalization, texture, 3D model]   \n",
      "58                                                                                                                       [Deep Learning, Coreset, PCB Inspection, OCR, ResNet]   \n",
      "59                                                                                             [Cervical cancer, Radiomics, Laplacian variance, Euclidean distance, ResNet-50]   \n",
      "60                                                                                     [고령친화도시, 인구고령화, 지표, GIS, Smart Farm, Deep Learning, ResNet, Convolution Neural Network]   \n",
      "61                                             [융합, 패션, T.P.O, 다중 레이블, 클래스 불균형, 딥러닝, Convergence, Fashion, T.P.O, Multi-label problem, Class imbalance problem, Deep learning]   \n",
      "62                                                                                    [Convolutional neural networks, lofar analysis, sonar analysis, underwater recognition.]   \n",
      "63                                                                            [Cerebral hemorrhage prediction · Cerebrovascular disease · Explainable artificial intelligence]   \n",
      "64                                                                            [Surface Inspection, Crack Detection, Computer Vision, Deep Learning, 표면 검사, 균열 탐지, 컴퓨터 비전, 딥러닝]   \n",
      "65                                                                    [Simultaneous Switching Noise, Electromagnetic Band Gap, Machine Learning, Convolutional Neural Network]   \n",
      "66                                                           [Convolutional Neural Network(CNN, 합성곱 신경망), Deep learning(딥러닝), Propeller(프로펠러), Cavitation(캐비테이션), Erosion(침식)]   \n",
      "67                                                                                                                      [AI, Biometric authentication, Finger vein recognizer]   \n",
      "68                                                             [Image Classification, Object Detection, Convolutional Neural Network, Deep Learning, Heatmap, Autonomous Ship]   \n",
      "69                                                                                                           [Time Series Classification, Deep Learning, Tribology, Cosmetics]   \n",
      "70                                                                                          [Deep-learning, EfficientNet, Intracranial hemorrhage, Computed tomography images]   \n",
      "71                                                                                          [Deep Learning, Multi-Label Classification, Skin Diseases, 딥 러닝, 다중 레이블 분류, 피부 질환]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           abstract  \\\n",
      "0   전통적으로 대부분의 악성코드는 도메인 전문가에 의해 추출된 특징 정보를 활용하여 분석되었다. 하지만 이러한 특징 기반의 분석방식은 분석가의 역량에 의존적이며 기존의 악성코드를 변형한 변종 악성코드를 탐지하는 데 한계를 가지고 있다. 본 연구에서는 도메인 전문가의 개입 없이도 변종 악성코드의 패밀리를 분류할 수 있는 ResNet-Variational AutoEncder 기반 변종 악성코드 분류 방법을 제안한다. Variational AutoEncoder 네트워크는 입력값으로 제공되는 훈련 데이터의 학습 과정에서 데이터의 특징을 잘 이해하며 정규 분포 내에서 새로운 데이터를 생성하는 특징을 가지고 있다. 본 연구에서는 Variational AutoEncoder의 학습 과정에서 잠재 변수를 추출을 통해 악성코드의 중요 특징을 추출할 수 있었다. 또한 훈련 데이터의 특징을 더욱 잘 학습하고 학습의 효율성을 높이기 위해 전이 학습을 수행했다. ImageNet Dataset으로 사전학습된 ResNet-152 모델의 학습 파라미터를 Encoder Network의 학습 파라미터로 전이했다. 전이학습을 수행한 ResNet-Variational AutoEncoder의 경우 기존 Variational AutoEncoder에 비해 높은 성능을 보였으며 학습의 효율성을 제공하였다. 한편 변종 악성코드 분류를 위한 방법으로는 앙상블 모델인 Stacking Classifier가 사용되었다. ResNet-VAE 모델의 Encoder Network로 추출한 변종 악성코드 특징 데이터를 바탕으로 Stacking Classifier를 학습한 결과 98.66%의 Accuracy와 98.68의 F1-Score를 얻을 수 있었다.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None   \n",
      "2                                                                                                                                                                                                                                                                                                                                                           ResNet은 residual learning으로 학습 최적화를 용이하게 만들어 gradient vanishing과 상관없이 신경망이 깊어질 때 성능이 하락하는 degradation 문제를 해결한다. 하지만 기존 ResNet에서는 모든 residual block에 동일한 비율로 residual connection을 적용하여 residual learning의 최적화 효과를 극대화하지 못하는 한계가 있다. 따라서 본 논문에서는 residual learning의 최적화 효과를 극대화하기 위해 깊이 별 residual connection 비율을 달리하여 ResNet 정확도를 향상시키는 variational scaling 방법을 제안한다. 성능을 검증하기 위해 두가지 다른 데이터셋인 CIFAR-10과 CIFAR-100에서 다른 깊이를 갖는 ResNet-32, ResNet-56를 사용해 실험을 수행한다. 실험 결과 모든 케이스에서 기존 ResNet과 비교하여 연산량 증가 없이 정확도 향상을 이룬다.   \n",
      "3                                                                         본 논문에서는 스펙트럼 저장 시 데이터용량을 줄이기 위해 합성곱 오토인코더(convolutional autoencoder) 구조에 ResNet(Residual Neural Network) 알고리즘을 적용한 스펙트럼 데이터 압축 신경망을 제안한다. 최근 분광법(spectroscopy)의 적용 분야가 넓어짐에 따라 스펙트럼 데이터베이스가 대용량화되어 효율적인 전송이 어렵고 많은 저장 공간을 필요로 한다. 이러한 대용량의 데이터베이스를 효율적으로 관리하기 위해 데이터 압축을 수행한다. 기존 데이터 압축에 주로 사용되는 PCA(Principal Component Analysis)는 주성분의 개수에 따라 압축률이 결정된다. 주성분 개수가 적을수록 압축률은 높아지지만 정보 손실이 보다 쉽게 발생하기 복원 시 원본 데이터와의 크게 오차가 발생한다. 이러한 한계점을 극복하기 위해 본 논문에서는 제안한 신경망인 CAER(Convolutional AutoEncoder+ResNet)을 통하여 데이터 압축을 수행하였다. 신경망 학습은 실제 스펙트럼 데이터를 묘사하여 생성한 모의실험 데이터를 통해 수행하였다. CAER 신경망의 성능 검증을 위해 라만 스펙트럼을 PCA와 신경망을 통하여 75%, 87.5%, 93.75%의 압축률로 압축과 복원을 수행한 후 각각의 결과를 비교 분석하였다. 원본과 복원 데이터의 오차 비교를 하였을 때 CAER 신경망은 PCA보다 평균 94.2%의 낮은 오차를 보인다. 이 결과를 통해 CAER 신경망이 스펙트럼 데이터 압축에 효과적으로 적용될 수 있음을 확인하였다.   \n",
      "4                                                                                                                 본 연구에서는 딥러닝 기술(deep learning technology)을 이용하여 분광 신호의 ROI(region of interest)를 찾는 방법을 제안한다. 제안한 방법은 모의실험 데이터로 학습된 딥러닝 모델을 이용하여 분광 신호의 ROI를 검출하는 방법이다. 분광 신호의 피크는 물질의 물리 화학적인 정보를 포함하고 있으므로 정확한 피크 검출은 분석 시스템의 성능에 영향을 미치는 중요한 과정이다. 지금까지 가장 많이 사용되는 방법은 진폭을 기반으로 피크 검출을 진행하는 것이다. 하지만 이런 방법들은 전처리 과정을 포함하거나 분광 신호에 따라 파라미터를 육안 검사로 선택하여 추정하므로 복잡하고 주관적이다. 이러한 문제점 개선을 위해 딥러닝 모델을 통해 분광 신호의 ROI 검출을 수행하였다. 제안한 방법은 전처리 과정이 없고 파라미터를 설정하지 않아도 되는 장점을 갖는다. 또한 검출한 ROI에 따라 분광 신호에 후처리(post-processing)를 수행하여 피크를 얻을 수 있다. 디폴트 손실 함수에 3만개 테스트 데이터를 적용하여 얻은 손실값을 통해 성능 평가를 수행하였다. 제안된 ResNet과 Unet을 결합한 딥러닝 모델은 일반적인 컨볼루션 신경망(CNN: Convolutional Neural Network), ResNet, 그리고 Unet에 비해 각각 76.5%, 69.8%, 5.9%의 성능 향상을 보였으며, 실제 라만 분광 신호의 ROI 검출에도 효과적으로 적용될 수 있음을 확인하였다.   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                               본 논문에서는 운전자의 주의산만을 유발하는 운전자, 탑승자의 동작을 분석하고 핸드폰과 관련된 운전자의 행동 10가지를 인식하였다. 먼저 주의산만을 유발하는 동작을 환경 및 요인으로 분류하고 관련 최근 논문을 분석하였다. 분석된 논문을 기반으로 주의산만을 유발하는 주요 원인인 핸드폰과 관련된 10가지 운전자의 행동을 인식하였다. 약 10만 개의 이미지 데이터를 기반으로 실험을 진행하였다. SURF를 통해 특징을 추출하고 3가지 모델(CNN, ResNet-101, 개선된 ResNet-101)로 실험하였다. 개선된 ResNet-101 모델은 CNN보다 학습 오류와 검증 오류가 8.2배, 44.6배가량 줄어들었으며 평균적인 정밀도와 f1-score는 0.98로 높은 수준을 유지하였다. 또한 CAM(class activation maps)을 활용하여 딥러닝 모델이 운전자의 주의 분산 행동을 판단할 때, 핸드폰 객체와 위치를 결정적 원인으로 활용했는지 검토하였다.   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None   \n",
      "10                                                                                                                                                                                                                                                                                                                                    자동차 번호판 인식은 지능형 교통시스템에서 핵심적인 역할을 담당한다. 따라서 효율적으로 자동차 번호판의 숫자 및 문자영역을 검출하는 것은 매우 중요한 과정이다. 본 연구에서는 딥러닝과 의미론적 영상분할 알고리즘을 적용 하여 효과적으로 자동차 번호판의 번호영역을 검출하는 방법을 제안한다. 제안된 방법은 화소 투영과 같은 전처리과정 없이 번호판 영상에서 바로 숫자 및 문자영역을 검출하는 알고리즘이다. 번호판 영상은 도로 위에 설치된 고정 카메라로 부터 획득한 영상으로 날씨 및 조명변화 등을 모두 포함한 다양한 실제 상황에서 촬영된 것을 사용하였다. 입력 영상은 색상변화를 줄이기 위해 정규화하고 실험에 사용된 딥러닝 신경망 모델은 Vgg16, Vgg19, ResNet18 및 ResNet50이 다. 제안방법의 성능을 검토하기 위해 번호판 영상 500장으로 실험하였다. 학습을 위해 300장을 할당하였으며 테스트용 으로 200장을 사용하였다. 컴퓨터모의 실험결과 ResNet50을 사용할 때 가장 우수하였으며 95.77% 정확도를 얻었다.   \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              농장주들이 과일을 분류하는 데까지의 시간소모를 줄이고 향후 과일의 등급 판정 기준을 정량화하기 위하여 우리나라의 대표적인 과일이며 다양한 품종을 가지고 있는 사과를 대상으로 신경망 기반 분류 자동화 시스템을 제안한다. 컨베이어 벨트를 통해 지나가는 객체를 카메라모듈로 촬영하여 이를 통신 및 연산을 수행하는 라즈베리파이 기반 시스템을 설계, 구현한다. 깊은 네트워크와 나머지(residual)를 학습하는 ResNet 기반 알고리즘이 구동되는 딥러닝 서버와는 SSH통신을 통해 이미지와 학습된 모델을 주고받는다. 향후 품종뿐만 아니라 등급까지 분류 자동화한다면 다양한 품종의 과일을 대상으로 한 출하 자동화 시스템으로의 적용이 가능하다.   \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                     코로나바이러스-19(COVID-19)의 대유행에 따라 전 세계 수많은 확진자가 발생하고 있으며 국민을 불안에 떨게 하고 있다. 바이러스 감염 확산을 방지하기 위해서는 마스크를 제대로 착용하는것이 필수적이지만 몇몇 사람들은 마스크를 쓰지 않거나 제대로 착용하지 않고 있다. 본 논문에서는 영상 이미지에서의 효율적인 마스크 감지 시스템을 제안한다. 제안 방법은 우선 입력 이미지의 모든 얼굴의 영역을 YOLOv5를 사용하여 감지하고 감지된 얼굴의 수에 따라 3가지의 장면복잡도(Simple, Moderate, Complex) 중 하나로 분류한다. 그 후 장면 복잡도에 따라 3가지ResNet(ResNet-18, 50, 101) 중 하나를 기반으로 한 Faster-RCNN을 사용하여 얼굴 부위를 감지하고마스크를 제대로 착용하였는지 식별한다. 공개 마스크 감지 데이터셋을 활용하여 실험한 결과 제안한 장면 복잡도 기반 적응적인 모델이 다른 모델에 비해 가장 성능이 뛰어남을 확인하였다.   \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "16                                                                                                                                                                                                                                                                                                                                             디자인에 있어서 유사한 디자인들을 그룹핑하여 분류하는 것은 관리적인 측면에서 효율성을 높여주고 사용적인 측면에서는 편의성을 제공한다. 본 연구는 인공지능 알고리즘을 이용하여 텍스타일 디자인을 도트, 꽃무늬, 줄무늬, 그리고 기하학으로 4개의 카테고리로 분류하고자 하였다. 특히, 인공지능의 관점에서 분류의 근거가 되는 관심 영역을 찾아내고 설명할 수 있는 지를 탐색하였다. 총 4,536개의 디자인을 8:2의 비율로 무작위 추출하여 학습용 데이터 3,629개와 테스트용 데이터 907개로 구성하였다. 분류에 사용된 모델은 VGG-16과 ResNet-34로 두 모델의 꽃무늬 디자인에 대한 정밀도는 각각 0.79%, 0.89%이며, 재현율은 0.95%, 0.38%로 우수한 분류 성과를 보였다. LIME(Local Interpretable Model-agnostic Explanation) 기법을 이용하여 분석한 결과에 따르면, 기하학과 꽃무늬 디자인의 경우 도형과 꽃잎 부분이 분류의 근거가 되는 관심 영역으로 도출되었다.   \n",
      "17                                                                                                                                                                                                공연예술 기관에서의 공연에 대한 흥행 예측은 공연예술 산업 및 기관에서 매우 흥미롭고도 중요한 문제이다. 이를 위해 출연진, 공연장소, 가격 등 정형화된 데이터를 활용한 전통적인 예측방법론, 데이터마이닝 방법론이 제시되어 왔다. 그런데 관객들은 공연안내 포스터에 의하여 관람 의도가 소구되는 경향이 있음에도 불구하고, 포스터 이미지 분석을 통한 흥행 예측은 거의 시도되지 않았다. 그러나 최근 이미지를 통해 판별하는 CNN 계열의 딥러닝 방법이 개발되면서 포스터 분석의 가능성이 열렸다. 이에 본 연구의 목적은 공연 관련 포스터 이미지를 통해 흥행을 예측할 수 있는 딥러닝 방법을 제안하는 것이다. 이를 위해 KOPIS 공연예술 통합전산망에 공개된 포스터 이미지를 학습데이터로 하여 Pure CNN, VGG-16, Inception-v3, ResNet50 등 딥러닝 알고리즘을 통해 예측을 수행하였다. 또한 공연 관련 정형데이터를 활용한 전통적 회귀분석 방법론과의 앙상블을 시도하였다. 그 결과 흥행 예측 정확도 85%를 상회하는 높은 판별 성과를 보였다. 본 연구는 공연예술 분야에서 이미지 정보를 활용하여 흥행을 예측하는 첫 시도이며 본 연구에서 제안한 방법은 연극 외에 영화, 기관 홍보, 기업 제품 광고 등 포스터 기반의 광고를 하는 영역으로도 적용이 가능할 것이다.   \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     본 연구에서는 소비자 데이터를 딥러닝 기반의 분류(Classification) 모델을 학습 시켜 추천 알고리즘을 구현하였다. 이를 위하여 사용자 데이터를 이미지로 변환 시켜 분류 과제에서 보편적으로 사용되는 ResNet50을 사용하여 학습한 결과로서 유의미한 결과에 대하여 제시함   \n",
      "24                                                    영농 폐기물의 증가로 인해, 빠르고 효율적으로 수거할 수 있는 스마트 영농 폐기물 모니터링 시스템 개발이 필요하다. 본 논문에서는 영농 폐기물 분류 시스템을 제안하고 실제 지역 농촌에서 직접 수집한 영상을 이용하여 CNN 기반의 전이학습 모델들을 구현하고 비교하였다. 영농 폐기물 영상 분류에 적합한 모델과 학습 조건을 찾기 위해, 3가지의 학습 자료군 구성 조건 (2종 분류, 6종 분류, 6종 하위분류를 가진 2종 분류)을 달리하여 미세 조정된 6개의 사전 훈련 CNN 모델들의 검증 정확도를 비교하였다. 그 결과, ResNet-50 모델의 성능이 모든 학습 조건에서 평균 90.9%의 정확도로 가장 높았고, 폐기물 영상을 6종 분류했을 때보다 2종 분류로 했을 때의 검증 정확도가 10% 더 높았다. 특히, 학습 자료군 구성 방법 중 6종 하위분류를 가진 2종 분류했을 때의 검증 정확도는 2종 분류했을 때와 유사했다. 이를 통해 영농 폐기물은 한 종류만 모여 있지 않을뿐더러 다양한 폐기물들이 한데 섞여 있어서 영농 폐기물의 특정한 세부 종류로 분류하는 것보다 폐기물인지 아닌지를 이진 분류하는 것이 더 효과적임을 확인하였다. 나아가, 제안된 시스템의 동작을 확인하기 위해, 영농 환경 모니터링 서버와 영농 폐기물 영상 분류 서버 사이에 TCP / IP 기반의 통신 환경을 구축하고, 모의실험을 통해 구현한 영농 폐기물 영상 분류 시스템이 스마트 영농 폐기물 모니터링 시스템으로 사용될 가능성을 확인하였다. 본 연구의 결과는 정형화되지 않거나 여러 병변이 혼합된 의료 영상을 분류하는 경우에도 활용될 수 있을 것이다.   \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     영상 콘텐츠를 사용자에게 추천하기 위해서는 메타데이터가 필수적인 요소로 자리 잡고 있다. 하지만 이러한 메타데이터는 영상 콘텐츠 제공자에 의해 수동적으로 생성되고 있다. 본 논문에서는 기존 수동으로 직접 메타데이터를 입력하는 방식에서 자동으로 메타데이터를 생성하는 방법을 연구하였다. 기존 연구에서 감정 태그를 추출하는 방법에 추가로 영화 오디오를 통한 장르와 제작국가에 대한 메타데이터 자동 생성 방법에 대해 연구를 진행하였다. 전이학습 모델인 ResNet34 인공 신경망 모델을 이용하여 오디오의 스펙트로그램으로부터 장르를 추출하고, 영화 속 화자의 음성을 음성인식을 통해 언어를 감지하였다. 이를 통해 메타데이터를 생성 인공지능을 통해 자동 생성 가능성을 확인할 수 있었다.   \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "28                                                                                                                                                                                                                                                                                                                                                  Convolution Neural Network 가속기는 AI시대에 중요한 요소 중 하나로 떠오르게 되었다. CNN 가속기의 내부 구성을 몇 가지로 나눈다면 연산을 위한 Multiplier-Accumulator (MAC unit), 데이터 저장을 위한 SRAM, 데이터 이동을 위한 메모리 인터페이스 그리고 제어 로직으로 구분할 수 있다. 다양한 CNN 가속기들의 경우, 각기 다른 공정과 동작 주파수를 기준으로 제안되었으며, 또한 아키텍처 형태에 따라 내부 MAC unit의 수와 SRAM의 크기가 매우 큰 차이를 갖는 형태로 구성되어있다. 각 가속기들의 기본 사양으로 면적, 에너지, 성능을 비교하였을 때는 공정이나 동작 주파수 등 여러 조건들에 의해서 아키텍처에 따른 정량적인 비교가 용이하지 않게 된다. 따라서, 본 논문에서는 다양한 CNN 가속기에서 여러 조건들을 동일하게 재구성하였을 때, ResNet-50 추론 동작 시에 요구되는 면적, 에너지, 성능을 비교하여 아키텍처의 특징과 경향성을 분석하였다.   \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "40                                                                                                                                                                                                                                                                                                                                                            요검사는 물리적 성상 검사, 화학적 검사, 현미경 검사 세 가지가 있다. 이 중에서 화학적 요검사는 일반인이 쉽게 접근하는 방법으로 요검사지의 화학반응을 눈으로 표준비색표와 비교하거나 휴대용 요검사기를 별도로 구매하여 검사를 진행한다. 현재는 스마트폰의 보급이 대중화되어 스마트폰을 활용한 요검사 서비스 연구가 높아지고 있다. 요검사 스크리닝 애플리케이션은 스마트폰을 활용한 요검사 서비스 중 하나이다. 그러나 요검사 스크리닝 애플리케이션으로 촬영한 요검사 패드 RGB 값은 조명영향으로 인해 큰 편차가 발생한다. 요검사 패드 RGB 값의 편차는 요검사 판별의 정확도를 떨어뜨린다. 따라서 본 논문에서는 스마트폰 기반 요검사 스크리닝 애플리케이션으로 촬영한 요검사지를 검사 항목별 요검사 패드로 분류한 후 CNN을 통해 요검사 패드 이미지 판별의 정확도를 높인다. 요검사지는 다양한 배경에서 촬영하여 CNN 이미지를 생성하였으며 ResNet-50 CNN 모델을 사용하여 요검사 판별을 분석하였다.   \n",
      "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "45                                                                                                                                                                                                                                                                                                                                                                       딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다.   \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "52                                                                                                                                                                                                                                     토마토 작물은 병해충의 영향을 많이 받기 때문에 이를 예방하지 않으면 농업 경제에 막대한 손실을 초래할 수 있다. 따라서 토마토의 다양한 병해충의 진단을 빠르고 정확하게 진단하는 시스템이 요구된다. 본 논문에서는 ImageNet 데이터 셋 상에서 다양하게 사전 학습된 딥러닝 기반 CNN 모델을 적용하여 토마토의 9가지 병해충 및 정상인 경우의 클래스를 분류하는 시스템을 제안한다. PlantVillage 데이터 셋으로부터 발췌한 토마토 잎의 이미지 셋을 3가지 딥러닝 기반 CNN 구조를 갖는 ResNet, Xception, DenseNet의 입력으로 사용한다. 기본 CNN 모델 위에 톱-레벨 분류기를 추가하여 제안 모델을 구성하였으며, 훈련 데이터 셋에 대해 5-fold 교차검증 기법을 적용하여 학습시켰다. 3가지 제안 모델의 학습은 모두 기본 CNN 모델의 계층을 동결하여 학습시키는 전이 학습과 동결을 해제한 후 학습률을 매우 작은 수로 설정하여 학습시키는 미세 조정 학습 두 단계로 진행하였다. 모델 최적화 알고리즘으로는 SGD, RMSprop, Adam을 적용하였다. 실험 결과는 RMSprop 알고리즘이 적용된 DenseNet CNN 모델이 98.63%의 정확도로 가장 우수한 결과를 보였다.   \n",
      "53                                               COVID-19 팬데믹으로 인한 피해는 공중 보건적 측면 뿐 만 아니라 정치, 경제, 사회, 문화 전반에 심각한 영향을 미치고 있다. 현재까지 COVID-19 표준 진단검사인 RT-PCR 검사는 검체의 종류, 검체 채취 방법 및 보관에 따라 검사 결과가 달라질 수 있고 코로나바이러스 (SARS-CoV-2) 감염 후 검사 시점에도 영향을 받는다. 본 논문은 전이학습 (transfer learning) 기반 앙상블 딥러닝을 사용하여 COVID-19 환자 X-ray/CT 영상을 분류하고자 한다. 여기서 사용된 전이학습은 CNN (convolutional neural network) 기반인 AlexNet, ResNet, Inception V3, DenseNet 모형이다. 본 연구에서 제안한 스태킹 앙상블 (stacking ensemble) 모형은 세 단계에 걸쳐 이루어진다. 첫 번째 단계에서는 기본모형 (base model)로서 여러 전이학습 모형을 이용하여 예측된 결과들을 얻고, 두 번째 단계에서는 concatenate layer를 통해 이들 결과들을 결합한 다음, 세 번째 단계에서는 메타모형(meta model), 여기서는 DNN (deep neural network) 모형을 적용하여 최종 분류한다. 본 논문에서 제안된 앙상블 모형의 성능평가를 위해 3가지 실제 COVID-19 환자의 X-ray/CT 영상데이터셋을 고려하였으며 여러 가지 성능평가 지표를 가지고 기존의 전이학습 모형과 앙상블 모형과 비교 분석하였다. 성능실험결과, 전반적으로 제안된 앙상블 모형이 기존의 전이학습 모형과 앙상블 모형보다 우수함을 보였다.   \n",
      "54                                                                                                                                                                                                                                자동차 번호판인식 시스템은 기존에는 영상처리만을 이용한 방식으로 매우 빠른 실시간 처리가 가능하나 다양한 번호판에는 적용하기 어렵다는 한계가 있었고, 딥러닝을 이용하는 경우 다양성과 정확성이 좋아지나 고성능의 그래픽카드가 필요하고 처리하는 데 시간이 매우 오래 걸리는 문제점이 있었다. 본 논문은 각 방식의 장점을 살려 그래픽카드가 없는 일반 사무용 PC에서도 실시간 처리가 가능하며 높은 정확성을 가진 자동차 번호판인식 시스템을 제안하며 더 나아가 사무용 PC가 아닌 임베디드 환경에서도 사용할 수 있도록 경량화한 시스템을 제안한다. 제안하는 시스템은 기존의 번호판인식 시스템과 동일하게 [번호판검출]-[문자영역 분할]-[문자인식]의 3단계 과정을 거치며 각 과정에는 딥러닝 모델로서는 SSD-MobileNet, ResNet 네트워크를 사용하였고, 영상처리 기법으로는 Edge를 검출한 후 수직, 수평으로 전파하면서 관심 영역을 찾는 CLNF 알고리즘을 사용하였다. 제안하는 시스템으로 지하주차장 및 톨게이트 등의 장소에서 얻은 4,389장의 이미지로 테스트하였을 때 충분히 레이어가 깊은 경우 98.2% 정확성을 보여 주었고, 레이어가 얕아질수록 영상처리 결합 여부에 따른 정확성 차이가 커짐을 확인할 수 있었다.   \n",
      "55                                                                                                                                                                        흡연자 중에서 담배가 인체에 유해하다는 사실을 모르는 사람은 없을 것임에도 불구하고, 정작 금연 성공률은 높지 않다. 금연을 위한 의지를 지속적으로 굳건하게 다지기 위하여 병원에서 실시하는 건강검진과 PET(positron emission tomography) 이미지를 통한 암 검사의 결과가 도움이 되지만, 일상생활 중에 간단히 실시할 수 있는 방법이 아니다. 본 연구에서는 일상생활 중에 관찰 가능한 흡연자의 신체 부위를 딥러닝 기반 마이크로스코프 이미지 측정 및 분석을 통하여 흡연자와 비흡연자의 차이를 검출할 수 있는 비침습적 방법을 제안하였다. 우선, 관찰 부위를 흡연시 직접적인 접촉을 하는 혓바닥 표면으로 설정하였다. 다음으로, 마이크로스코프로 혓바닥 표면(410배 확대)을 흡연자 10명과 비흡연자 10명의 실험 참가자를 통하여 데이터 셋(총 1,000장)을 구축하여 그 중 80%를 딥러닝 모델의 학습에 사용하였고, 나머지 20%는 예측에 사용하였다. 딥러닝 모델을 스케일링하는 방법(width scaling, depth scaling, resolution scaling) 중 한 가지 방법만 적용하는 VGG, ResNet, DenseNet과 세 가지를 모두 적용하여 스케일링하는 EfficientNet의 성능을 비교하여 모세혈관 이미지 처리에 EfficientNet의 우수성을 확인해 볼 수 있었다.   \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                         본 논문에서는 딥러닝 모델을 이용하여 모바일 기기의 심전도 신호 측정 데이터를 분류한다. 비정상 심장박동을 높은 정확도로 분류하기 위해 딥러닝 모델의 구성 요소 세 가지를 선정하고 요소의 조건 변화에 따른 분류 정확도를 비교한다. 심전도 신호 데이터의 특징을 스스로 추출할 수 있는 CNN 모델을 적용하고 모델을 구성하는 모델의 깊이, 최적화 방법, 활성화 함수의 조건을 변경하여 총 48개의 조합의 성능을 비교한다. 가장 높은 정확도를 보이는 조건의 조합을 도출한 결과 컨볼루션 레이어 19개, 최적화 방법 SGD, 활성화 함수 Mish를 적용하였을 때 정확도 97.88%로 모든 조합 중 가장 높은 분류 정확도를 얻었다. 이 실험에서 CNN을 활용한 1-채널 심전도 신호의 특징 추출과 비정상 박동 검출의 적합성을 확인하였다.   \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "60                                                                                                                                                                                                                                                                                                                                                                                                                                   전 세계적인 인구고령화 현상이 사회적으로 문제가 되고 있다. 우리나라 역시 초고령화 사회에 급속도로 진입하면서 사회적 생활공간의 실질적 변화가 필요하게 되었다. 이에 본 연구에서는 WHO가 제시한 ‘고령친화도시’ 개념과 가이드라인을 중심으로 군집분석과 EDA를 활용해 국내 고령친화도 현황을 분석하였다. 더불어 경기도의 이천시와 안성시를 중심으로 관련 데이터를 비교·분석 후 GIS 기반 데이터 시각화를 수행하였다. 분석 결과 B등급인 이천시에 비교하여 안성시의 고령친화도는 인구, 면적 등의 조건이 비슷함에도 불구하고 D등급으로 측정되어 매우 낮은 것으로 나타났다. 본 연구의 결과를 통해 정책적으로는 전국 단위의 고령친화도를 파악하여 효과적인 복지 정책 가이드라인 제시와 더불어, 한정된 정부예산에서 정비가 시급한 부분에 대한 우선적 고려가 가능할 것으로 기대된다.   \n",
      "61                                                                                                                                                                                                                                                                                                                                                                                                                                          의복을 착용하는데 있어 목적 상황에 부합하는 옷차림을 구성하는 것은 중요하다. 따라서 인공지능 기반의 다양 한 패션 추천 시스템에서 의복 착용의 T.P.O(Time, Place, Occasion)를 고려하고 있다. 하지만 옷차림으로부터 직접 T.P.O를 추론하는 연구는 많지 않은데, 이는 문제 특성 상 다중 레이블 및 클래스 불균형 문제가 발생하여 모델 학습을 어렵게 하기 때문이다. 이에 본 연구에서는 label-distribution-aware margin(LDAM) loss를 도입하여 옷차림의 T.P.O를 추론할 수 있는 모델을 제안한다. 모델의 학습 및 평가를 위한 데이터셋은 패션 쇼핑몰로부터 수집되었고 이를 바탕으로 성능을 측정한 결과, 제안 모델은 비교 모델 대비 모든 T.P.O 클래스에서 균형잡힌 성능을 보여주는 것을 확인할 수 있었다.   \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "65                                                                                                                                                                                                                                                                           기술이 빠르게 발전하여 디지털 시스템의 동작 주파수는 수 GHz 대역까지 증가했다. 이로 인하여 Simultaneous Switching Noise 문제가 증가했고, 이를 줄이기 위해 Electromagnetic Band Gap(EBG) 구조가 많이 연구된다. EBG 구조 설계에서 중요한 과정 중 하나는 노이즈를 저감하는 Stopband 대역을 예측하는 것이다. 기존에 3차원 전자장 시뮬레이션 프로그램을 이용하는 방법과 Floquet 이론 기반의 수식을 이용하는 방법이 있으나, 한계점이 존재한다. 본 논문에서는 Convolutional Neural Network(CNN)을 이용하여 EBG 구조의 Stopband 대역을 예측하는 새로운 방법을 제안한다. 또한 기본 CNN 구조, GoogLeNet, ResNet, DenseNet과 같은 CNN Architecture 모델을 활용하여 어떤 CNN 구조가 Stopband 대역 예측에 높은 성능을 보이는지 분석한다. 900개의 EBG 구조 모델에 대해서 학습시킨 후 CNN 구조의 mean absolute error를 비교한 결과, DenseNet이 가장 우수한 성능을 보임을 확인하였다.   \n",
      "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Traditionally, most malicious codes have been analyzed using feature information extracted by domain experts. However, this feature-based analysis method depends on the analyst's capabilities and has limitations in detecting variant malicious codes that have modified existing malicious codes. In this study, we propose a ResNet-Variational AutoEncder-based variant malware classification method that can classify a family of variant malware without domain expert intervention. The Variational AutoEncoder network has the characteristics of creating new data within a normal distribution and understanding the characteristics of the data well in the learning process of training data provided as input values. In this study, important features of malicious code could be extracted by extracting latent variables in the learning process of Variational AutoEncoder. In addition, transfer learning was performed to better learn the characteristics of the training data and increase the efficiency of learning. The learning parameters of the ResNet-152 model pre-trained with the ImageNet Dataset were transferred to the learning parameters of the Encoder Network. The ResNet-Variational AutoEncoder that performed transfer learning showed higher performance than the existing Variational AutoEncoder and provided learning efficiency. Meanwhile, an ensemble model, Stacking Classifier, was used as a method for classifying variant malicious codes. As a result of learning the Stacking Classifier based on the characteristic data of the variant malware extracted by the Encoder Network of the ResNet-VAE model, an accuracy of 98.66% and an F1-Score of 98.68 were obtained.  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Unlike optical equipment, SAR(Synthetic Aperture Radar) has the advantage of obtaining images in all weather, and object detection in SAR images is an important issue. Generally, deep learning-based object detection was mainly performed in real-valued network using only amplitude of SAR image. Since the SAR image is complex data consist of amplitude and phase data, a complex-valued network is required. In this paper, a complex-valued ResNet network is proposed. SAR image object detection was performed by combining the ROI transformer detector specialized for aerial image detection and the proposed complex-valued ResNet. It was confirmed that higher accuracy was obtained in complex-valued network than in existing real-valued network.  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ResNet resolves a degradation problem by residual learning which allows ease to the learning optimization. However, original ResNet has such a limitation that do not maximize the optimization of residual learning by applying residual connections with the constant scale. This paper suggests a variational scaling method that adjusts scales of residual connection by depth in order to maximize the optimization effect of residual learning hence to enhance the model accuracy. Experiments are conducted in two different datasets CIFAR-10 and CIFAR-100, and with 2 models with different depth, ResNet –32 and ResNet-56. As the result of experiments, the variational scaling method enhanced accuracy without the computational amount increased compared to the original ResNet in all cases.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In this paper, we propose a spectrum compression neural network that applied the ResNet (Residual Neural Network) algorithm to the convolutional autoencoder structure to reduce data capacity requirement in storing the spectrum. Recently, as the field of application of spectroscopy widens, the spectrum database is becoming larger, making efficient transmission difficult and requiring large amount of storage. Therefore, data compression is performed to manage large amounts of data efficiently. In PCA (Principal Component Analysis), which is mainly used for data compression, the compression ratio is determined by the number of principal components. As the number of principal components decreases, the compression rate increases, but at the same time, it is easier for information loss to occur. Hence, errors occur between reconstruction and the raw spectrum. To overcome these limitations, we perform compression through the proposed CAER (Convolutional AutoEncoder+ResNet) network. The training of the network was performed through simulated data describing the real spectrum. To verify the performance of the CAER network, the Raman spectrum was compressed and reconstructed at compression rates of 75%, 87.5%, and 93.75% through the PCA and CAER networks. Comparing the errors between raw and reconstructed data, the CAER network shows an average error of 94.2% lower than that of the PCA. The results obtained confirm that the CAER network can be effectively applied to spectrum compression.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This study proposes a method to find the ROI (region of interest) of spectral signals using deep learning technology. The proposed method detects the ROI of spectral signals using a deep learning model trained with simulated data. Since the peak of the spectral signal contains physical and chemical information of the substance, accurate peak detection is an important process affecting the performance of the analyzed system. The widely used method for peak detection is the one based on the amplitude. However, this method is complex and subjective because it involves pre-processing or select and estimate parameters using visual inspection according to spectral signals. To overcome this problem, ROI detection of the spectral signal was performed through a deep learning model. The proposed method has the advantage of requiring no pre-processing and parameter setting. In addition, a peak may be obtained by performing post-processing of the spectral signal according to the detected ROI. Performance evaluation was performed through loss values obtained by applying 30,000 test data to the custom loss function. The proposed deep learning model combining ResNet and Unet showed performance improvements of 76.5%, 69.8%, and 5.9% compared to the general convolutional neural network (CNN), ResNet, and Unet, respectively. It was also confirmed that the proposed method could be effectively applied to measured spectral signals.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Initializing the weights plays an essential role in a convolutional neural network model. This paper investigates how Glorot and Hes initialization methods behave in Mobilenet and Resnet models on the weeds classification problem. Experiments show that pointwise and depthwise convolution in Mobilenet reduces the variance of feature maps from earlier layers. Using the He’s method, shortcut connection in Resnet saturate values in logistic classify layer. The accuracy of Mobilenet and Resnet, using Glorots method, are 0.9568 and 0.9711, respectively. While using Hes method, we obtain 0.9471 using Mobilenet and 0.9645 using Resnet. Also, both models converge faster and better generalization using Glorots method than using Hes method.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None  \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        None  \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In this paper, we analyzed driver\"s and passenger\"s motions that cause driver\"s distraction, and recognized 10 driver\"s behaviors related to mobile phones. First, distraction-inducing behaviors were classified into environments and factors, and related recent papers were analyzed. Based on the analyzed papers, 10 driver\"s behaviors related to cell phones, which are the main causes of distraction, were recognized. The experiment was conducted based on about 100,000 image data. Features were extracted through SURF and tested with three models (CNN, ResNet-101, and improved ResNet-101). The improved ResNet-101 model reduced training and validation errors by 8.2 times and 44.6 times compared to CNN, and the average precision and f1-score were maintained at a high level of 0.98. In addition, using CAM (class activation maps), it was reviewed whether the deep learning model used the cell phone object and location as the decisive cause when judging the driver\"s distraction behavior.  \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The conventional butterfly identification method is based on their different morphological characters namely wing-venation, color, shape, patterns and through the dissection studies and molecular techniques which are tedious, expensive and highly time-consuming. To overcome the above aforesaid challenges, a new butterfly identification system using butterfly images has been designed to instantly identify the butterfly with high ac curacy. In this study, we construct a new butterfly dataset with 34,024 butterfly images belonging to 315 species from India. We propose and prove the effectiveness of new data augmentation techniques on our dataset. To identify butterflies using photographic images, we built eleven new Deep Convolutional Neural Network (DCNN) butterfly classifier models using eleven pre-trained architectures namely ResNet-18, ResNet-34, ResNet-50, ResNet-121, ResNet-152, Alex-Net, DenseNet-121, DenseNet-161, VGG-16, VGG-19 and SqueezeNet-v1.1. The different model’s classification results were compared and the proposed technique achieved a maximum top-1 accuracy(94.44%), top-3 accuracy(98.46%) and top-5 accuracy(99.09%) using ResNet-152 model, followed by DenseNet-161 model achieved the top-1 accuracy(94.31%), top-3 accuracy (98.07%) and top-5 accuracy (98.66%). The results suggest that models can be assertively used to identify butterflies in India.  \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   License plate recognition plays a key role in intelligent transportation systems. Therefore, it is a very important process to efficiently detect the number and character areas. In this paper, we propose a method to effectively detect license plate number area by applying deep learning and semantic image segmentation algorithm. The proposed method is an algorithm that detects number and text areas directly from the license plate without preprocessing such as pixel projection. The license plate image was acquired from a fixed camera installed on the road, and was used in various real situations taking into account both weather and lighting changes. The input images was normalized to reduce the color change, and the deep learning neural networks used in the experiment were Vgg16, Vgg19, ResNet18, and ResNet50. To examine the performance of the proposed method, we experimented with 500 license plate images. 300 sheets were used for learning and 200 sheets were used for testing. As a result of computer simulation, it was the best when using ResNet50, and 95.77% accuracy was obtained.  \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              This paper propose a neural network-based classification automation system for apples, which are representative fruits of Korea and have a variety of varieties, in order to reduce the consumption of time for farmers to classify fruits and to quantify the criteria for grading the fruits. Raspberry Pi-based system that communicates and performs calculations by photographing objects passing through a conveyor belt with a camera module is designed and implemented. Then it receive the trained model using ResNet-based algorithm that runs on the deep-learning server. In the future, if classifying not only varieties but also grades is automated, it can be applied as a shipping automation system targeting various varieties of fruit.  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Brain–Computer Interface technology plays a vital role in facilitating post-stroke patients’ ability to carry out their daily activities of living. The extraction of features and the classification of electroencephalogram (EEG) signals are pertinent parts in enabling such a system. This research investigates the efficacy of Transfer Learning models namely ResNet50 V2, ResNet101 V2, and ResNet152 V2 in extracting features from CWT converted wink-based EEG signals, prior to its classification via a fine-tuned Support Vector Machine (SVM) classifier. It was shown that ResNet152 V2-SVM pipeline could achieve an excellent accuracy on all train, test and validation datasets.  \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Objective Recent studies have suggested that deep-learning models can satisfactorily assist in fracture diagnosis. We aimed to evaluate the performance of two of such models in wrist fracture detection. Methods We collected image data of patients who visited with wrist trauma at the emergency department. A dataset extracted from January 2018 to May 2020 was split into training (90%) and test (10%) datasets, and two types of convolutional neural networks (i.e., DenseNet-161 and ResNet-152) were trained to detect wrist fractures. Gradient-weighted class activation mapping was used to highlight the regions of radiograph scans that contributed to the decision of the model. Performance of the convolutional neural network models was evaluated using the area under the receiver operating characteristic curve. Results For model training, we used 4,551 radiographs from 798 patients and 4,443 radiographs from 1,481 patients with and without fractures, respectively. The remaining 10% (300 radiographs from 100 patients with fractures and 690 radiographs from 230 patients without fractures) was used as a test dataset. The sensitivity, specificity, positive predictive value, negative predictive value, and accuracy of DenseNet-161 and ResNet-152 in the test dataset were 90.3%, 90.3%, 80.3%, 95.6%, and 90.3% and 88.6%, 88.4%, 76.9%, 94.7%, and 88.5%, respectively. The area under the receiver operating characteristic curves of DenseNet-161 and ResNet-152 for wrist fracture detection were 0.962 and 0.947, respectively. Conclusion We demonstrated that DenseNet-161 and ResNet-152 models could help detect wrist fractures in the emergency room with satisfactory performance.  \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Coronavirus disease 2019 (COVID-19) has affected the world seriously. Every person is required for wearing a mask properly in a public area to prevent spreading the virus. However, many people are not wearing a mask properly. In this paper, we propose an efficient mask detection system. In our proposed system, we first detect the faces of input images using YOLOv5 and classify them as the one of three scene complexity classes (Simple, Moderate, and Complex) based on the number of detected faces. After that, the image is fed into the Faster-RCNN with the one of three ResNet (ResNet-18, 50, and 101) as backbone network depending on the scene complexity for detecting the face area and identifying whether the person is wearing the mask properly or not. We evaluated our proposed system using public mask detection datasets. The results show that our proposed system outperforms other models.  \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        User interface for cancer classification system is a software application with clinician's friendly tools and functions to diagnose cancer from pathology images. Pathology evolved from manual diagnosis to computer-aided diagnosis with the help of Artificial Intelligence tools and algorithms. In this paper, we explained each block of the project life cycle for the implementation of automated breast cancer classification software using AI and machine learning algorithms to classify normal and invasive breast histology images. The system was designed to help the pathologists in an automatic and efficient diagnosis of breast cancer. To design the classification model, Hematoxylin and Eosin (H&E) stained breast histology images were obtained from the ICIAR Breast Cancer challenge. These images are stain normalized to minimize the error that can occur during model training due to pathological stains. The normalized dataset was fed into the ResNet-34 for the classification of normal and invasive breast cancer images. ResNet-34 gave 94% accuracy, 93% F Score, 95% of model Recall, and 91% precision.  \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Grouping and classifying similar designs in design increase efficiency in terms of management and provide convenience in terms of use. Using artificial intelligence algorithms, this study attempted to classify textile designs into four categories: dots, flower patterns, stripes, and geometry. In particular, we explored whether it is possible to find and explain the regions of interest underlying classification from the perspective of artificial intelligence. We randomly extracted a total of 4,536 designs at a ratio of 8:2, comprising 3,629 for training and 907 for testing. The models used in the classification were VGG-16 and ResNet-34, both of which showed excellent classification performance with precision on flower pattern designs of 0.79%, 0.89% and recall of 0.95% and 0.38%. Analysis using the Local Interpretable Model-agnostic Explanation (LIME) technique has shown that geometry and flower-patterned designs derived shapes and petals from the region of interest on which classification was based.  \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The prediction of box office performance in performing arts institutions is an important issue in the performing arts industry and institutions. For this, traditional prediction methodology and data mining methodology using standardized data such as cast members, performance venues, and ticket prices have been proposed. However, although it is evident that audiences tend to seek out their intentions by the performance guide poster, few attempts were made to predict box office performance by analyzing poster images. Hence, the purpose of this study is to propose a deep learning application method that can predict box office success through performance-related poster images. Prediction was performed using deep learning algorithms such as Pure CNN, VGG-16, Inception-v3, and ResNet50 using poster images published on the KOPIS as learning data set. In addition, an ensemble with traditional regression analysis methodology was also attempted. As a result, it showed high discrimination performance exceeding 85% of box office prediction accuracy. This study is the first attempt to predict box office success using image data in the performing arts field, and the method proposed in this study can be applied to the areas of poster-based advertisements such as institutional promotions and corporate product advertisements.  \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Nondestructive evaluation methods play an important role in ensuring component integrity and safety in many industries. Operator fatigue can play a critical role in the reliability of such methods. This is important for inspecting high value assets or assets with a high consequence of failure, such as aerospace and nuclear components. Recent advances in convolution neural networks can support and automate these inspection efforts. This paper proposes using residual neural networks (ResNets) for real-time detection of corrosion, including iron oxide discoloration, pitting and stress corrosion cracking, in dry storage stainless steel canisters housing used nuclear fuel. The proposed approach crops nuclear canister images into smaller tiles, trains a ResNet on these tiles, and classifies images as corroded or intact using the per-image count of tiles predicted as corroded by the ResNet. The results demonstrate that such a deep learning approach allows to detect the locus of corrosion via smaller tiles, and at the same time to infer with high accuracy whether an image comes from a corroded canister. Thereby, the proposed approach holds promise to automate and speed up nuclear fuel canister inspections, to minimize inspection costs, and to partially replace human-conducted onsite inspections, thus reducing radiation doses to personnel.  \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In this study, deep neural networks were applied to a similar object classification method, and the classification performance was analyzed. For the similar object classification performance analysis, ResNet50 and DenseNet169 models, which are known to show similar behaviors, were selected. To verify the performance of these deep neural networks, a bolt recognition for smart factories and an armored fighting vehicle recognition were performed. In addition, image preprocessing methods to improve the similar object classification performance were proposed. The experimental results confirmed that appropriate image preprocessing methods should be applied according to the type of similar object to be classified.  \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We developed a model to classify the absence of cervical cancer using deep learning from the cervical image to which the histogram equalization algorithm was applied, and to compare the performance of each model. A total of 4259 images were used for this study, of which 1852 images were normal and 2407 were abnormal. And this paper applied Image Sharpening(IS), Histogram Equalization(HE), and Contrast Limited Adaptive Histogram Equalization(CLAHE) to the original image. Peak Signal-to-Noise Ratio(PSNR) and Structural Similarity index for Measuring image quality(SSIM) were used to assess the quality of images objectively. As a result of assessment, IS showed 81.75dB of PSNR and 0.96 of SSIM, showing the best image quality. CLAHE and HE showed the PSNR of 62.67dB and 62.60dB respectively, while SSIM of CLAHE was shown as 0.86, which is closer to 1 than HE of 0.75. Using ResNet-50 model with transfer learning, digital image-processed images are classified into normal and abnormal each. In conclusion, the classification accuracy of each model is as follows. 90.77% for IS, which shows the highest, 90.26% for CLAHE and 87.60% for HE. As this study shows, applying proper digital image processing which is for cervical images to Computer Aided Diagnosis(CAD) can help both screening and diagnosing.  \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Recently, research on image recognition and object extraction has been actively carried out through deep learning. In addition, research on transfer learning, which is used for new neural network learning by maintaining or partially changing the neural network function of a deep learning model pre-trained in a specific field, is also being actively conducted. However, although a large number of datasets are required for image learning, it is difficult to obtain desired quantities of images. In this paper, a large number of images were collected through Google and bird-related professional website crawling for bird breed classification, and images that can be used for learning were selected through various preprocessing. Then, using the ResNet50 model pre-trained with ImageNet, fine-tuning transfer learning was performed to classify bird breeds. In addition, the trained model was tested using the CUB_200-2011 data set, which is a data set for classifying new breeds, and the reliability of the search image was obtained with an accuracy of 87.87%.  \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We compare three approaches of structure reduction, pruning, and knowledge distillation for lightning of a deep learning network. Structure reduction eliminates a set of layers of the model, but pruning deletes filters within a layer. Knowledge distillation effectively learns a small student model from a large teacher model using KL Divergence. Therefore, it has a similar effect of reduction of the model. The above three methods for lightning are rarely compared to each other in terms of performance. To compare these approaches for network reduction problem, we investigate the accuracy and flops of the methods on CIFAR10 and CIFAR100 data for ResNet models. A systematic analysis for the fundamental orientations and differences of each method is supplemented.  \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In this study, a recommendation algorithm was implemented by learning a deep learning-based classification model for consumer data. For this purpose, a meaningful result is presented as a result of learning using ResNet50, which is commonly used in classification tasks by converting user data into images.  \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Due to the increase of farm waste in many countries, there’s a need to develop a smart farm waste monitoring system that can collect it promptly and efficiently. In this paper, we proposed, compared the performance of a convolutional neural network (CNN) -based transfer learning models and implement a farm waste image classification system, which is crucial component for the monitoring system. To find an appropriate model and labelling methods for farm waste image classification, we compared each validation accuracy of six different pre-trained CNN methods with three types of labelling scheme, using the waste images taken directly from the farming area. As a result, the ResNet-50 model performed best with an accuracy of 90.9% on average. Also, when classified into 2 categories, the accuracy was about 10% higher than that of the 6 categories. Furthermore, when the image was classified into 2 main categories with 6 sub-categories, the validation accuracy was similar to that of the 2 categories. Through these results, it seemed to be more effective to classify with binary labels such as ‘trash’ and ‘non-trash’, rather than with multiple labels of specific categories because farm waste is generated not only by single type of waste but also by various types of mixed waste. And a TCP / IP based communication environment between farm environment monitoring server and farm waste image classification server has been implemented. Experimental results using the system implemented for a smart farm waste monitoring showed that the proposed system can be used for a smart farm waste collection system. Also, the result of this study could be applied to classify medical images of unstructured and/or mixed lesion.  \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Objectives: Different complex strategies of fusing handcrafted descriptors and features from convolutional neural network(CNN) models have been studied, mainly for two-class Papanicolaou (Pap) smear image classification. This paper explores asimplified system using combined binary coding for a five-class version of this problem. Methods: This system extracted featuresfrom transfer learning of AlexNet, VGG19, and ResNet50 networks before reducing this problem into multiple binarysub-problems using error-correcting coding. The learners were trained using the support vector machine (SVM) method. The outputs of these classifiers were combined and compared to the true class codes for the final prediction. Results: Despitethe superior performance of VGG19-SVM, with mean ± standard deviation accuracy and sensitivity of 80.68% ± 2.00% and80.86% ± 0.45%, respectively, this model required a long training time. There were also false-negative cases using both theVGGNet-SVM and ResNet-SVM models. AlexNet-SVM was more efficient in terms of running speed and prediction consistency. Our findings also showed good diagnostic ability, with an area under the curve of approximately 0.95. Further investigationalso showed good agreement between our research outcomes and that of the state-of-the-art methods, with specificityranging from 93% to 100%. Conclusions: We believe that the AlexNet-SVM model can be conveniently applied for clinicaluse. Further research could include the implementation of an optimization algorithm for hyperparameter tuning, as well asan appropriate selection of experimental design to improve the efficiency of Pap smear image classification.  \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        A meatadata has become an essential element in order to recommend video content to users. However, it is passively generated by video content providers. In the paper, a method for automatically generating metadata was studied in the existing manual metadata input method. In addition to the method of extracting emotion tags in the previous study, a study was conducted on a method for automatically generating metadata for genre and country of production through movie audio. The genre was extracted from the audio spectrogram using the ResNet34 artificial neural network model, a transfer learning model, and the language of the speaker in the movie was detected through speech recognition. Through this, it was possible to confirm the possibility of automatically generating metadata through artificial intelligence.  \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Engineering drawing inspection is important to CAD modeling of mechanical parts. Traditional inspection methods mainly rely on manual analysis by using the CAD software, which requires expert knowledge and massive time. In view of simplifying the analysis for non-experts and improving detection efficiency and accuracy, this study proposes a generic approach combining object detection and image recognition methods to identify surface roughness of mechanical drawings. For both the object detection and image recognition methods, deep learning models with different backbone networks are trained and tested independently.Experimental results show that a combination of Faster-RCNN with ResNet101 as backbone network, and SSD with ResNet50 as backbone network achieves the best performance under our evaluation metrics.  \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The Convolution Neural Network accelerator has emerged as an important element in the AI era. The primary components of a CNN accelerator include the Multiplier-Accumulator (MAC unit) for calculation, SRAM for data storage, memory interface for data movement, and control logic. Different CNN accelerators have been designed based on different assumptions regarding the process technologies and operating frequencies. In addition, the number of internal MAC units and the size of SRAM vary substantially between different types of architectures. These factors make it difficult to design a fair comparison of the area, energy, and performance of different CNN accelerators. In this paper, we attempt to compare the area, energy, and performance of different CNN accelerator architectures by constructing them all with the same fabrication process and operating frequency while making inferences using the ResNet-50 network.  \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Recently, various lightweight methods for using Convolutional Neural Network(CNN) models in mobile devices have emerged. Weight quantization, which lowers bit precision of weights, is a lightweight method that enables a model to be used through integer calculation in a mobile environment where GPU acceleration is unable. Weight quantization has already been used in various models as a lightweight method to reduce computational complexity and model size with a small loss of accuracy. Considering the size of memory and computing speed as well as the storage size of the device and the limited network environment, this paper proposes a method of compressing integer weights after quantization using a video codec as a method. To verify the performance of the proposed method, experiments were conducted on VGG16, Resnet50, and Resnet18 models trained with ImageNet and Places365 datasets. As a result, loss of accuracy less than 2% and high compression efficiency were achieved in various models. In addition, as a result of comparison with similar compression methods, it was verified that the compression efficiency was more than doubled.  \n",
      "30  The biggest reason for using a deep learning model in image classification is that it is possible to consider the relationship between each region by extracting each regions features from the overall information of the image. However, the CNN model may not be suitable for emotional image data without the images regional features. To solve the difficulty of classifying emotion images, many researchers each year propose a CNN-based architecture suitable for emotion images. Studies on the relationship between color and human emotion were also conducted, and results were derived that different emotions are induced according to color. In studies using deep learning, there have been studies that apply color information to image subtraction classification. The case where the images color information is additionally used than the case where the classification model is trained with only the image improves the accuracy of classifying image emotions.  This study proposes two ways to increase the accuracy by incorporating the result value after the model classifies an images emotion. Both methods improve accuracy by modifying the result value based on statistics using the color of the picture. When performing the test by finding the two-color combinations most distributed for all training data, the two-color combinations most distributed for each test data image were found. The result values were corrected according to the color combination distribution. This method weights the result value obtained after the model classifies an images emotion by creating an expression based on the log function and the exponential function.  Emotion6, classified into six emotions, and Artphoto classified into eight categories were used for the image data. Densenet169, Mnasnet, Resnet101, Resnet152, and Vgg19 architectures were used for the CNN model, and the performance evaluation was compared before and after applying the two-stage learning to the CNN model.  Inspired by color psychology, which deals with the relationship between colors and emotions, when creating a model that classifies an images sentiment, we studied how to improve accuracy by modifying the result values based on color. Sixteen colors were used: red, orange, yellow, green, blue, indigo, purple, turquoise, pink, magenta, brown, gray, silver, gold, white, and black. It has meaning. Using Scikit-learns Clustering, the seven colors that are primarily distributed in the image are checked. Then, the RGB coordinate values of the colors from the image are compared with the RGB coordinate values of the 16 colors presented in the above data. That is, it was converted to the closest color. Suppose three or more color combinations are selected. In that case, too many color combinations occur, resulting in a problem in which the distribution is scattered, so a situation fewer influences the result value. Therefore, to solve this problem, two-color combinations were found and weighted to the model. Before training, the most distributed color combinations were found for all training data images. The distribution of color combinations for each class was stored in a Python dictionary format to be used during testing. During the test, the two-color combinations that are most distributed for each test data image are found. After that, we checked how the color combinations were distributed in the training data and corrected the result. We devised several equations to weight the result value from the model based on the extracted color as described above.  The data set was randomly divided by 80:20, and the model was verified using 20% of the data as a test set. After splitting the remaining 80% of the data into five divisions to perform 5-fold cross-validation, the model was trained five times using different verification datasets. Finally, the performance was checked using the test dataset that was previously separated. Adam was used as the activation function, and the learning rate  \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           In this paper, we propose a new hand gesture classification strategy using early fusion based multimodal deep learning. The structure and parameters of the state-of-the-art deep learning models such as ResNet152, DenseNet201, EfficientNetB0 for the source task of image classification are reused in the target task of hand gesture classification using surface electromyograph(EMG) and finger\"s kinematic data. The time-domain EMG and kinematic signals are normalized and then transformed into combined 2-D images for the early-fusion network. The experimental results support the superiority of the proposed method in terms of classification accuracy. The transfer learning model with the EfficientNetB0 shows the 93.94% accuracy for 40 gestures of 40 participants in the Ninapro DB2.  \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In this paper, we propose a new hand gesture recognition strategy using network-based transfer learning(TL) and reference voluntary contraction(RVC) normalization. The structure and parameters of the state-of-the-art deep learning models such as VGG19, ResNet152 and DenseNet121 for source task of image classification are reused in the target task of hand gesture recognition based on surface electromyography(EMG) signals. To mitigate the difficulty in handling the subject-dependent EMG signals, the RVC normalization is adopted in the signal pre-processing. The time-domain EMG signals are transformed into 2-D images for TL networks. The experimental results verify the validity of the proposed method in terms of recognition accuracy. The TL using VGG19, RVC normalization and gray image transformation shows 99.78% accuracy for the data from 15 participants performing 20 different gestures.  \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       This paper proposes an approach to the real-time implementation of a convolutional neural network (CNN)-based object detector for a former Unmanned Surface Vehicle (USV). The original network VGG-16 of the Single Shot MultiBox Detector (SSD) is first replaced with ResNet-18, as the basic feature extraction network. The classifying network is then redesigned by reducing half of the convolutional kernel numbers, where kernel sizes of 1×1 and 3×3 are mainly used. Simultaneously, a monocular camera installed on the tracking system, is used to calculate the distance and azimuth of the former USV. The experimental results show that the proposed method has advantages of higher accuracy and lower computational complexity, compared with other existing approaches. Therefore, the proposed approach can be efficiently used on real-time tracking systems.  \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Under natural conditions, Apple detection has the problems of occlusion and small object detection difficulties. This paper proposes an improved model based on SSD. The SSD backbone network VGG16 is replaced with the ResNet50 network model, and the receptive field structure RFB structure is introduced. The RFB model amplifies the feature information of small objects and improves the detection accuracy of small objects. Combined with the attention mechanism (SE) to filter out the information that needs to be retained, the semantic information of the detection objectis enhanced. An improved SSD algorithm is trained on the VOC2007 data set. Compared with SSD, the improved algorithm has increased the accuracy of occlusion and small object detection by 3.4% and 3.9%. The algorithm has improved the false detection rate and missed detection rate. The improved algorithm proposed in this paper has higher efficiency.  \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Detecting brain tumors of different sizes is a challenging task. This study aimed to identify brain tumors using detection algorithms. Most studies in this area use segmentation; however, we utilized detection owing to its advantages. Data were obtained from 64 patients and 11,200 MR images. The deep learning model used was RetinaNet, which is based on ResNet152. The model learned three different types of pre-processing images: normal, general histogram equalization, and contrast-limited adaptive histogram equalization (CLAHE). The three types of images were compared to determine the pre-processing technique that exhibits the best performance in the deep learning algorithms. During pre-processing, we converted the MR images from DICOM to JPG format. Additionally, we regulated the window level and width. The model compared the pre-processed images to determine which images showed adequate performance; CLAHE showed the best performance, with a sensitivity of 81.79%. The RetinaNet model for detecting brain tumors through deep learning algorithms demonstrated satisfactory performance in finding lesions. In future, we plan to develop a new model for improving the detection performance using well-processed data. This study lays the groundwork for future detection technologies that can help doctors find lesions more easily in clinical tasks.  \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In the article, the authors study the possibility of detecting some fungal diseases of rice using visual computing and machine learning techniques. Leaf blast and brown spot diseases are considered. Modern computer vision methods based on convolutional neural networks are used to identify a particular disease on an image. The authors compare the four most successful and compact convolutional neural network architectures: GoogleNet, ResNet-18, SqueezeNet-1.0, and DenseNet-121. The authors show that in the dataset used for the analysis, the disease can be detected with an accuracy of at least 95%. Testing the algorithm on real data not used in training showed an accuracy of up to 95.6%. This is a good indicator of the reliability and stability of the obtained solution even to a change in the data distribution. Data not used in training showed an accuracy of up to 95.6%. This is a good indicator of the reliability and stability of the obtained solution even to a change in the data distribution.  \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The lontar manuscript is an ancient Balinese cultural heritage written using Balinese characters on palm leaves. The recognition of Balinese characters in lontar is challenging because it has noise and limited data availability. To solve these problems, data augmentation is needed to increase the variety and amount of data to improve recognition performance. In this study, we collected Balinese character images from 50 lontar manuscript writers. We proposed MAT-AGCA that combines Adaptive Gaussian Thresholding and Convolutional Autoencoder for data augmentation. Based on experiments using InceptionResnetV2, DenseNet169, ResNet152V2, VGG19, and MobileNetV2, our proposed method achieved the best performance with 96.29% accuracy.  \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Background: Ultrasonography is an effective noninvasive imaging modality for the diagnosis of subcutaneous masses. To date, few studies have reported skin ultrasonography using deep convolutional neural networks (DCNNs). We investigated the accuracy of DCNNs for the diagnosis of epidermal cysts, lipomas, and other subcutaneous masses.Objective: The purpose of this study was to evaluate whether DCNNs could diagnose subcutaneous masses with ultrasonographic images at level of competence comparable to dermatologists.Methods: We created a dataset of 1,361 skin ultrasonography images obtained from 202 patients diagnosed with epidermal cysts, lipomas, and other subcutaneous masses, to train the DCNNs using ResNet18. Performance was compared with another set of 93 ultrasonographic images (24 epidermal cysts, 25 lipomas, and 44 other subcutaneous masses) from open-access articles.Results: The DCNNs yielded 87.10% classification accuracy and 86.10% F1-scores. The area under the curve, sensitivity, and specificity were 0.92 (95% confidence interval [CI] 0.86∼0.98), 75.00%, and 98.55% for epidermal cysts; 0.93 (95% CI 0.88∼0.98), 80.00%, and 94.12% for lipomas; and 0.97 (95% CI 0.93∼1.00), 97.73%, and 85.71% for other subcutaneous masses, respectively. Analysis using gradient-weighted class activation mapping revealed that the DCNNs could detect specific ultrasonographic findings of epidermal cysts and lipomas.Conclusion: We propose that DCNNs combined with ultrasonography may aid in the diagnosis of subcutaneous masses in outpatient settings. (Korean J Dermatol 2021;59(7):513∼520)  \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In the field of automatic target recognition(ATR) with synthetic aperture radar(SAR) imagery, it is usually impractical to obtain SAR target images covering a full range of aspect views. When the database consists of SAR target images with limited angular diversity, it can lead to performance degradation of the SAR-ATR system. To address this problem, this paper proposes a deep learning-based method where channel attention modules(CAMs) are inserted to a convolutional neural network(CNN). Motivated by the idea of the squeeze-and-excitation(SE) network, the CAM is considered to help improve recognition performance by selectively emphasizing discriminative features and suppressing ones with less information. After testing various CAM types included in the ResNet18-type base network, the SE CAM and its modified forms are applied to SAR target recognition using MSTAR dataset with different reduction ratios in order to validate recognition performance improvement under the limited angular diversity condition.  \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None  \n",
      "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The purpose of this study is to classify TIFF images, PNG images, and JPEG images using deep learning, and to compare the accuracy by verifying the classification performance. The TIFF, PNG, and JPEG images converted from chest X-ray DICOM images were applied to five deep neural network models performed in image recognition and classification to compare classification performance. The data consisted of a total of 4,000 X-ray images, which were converted from DICOM images into 16-bit TIFF images and 8-bit PNG and JPEG images. The learning models are CNN models - VGG16, ResNet50, InceptionV3, DenseNet121, and EfficientNetB0. The accuracy of the five convolutional neural network models of TIFF images is 99.86%, 99.86%, 99.99%, 100%, and 99.89%. The accuracy of PNG images is 99.88%, 100%, 99.97%, 99.87%, and 100%. The accuracy of JPEG images is 100%, 100%, 99.96%, 99.89%, and 100%. Validation of classification performance using test data showed 100% in accuracy, precision, recall and F1 score. Our classification results show that when DICOM images are converted to TIFF, PNG, and JPEG images and learned through preprocessing, the learning works well in all formats. In medical imaging research using deep learning, the classification performance is not affected by converting DICOM images into any format.  \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This paper proposes a novel image classification method based on few-shot learning, which is mainly used to solve model overfitting and non-convergence in image classification tasks of small datasets and improve the accuracy of classification. This method uses model structure optimization to extend the basic convolutional neural network (CNN) model and extracts more image features by adding convolutional layers, thereby improving the classification accuracy. We incorporated certain measures to improve the performance of the model. First, we used general methods such as setting a lower learning rate and shuffling to promote the rapid convergence of the model. Second, we used the data expansion technology to preprocess small datasets to increase the number of training data sets and suppress over-fitting. We applied the model to 10 monkey species and achieved outstanding performances. Experiments indicated that our proposed method achieved an accuracy of 87.92%, which is 26.1% higher than that of the traditional CNN method and 1.1% higher than that of the deep convolutional neural network ResNet50.  \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Steel surface defect should be detected and repaired in steel industry. Therefore, automatic detection of the steel surface defects plays a vital role in the steel manufacturing process. For the defect detection, machine learning based classification methods have been widely used such as HAAR feature-based cascade classifiers and support vector machines (SVM). As deep learning methods have been popular, the neural network based surface defect detection has been recently introduced. As for the methods, many researchers, in general, adopt a trained neural network, which is mainly winner in the recent ILSVRC (ImageNet Large Scale Visual Recognition Challenge). Then, the weights and last layers are modified to be used for surface defect detection (SDD), which is called transfer learning. In the previous researches, ResNet152 (winner in ILSVRC 2015) was used and the resulting performances were F1=0.975 and F1=0.912 in two different studies, respectively. However, the neural network used in their research has very wide and deep. Therefore, huge memories to save the trained weights and many multiplier–accumulators (MAC) are necessary, which means expensive hardware systems are essential to predict surface defect on the steel surface. This paper suggests a small neural network dedicated to surface defect detection. The proposed network has only three convolution layers and two fully connected layers. From the experimental results, we obtained F1=0.931 and minimum AUC (area under the curve)=0.995.  \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Wood identification is regularly performed by observing the wood anatomy, such as colour, texture, fibre direction, and other characteristics. The manual process, however, could be time consuming, especially when identification work is required at high quantity. Considering this condition, a convolutional neural networks (CNN)-based program is applied to improve the image classification results. The research focuses on the algorithm accuracy and efficiency in dealing with the dataset limitations. For this, it is proposed to do the sample selection process or only take a small portion of the existing image. Still, it can be expected to represent the overall picture to maintain and improve the generalisation capabilities of the CNN method in the classification stages. The experiments yielded an incredible F1 score average up to 93.4% for medium sample area sizes (200 × 200 pixels) on each CNN architecture (VGG16, ResNet50, MobileNet, DenseNet121, and Xception based). Whereas DenseNet121-based architecture was found to be the best architecture in maintaining the generalisation of its model for each sample area size (100, 200, and 300 pixels). The experimental results showed that the proposed algorithm can be an accurate and reliable solution.  \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Deep learning in computer vision has made accelerated improvement over a short period but large-scale learning data and computing power are still essential that required time-consuming trial and error tasks are involved to derive an optimal network model. In this study, we propose a similar image classification performance improvement method based on CR (Confusion Rate) that considers only the characteristics of the data itself regardless of network optimization or data reinforcement. The proposed method is a technique that improves the performance of the deep learning model by calculating the CRs for images in a dataset with similar characteristics and reflecting it in the weight of the Loss Function. Also, the CR-based recognition method is advantageous for image identification with high similarity because it enables image recognition in consideration of similarity between classes. As a result of applying the proposed method to the Resnet18 model, it showed a performance improvement of 0.22% in HanDB and 3.38% in Animal-10N. The proposed method is expected to be the basis for artificial intelligence research using noisy labeled data accompanying large-scale learning data.  \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None  \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Breast ultrasound has been widely utilized for classifying tumors into benignancy and malignancy. The limitations of traditional breast ultrasound are the handcrafted features obtained by well-trained sonographers and subjective decision according to different individual experiences. Recently, CNN-based deep learning techniques have exhibited better performance in medical images. However, most research for deep learning in medical ultrasound adopts CNN models developed for natural images due to the lack of common standard and dataset. In this paper, we compare six DCNN models which exhibit good performance for natural images - VGGNet, ResNet, InceptionNet, DenseNet, and EfficientNet. Our classification results demonstrate that CNN models of relatively lower performance on natural images show better performance on gray-scale ultrasound images and further study of CNN models are needed focusing on the features of medical images.  \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Conformal coating is a technology that protects PCB(Printed Circuit Board) and minimizes PCB failures. Since the defects in the coating are linked to failure of the PCB, the coating surface is examined for air bubbles to satisfy the successful conditions of the conformal coating. In this paper, we propose an algorithm for detecting problematic bubbles in high-risk groups by applying image signal processing. The algorithm consists of finding candidates for problematic bubbles and verifying candidates. Bubbles do not appear in visible light images, but can be visually distinguished from UV(Ultra Violet) light sources. In particular the center of the problematic bubble is dark in brightness and the border is high in brightness. In the paper, these brightness characteristics are called valley and mountain features, and the areas where both characteristics appear at the same time are candidates for problematic bubbles. However, it is necessary to verify candidates because there may be candidates who are not bubbles. In the candidate verification phase, we used convolutional neural network models, and ResNet performed best compared to other models. The algorithms presented in this paper showed the performance of precision 0.805, recall 0.763, and f1-score 0.767, and these results show sufficient potential for bubble test automation.  \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None  \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The purpose of this study is to improve the performance of the artificial neural network system for facial image analysis through the image landmark selection technique. For landmark selection, a CNN-based multi-layer ResNet model for classification of facial image age is required. From the configured ResNet model, a heat map that detects the change of the output node according to the change of the input node is extracted. By combining a plurality of extracted heat maps, facial landmarks related to age classification prediction are created. The importance of each pixel location can be analyzed through facial landmarks. In addition, by removing the pixels with low weights, a significant amount of input data can be reduced.  \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Recently, due to COVID-19, studies have been popularly worked to apply neural network to mask wearing automatic detection system. For applying neural networks, the 1-stage detection or 2-stage detection methods are used, and if data are not sufficiently collected, the pretrained neural network models are studied by applying fine-tuning techniques. In this paper, the system is consisted of 2-stage detection method that contain MTCNN model for face recognition and ResNet model for mask detection. The mask detector was experimented by applying five ResNet models to improve accuracy and fps in various environments. Training data used 17,217 images that collected using web crawler, and for inference, we used 1,913 images and two one-minute videos respectively. The experiment showed a high accuracy of 96.39% for images and 92.98% for video, and the speed of inference for video was 10.78fps.  \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Tomato crops are highly affected by tomato diseases, and if not prevented, a disease can cause severe losses for the agricultural economy. Therefore, there is a need for a system that quickly and accurately diagnoses various tomato diseases. In this paper, we propose a system that classifies nine diseases as well as healthy tomato plants by applying various pretrained deep learning-based CNN models trained on an ImageNet dataset. The tomato leaf image dataset obtained from PlantVillage is provided as input to ResNet, Xception, and DenseNet, which have deep learning-based CNN architectures. The proposed models were constructed by adding a top-level classifier to the basic CNN model, and they were trained by applying a 5-fold cross-validation strategy. All three of the proposed models were trained in two stages: transfer learning (which freezes the layers of the basic CNN model and then trains only the top-level classifiers), and fine-tuned learning (which sets the learning rate to a very small number and trains after unfreezing basic CNN layers). SGD, RMSprop, and Adam were applied as optimization algorithms. The experimental results show that the DenseNet CNN model to which the RMSprop algorithm was applied output the best results, with 98.63% accuracy.  \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The damage caused by the COVID-19 pandemic has a serious impact not only on public health but also on politics, economy, society, and culture as a whole. To date, the RT-PCR test, a COVID-19 standard diagnostic test, may vary depending on the type of sample, sample collection method, and storage, and is also affected by the time of the test after infection with COVID-19. This paper attempts to classify COVID-19 patients with X-ray/CT images using transfer learning-based ensemble deep learning. The transfer learning used here is the AlexNet, ResNet, Inception V3, and DenseNet models based on the convolutional neural network (CNN). The stacking ensemble model proposed in this study takes place over three stages. In the first step, predicted results are obtained using several transfer learning models, in the second step, they are combined through a concatenate layer, and in the third step, a deep neural network (DNN) model is applied and finally classified. For the performance evaluation of the ensemble model proposed in this paper, three actual COVID-19 X-ray/CT image datasets were considered, and various performance evaluation indicators were compared and analyzed with the transfer learning model and the existing ensemble model. As a result of the performance experiment, the overall proposed ensemble model was superior to the transfer learning model and the existing ensemble model.  \n",
      "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In the previous vehicle license plate recognition(LPR) systems, image processing method is able to process very fast in real time, but there is a limitation that it is difficult to apply to various license plates. Deep learning enables the variety and accuracy of LPR to be good, but ir reauires high-performance graphic cards and very long time processing time. In order to get around the advantages and disadvantages of both approaches, this paper proposes a light-weight vehicle license plate recognition system that can be processed in real time even using an embedded board or a general office PC without graphic cards. The proposed system consists of the three steps [license plate detection]-[character segmentation]-[character recognition] in the same with the conventional license plate recognition system. For each step, SSD-MobileNet and ResNet networks were used as deep learning models. As an image processing technique, the CLNF algorithm was used to detect an edge and to propagate vertically and horizontally for finding an ROI(region of interest). In the experiments for testing of the proposed system using 4,389 images obtained at places such as underground parking lots and toll gates, the accuracy was 98.2%. As the layer became shallower, the accuracy difference according to the image processing combination was bigger.  \n",
      "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The success rate of the no-smoking campaign has been low, although everybody knows that cigarettes are harmful to the human health. The results of both regular health and cancer checks in the hospital are useful for strengthening the human intention for quitting the smoking, however, those methods are difficult to use in daily life because of the use of large-scaled particular devices such as PET(positron emission tomography). Thus, this study proposed a non-invasive method that detects the difference between smokers and non-smokers through deep-learning-based analysis. At first, observing parts were decided to the tongue surface. Then, a data set(total 1,000) was made through the experiment to measure the tongue surface(410 times magnification) with the participants of 10 smokers and 10 non-smokers. The 80% ratio of data set was used for the train, and the left 20% was for the prediction. As a result, it was found that the classification through EfficientNet with the compound scaling including three scaling methods of width scaling, depth scaling and resolution scaling was much better than other models including VGG, ResNet, and DenseNet with the only one scaling.  \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this paper, we classify ECG signal data for mobile devices using deep learning models. To classify abnormal heartbeats with high accuracy, three factors of the deep learning model are selected, and the classification accuracy is compared according to the changes in the conditions of the factors. We apply a CNN model that can self-extract features of ECG data and compare the performance of a total of 48 combinations by combining conditions of the depth of model, optimization method, and activation functions that compose the model. Deriving the combination of conditions with the highest accuracy, we obtained the highest classification accuracy of 97.88% when we applied 19 convolutional layers, an optimization method SGD, and an activation function Mish. In this experiment, we confirmed the suitability of feature extraction and abnormal beat detection of 1-channel ECG signals using CNN.  \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Research and development of smart vessels has progressed significantly in recent years, and ships have become high-value technology-intensive resources. These ships entail high production costs and long-life cycles. Thus, modernized technical design, professional training, and aggressive maintenance are important factors in the efficient management of ships. With the continuing digital revolution, the industrial shipbuilding applicability of augmented reality (AR) and virtual reality (VR) technologies as well as related 3D system modeling and processes has increased. However, resolving the differences between AR/VR and real-world models remains burdensome. This problem is particularly evident when mapping various texture characteristics to virtual objects. To mitigate the burden and improve the performance of such technologies, it is necessary to directly define various texture characteristics or to express them using expensive equipment. The use of deep-learning-based CycleGAN, however, has gained attention as a method of learning and automatically mapping real-object textures. Thus, we seek to use CycleGAN to improve the immersive capacities of AR/VR models and to reduce production costs for shipbuilding. However, when applying CycleGAN’s textures to pipe structures, the performance is insufficient for direct application to industrial piping networks. Therefore, this study investigates an improved CycleGAN algorithm that can be specifically applied to the shipbuilding industry by combining a modified object-recognition algorithm with a double normalization method. Thus, we demonstrate that basic knowledge on the production of AR industrial pipe models can be applied to virtual models through machine learning to deliver low-cost and high-quality textures. Our results provide an on-ramp for future CycleGAN studies related to the shipbuilding industry.  \n",
      "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None  \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Recently, artificial intelligence for diagnosis system of obstetric diseases have been actively studied. Artificial intelligence diagnostic assist systems, which support medical diagnosis benefits of efficiency and accuracy, may experience problems of poor learning accuracy and reliability when inappropriate images are the model's input data. For this reason, before learning, We proposed an algorithm to exclude unread cervical imaging. 2,000 images of read cervical imaging and 257 images of unread cervical imaging were used for this study. Experiments were conducted based on the statistical method Radiomics to extract feature values of the entire images for classification of unread images from the entire images and to obtain a range of read threshold values. The degree to which brightness, blur, and cervical regions were photographed adequately in the image was determined as classification indicators. We compared the classification performance by learning read cervical imaging classified by the algorithm proposed in this paper and unread cervical imaging for deep learning classification model. We evaluate the classification accuracy for unread Cervical imaging of the algorithm by comparing the performance. Images for the algorithm showed higher accuracy of 91.6% on average. It is expected that the algorithm proposed in this paper will improve reliability by effectively excluding unread cervical imaging and ultimately reducing errors in artificial intelligence diagnosis.  \n",
      "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The population aging around the world is becoming a social problem. As Korea also entered a super-aged society rapidly, a substantial change in social living space became necessary. Therefore, this study analyzes the current status of elderly affinity in Korea using cluster analysis and EDA, focusing on the WHO's concept of ‘aged-friendly city’ and eight area guidelines. In addition, GIS-based data visualization carries after comparing and analyzing related data, centering on Icheon and Anseong in Gyeonggi province. As a result of the analysis, compared to Icheon, which is grade B, the elderly affinity of Anseong was measured as grade D despite similar conditions such as population and area. Through the results of this study, it is expected that it will be possible to identify the relative Age-Friendliness the country in terms of policy, present effective welfare policy guidelines, and prioritize areas where maintenance is urgent in the limited government budget  \n",
      "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             When a person wears clothing, it is important to configure an outfit appropriate to the intended occasion. Therefore, T.P.O(Time, Place, Occasion) of the outfit is considered in various fashion recommendation systems based on artificial intelligence. However, there are few studies that directly infer the T.P.O from outfit images, as the nature of the problem causes multi-label and class imbalance problems, which makes model training challenging. Therefore, in this study, we propose a model that can infer the T.P.O of outfit images by employing a label-distribution-aware margin(LDAM) loss function. Datasets for the model training and evaluation were collected from fashion shopping malls. As a result of measuring performance, it was confirmed that the proposed model showed balanced performance in all T.P.O classes compared to baselines.  \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Advances in convolutional neural networks (CNNs) have driven the development of computer vision. Recent CNN architectures, such as those with skip residual connections (ResNets) or densely connected architectures (DenseNets), have facilitated backpropagation and improved the performance of feature extraction and classification. Detecting objects in underwater environments by analyzing sound navigation and ranging (sonar) signals is considered an important process that should be automated. Several previous approaches have addressed this challenge; however, there has been no in-depth study of CNN architectures that effectively analyze sonar grams. In this paper, we have presented the identification of tonal frequencies in lofargrams using recent CNN architectures. Our study includes 175 CNN models that are derived from five different CNN architectures and 35 different input patch sizes. The study results showed that the accuracy of the best model was as high as 96.2% for precision and 99.5% for recall, with an inference time of 0.184 s.  \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Cerebral hemorrhages require rapid diagnosis and intensive treatment. This study aimed to detect cerebral hemorrhages and their locations in images using a deep learning model applying explainable deep learning. Normal brain images with no hemorrhages and images with subarachnoid, intraventricular, subdural, epidural, and intraparenchymal hemorrhages according to computed tomography (CT) (n = 200) were analyzed. A ResNet deep learning model, including image processing, was utilized. The visual explanation from a heatmap was made at the hemorrhage location using a gradient-class activation map (Grad-CAM). To evaluate the performance of the deep learning system, the accuracy, sensitivity, and specificity were determined. A hemorrhage prediction system for images of normal brains and brains with subarachnoid, intraventricular, subdural, epidural, and intraparenchymal hemorrhages was built. The Grad-CAM representation indicated the location of the hemorrhages in these images. In the prediction results, accurate predictions of the hemorrhage areas were made and visualizations of the corresponding locations overlapped in the images within (− 4, 1) pixel difference. The evaluation of the system performance showed an accuracy of 0.81 with a sensitivity of 0.67 and specificity of 0.86. These results constitue a proof of concept for the use of explainable artificial intelligence (XAI) to detect cerebral hemorrhages and visualize their locations in medical images, which will allow rapid diagnosis and treatment.  \n",
      "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Cracks affect the robustness of infrastructures such as buildings, bridge, pavement, and pipelines. This paper presents an automatedcrack detection system which detect cracks in diverse surfaces. We first constructed the combined crack dataset, consists of multiplecrack datasets in diverse domains presented in prior studies. Then, state-of-the-art deep learning models in computer vision tasks includingVGG, ResNet, WideResNet, ResNeXt, DenseNet, and EfficientNet, were used to validate the performance of crack detection. We dividedthe combined dataset into train (80%) and test set (20%) to evaluate the employed models. DenseNet121 showed the highest accuracyat 96.20% with relatively low number of parameters compared to other models. Based on the validation procedures of the advanced deeplearning models in crack detection task, we shed light on the cost-effective automated crack detection system which can be appliedto different surfaces and structures with low computing resources.  \n",
      "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   With rapid advances in technology, the operating frequencies of digital systems have increased to several GHz bands. This has led to an increase in simultaneous switching noise(SSN). To reduce SSN, electromagnetic bandgap(EBG) structures have been intensively studied. One of the critical steps in the design of an EBG structure is to predict the stopband that reduces SSN. Existing methods include using a 3D electromagnetic field simulation program or equations based on the Floquet theory. However, these have limitations. In this study, we verified a new method for predicting the stopband using a convolutional neural network(CNN). Specifically, a CNN architectural model was used to compare structures that perform well in predicting the stopband. It was also used to confirm that the DenseNet showed high performance.  \n",
      "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Cavitation erosion is one of the major factors causing damage by lowering the structural strength of the marine propeller and the risk of it has been qualitatively evaluated by each institution with their own criteria based on the experiences. In this study, in order to quantitatively evaluate the risk of cavitation erosion on the propeller, we implement a deep learning algorithm based on a convolutional neural network. We train and verify it using the model tests results, including cavitation characteristics of various ship types. Here, we adopt the validated well-known networks such as VGG, GoogLeNet, and ResNet, and the results are compared with the expert’s qualitative prediction results to confirm the feasibility of the prediction algorithm using a convolutional neural network.  \n",
      "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Biometric technology using finger veins is receiving a lot of attention due to its high security, convenience and accuracy. And the recent development of deep learning technology has improved the processing speed and accuracy for authentication. However, the training data is a subset of real data not in a certain order or method and the results are not constant. so the amount of data and the complexity of the artificial neural network must be considered. In this paper, the deep learning model of Inception-Resnet-v2 was used to improve the high accuracy of the finger vein recognizer and the performance of the authentication system, We compared and analyzed the performance of the deep learning model of DenseNet-201. The simulations used data from MMCBNU_6000 of Jeonbuk National University and finger vein images taken directly. There is no preprocessing for the image in the finger vein authentication system, and the results are checked through EER.  \n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In the last few years, researches on autonomous ships have attracted attention. One of the essential techniques required for autonomous ships is the awareness of surroundings, including detection and classification of objects. Although researches on computer vision regarding the classification of ship images are still making progress, it is challenging to encounter a lack of enough database that was adequately labeled for ship classification. In this study, data obtained from Singapore Maritime Dataset (SMD) and public datasets such as MARVEL, FleetMon, and VesselFinder were labeled and integrated into a unified dataset for further study for ship classification. The ship image dataset was classified into seven classes, including bulk carrier, container ship, cruise ship, naval surface ship, tanker, tug boat, and buoy. Subsequently, Convolutional Neural Networks (CNNs) based on GoogleNet, VGG16, and ResNet were implemented for ship image classification, and a comparative test was done. As a result, the CNNs which were trained with the unified dataset showed high accuracy. The classification results were analyzed by the heatmap visualization with Grad-CAM, which indicates critical features best activating each class of ships, and further discussion was made.  \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The sensory stimulation of a cosmetic product has been deemed to be an ancillary aspect until a decade ago. That point of view has drastically changed on different levels in just a decade. Nowadays cosmetic formulators should unavoidably meet the needs of consumers who want sensory satisfaction, although they do not have much time for new product development. The selection of new products from candidate products largely depend on the panel of human sensory experts. As new product development cycle time decreases, the formulators wanted to find systematic tools that are required to filter candidate products into a short list. Traditional statistical analysis on most physical property tests for the products including tribology tests and rheology tests, do not give any sound foundation for filtering candidate products. In this paper, we suggest a deep learning-based analysis method to identify hand cream products by raw electric signals from tribological sliding test. We compare the result of the deep learning-based method using raw data as input with the results of several machine learning-based analysis methods using manually extracted features as input. Among them, ResNet that is a deep learning model proved to be the best method to identify hand cream used in the test. According to our search in the scientific reported papers, this is the first attempt for predicting test cosmetic product with only raw time-series friction data without any manual feature extraction. Automatic product identification capability without manually extracted features can be used to narrow down the list of the newly developed candidate products.  \n",
      "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Intracranial hemorrhage (ICH) refers to acute bleeding inside the intracranial vault. Not only does this devastating disease record a very high mortality rate, but it can also cause serious chronic impairment of sensory, motor, and cognitive functions. Therefore, a prompt and professional diagnosis of the disease is highly critical. Noninvasive brain imaging data are essential for clinicians to efficiently diagnose the locus of brain lesion, volume of bleeding, and subsequent cortical damage, and to take clinical interventions. In particular, computed tomography (CT) images are used most often for the diagnosis of ICH. In order to diagnose ICH through CT images, not only medical specialists with a sufficient number of diagnosis experiences are required, but even when this condition is met, there are many cases where bleeding cannot be successfully detected due to factors such as low signal ratio and artifacts of the image itself. In addition, discrepancies between interpretations or even misinterpretations might exist causing critical clinical consequences. To resolve these clinical problems, we developed a diagnostic model predicting intracranial bleeding and its subtypes (intraparenchymal, intraventricular, subarachnoid, subdural, and epidural) by applying deep learning algorithms to CT images. We also constructed a visualization tool highlighting important regions in a CT image for predicting ICH. Specifically, 1) 27,758 CT brain images from RSNA were pre-processed to minimize the computational load. 2) Three different CNN-based models (ResNet, EfficientNet-B2, and EfficientNet-B7) were trained based on a training image data set. 3) Diagnosis performance of each of the three models was evaluated based on an independent test image data set: As a result of the model comparison, EfficientNet-B7's performance (classification accuracy = 91%) was a way greater than the other models. 4) Finally, based on the result of EfficientNet-B7, we visualized the lesions of internal bleeding using the Grad-CAM. Our research suggests that artificial intelligence-based diagnostic systems can help diagnose and treat brain diseases resolving various problems in clinical situations.  \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Recently, as people's interest in facial skin beauty has increased, research on skin disease recognition for facial skin beauty is being conducted by using deep learning. These studies recognized a variety of skin diseases, including acne. Existing studies can recognize only the single skin diseases, but skin diseases that occur on the face can enact in a more diverse and complex manner. Therefore, in this paper, complex skin diseases such as acne, blackheads, freckles, age spots, normal skin, and whiteheads are identified using the Inception-ResNet V2 deep learning mode with multi-label classification. The accuracy was 98.8%, hamming loss was 0.003, and precision, recall, F1-Score achieved 96.6% or more for each single class.  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                                                                                                                       keyword  \\\n",
      "21                                                                                                               Deep Learning   \n",
      "10                                                                                                                      ResNet   \n",
      "79                                                                                                               Deep learning   \n",
      "22                                                                                                                         CNN   \n",
      "44                                                                                                                         딥러닝   \n",
      "120                                                                                                             Classification   \n",
      "39                                                                                                           Transfer Learning   \n",
      "38                                                                                                Convolutional Neural Network   \n",
      "109                                                                                              Convolutional neural networks   \n",
      "122                                                                                                              deep learning   \n",
      "32                                                                                                Convolutional neural network   \n",
      "87                                                                                                     Artificial intelligence   \n",
      "95                                                                                                                          AI   \n",
      "141                                                                                                       Image classification   \n",
      "70                                                                                                                  Smart Farm   \n",
      "71                                                                                                  Convolution Neural Network   \n",
      "118                                                                                                            Cervical cancer   \n",
      "89                                                                                                            Object detection   \n",
      "208                                                                                                                       딥 러닝   \n",
      "41                                                                                                       Semantic Segmentation   \n",
      "123                                                                                                          transfer learning   \n",
      "100                                                                                                                     VGG-16   \n",
      "279                                                                                                                      T.P.O   \n",
      "136                                                                                                      Recommendation System   \n",
      "223                                                                                                               EfficientNet   \n",
      "133                                                                                                                         분류   \n",
      "121                                                                                                                  ResNet-50   \n",
      "115                                                                                                                   DenseNet   \n",
      "12                                                                                                           Residual Learning   \n",
      "88                                                                                                            Machine learning   \n",
      "3                                                                                                                         전이학습   \n",
      "172                                                                                                                 Ninapro DB   \n",
      "93                                                                                                                   ResNet-34   \n",
      "170                                                                                                                        EMG   \n",
      "91                                                                                                                    COVID-19   \n",
      "106                                                                                                                   ResNet50   \n",
      "60                                                                                                             Computer vision   \n",
      "82                                                                                                                        인공지능   \n",
      "235                                                                                              Convolutional Neural Networks   \n",
      "226                                                                                                          Conformal Coating   \n",
      "241                                                                                                             X-ray/CT image   \n",
      "222                                                                                                               InceptionNet   \n",
      "230                                                                                                                Resnet 알고리즘   \n",
      "240                                                                                                          stacking ensemble   \n",
      "239                                                                                                                X-ray/CT 영상   \n",
      "224                                                                                                         Problematic Bubble   \n",
      "225                                                                                                           Bubble Detection   \n",
      "229                                                                                                                         열흔   \n",
      "232                                                                                                                  Arc beads   \n",
      "236                                                                                                                Fine Tuning   \n",
      "227                                                                                                                      전기 화재   \n",
      "233                                                                                                                Molten mark   \n",
      "238                                                                                                                    스태킹 앙상블   \n",
      "221                                                                                                                        VGG   \n",
      "228                                                                                                                        단락흔   \n",
      "237                                                                                              Plant Diseases Classification   \n",
      "234                                                                                                                     Resnet   \n",
      "231                                                                                                              Electric fire   \n",
      "214                                                                                                  MIT-BIH arrhythmia 데이터베이스   \n",
      "220                                                                                                                      Tumor   \n",
      "219                                                                                                              Breast Cancer   \n",
      "196                                                                                                                 Urinalysis   \n",
      "195                                                                                                            Ultrasonography   \n",
      "194                                                                                                                     Lipoma   \n",
      "193                                                                                                             Epidermal cyst   \n",
      "192                                                                                                  Convolutional Autoencoder   \n",
      "191                                                                                             Adaptive Gaussian Thresholding   \n",
      "190                                                                                                          Data augmentation   \n",
      "189                                                                                                          Lontar manuscript   \n",
      "188                                                                                                         Balinese character   \n",
      "187                                                                                                            Fungal diseases   \n",
      "186                                                                                                                       Rice   \n",
      "185                                                                                                     Histogram Equalization   \n",
      "184                                                                                                                  RetinaNet   \n",
      "183                                                                                                                Brain Tumor   \n",
      "182                                                                                                        Objection detection   \n",
      "181                                                                                                            Apple detection   \n",
      "180                                                                                                                        SSD   \n",
      "197                                                                                                       Image Discrimination   \n",
      "198                                                                                                                      X-ray   \n",
      "199                                                                                                         Feature extraction   \n",
      "210                                                                                                              컨볼루셔널 뉴럴 네트워크   \n",
      "218                                                                                                          Breast Ultrasound   \n",
      "217                                                                                                                       주입기법   \n",
      "216                                                                                                                   Adabound   \n",
      "215                                                                                                                    ResNeXt   \n",
      "213                                                                                                             Confusion Rate   \n",
      "212                                                                                                              Similar Image   \n",
      "211                                                                                                                        혼동률   \n",
      "209                                                                                                                     유사 이미지   \n",
      "200                                                                                                          Few-shot learning   \n",
      "207                                                                                               convolutional neural network   \n",
      "206                                                                                                             classification   \n",
      "205                                                                                                           sample selection   \n",
      "204                                                                                                          microscopic image   \n",
      "203                                                                                                                       wood   \n",
      "202                                                                                             Surface defect detection (SDD)   \n",
      "201                                                                                                       Metal surface defect   \n",
      "242                                                                                                                 자동차 번호판 인식   \n",
      "0                                                                                                                      변종 악성코드   \n",
      "243                                                                                                                    영상처리 결합   \n",
      "292                                                                                                            Computer Vision   \n",
      "299                                                                                           Convolutional Neural Network(CNN   \n",
      "298                                                                                                           Machine Learning   \n",
      "297                                                                                                   Electromagnetic Band Gap   \n",
      "296                                                                                               Simultaneous Switching Noise   \n",
      "295                                                                                                                     컴퓨터 비전   \n",
      "294                                                                                                                      균열 탐지   \n",
      "293                                                                                                                      표면 검사   \n",
      "291                                                                                                            Crack Detection   \n",
      "301                                                                                                         Deep learning(딥러닝)   \n",
      "290                                                                                                         Surface Inspection   \n",
      "289                             Cerebral hemorrhage prediction · Cerebrovascular disease · Explainable artificial intelligence   \n",
      "288                                                                                                    underwater recognition.   \n",
      "287                                                                                                             sonar analysis   \n",
      "286                                                                                                             lofar analysis   \n",
      "285                                                                                                    Class imbalance problem   \n",
      "284                                                                                                        Multi-label problem   \n",
      "300                                                                                                                   합성곱 신경망)   \n",
      "302                                                                                                            Propeller(프로펠러)   \n",
      "244                                                                                                                       임베디드   \n",
      "312                                                                                                                  Tribology   \n",
      "319                                                                                                                  다중 레이블 분류   \n",
      "318                                                                                                              Skin Diseases   \n",
      "317                                                                                                 Multi-Label Classification   \n",
      "316                                                                                                 Computed tomography images   \n",
      "315                                                                                                    Intracranial hemorrhage   \n",
      "314                                                                                                              Deep-learning   \n",
      "313                                                                                                                  Cosmetics   \n",
      "311                                                                                                 Time Series Classification   \n",
      "303                                                                                                          Cavitation(캐비테이션)   \n",
      "310                                                                                                            Autonomous Ship   \n",
      "309                                                                                                                    Heatmap   \n",
      "308                                                                                                           Object Detection   \n",
      "307                                                                                                       Image Classification   \n",
      "306                                                                                                     Finger vein recognizer   \n",
      "305                                                                                                   Biometric authentication   \n",
      "304                                                                                                                Erosion(침식)   \n",
      "283                                                                                                                    Fashion   \n",
      "282                                                                                                                Convergence   \n",
      "281                                                                                                                    클래스 불균형   \n",
      "252                                                                                                                         흡연   \n",
      "259                                                                                                       Arrhythmia Detection   \n",
      "258                                                                                                          Electrocardiogram   \n",
      "257                                                                                                                      smoke   \n",
      "256                                                                                                             tongue surface   \n",
      "255                                                                                                                 microscope   \n",
      "254                                                                                                         abnormal detection   \n",
      "253                                                                                                                        혓바닥   \n",
      "251                                                                                                                       이상검출   \n",
      "280                                                                                                                     다중 레이블   \n",
      "250                                                                                                                    마이크로스코프   \n",
      "178                                                                                                                        RFB   \n",
      "249                                                                                                                 Lightening   \n",
      "248                                                                                                                   Embedded   \n",
      "247                                                                                               Image processing combination   \n",
      "246                                                                                                  License plate recognition   \n",
      "245                                                                                                                        경량화   \n",
      "260                                                                                                                        심전도   \n",
      "261                                                                                                                     부정맥 검출   \n",
      "262                                                                                                          augmented reality   \n",
      "263                                                                                                                   CycleGAN   \n",
      "278                                                                                                                         패션   \n",
      "277                                                                                                                         융합   \n",
      "276                                                                                                                        GIS   \n",
      "275                                                                                                                         지표   \n",
      "274                                                                                                                      인구고령화   \n",
      "273                                                                                                                     고령친화도시   \n",
      "272                                                                                                         Euclidean distance   \n",
      "271                                                                                                         Laplacian variance   \n",
      "270                                                                                                                  Radiomics   \n",
      "269                                                                                                                        OCR   \n",
      "268                                                                                                             PCB Inspection   \n",
      "267                                                                                                                    Coreset   \n",
      "266                                                                                                                   3D model   \n",
      "265                                                                                                                    texture   \n",
      "264                                                                                                              normalization   \n",
      "179                                                                                                            Attention Model   \n",
      "160                                                                                 Deep Learning Model Parameter Quantization   \n",
      "177                                                                                                           Monocular camera   \n",
      "55                                                                                                                       동작 인식   \n",
      "62                                                                                                                     의미론적 분할   \n",
      "61                                                                                                                합성곱 신경망(CNN)   \n",
      "59                                                                                                          classification CNN   \n",
      "58                                                                                                                   Butterfly   \n",
      "57                                                                                                                ButterflyNet   \n",
      "56                                                                                             Indian butterfly identification   \n",
      "54                                                                                                                  운전자의 주의 분산   \n",
      "64                                                                                                                   영상분할 및 인식   \n",
      "53                                                                                                                     운전자의 동작   \n",
      "52                                                                                                                         CAM   \n",
      "51                                                                                                                  ResNet-101   \n",
      "50                                                                                                        Behavior Recognition   \n",
      "49                                                                                                        Driver’s Distraction   \n",
      "48                                                                                                           Driver’s Behavior   \n",
      "63                                                                                                                     자동차 번호판   \n",
      "65                                                                                             Convolution Neural Network(CNN)   \n",
      "46                                                                                                                     횡단보도 검출   \n",
      "76                                                                                                                       Wrist   \n",
      "84                                                                                                                       객체 감지   \n",
      "83                                                                                                                        기계학습   \n",
      "81                                                                                                                    computer   \n",
      "80                                                                                                             Neural networks   \n",
      "78                                                                                                                        bone   \n",
      "77                                                                                                                   Fractures   \n",
      "75                                                                                                                         SVM   \n",
      "66                                                                                                               License Plate   \n",
      "74                                                                                                                         EEG   \n",
      "73                                                                                                                         CWT   \n",
      "72                                                                                                                         BCI   \n",
      "69                                                                                                                         신경망   \n",
      "68                                                                                                                        스마트팜   \n",
      "67                                                                                          Image Segmentation and Recognition   \n",
      "47                                                                                                                     신경 네트워크   \n",
      "45                                                                                                                      시맨틱 분할   \n",
      "176                                                                                                            Tracking system   \n",
      "11                                                                                                         Residual Connection   \n",
      "18                                                                                                              Raman Spectrum   \n",
      "17                                                                                                                 Autoencoder   \n",
      "16                                                                                                                         PCA   \n",
      "15                                                                                                            Data Compression   \n",
      "14                                                                                                                 Degradation   \n",
      "13                                                                                                         Variational Scaling   \n",
      "9                                                                                                            Ensemble Learning   \n",
      "20                                                                                                          Region of Interest   \n",
      "8                                                                                                             Tranfer Learning   \n",
      "7                                                                                                      Variational AutoEncoder   \n",
      "6                                                                                                       Malware Classification   \n",
      "5                                                                                                              Variant Malware   \n",
      "4                                                                                                                       앙상블 학습   \n",
      "2                                                                                                                     변이 오토인코더   \n",
      "19                                                                                                              Peak Detection   \n",
      "23                                                                                                          Raman Spectroscopy   \n",
      "43                                                                                                              Neural Network   \n",
      "33                                                                                                        Weeds classification   \n",
      "42                                                                                                    Zebra-crossing Detection   \n",
      "40                                                                                                    Traffic Speed Prediction   \n",
      "37                                                                                                                    교통 속도 예측   \n",
      "36                                                                                                                       전이 학습   \n",
      "35                                                                                                                       잔차 학습   \n",
      "34                                                                                                                     합성곱 신경망   \n",
      "31                                                                                                           He initialization   \n",
      "24                                                                                                                     가중치 초기화   \n",
      "30                                                                                                       Glorot initialization   \n",
      "29                                                                                                      Weights initialization   \n",
      "28                                                                                                                       잡초 분류   \n",
      "27                                                                                                               컨볼루션 신경망 네트워크   \n",
      "26                                                                                                                      He 초기화   \n",
      "25                                                                                                                  Glorot 초기화   \n",
      "85                                                                                                                      마스크 감지   \n",
      "86                                                                                                                  코로나바이러스-19   \n",
      "90                                                                                                              Mask detection   \n",
      "149                                                                                                                   Metadata   \n",
      "155                                                                                                                    뉴럴 네트워크   \n",
      "154                                                                                                                    메모리 대역폭   \n",
      "153                                                                                                                       아키텍처   \n",
      "152                                                                                                                    CNN 가속기   \n",
      "151  · Mechanical drawings · Image analysis · Object detection · Image recognition · Deep learning models · Evaluation metrics   \n",
      "150                                                                                                          Voice Recognition   \n",
      "148                                                                                                                      Audio   \n",
      "157                                                                                                               architecture   \n",
      "147                                                                                                     Support Vector Network   \n",
      "146                                                                                                          Papanicolaou Test   \n",
      "145                                                                                                                  Nerve Net   \n",
      "144                                                                                                                  Diagnosis   \n",
      "143                                                                                                               Cervix Uteri   \n",
      "142                                                                                                      Farm waste collection   \n",
      "156                                                                                                           CNN accelerators   \n",
      "158                                                                                                           memory bandwidth   \n",
      "92                                                                                                       Histopathology images   \n",
      "167                                                                                                             Sense of Color   \n",
      "175                                                                                                                   Accuracy   \n",
      "174                                                                                                   Hand Gesture Recognition   \n",
      "173                                                                                            Reference Voluntary Contraction   \n",
      "171                                                                                                        Multimodal Learning   \n",
      "169                                                                                                Hand Gesture Classification   \n",
      "168                                                                                                         Two-stage learning   \n",
      "166                                                                                                Sentiment Analysis of Image   \n",
      "159                                                                                                            neural networks   \n",
      "165                                                                                                                    이-단계 학습   \n",
      "164                                                                                                                         색감   \n",
      "163                                                                                                                   이미지 감성분류   \n",
      "162                                                                                                          Lightweight model   \n",
      "161                                                                                                         Weight compression   \n",
      "1                                                                                                                      악성코드 분류   \n",
      "140                                                                                                          Transfer-learning   \n",
      "139                                                                                                                 Restaurant   \n",
      "138                                                                                                         Sentiment Analysis   \n",
      "103                                                                                                                       공연예술   \n",
      "111                                                                                                      Dry storage canisters   \n",
      "110                                                                                                                  Corrosion   \n",
      "108                                                                                                      Box Office Prediction   \n",
      "107                                                                                                            Performing Arts   \n",
      "105                                                                                                               Inception-v3   \n",
      "104                                                                                                                      흥행 예측   \n",
      "102                                                                                                         region of interest   \n",
      "137                                                                                                    Artificial Intelligence   \n",
      "101                                                                                                                       LIME   \n",
      "99                                                                                                              Textile design   \n",
      "98                                                                                                                    관심 영역 도출   \n",
      "97                                                                                                                 텍스타일 디자인 분류   \n",
      "96                                                                                                                         CAD   \n",
      "94                                                                                                           Digital Pathology   \n",
      "112                                                                                                          Feature detection   \n",
      "113                                                                                                   Residual neural networks   \n",
      "114                                                                                                       Deep Neural Networks   \n",
      "116                                                                                                              Smart Factory   \n",
      "117                                                                                                   Armored Fighting Vehicle   \n",
      "119                                                                                                     Histogram equalization   \n",
      "124                                                                                                     bird breed recognition   \n",
      "125                                                                                                               web crawling   \n",
      "126                                                                                                                   ImageNet   \n",
      "127                                                                                                        Structure Reduction   \n",
      "128                                                                                                                    Pruning   \n",
      "129                                                                                                     Knowledge Distillation   \n",
      "130                                                                                                                CIFAR10/100   \n",
      "131                                                                                                               ResNet56/110   \n",
      "132                                                                                                                     추천 시스템   \n",
      "134                                                                                                                      감성 분석   \n",
      "135                                                                                                                       외식업체   \n",
      "320                                                                                                                      피부 질환   \n",
      "\n",
      "     count  \n",
      "21      16  \n",
      "10      13  \n",
      "79      11  \n",
      "22      11  \n",
      "44      11  \n",
      "120      5  \n",
      "39       4  \n",
      "38       4  \n",
      "109      3  \n",
      "122      3  \n",
      "32       3  \n",
      "87       3  \n",
      "95       3  \n",
      "141      3  \n",
      "70       2  \n",
      "71       2  \n",
      "118      2  \n",
      "89       2  \n",
      "208      2  \n",
      "41       2  \n",
      "123      2  \n",
      "100      2  \n",
      "279      2  \n",
      "136      2  \n",
      "223      2  \n",
      "133      2  \n",
      "121      2  \n",
      "115      2  \n",
      "12       2  \n",
      "88       2  \n",
      "3        2  \n",
      "172      2  \n",
      "93       2  \n",
      "170      2  \n",
      "91       2  \n",
      "106      2  \n",
      "60       2  \n",
      "82       2  \n",
      "235      1  \n",
      "226      1  \n",
      "241      1  \n",
      "222      1  \n",
      "230      1  \n",
      "240      1  \n",
      "239      1  \n",
      "224      1  \n",
      "225      1  \n",
      "229      1  \n",
      "232      1  \n",
      "236      1  \n",
      "227      1  \n",
      "233      1  \n",
      "238      1  \n",
      "221      1  \n",
      "228      1  \n",
      "237      1  \n",
      "234      1  \n",
      "231      1  \n",
      "214      1  \n",
      "220      1  \n",
      "219      1  \n",
      "196      1  \n",
      "195      1  \n",
      "194      1  \n",
      "193      1  \n",
      "192      1  \n",
      "191      1  \n",
      "190      1  \n",
      "189      1  \n",
      "188      1  \n",
      "187      1  \n",
      "186      1  \n",
      "185      1  \n",
      "184      1  \n",
      "183      1  \n",
      "182      1  \n",
      "181      1  \n",
      "180      1  \n",
      "197      1  \n",
      "198      1  \n",
      "199      1  \n",
      "210      1  \n",
      "218      1  \n",
      "217      1  \n",
      "216      1  \n",
      "215      1  \n",
      "213      1  \n",
      "212      1  \n",
      "211      1  \n",
      "209      1  \n",
      "200      1  \n",
      "207      1  \n",
      "206      1  \n",
      "205      1  \n",
      "204      1  \n",
      "203      1  \n",
      "202      1  \n",
      "201      1  \n",
      "242      1  \n",
      "0        1  \n",
      "243      1  \n",
      "292      1  \n",
      "299      1  \n",
      "298      1  \n",
      "297      1  \n",
      "296      1  \n",
      "295      1  \n",
      "294      1  \n",
      "293      1  \n",
      "291      1  \n",
      "301      1  \n",
      "290      1  \n",
      "289      1  \n",
      "288      1  \n",
      "287      1  \n",
      "286      1  \n",
      "285      1  \n",
      "284      1  \n",
      "300      1  \n",
      "302      1  \n",
      "244      1  \n",
      "312      1  \n",
      "319      1  \n",
      "318      1  \n",
      "317      1  \n",
      "316      1  \n",
      "315      1  \n",
      "314      1  \n",
      "313      1  \n",
      "311      1  \n",
      "303      1  \n",
      "310      1  \n",
      "309      1  \n",
      "308      1  \n",
      "307      1  \n",
      "306      1  \n",
      "305      1  \n",
      "304      1  \n",
      "283      1  \n",
      "282      1  \n",
      "281      1  \n",
      "252      1  \n",
      "259      1  \n",
      "258      1  \n",
      "257      1  \n",
      "256      1  \n",
      "255      1  \n",
      "254      1  \n",
      "253      1  \n",
      "251      1  \n",
      "280      1  \n",
      "250      1  \n",
      "178      1  \n",
      "249      1  \n",
      "248      1  \n",
      "247      1  \n",
      "246      1  \n",
      "245      1  \n",
      "260      1  \n",
      "261      1  \n",
      "262      1  \n",
      "263      1  \n",
      "278      1  \n",
      "277      1  \n",
      "276      1  \n",
      "275      1  \n",
      "274      1  \n",
      "273      1  \n",
      "272      1  \n",
      "271      1  \n",
      "270      1  \n",
      "269      1  \n",
      "268      1  \n",
      "267      1  \n",
      "266      1  \n",
      "265      1  \n",
      "264      1  \n",
      "179      1  \n",
      "160      1  \n",
      "177      1  \n",
      "55       1  \n",
      "62       1  \n",
      "61       1  \n",
      "59       1  \n",
      "58       1  \n",
      "57       1  \n",
      "56       1  \n",
      "54       1  \n",
      "64       1  \n",
      "53       1  \n",
      "52       1  \n",
      "51       1  \n",
      "50       1  \n",
      "49       1  \n",
      "48       1  \n",
      "63       1  \n",
      "65       1  \n",
      "46       1  \n",
      "76       1  \n",
      "84       1  \n",
      "83       1  \n",
      "81       1  \n",
      "80       1  \n",
      "78       1  \n",
      "77       1  \n",
      "75       1  \n",
      "66       1  \n",
      "74       1  \n",
      "73       1  \n",
      "72       1  \n",
      "69       1  \n",
      "68       1  \n",
      "67       1  \n",
      "47       1  \n",
      "45       1  \n",
      "176      1  \n",
      "11       1  \n",
      "18       1  \n",
      "17       1  \n",
      "16       1  \n",
      "15       1  \n",
      "14       1  \n",
      "13       1  \n",
      "9        1  \n",
      "20       1  \n",
      "8        1  \n",
      "7        1  \n",
      "6        1  \n",
      "5        1  \n",
      "4        1  \n",
      "2        1  \n",
      "19       1  \n",
      "23       1  \n",
      "43       1  \n",
      "33       1  \n",
      "42       1  \n",
      "40       1  \n",
      "37       1  \n",
      "36       1  \n",
      "35       1  \n",
      "34       1  \n",
      "31       1  \n",
      "24       1  \n",
      "30       1  \n",
      "29       1  \n",
      "28       1  \n",
      "27       1  \n",
      "26       1  \n",
      "25       1  \n",
      "85       1  \n",
      "86       1  \n",
      "90       1  \n",
      "149      1  \n",
      "155      1  \n",
      "154      1  \n",
      "153      1  \n",
      "152      1  \n",
      "151      1  \n",
      "150      1  \n",
      "148      1  \n",
      "157      1  \n",
      "147      1  \n",
      "146      1  \n",
      "145      1  \n",
      "144      1  \n",
      "143      1  \n",
      "142      1  \n",
      "156      1  \n",
      "158      1  \n",
      "92       1  \n",
      "167      1  \n",
      "175      1  \n",
      "174      1  \n",
      "173      1  \n",
      "171      1  \n",
      "169      1  \n",
      "168      1  \n",
      "166      1  \n",
      "159      1  \n",
      "165      1  \n",
      "164      1  \n",
      "163      1  \n",
      "162      1  \n",
      "161      1  \n",
      "1        1  \n",
      "140      1  \n",
      "139      1  \n",
      "138      1  \n",
      "103      1  \n",
      "111      1  \n",
      "110      1  \n",
      "108      1  \n",
      "107      1  \n",
      "105      1  \n",
      "104      1  \n",
      "102      1  \n",
      "137      1  \n",
      "101      1  \n",
      "99       1  \n",
      "98       1  \n",
      "97       1  \n",
      "96       1  \n",
      "94       1  \n",
      "112      1  \n",
      "113      1  \n",
      "114      1  \n",
      "116      1  \n",
      "117      1  \n",
      "119      1  \n",
      "124      1  \n",
      "125      1  \n",
      "126      1  \n",
      "127      1  \n",
      "128      1  \n",
      "129      1  \n",
      "130      1  \n",
      "131      1  \n",
      "132      1  \n",
      "134      1  \n",
      "135      1  \n",
      "320      1  \n",
      "데이터가 저장되었습니다: resnet_2021_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2021_academic_riss.csv\n",
      "ResNet 기반 작물 생육단계 추정 모델 개발 2022 ['딥러닝', '컴퓨터 비전', 'Convolution Neural Network', '작물 생육단계', 'Deep Learing', 'Computer Vision', 'CNN', 'Crop Growth Stage'] 산업화 이후 가속화된 지구 온난화 현상으로 인해 기존환경 변화 및 이상기후 발생 빈도가 증가하고 있다. 농업은 기후변화에 매우 민감한 분야의 산업으로 지구 온난화는 작물의 생산량을 감소시키고 재배 지역이 변하는 등의 문제를 발생시킨다. 또한, 환경 변화는 작물의 생육 시기를 불규칙하게 만들어 숙련된 농사꾼들도 작물의 생육단계를 쉽게 추정할 수 없도록 만들어 여러 문제를 발생시킨다. 이에 본 논문에서는 작물의 생육단계를 추정하기 위한 CNN(Convolution Neural Network) 모델을 제안한다. 제안한 모델은 ResNet의 Pooling Layer를 수정한 모델로 ResNet, DenseNet 모델의 생육단계 추정보다 높은 성능 결과를 확인하였다. Due to the accelerated global warming phenomenon after industrialization, the frequency of changes in the existing environment and abnormal climate is increasing. Agriculture is an industry that is very sensitive to climate change, and global warming causes problems such as reducing crop yields and changing growing regions. In addition, environmental changes make the growth period of crops irregular, making it difficult for even experienced farmers to easily estimate the growth stage of crops, thereby causing various problems. Therefore, in this paper, we propose a CNN model for estimating the growth stage of crops. The proposed model was a model that modified the pooling layer of ResNet, and confirmed the accuracy of higher performance than the growth stage estimation of the ResNet and DenseNet models.\n",
      "스펙트럼 첨도를 이용한 ResNet 기반의 표적 분류 성능 분석 2022 ['Micro-Doppler', 'Characteristic vector', 'Spectral kurtosis', 'Target classification', 'ResNet34'] 미세 도플러 (micro-Doppler) 변조는 각 개체의 구분 및 각각의 움직임에 대한 미세한 운동 상태를 나타내는 표적 특징으로서, 표적을 인식하고 분류하는 기술에 활용되고 있다. 미세 도플러 주파수는 물체의 회전과 진동 등의 기본적인 운동 특징에 의한 도플러 주파수의 변조 형태로 나타나며, 이를 이용하면 높은 표적 인식 정확도로 표적을 추적하고 분류할 수 있다. 본 논문에서는 드론, 조류, 사람 표적에 따른 미세 운동 신호를 모델링하고, 미세 도플러 영상의 스펙트럼 첨도를 계산하여 표적의 미세 도플러 특징 벡터를 추출한다. 그리고 서로 다른 미세 운동을 하는 표적을 분류하기 위해 스펙트럼 첨도를 입력으로 하는 ResNet34 심층 신경망 네트워크를 적용한다. 모의실험을 통해 각 표적의 레이더 실측 데이터 입력 세트에 따른 ResNet34 알고리즘의 분류 성능을 분석한다. 모의실험 결과를 통해 제안하는 기법이 정확도, 정밀도, 재현도 측면에서 평균 95% 이상의 성능을 제시함으로써, 미세 도플러 영상을 이용하는 기존 기법보다 우수함을 보인다. None\n",
      "A ResNet based multiscale feature extraction for classifying multi-variate medical time series 2022 ['Multi-scale convolutional feature extraction methods', 'ResNet50 structure', 'Squeeze-and-Excitation Modules'] None We construct a deep neural network model named ECGResNet. This model can diagnosis diseases based on 12-lead ECG data of eight common cardiovascular diseases with a high accuracy. We chose the 16 Blocks of ResNet50 as the main body of the model and added the Squeeze-and-Excitation module to learn the data information between channels adaptively. We modified the first convolutional layer of ResNet50 which has a convolutional kernel of 7 to a superposition of convolutional kernels of 8 and 16 as our feature extraction method. This way allows the model to focus on the overall trend of the ECG signal while also noticing subtle changes. The model further improves the accuracy of cardiovascular and cerebrovascular disease classification by using a fully connected layer that integrates factors such as gender and age. The ECGResNet model adds Dropout layers to both the residual block and SE module of ResNet50, further avoiding the phenomenon of model overfitting. The model was eventually trained using a five-fold cross-validation and Flooding training method, with an accuracy of 95% on the test set and an F1-score of 0.841.We design a new deep neural network, innovate a multi-scale feature extraction method, and apply the SE module to extract features of ECG data.\n",
      "ResNet-Based Simulations for a Heat-Transfer Model Involving an Imperfect Contact 2022 ['Composite material', 'deep learning', 'heat transfer', 'Kapitza thermal resistance', 'ResNet'] None Simulating the heat transfer in a composite material is an important topic in material science. Difficulties arise from the fact that adjacent materials cannot match perfectly, resulting in discontinuity in the temperature variables. Although there have been several numerical methods for solving the heat-transfer problem in imperfect contact conditions, the methods known so far are complicated to implement, and the computational times are non-negligible. In this study, we developed a ResNet-type deep neural network for simulating a heat transfer model in a composite material. To train the neural network, we generated datasets by numerically solving the heat-transfer equations with Kapitza thermal resistance conditions. Because datasets involve various configurations of composite materials, our neural networks are robust to the shapes of material-material interfaces. Our algorithm can predict the thermal behavior in real time once the networks are trained. The performance of the proposed neural networks is documented, where the root mean square error (RMSE) and mean absolute error (MAE) are below 2.47E-6, and 7.00E-4, respectively.\n",
      "Rotate vector reducer design using resnet-based model and integration of discretized optimization 2022 ['Sequential engineering', 'RV reducer', 'Design automation', 'Multidisciplinary optimization', 'Secondary development'] None The author present an artificial intelligent (AI)-based deep generative model that demonstrate how to generate design options of mechanical systems, which are not only suitable for specific working conditions but also optimized for engineering performance. In current study, (1) a structural generative residual netowork (SG-Resnet) model is developed to establish the non-linear mapping between the working conditions and the external dimensions of the reducer, the main hyperparameters influencing the prediction ability and learning rate of the SG-Resnet are analyzed. (2) The mixed population non dominated sorting genetic algorithm-II (MP-NSGA-II) is proposed, and used to obtain pareto optimal solutions of the internal dimensions of the reducer. Experiments are performed to validate the positive effect of the structural generative model on the stiffness of the reducer. This research provides a novel method for reducer design and lays a solid foundation for the development of sequential engineering software for integrated rotate vector (RV) reducer.\n",
      "딥러닝을 이용한 직물의 결함 검출에 관한 연구 2022 ['fabric defects', 'VGGNet', 'ResNet', 'Grad-CAM', '직물결함', 'VGGNet', 'ResNet', 'Grad-CAM'] 섬유산업에서 생산된 직물의 결함을 식별하는 것은 품질관리를 위한 핵심적인 절차이다. 본 연구는 직물의 이미지를 분석하여 결함을 검출하는 모델을 만들고자 하였다. 연구에 사용된 모델은 딥러닝 기반의 VGGNet과 ResNet이었고, 두 모델의 결함 검출 성능을 비교하여 평가하였다. 정확도는 VGGNet 모델이 0.859, ResNet 모델이 0.893으로 ResNet 모델의 정확도가 더 높은 결과를 보여주었다. 추가적으로 딥러닝 모델이 직물의 이미지 내에서 결함으로 인식한 부분의 위치를 알아보기 위하여 XAI(eXplainable Artificial Intelligence)기법인 Grad-CAM 알고리즘을 사용하여 모델의 관심영역을 도출하였다. 그 결과 딥러닝 모델이 직물의 결함으로 인식한 부분이 육안으로도 실제 결함이 있는 것으로 확인되었다. 본 연구의 결과는 직물의 결함 검출에 있어서 딥러닝 기반의 인공지능을 활용함으로써 섬유의 생산과정에서 발생하는 시간과 비용을 줄일 수 있을 것으로 기대된다. Identifying defects in textiles is a key procedure for quality control. This study attempted to create a model that detects defects by analyzing the images of the fabrics. The models used in the study were deep learning-based VGGNet and ResNet, and the defect detection performance of the two models was compared and evaluated. The accuracy of the VGGNet and the ResNet model was 0.859 and 0.893, respectively, which showed the higher accuracy of the ResNet. In addition, the region of attention of the model was derived by using the Grad-CAM algorithm, an eXplainable Artificial Intelligence (XAI) technique, to find out the location of the region that the deep learning model recognized as a defect in the fabric image. As a result, it was confirmed that the region recognized by the deep learning model as a defect in the fabric was actually defective even with the naked eyes. The results of this study are expected to reduce the time and cost incurred in the fabric production process by utilizing deep learning-based artificial intelligence in the defect detection of the textile industry.\n",
      "ArcFace를 사용한 경량 신원인식 네트워크 2022 ['ArcFace', 'lightweight CNN', 'identity recognition', 'edge AI'] None In this paper, we propose a lightweight identity recognition network combining ArcFace, a loss function with robust characteristics in the field of face recognition, and ResNet-18. In this paper, ResNet-18 is modified to a single-channel-based network to reduce the computational amount of the ResNet-18 network. And to compensate for the network performance degradation due to the decrease in the number of channels, ArcFace, which uses an angle margin-based loss function, was combined with a single-channel-based ResNet-18 network to compensate for the network performance degradation. The proposed network performed training and inference with 10,056 experimental data sets consisting of face photos of 33 people. As a result of the experiment, it was confirmed that the proposed ArcFace-based lightweight ResNet-18 improved processing speed by about 1.3 times compared to the existing ResNet-18. In addition, an inference accuracy of 96.9%, similar to that of the existing network ResNet-18, which is 97.6%, was derived.\n",
      "비소세포폐암 환자의 재발 예측을 위한 흉부 CT 영상 패치 기반 CNN 분류 및 시각화 2022 ['Non-Small Cell Lung Cancer(NSCLC)', 'Recurrence Prediction', 'Deep Learning', 'Classification', 'Ensemble Learning', 'Convolutional Neural Network(CNN)1', '비소세포폐암', '재발 예측', '딥러닝', '분류', '앙상블 학습', '합성곱 신경망'] 비소세포폐암(NSCLC)은 전체 폐암 중 85%의 높은 비중을 차지하며 사망률(22.7%)이 다른 암에 비해 현저히 높은 암으로비소세포폐암 환자의 수술 후 예후에 대한 예측은 매우 중요하다. 본 연구에서는 종양을 관심영역으로 갖는 비소세포폐암환자의 수술 전 흉부 CT 영상 패치의 종류를 종양 관련 정보에 따라 총 다섯 가지로 다양화하고, 이를 입력데이터로 갖는사전 학습 된 ResNet 과 EfficientNet CNN 네트워크를 사용하여 단일 모델과 간접 투표 방식을 이용한 앙상블 모델, 그리고3 개의 입력 채널을 활용한 앙상블 모델에서의 실험 결과 및 성능을 오분류의 사례와 Grad-CAM 시각화를 통해 비교분석한다. 실험 결과, 종양 주변부 패치를 학습한 ResNet152 단일 모델과 EfficientNet-b7 단일 모델은 각각 87.93%와81.03%의 정확도를 보였다. 또한 ResNet152 에서 총 3 개의 입력 채널에 각각 영상 패치, 종양 주변부 패치, 형상 집중 종양내부 패치를 넣어 앙상블 모델을 구성한 경우에는 정확도 87.93%를, EfficientNet-b7 에서 간접 투표 방식으로 영상 패치와종양 주변부 패치 학습 모델을 앙상블 한 경우에는 정확도 84.48%를 도출하며 안정적인 성능을 보였다. Non-small cell lung cancer (NSCLC) accounts for a high proportion of 85% among all lung cancer and has a significantly higher mortality rate (22.7%) compared to other cancers. Therefore, it is very important to predict the prognosis after surgery in patients with non-small cell lung cancer. In this study, the types of preoperative chest CT image patches for non-small cell lung cancer patients with tumor as a region of interest are diversified into five types according to tumor-related information, and performance of single classifier model, ensemble classifier model with soft-voting method, and ensemble classifier model using 3 input channels for combination of three different patches using pre-trained ResNet and EfficientNet CNN networks are analyzed through misclassification cases and Grad-CAM visualization. As a result of the experiment, the ResNet152 single model and the EfficientNet-b7 single model trained on the peritumoral patch showed accuracy of 87.93% and 81.03%, respectively. In addition, ResNet152 ensemble model using the image, peritumoral, and shape-focused intratumoral patches which were placed in each input channels showed stable performance with an accuracy of 87.93%. Also, EfficientNet-b7 ensemble classifier model with soft-voting method using the image and peritumoral patches showed accuracy of 84.48%.\n",
      "무선 단말기 Fingerprint 식별을 위한 딥러닝 구조 개발 2022 ['DMR Fingerprint', 'ResNet-1D', 'fingerprinting feature', 'polar coordinate'] RF-Fingerprint 기술은 전송된 파형에서 송신기의 하드웨어 고유 특성을 추출하는 기술로써, 디바이스 보안분야에 매우 유용한 기술 중의 하나이다. 본 논문은 무선 단말기의 In-phase(I)와 Quadrature(Q) 값을 입력으로 동종무선 단말기 및 이기종 무선 단말기를 식별할 수 있는 fingerprint 특징을 추출하고 이를 식별할 수 있는 딥러닝 구조를제안한다. 동종/이기종 무선 단말기를 식별하기 위한 특징으로 I/Q를 극좌표로 변환한 후 크기 값을 시간축으로 배열한데이터를 무선 단말기의 fingerprinting 특징으로 제안하고 이를 식별하기 위해서 수정된 1차원 ResNet 모델을 제안한다. 실험을 위해서 동일 모델 10대의 두 종류 무선 단말기를 대상으로 제안한 딥러닝 구조의 성능을 분석한다. 제안한딥러닝 구조 및 fingerprint 특징의 성능 검증을 위해서 4000개의 데이터셋 중에서 20%인 800개 데이터셋을 이용하여성능 분석한 결과 약 99.5%의 식별 성능을 보였다. Radio frequency fingerprinting refers to a methodology that extracts hardware-specific characteristics of a transmitter that are unintentionally embedded in a transmitted waveform. In this paper, we put forward a fingerprinting feature and deep learning structure that can identify the same type of Digital Mobile Radio(DMR) by inputting the in-phase(I) and quadrature(Q). We proposes using the magnitude in polar coordinates of I/Q as RF fingerprinting feature and a modified ResNet-1D structure that can identify them. Experimental results show that our proposed modified ResNet-1D structure can achieve recognition accuracy of 99.5% on 20 DMR.\n",
      "Development of an Artificial Intelligence-Based Support  Technology for Urethral and Ureteral Stricture Surgery 2022 ['Urethral stricture', 'Ureteral stricture', 'ResNet-50', 'Surgical support technology', 'Endoscope', 'Artificial intelligence'] None Purpose: This paper proposes a technological system that uses artificial intelligence to recognize and guide the operator to the exact stenosis area during endoscopic surgery in patients with urethral or ureteral strictures. The aim of this technological solution was to increase surgical efficiency.Methods: The proposed system utilizes the ResNet-50 algorithm, an artificial intelligence technology, and analyzes images entering the endoscope during surgery to detect the stenosis location accurately and provide intraoperative clinical assistance.The ResNet-50 algorithm was chosen to facilitate accurate detection of the stenosis site.Results: The high recognition accuracy of the system was confirmed by an average final sensitivity value of 0.96. Since sensitivity is a measure of the probability of a true-positive test, this finding confirms that the system provided accurate guidance to the stenosis area when used for support in actual surgery.Conclusions: The proposed method supports surgery for patients with urethral or ureteral strictures by applying the ResNet-50 algorithm. The system analyzes images entering the endoscope during surgery and accurately detects stenosis, thereby assisting in surgery. In future research, we intend to provide both conservative and flexible boundaries of the strictures.\n",
      "다양한 CNN모델을 사용한 컬러 콘택트렌즈 불량 검출 2022 ['컬러 콘택트렌즈', '딥러닝', 'ResNet', 'GoogLeNet', 'DenseNet', 'MobileNet', 'Contact Lens', 'Deep Learning', 'ResNet', 'GoogLeNet', 'DenseNet', 'MobileNet'] 4차 산업혁명과 함께 디지털 전환(Digital Transformation, DX) 기술이 중요해지고 있다. 이와 함께 인공지능을 통한 생산공정에서의 불량 검출 및 분류에 대한 연구가 활발히 이루어지고 있다. 본 논문에서는 다양한 CNN 모델을 사용하여 컬러 콘택트렌즈 생산공정에서 발생하는 불량 검출을 효과적으로 수행하는 모델을 선정하고자 하며, 이를 통해 생산 및 품질의 향상을 이루어, 자원의 낭비와 소비자의 안전을 확보하고자 한다. 이를 위해 컬러 콘택트렌즈 영상에 대한 전처리와 증강을 통해 학습 및 검증  데이터를 생성하였으며, RGB 및 HSV 채널 영상에 대해 ResNet101, GoogLeNet V2, GoogLeNet V4, DenseNet121, MobileNet의 CNN 기법을 활용하여 RGB와 HSV 채널별로 불량 탐지율 비교 분석하였다. 위 모델의 정확도는 순서대로 각각 89.74%, 84.46%, 95.43%, 82.80%, 89.74%로, RGB 채널의 GoogLeNet V4가 가장 높은 불량 검출 정확도를 얻었으며, 대부분의 모델에서 RGB 채널이 HSV 채널보다 더 좋은 결과를 얻어냄을 알 수 있었다. The importance of Digital Transformation (DX) technology has increased with the Fourth Industrial Revolution. At the same time, research on defect detection and classification in the production process through artificial intelligence has been actively applied. In this paper, we select a model that effectively detects defects that occur in the production process of color contact lenses using various models, secure reducing resource waste and consumer safety by improving production and quality. For this purpose, data for training and validation were generated through preprocessing and augmentation of color contact lens images, using CNN technologies such as ResNet101, GoogLeNet V2, GoogLeNet V4, DenseNet121, MobileNet compared and analyzed the defect detection rate for each RGB channel and HSV channel. The accuracies of the above models are 89.74%, 84.46%, 95.43%, 82.80%, and 89.74% respectively, with GoogLeNet V4 on the RGB channel having the highest defect detection accuracy, and in most models, the RGB channel is higher than the HSV channel.\n",
      "심혈관 상태 식별을 위한 CNN Block 구조를 적용한 고성능 ECG 데이터 분석시스템 2022 ['CNN block structure', 'ECG data analysis scheme', 'ResNeXt', 'MIT-BIH database', 'F1-score'] None None\n",
      "딥러닝 기반의 핵의학 폐검사 분류 모델 적용 2022 ['컨볼루션 신경망', '딥러닝', '폐 신티그라피', '분류-활성화 맵', '핵의학', 'Convolutional neural network', 'Deep learning', 'Lung scintigraphy', 'Class activation map', 'Nuclear medicine'] None The purpose of this study is to apply a deep learning model that can distinguish lung perfusion and lung ventilation images in nuclear medicine, and to evaluate the image classification ability. Image data pre-processing was performed in the following order: image matrix size adjustment, min-max normalization, image center position adjustment, train/validation/test data set classification, and data augmentation. The convolutional neural network(CNN) structures of VGG-16, ResNet-18, Inception-ResNet-v2, and SE-ResNeXt-101 were used. For classification model evaluation, performance evaluation index of classification model, class activation map(CAM), and statistical image evaluation method were applied. As for the performance evaluation index of the classification model, SE-ResNeXt-101 and Inception-ResNet-v2 showed the highest performance with the same results. As a result of CAM, cardiac and right lung regions were highly activated in lung perfusion, and upper lung and neck regions were highly activated in lung ventilation. Statistical image evaluation showed a meaningful difference between SE-ResNeXt-101 and Inception-ResNet-v2. As a result of the study, the applicability of the CNN model for lung scintigraphy classification was confirmed. In the future, it is expected that it will be used as basic data for research on new artificial intelligence models and will help stable image management in clinical practice.\n",
      "Feasibility of Deep Learning-Based Analysis of Auscultation for Screening Significant Stenosis of Native Arteriovenous Fistula for Hemodialysis Requiring Angioplasty 2022 ['Angioplasty', 'Deep learning', 'Arteriovenous fistula', 'Auscultation', 'Renal dialysis'] None Objective: To investigate the feasibility of using a deep learning-based analysis of auscultation data to predict significant stenosis of arteriovenous fistulas (AVF) in patients undergoing hemodialysis requiring percutaneous transluminal angioplasty (PTA).Materials and Methods: Forty patients (24 male and 16 female; median age, 62.5 years) with dysfunctional native AVF were prospectively recruited. Digital sounds from the AVF shunt were recorded using a wireless electronic stethoscope before (pre-PTA) and after PTA (post-PTA), and the audio files were subsequently converted to mel spectrograms, which were used to construct various deep convolutional neural network (DCNN) models (DenseNet201, EfficientNetB5, and ResNet50). The performance of these models for diagnosing ≥ 50% AVF stenosis was assessed and compared. The ground truth for the presence of ≥ 50% AVF stenosis was obtained using digital subtraction angiography. Gradient-weighted class activation mapping (Grad-CAM) was used to produce visual explanations for DCNN model decisions.Results: Eighty audio files were obtained from the 40 recruited patients and pooled for the study. Mel spectrograms of “pre-PTA” shunt sounds showed patterns corresponding to abnormal high-pitched bruits with systolic accentuation observed in patients with stenotic AVF. The ResNet50 and EfficientNetB5 models yielded an area under the receiver operating characteristic curve of 0.99 and 0.98, respectively, at optimized epochs for predicting ≥ 50% AVF stenosis. However, Grad- CAM heatmaps revealed that only ResNet50 highlighted areas relevant to AVF stenosis in the mel spectrogram.Conclusion: Mel spectrogram-based DCNN models, particularly ResNet50, successfully predicted the presence of significant AVF stenosis requiring PTA in this feasibility study and may potentially be used in AVF surveillance.\n",
      "A Study on the Optimal Artificial Intelligence Model for  Determination of Urolithiasis 2022 ['Urolithiasis', 'Ureter stones', 'ResNet-50', 'Fast R-CNN', 'Surgical support technology'] None Purpose: This paper aims to develop a clinical decision support system (CDSS) that can help detect the stone that is most important to the diagnosis of urolithiasis. Among them, especially for the development of artificial intelligence (AI) models that support a final judgment in CDSS, we would like to study the optimal AI model by comparing and evaluating them.Methods: This paper proposes the optimal ureter stone detection model using various AI technologies. The use of AI technology compares and evaluates methods such as machine learning (support vector machine), deep learning (ResNet-50, Fast RCNN), and image processing (watershed) to find a more effective method for detecting ureter stones.Results: The final value of sensitivity, which is calculated using true positive (TP) and false negative and is a measure of the probability of TP results, showed high recognition accuracy, with an average value of 0.93 for ResNet-50. This finding confirmed that accurate guidance to the stones area was possible when the developed platform was used to support actual surgery.Conclusions: The general situation in the most effective way to the detection stone can be found. But a variety of variables may be slightly different the difference through the term could tell. Future works, on urological diseases, are diverse and the research will be expanded by customizing AI models specialized for those diseases.\n",
      "새로운 반려견 등록방식 도입을 위한 안면 인식 성능 개선 연구 2022 ['Deep Learning', 'Face Recognition', 'Dog Recognition', 'ResNet', 'Triplet Loss'] 동물보호법 개정에 따라 반려견 등록이 의무화 되었음에도 불구하고, 현재 등록 방법의 불편함으로 등록율이 저조한 상태이다. 본논문에서는 새로운 등록 방법으로 검토되고 있는 반려견 안면 인식 기술에 대한 성능 개선 연구를 진행하였다. 딥러닝 학습을 통해,반려견의 안면 인식을 위한 임베딩 벡터를 생성하여 반려견 개체별로 식별하기 위한 방법을 실험하였다. 딥러닝 학습을 위한 반려견이미지 데이터셋을 구축하고, InceptionNet과 ResNet-50을 백본 네트워크로 사용하여 실험하였다. 삼중항 손실 방법으로 학습하였으며,안면 검증과 안면 식별로 나뉘어 실험하였다. ResNet-50 기반의 모델에서 최고 93.46%의 안면 검증 성능을 얻을 수 있었으며, 안면식별 시험에서는 rank-5에서 91.44%의 최고 성능을 각각 얻을 수 있었다. 본 논문에서 제시한 실험 방법과 결과는 반려견의 등록 여부 확인, 반려견 출입시설에서의 개체 확인 등 다양한 분야로 활용이 가능하다. None\n",
      "딥러닝기반 토마토 병해 진단 서비스 연구 2022 ['딥러닝', '토마토병해충', '모바일넷', '레스넷', 'deep learning', 'tomato disease', 'MobileNet', 'ResNet'] 토마토 작물은 병해에 노출이 쉽고 단시간에 퍼지므로 병해에 대한 늦은 조치로 인한 피해는 생산량과 매출에 직접적인 영향을 끼친다.  따라서, 토마토의 병해에 대해 누구나 현장에서 간편하고 정확하게 진단하여 조기 예방을 가능하게 하는 서비스가 요구된다. 본 논문에서는 사전에 ImageNet 전이 학습된 딥러닝 기반 모델을 적용하여 토마토의 9가지 병해 및 정상인 경우의 클래스를 분류하고 서비스를 제공하는 시스템을 구성한다. Plant Village 데이터 셋으로부터 토마토 병해 및 정상을 분류한 잎의 이미지 셋을 합성곱을 사용하여 조금 더 가벼운 신경망을 구축한 딥러닝 기반 CNN구조를 갖는 MobileNet, ResNet의 입력을 사용한다. 2가지 제안 모델의 학습을 통해 정확도와 학습속도가 빠른 MobileNet를 사용하여 빠르고 편리한 서비스를 제공할 수 있다. Tomato crops are easy to expose to disease and spread in a short period of time, so late measures against disease are directly related to production and sales, which can cause damage. Therefore, there is a need for a service that enables early prevention by simply and accurately diagnosing tomato diseases in the field. In this paper, we construct a system that applies a deep learning-based model in which ImageNet transition is learned in advance to classify and serve nine classes of tomatoes for disease and normal cases. We use the input of MobileNet, ResNet, with a deep learning-based CNN structure that builds a lighter neural network using a composite product for the image set of leaves classifying tomato disease and normal from the Plant Village dataset. Through the learning of two proposed models, it is possible to provide fast and convenient services using MobileNet with high accuracy and learning speed.\n",
      "Two-phase flow pattern online monitoring system based on convolutional neural network and transfer learning 2022 ['Flow pattern', 'Online monitoring system', 'Artificial neural network (ANN)', 'Convolutional neural network (CNN)', 'Transfer learning', 'ResNet50'] None Two-phase flow may almost exist in every branch of the energy industry. For the corresponding engineering design, it is very essential and crucial to monitor flow patterns and their transitions accurately.With the high-speed development and success of deep learning based on convolutional neural network (CNN), the study of flow pattern identification recently almost focused on this methodology. Additionally, the photographing technique has attractive implementation features as well, since it is normally considerably less expensive than other techniques. The development of such a two-phase flow pattern online monitoring system is the objective of this work, which seldom studied before. The ongoing preliminary engineering design (including hardware and software) of the system are introduced. The flow pattern identification method based on CNNs and transfer learning was discussed in detail. Several potential CNN candidates such as ALexNet, VggNet16 and ResNets were introduced and compared with each other based on a flow pattern dataset. According to the results, ResNet50 is the most promising CNN network for the system owing to its high precision, fast classification and strong robustness. This work can be a reference for the online monitoring system design in the energy system.\n",
      "안전벨트에 관성측정장치 장착을 통한 운전자의 호흡 상태 모니터링 2022 ['운전자 모니터링 시스템', '생체신호 분석', '관성측정장치', '운전자 상태 분류', 'Driver Monitoring Systems', 'Bio-Signal Data Measurement', 'Inertial Measurement Unit', 'ResNet', 'Driver States Classification'] 사람의 건강 상태를 파악하는 것은 응급상황 예방 및 대처에 있어 매우 중요하다. 호흡 상태와 같은 생활 징후를 파악하는 것은 응급상황 발생 여부를 확인할 수 있는 주요한 요소로 작용한다. 이때, 운전자의 생체신호로부터 운전 중에 발생하는 응급상황을 모니터링할 수 있다면, 운전자의 제어권 상실로 인해 발생하는 교통사고에 대한 대처가 가능하다. 본 논문에서는 운전자의 응급상황을 모니터링하기 위해 운전자의 흉부 움직임을 측정하고, 측정된 데이터를 통해 운전자의 호흡 상태를 분류한다. 운전자의 호흡 상태는 관성측정장치를 사용하여 측정하고, Kalman filter를 통해 가속도와 자이로 센서값에 대한 노이즈를 제거한다. 이후, 운전자의 호흡 상태를 정확히 분류하기 위해 데이터를 이미지로 가시화하며, ResNet을 통한 학습을 진행한다. 각 이미지화된 데이터에 대한 분류 성능을 비교하고, 가장 높은 정확도를 보이는 관성측정장치의 데이터를 확인한다. Monitoring a person\"s health status is a crucial factor to prevent and react to emergencies. Especially, identifying vital signs such as respiratory status is directly related to recognizing urgent situations. Therefore, we can initiatively respond to traffic accidents caused by the loss of control/consciousness if the emergencies can be detected based on a person’s vital reaction while driving a car. To solve this issue, this paper proposes a methodology to monitor the emergencies in driver’s health condition by measuring the dynamic movement of a chest and then classifying the respiratory conditions. First, the driver’s respiratory status is measured by a seat belt equipped with an inertial measurement unit (IMU) and the measured data is denoised by applying the Kalman filter. To classify respiratory status, next, each IMU data is visualized as an image, and then the image is used to train the ResNet. Finally, the classification accuracies under various combinations of visualization approaches and IMU data types are compared to scrutinize the combination yielding the best classification accuracy. The result from this study allows us to design a driver respiratory monitoring system with the IMU installed seat belt.\n",
      "딥러닝 기반의 반도체 패키지 다이면 스크래치 검출 방법 2022 ['Scratch Detection', 'Deep Learning', 'Image Processing', 'Semiconductor Package', '.'] None .\n",
      "설명 가능한 합성곱 신경망을 활용한 센서 기반의 시계열 데이터 분류 모델 제안 2022 ['Sensor Data', 'Time Series Classification', 'Pattern Recognition', 'Deep Learning', 'eXplainable Artificial Intelligence(XAI)', '센서 데이터', '시계열 데이터 분류', '패턴 인식', '딥러닝', '설명가능한 인공지능'] 센서 데이터를 활용하여 설비의 이상 진단이 가능해졌다. 하지만 설비 이상에 대한 원인 분석은 미비한 실정이다. 본 연구에서는 센서 기반 시계열 데이터 분류 모델을 위한 해석가능한 합성곱 신경망 프레임워크를 제안한다. 연구에서 사용된 센서 기반 시계열 데이터는 실제 차량에 부착된 센서를 통해 수집되었고, 반도체의 웨이퍼 데이터는 공정 과정에서 수집되었다. 추가로 실제 기계 설비에서 수집된 주기 신호 데이터를 이용 하였으며, 충분한 학습을 위해 Data augmentation 방법론인 Scaling과 Jittering을 적용하였다. 또한, 본 연구에서는 3가지 합성곱 신경망 기반 모델들을 제안하고 각각의 성능을 비교하였다. 본 연구에서는 ResNet에 Jittering을 적용한 결과 정확도 95%, F1 점수 95%로 가장 뛰어난 성능을 보였으며, 기존 연구 대비 3%의 성능 향상을 보였다. 더 나아가 결과의 해석을 위한 XAI 방법론으로 Class Activation Map과 Layer Visualization을 제안하였으며, 센서 데이터 분류에 중요 영향을 끼치는 시계열 구간을 시각적으로 확인하였다. Sensor data can provide fault diagnosis for equipment. However, the cause analysis for fault results of equipment is not often provided. In this study, we propose an explainable convolutional neural network framework for the sensor-based time series classification model. We used sensor-based time series dataset, acquired from vehicles equipped with sensors, and the Wafer dataset, acquired from manufacturing process. Moreover, we used Cycle Signal dataset, acquired from real world mechanical equipment, and for Data augmentation methods, scaling and jittering were used to train our deep learning models. In addition, our proposed classification models are convolutional neural network based models, FCN, 1D-CNN, and ResNet, to compare evaluations for each model. Our experimental results show that the ResNet provides promising results in the context of time series classification with accuracy and F1 Score reaching 95%, improved by 3% compared to the previous study. Furthermore, we propose XAI methods, Class Activation Map and Layer Visualization, to interpret the experiment result. XAI methods can visualize the time series interval that shows important factors for sensor data classification.\n",
      "반려견 자동 품종 분류를 위한 전이학습 효과 분석 2022 ['Deep Learning', 'Transfer Learning', 'Resnet', 'VGGNet', 'Dog Breed'] None Compared to the continuously increasing dog population and industry size in Korea, systematic analysis of related data and research on breed classification methods are very insufficient. In this paper, an automatic breed classification method is proposed using deep learning technology for 14 major dog breeds domestically raised. To do this, dog images are collected for deep learning training and a dataset is built, and a breed classification algorithm is created by performing transfer learning based on VGG-16 and Resnet-34 as backbone networks. In order to check the transfer learning effect of the two models on dog images, we compared the use of pre-trained weights and the experiment of updating the weights. When fine tuning was performed based on VGG-16 backbone network, in the final model, the accuracy of Top 1 was about 89% and that of Top 3 was about 94%, respectively. The domestic dog breed classification method and data construction proposed in this paper have the potential to be used for various application purposes, such as classification of abandoned and lost dog breeds in animal protection centers or utilization in pet-feed industry.\n",
      "Scaling Up Face Masks Classification Using a Deep Neural Network and Classical Method Inspired Hybrid Technique 2022 ['CNNs', 'Face masks', 'Machine learning', 'Multi-layer perceptron', 'ResNet-101'] None Classification of persons wearing and not wearing face masks in images has emerged as a new computer vision problem during the COVID-19 pandemic. In order to address this problem and scale up the research in this domain, in this paper a hybrid technique by employing ResNet-101 and multi-layer perceptron (MLP) classifier has been proposed. The proposed technique is tested and validated on a self-created face masks classification dataset and a standard dataset. On self-created dataset, the proposed technique achieved a classification accuracy of 97.3%. To embrace the proposed technique, six other state-of-the-art CNN feature extractors with six other classical machine learning classifiers have been tested and compared with the proposed technique. The proposed technique achieved better classification accuracy and 1-6% higher precision, recall, and F1 score as compared to other tested deep feature extractors and machine learning classifiers.\n",
      "합성곱 신경망 기반 분류 모델의 화재 예측 성능 분석 2022 ['Fire detection', 'Fire image classification', 'Convolutional neural network', 'Fire prediction performance'] 본 연구에서는 화재 안전 향상을 위한 엣지 컴퓨팅(edge computing) 기반 화재감지시스템에 적용 가능한 합성곱 신경망 기반 이미지 분류 모델들인 MobileNetV2, ResNet101, EfficientNetB0를 이용하여 화재 예측 성능 해석을 수행하였다. 성능평가지표인 정확도, 재현율, 정밀도, F1-score와 혼동 행렬을 이용하여 화재 예측 성능을 비교 분석하였다. 또한 분류 모델의 경량화와 관련한 모델 용량 및 추론시간에 대한 비교 분석을 수행하였다. 비교 분석 결과로서 화재 예측 정확도는 EfficientNetB0 모델이 가장 높았으며 경량성 측면에서는 MobileNetV2가 가장 우수한 것으로 확인하였다. 더하여 화재와 유사한 특징을 갖는 비 화재 이미지인 빛과 연무에 대한 이미지 특성을 추가 학습한 결과, 경량성은 우수하나 예측 성능이 낮은 MobileNetV2의 화재 예측 정확도가 개선되는 것을 확인하였다. In this study, fire prediction performance was analyzed using convolutional neural network (CNN)-based classification models such as MobileNetV2, ResNet101, and EfficientNetB0 applicable to an edge computing-based fire detection system for improving fire safety. The fire prediction performance was evaluated using the performance evaluation measures including accuracy, recall, precision, F1-score, and the confusion matrix. The model size and inference time were assessed in terms of the light-weight classification model for the practical deployment and use. The analysis results confirmed that the EfficientNetB0 model had the highest fire prediction accuracy, and the MobileNetV2 was the best light-weight classification model. Notably, additionally learning the image features about light and haze images having similar features with those of the fire images improved the fire prediction accuracy of the light-weight MobileNetV2 model.\n",
      "포인트 클라우드 및 투영 이미지를 이용한 다중 모달 형상 분류 2022 ['포인트 클라우드', '심층신경망', '딥러닝', '다중 모달', '형상 분류', 'Point Cloud', 'Deep Neural Network', 'Deep Learning', 'Multi-modal', 'Shape Classification'] 포인트 클라우드의 형상을 분류하기 위해 심층신경망을 활용할 때, 점들의 좌푯값만을 활용하거나 연산 부담이 큰 3차원 렌더링을 통해 생성한 이미지를 활용하여 형상 분류를 수행한다. 본 연구에서는 좌푯 값과 해당 좌표를 활용해 생성한 투영 이미지를 함께 다중 모달로 심층신경망의 입력으로 활용하는 포인트 클라우드의 형상 분류 기법을 제안한다. 성능 향상 여부를 확인하고자, 좌푯값 기반으로 형상을 분류하는 PointNet과 이미지 기반 분류 모델인 ResNet-18을 조합하여 다중 모달 모델을 구성하고 ModelNet40 데이터셋에 대해서 투영 이미지의 여부 및 방향(등각면, 정면, 측면, 상면)에 따른 성능 평가를 수행하였다. 그 결과 측면 투영 이미지가 함께 고려될 때 가장 성능이 좋았으며, 정면 투영 이미지의 경우가 두 번째로 우수한 성능을 보였다. 이는 포인트 클라우드의 형상 분류에 있어서 좌푯값과 더불어 투영 이미지를 함께 입력으로 활용하는 것이 형상 분류에 효과적임을 뒷받침한다. A deep neural network is used to classify the shape of the point cloud using only the coordinate values of the points or the rendered image. In this study, we proposed a point cloud shape classification technique using the coordinate values and the projection image generated using the coordinates as an input to a multi-modal deep neural network. To verify the performance improvement, the multi-modal model built using PointNet, which classifies the shapes based on coordinate values, and ResNet-18, an image-based classification model, was evaluated for the shape classification performance on a ModelNet40 dataset. The result showed that the performance was the best when the side projection image was additionally considered and the second best when the front projection image was considered. This supports the idea that the shape classification performance of the point cloud can be improved using the coordinate values and its projection image as the input to the deep neural network in a multi-modal manner.\n",
      "재구성 가능한 모듈 기반 CNN 가속기 구현 2022 ['CNN accelerator', 'module-based architecture', 'FPGA', 'Verilog-HDL', '.'] 본 논문에서는 CNN(Convolutional Neural Network)을 구성하는 주요 연산 모듈을 모듈 제어 명령어를 통해 구동함으로써 네트워크를 구현할 수 있는 CNN 가속기를 제안한다. 모듈 기반 CNN 가속기는 합성곱(Convolution), 풀링(Pooling) 등 CNN의 주요 연산 모듈로 구성되어 있으며, 프로세서에서 모듈 제어 명령어를 통해 네트워크 구성에 필요한 연산 모듈을 선택 및 내부 파라미터를 설정 할 수 있다. 본 논문에서 제안하는 모듈 기반 CNN 가속기를 사용하여 Xilinx SoC형 FPGA에 ResNet-18을 구현하였으며 CNN 프레임워크 모델인 PyTorch와 C 기반 검증 모델을 사용하여 출력 결과를 비교 검증하였다. 실험결과, CNN 가속기의 추론 결과는 92.87%의 정확도를 보였다. In this paper, we propose a CNN(convolution neural network) accelerator that can implement a network by driving main computation modules constituting the CNN through module control commands. The module-based CNN accelerator consists of CNN's main computation modules such as convolution and pooling, and the processor can select the computation module required for network configuration and set internal parameters through module control commands. In this paper, ResNet-18 was implemented on a Xilinx SoC-type FPGA using the proposed module-based CNN accelerator, and the output results were compared and verified using PyTorch, a CNN framework model, and a C-based verification model. As a result of the experiment, the inference result of the CNN accelerator showed an accuracy of 92.87%.\n",
      "SVM on Top of Deep Networks for Covid-19 Detection from Chest X-ray Images 2022 ['Covid-19', 'X-ray image', 'Deep learning', 'Support vector machines'] None In this study, we propose training a support vector machine (SVM) model on top of deep networks for detecting Covid-19 from chest X-ray images. We started by gathering a real chest X-ray image dataset, including positive Covid-19, normal cases, and other lung diseases not caused by Covid-19. Instead of training deep networks from scratch, we fine-tuned recent pre-trained deep network models, such as DenseNet121, MobileNet v2, Inception v3, Xception, ResNet50, VGG16, and VGG19, to classify chest X-ray images into one of three classes (Covid-19, normal, and other lung). We propose training an SVM model on top of deep networks to perform a nonlinear combination of deep network outputs, improving classification over any single deep network. The empirical test results on the real chest X-ray image dataset show that deep network models, with an exception of ResNet50 with 82.44%, provide an accuracy of at least 92% on the test set. The proposed SVM on top of the deep network achieved the highest accuracy of 96.16%.\n",
      "감정 분류를 이용한 표정 연습 보조 인공지능 2022 ['감정 분류', '표정 연습', '얼굴 이미지 처리', '자연어 처리', 'Emotion Classification', 'Facial Expression Practice', 'Facial Image Processing', 'Natural Language Processing'] 본 연구에서는 감정을 표현하기 위한 표정 연습을 보조하는 인공지능을 개발하였다. 개발한 인공지능은 서술형 문장과 표정 이미지로 구성된 멀티모달 입력을 심층신경망에 사용하고 서술형 문장에서 예측되는 감정과 표정 이미지에서 예측되는 감정 사이의 유사도를 계산하여 출력하였다. 사용자는 서술형 문장으로 주어진 상황에 맞게 표정을 연습하고 인공지능은 서술형 문장과 사용자의 표정 사이의 유사도를 수치로 출력하여 피드백한다. 표정 이미지에서 감정을 예측하기 위해 ResNet34 구조를 사용하였으며 FER2013 공공데이터를 이용해 훈련하였다. 자연어인 서술형 문장에서 감정을 예측하기 위해 KoBERT 모델을 전이학습 하였으며 AIHub의 감정 분류를 위한 대화 음성 데이터 세트를 사용해 훈련하였다. 표정 이미지에서 감정을 예측하는 심층신경망은 65% 정확도를 달성하여 사람 수준의 감정 분류 능력을 보여주었다. 서술형 문장에서 감정을 예측하는 심층신경망은 90% 정확도를 달성하였다. 감정표현에 문제가 없는 일반인이 개발한 인공지능을 이용해 표정 연습 실험을 수행하여 개발한 인공지능의 성능을 검증하였다. In this study, an artificial intelligence(AI) was developed to help with facial expression practice in order to express emotions. The developed AI used multimodal inputs consisting of sentences and facial images for deep neural networks (DNNs). The DNNs calculated similarities between the emotions predicted by the sentences and the emotions predicted by facial images. The user practiced facial expressions based on the situation given by sentences, and the AI provided the user with numerical feedback based on the similarity between the emotion predicted by sentence and the emotion predicted by facial expression. ResNet34 structure was trained on FER2013 public data to predict emotions from facial images. To predict emotions in sentences, KoBERT model was trained in transfer learning manner using the conversational speech dataset for emotion classification opened to the public by AIHub. The DNN that predicts emotions from the facial images demonstrated 65% accuracy, which is comparable to human emotional classification ability. The DNN that predicts emotions from the sentences achieved 90% accuracy. The performance of the developed AI was evaluated through experiments with changing facial expressions in which an ordinary person was participated.\n",
      "발레 자세 교정을 위한 발레 동작 코칭 시스템 2022 ['발레', '심플 포즈', '오픈 포즈', '관절 좌표 추출', '특징 중요도 분석', 'Ballet', 'Simple pose', 'Open pose', 'Keypoints detection', 'Analysis of feature importance'] 발레는 몸의 가동 범위가 넓은 운동인 만큼 정확하지 않은 자세로 운동을 하게 되면 부상을 입을 수 있고, 운동 효과를 느끼기 어렵다는 문제점이 있다. 이에 사용자가 올바른 자세로 운동을 할 수 있도록 돕는 발레 코칭 시스템을 제안한다. 제안한 방법은 15개의 관절이 레이블링 된 발레 동작을 기반으로 ResNet 기반의 Simple pose 모델과 Open pose의 BODY-25 모델을 사용하여 관절의 좌표를 검출한다. 검출한 관절 좌표의 분포를 히스토그램의 분위 수 분석을 통해 분석했다. 6가지 분류기를 통해 3가지 발레 동작 분류를 4가지 성능 지표를 통해 분석한다. Gradient Boosting Classifier가 비교한 모델 중 가장 최적의 성능 획득했다. 특징 중요도 분석 결과, 발목 관절이 중요하게 사용됨을 확인했다. 또한 4가지 특징 추출 방법에 따른 모델의 성능을 분석했다. 그리고 Odds ratio를 구하여 동작 별 예측 성공 비율을 비교했다. 제안한 방법이 발레 동작 코칭을 할 수 있음을 확인했다. Since ballet is an exercise with a wide range of movement, if you exercise with an incorrect posture, you may get injured and it is difficult to feel the effect of the exercise. We propose a ballet coaching system that helps users to exercise with correct posture. The proposed method detects joint coordinates using the ResNet-based Simple pose model and the Open pose BODY-25 model based on ballet motions labeled with 15 joints. Analysis of the quantiles of the histogram was performed on the distribution of the detected joint coordinates. Using 6 classifiers, 3 ballet motion classifications are analyzed through 4 performance indicators. Gradient Boosting Classifier obtained the most optimal performance among the compared models. As a result of the feature importance analysis, it was confirmed that the ankle joint is important. We also analyzed the performance of the model according to four feature extraction method. And the Odds ratio was obtained and the prediction success rates for each motion were compared. The proposed method can coach ballet movements.\n",
      "1인 가구 환경에서 프라이버시 보호 영상을 활용한 위험 행동 인식에 관한 연구 2022 ['Deep Learning', 'Privacy', 'Action Recognition', 'YOLOv5', 'Single-person household', '딥러닝', '프라이버시', '행동 인식', '1인 가구'] 최근 딥러닝 기술의 발달로 사람의 행동을 인식하는 연구가 진행 중에 있다. 본 논문에서는 딥러닝 기술을 활용하여 1인 가구 환경에서 발생할 수 있는 위험 행동을 인식하는 연구를 진행하였다. 1인 가구의 특성상 개인의 프라이버시 보호가 필요하다. 본 논문에서는 개인의 프라이버시 보호를 위해 가우시안 블러 필터가 적용된 프라이버시 보호 영상에서 사람의 위험 행동을 인식한다. 위험 행동 인식 방법은 객체 검출 모델인 YOLOv5 모델을 활용하여 영상에서 사람 객체 검출 및 전처리 방법을 적용한 후 행동 인식 모델의 입력값으로 활용하여 위험 행동을 인식한다. 실험에는 ResNet3D, I3D, SlowFast 모델을 사용하였고, 실험 결과 SlowFast 모델이 프라이버시 보호 영상에서 95.7%로 가장 높은 정확도를 달성하였다. 이를 통해 개인의 프라이버시를 보호하면서 1인 가구 환경에서 사람의 위험 행동을 인식하는 것이 가능하다. Recently, with the development of deep learning technology, research on recognizing human behavior is in progress. In this paper, a study was conducted to recognize risky behaviors that may occur in a single-person household environment using deep learning technology. Due to the nature of single-person households, personal privacy protection is necessary. In this paper, we recognize human dangerous behavior in privacy protection video with Gaussian blur filters for privacy protection of individuals. The dangerous behavior recognition method uses the YOLOv5 model to detect and preprocess human object from video, and then uses it as an input value for the behavior recognition model to recognize dangerous behavior. The experiments used ResNet3D, I3D, and SlowFast models, and the experimental results show that the SlowFast model achieved the highest accuracy of 95.7% in privacy-protected video. Through this, it is possible to recognize human dangerous behavior in a single-person household environment while protecting individual privacy.\n",
      "The Evaluation of Deep Learning Using Convolutional Neural Network (CNN) Approach for Identifying Arabica and Robusta Coffee Plants 2022 ['Leaf classification', 'Robusta coffee', 'Arabica coffee', 'Convolutional neural network', 'Coffee species', 'Precision agriculture'] None Purpose Arabica and Robusta coffee plants are physically distinctive as manifested in their leaves, leaf shape, color, and size.However, for ordinary people or those who have just begun their business in coffee cultivation, identifying the type of coffee plant can be challenging. In this study, we incorporated and evaluated deep learning technology to identify the types of coffee based on leaf image identification.Methods In this study, we designed a deep learning architecture and compared it with the well-known approaches, including LeNet, AlexNet, ResNet-50, and GoogleNet. A total of 19,980 image datasets were split into training and testing data, consisting of 15,984 images and 3,996 images, respectively.Results The hyperparameters were taken into account where the use of 100 epoch and 0.0001 learning rate provided the highest accuracy. In addition, 10-fold cross-validation and ROC were used for evaluating the proposed architectures. The results show that the developed convolutional neural network (CNN) generated the highest accuracy of 97.67% compared to LeNet, AlexNet, ResNet-50, and GoogleNet with an accuracy rate of 97.20%, 95.10%, 72.35%, and 82,16%, respectively.Conclusions The modified-CNN algorithm had satisfactory accuracy in identifying different types of coffee. The underlying principles of such classification draw specific attention to the leaf shape, size, and color of Arabica and Robusta coffee. For future works, it is a potential method that can be used to rapidly identify diverse varieties of Robusta and Arabica coffee plants based on leaf tissue and above canopy characteristics.\n",
      "Corneal Ulcer Region Detection With Semantic Segmentation Using Deep Learning 2022 None None Traditional methods of measuring corneal ulcers were difficult to present objective basis for diagnosis because of the subjective judgment of the medical staff through photographs taken with special equipment. In this paper, we propose a method to detect the ulcer area on a pixel basis in corneal ulcer images using a semantic segmentation model. In order to solve this problem, we performed the experiment to detect the ulcer area based on the DeepLab model which has the highest performance in semantic segmentation model. For the experiment, the training and test data were selected and the backbone network of DeepLab model which set as Xception and ResNet, respectively were evaluated and compared the performances. We used Dice similarity coefficient and IoU value as an indicator to evaluate the performances. Experimental results show that when 'crop & resized' images are added to the dataset, it segment the ulcer area with an average accuracy about 93% of Dice similarity coefficient on the DeepLab model with ResNet101 as the backbone network. This study shows that the semantic segmentation model used for object detection also has an ability to make significant results when classifying objects with irregular shapes such as corneal ulcers. Ultimately, we will perform the extension of datasets and experiment with adaptive learning methods through future studies so that they can be implemented in real medical diagnosis environment.\n",
      "영상 기반 Semantic Segmentation 알고리즘을 이용한 도로 추출 2022 ['Drone Image', 'Semantic Segmentation', 'Remote Sensing', 'Road Extraction', '드론 정사영상', 'Semantic Segmentation', '원격 탐사', '도로 추출'] 현대에는 급속한 산업화와 인구 증가로 인해 도시들이 더욱 복잡해지고 있다. 특히 도심은 택지개발, 재건축, 철거 등으로 인해 빠르게 변화하는 지역에 해당한다. 따라서 자율주행에 필요한 정밀도로지도와 같은 다양한 목적을 위해 빠른 정보 갱신이 필요하다. 우리나라의 경우 기존 지도 제작 과정을 통해 지도를 제작하면 정확한 공간정보를 생성할 수 있으나 대상 지역이 넓은 경우 시간과 비용이 많이 든다는 한계가 있다. 지도 요소 중 하나인 도로는 인류 문명을 위한 많은 다양한 자원을 제공하는 중추이자 필수적인 수단에 해당한다. 따라서 도로 정보를 정확하고 신속하게 갱신하는 것이 중요하다. 이 목표를 달성하기 위해 본 연구는 Semantic Segmentation 알고리즘인 LinkNet, D-LinkNet 및 NL-LinkNet을 사용하여 광주광역시 도시철도 2호선 공사 현장을 촬영한 드론 정사영상에서 도로를 추출한 다음 성능이 가장 높은 모델에 하이퍼 파라미터 최적화를 적용하였다. 그 결과, 사전 훈련된 ResNet-34를 Encoder로 사용한 LinkNet 모델이 85.125 mIoU를 달성했다. 향후 연구 방향으로 최신 Semantic Segmentation 알고리즘 또는 준지도 학습 기반 Semantic Segmentation 기법을 사용하는 연구의 결과와의 비교 분석이 수행될 것이다. 본 연구의 결과는 기존 지도 갱신 프로세스의 속도를 개선하는 데 도움을 줄 수 있을 것으로 예상된다. Cities are becoming more complex due to rapid industrialization and population growth in modern times. In particular, urban areas are rapidly changing due to housing site development, reconstruction, and demolition. Thus accurate road information is necessary for various purposes, such as High Definition Map for autonomous car driving. In the case of the Republic of Korea, accurate spatial information can be generated by making a map through the existing map production process. However, targeting a large area is limited due to time and money. Road, one of the map elements, is a hub and essential means of transportation that provides many different resources for human civilization. Therefore, it is essential to update road information accurately and quickly. This study uses Semantic Segmentation algorithms Such as LinkNet, D-LinkNet, and NL-LinkNet to extract roads from drone images and then apply hyperparameter optimization to models with the highest performance. As a result, the LinkNet model using pre-trained ResNet-34 as the encoder achieved 85.125 mIoU. Subsequent studies should focus on comparing the results of this study with those of studies using state-of-the-art object detection algorithms or semi-supervised learning-based Semantic Segmentation techniques. The results of this study can be applied to improve the speed of the existing map update process.\n",
      "합성곱 신경망 및 영상처리 기법을 활용한피부 모공 등급 예측 시스템 2022 ['Skin', 'Pore', 'Image processing', 'CNN', 'Prediction'] 본 논문은 사용자들에 의해 촬영된 피부이미지를 가공하여 데이터 세트를 구축하고, 제안한 영상처리 기법에 의해 모공 특징이미지를 생성하여, CNN(Convolution Neural Network) 모델 기반의 모공 상태 등급 예측 시스템을 구현한다. 본 논문에서 활용하는피부이미지 데이터 세트는, 피부미용 전문가의 육안 분류 기준에 근거하여, 모공 특징에 대한 등급을 라벨링 하였다. 제안한 영상처리 기법을 적용하여 피부이미지로 부터 모공 특징 이미지를 생성하고, 모공 특징 등급을 예측하는 CNN 모델의 학습을 진행하였다.제안한 CNN 모델에 의한 모공 특징은 전문가의 육안 분류 결과와 유사한 예측 결과를 얻었으며, 비교 모델(Resnet-50)에 의한 결과보다 적은 학습시간과 높은 예측결과를 얻었다. 본 논문의 본론에서는 제안한 영상처리 기법과 CNN 적용의 결과에 대해 서술하며, 결론에서는 제안한 방법에 대한 결과와 향후 연구방안에 대해 서술한다. In this paper, we propose a prediction system for skin pore labeling based on a CNN(Convolution NeuralNetwork) model, where a data set is constructed by processing skin images taken by users, and a pore featureimage is generated by the proposed image processing algorithm. The skin image data set was labeled for porecharacteristics based on the visual classification criteria of skin beauty experts. The proposed image processingalgorithm was applied to generate pore feature images from skin images and to train a CNN model that predictspore feature ratings. The prediction results with pore features by the proposed CNN model is similar to expertsvisual classification results, where less learning time and higher prediction results were obtained than the resultsby the comparison model (Resnet-50). In this paper, we describe the proposed image processing algorithm andCNN model, the results of the prediction system and future research plans.\n",
      "A new lightweight network based on MobileNetV3 2022 ['MobileNetV3', 'Real-time image classification', 'Lightweight network', 'Deep convolutional neural network', 'residual structure'] None The MobileNetV3 is specially designed for mobile devices with limited memory and computing power. To reduce the network parameters and improve the network inference speed, a new lightweight network is proposed based on MobileNetV3. Firstly, to reduce the computation of residual blocks, a partial residual structure is designed by dividing the input feature maps into two parts. The designed partial residual structure is used to replace the residual block in MobileNetV3. Secondly, a dual-path feature extraction structure is designed to further reduce the computation of MobileNetV3. Different convolution kernel sizes are used in the two paths to extract feature maps with different sizes. Besides, a transition layer is also designed for fusing features to reduce the influence of the new structure on accuracy. The CIFAR-100 dataset and Image Net dataset are used to test the performance of the proposed partial residual structure. The ResNet based on the proposed partial residual structure has smaller parameters and FLOPs than the original ResNet. The performance of improved MobileNetV3 is tested on CIFAR-10, CIFAR-100 and ImageNet image classification task dataset. Comparing MobileNetV3, GhostNet and MobileNetV2, the improved MobileNetV3 has smaller parameters and FLOPs. Besides, the improved MobileNetV3 is also tested on CPU and Raspberry Pi. It is faster than other networks\n",
      "언어장애인의 스마트스피커 접근성 향상을 위한 개인화된 음성 분류 기법 2022 ['스마트스피커', '언어장애인', '장애인접근성', '개인화된 음성분류기법', '딥러닝', 'smart speaker', 'speech-impaired people', 'disabled accessibility', 'personalized speech classification scheme', 'deep learning'] 음성인식 기술과 인공지능 기술을 기반으로 한 스마트스피커의 보급으로 비장애인뿐만 아니라 시각장애인이나 지체장애인들도 홈 네트워크 서비스를 연동하여 주택의 전등이나 TV와 같은 가전제품을 음성을 통해 쉽게 제어할 수 있게 되어 삶의 질이 대폭 향상되었다. 하지만 언어장애인의 경우 조음장애나 구음장애 등으로 부정확한 발음을 하게 됨으로서 스마트스피커의 유용한 서비스를 사용하는 것이 불가능하다. 본 논문에서는 스마트스피커에서 제공되는 기능 중 일부 서비스를 대상으로 언어장애인이 이용할 수 있도록 개인화된 음성분류 기법을 제안한다. 본 논문에서는 소량의 데이터와 짧은 학습시간으로도 언어장애인이 구사하는 문장의 인식률과 정확도를 높여 스마트스피커가 제공하는 서비스를 실제로 이용할 수 있도록 하는 것이 목표이다. 본 논문에서는 ResNet18 모델을 fine tuning하고 데이터 증강과 one cycle learning rate 최적화 기법을 추가하여 적용하였으며, 실험을 통하여 30개의 스마트스피커 명령어 별로 10회 녹음한 후 3분 이내로 학습할 경우 음성분류 정확도가 95.2% 정도가 됨을 보였다. With the spread of smart speakers based on voice recognition technology and deep learning technology, not only non-disabled people, but also the blind or physically handicapped can easily control home appliances such as lights and TVs through voice by linking home network services. This has greatly improved the quality of life. However, in the case of speech-impaired people, it is impossible to use the useful services of the smart speaker because they have inaccurate pronunciation due to articulation or speech disorders. In this paper, we propose a personalized voice classification technique for the speech-impaired to use for some of the functions provided by the smart speaker. The goal of this paper is to increase the recognition rate and accuracy of sentences spoken by speech-impaired people even with a small amount of data and a short learning time so that the service provided by the smart speaker can be actually used. In this paper, data augmentation and one cycle learning rate optimization technique were applied while fine-tuning ResNet18 model. Through an experiment, after recording 10 times for each 30 smart speaker commands, and learning within 3 minutes, the speech classification recognition rate was about 95.2%.\n",
      "ShortcutFusion++: Optimizing an End-to-End CNN Accelerator for High PE Utilization 2022 ['CNN accelerator', 'Processing element', 'Hardware utilization', 'FPGA', 'YOLO-v3'] None ShorcutFusion [1] is an end-to-end framework that effectively maps many well-known deep neural networks (DNNs), such as MobileNet-v2, EfficientNet-B0, ResNet-50, and YOLO-v3, to a generic CNN accelerator on FPGA. Nevertheless, its processing elements are not fully utilized when supporting various networks, leading to relatively low hardware utilization (e.g., 68.42% for YOLO-v3). This study aimed to enhance the performance of ShortcutFusion and introduce ShortcutFusion++ by proposing two simple but effective techniques for eliminating unnecessary stalls in conventional design. First, the prefetching scheme was re-designed to avoid bubble cycles when feeding data to the PE array. Second, the output buffer was reconstructed to pipeline the operations of PEs and the process of writing output feature maps to off-chip memory. The experimental results show that ShortcutFusion++ achieves a PE utilization of 80.95% for the wellknown object detection network YOLO-v3, outperforming its baseline by 12.53%.\n",
      "위성 및 드론 영상을 이용한 해안쓰레기 모니터링 기법 개발 2022 ['Sentinel-2', 'Drone', 'Multispectral image', 'Deep learning', 'Marine debris'] None This study proposes a marine debris monitoring methods using satellite and drone multispectral images. A multi-layer perceptron (MLP) model was applied to detect marine debris using Sentinel-2 satellite image. And for the detection of marine debris using drone multispectral images, performance evaluation and comparison of U-Net, DeepLabv3+ (ResNet50) and DeepLabv3+ (Inceptionv3) among deep learning models were performed (mIoU 0.68). As a result of marine debris detection using satellite image, the F1-Score was 0.97. Marine debris detection using drone multispectral images was performed on vegetative debris and plastics. As a result of detection, when DeepLabv3+ (Inceptionv3) was used, the most model accuracy, mean intersection over union (mIoU), was 0.68. Vegetative debris showed an F1-Score of 0.93 and IoU of 0.86, while plastics showed low performance with an F1-Score of 0.5 and IoU of 0.33. However, the F1-Score of the spectral index applied to generate plastic mask images was 0.81, which was higher than the plastics detection performance of DeepLabv3+ (Inceptionv3), and it was confirmed that plastics monitoring using the spectral index was possible. The marine debris monitoring technique proposed in this study can be used to establish a plan for marine debris collection and treatment as well as to provide quantitative data on marine debris generation.\n",
      "심층학습 알고리즘을 활용한 인접면 우식 탐지 2022 ['Artificial intelligence', 'Deep learning', 'Proximal caries', 'Primary teeth', 'Intraoral radiography', '.'] 이번 연구는 소아의 인접면 우식을 진단하는데 있어 사용하고 있는 구내방사선 사진에서 심층학습(deep learning) 알고리즘을 활용하여 치아우식을 진단하는 모델의 성능을 평가하고자 하였다.제1유구치와 제2유구치 사이의 인접면이 포함된 500개의 구내방사선 사진을 대상으로 연구를 시행하였다. 치아우식을 진단하는 모델의 학습에는 Resnet50 기반의 인공신경망 모델을 사용하였다. 평가자료군에서 진단모델의 정확도, 민감도, 특이도를 구하고, ROC 곡선을 얻어 AUC 값을 바탕으로 분류 모델의 성능을 평가하였다.학습 모델의 정확도는 0.84, 민감도는 0.74, 특이도는 0.94로 나타났으며 AUC는 0.86으로 나타났다.인공신경망을 기반으로 하는 소아의 구내방사선 사진에서의 인접면 우식의 진단 모델은 비교적 높은 정확도를 보여주었다. 심층학습 모델은 구내방사선 사진상에서 인접면 우식을 진단하는데 있어 향후 치과의사를 보조하는 진단 도구로서 활용될 수 있을 것이다. None\n",
      "Steel Surface Defect Detection using the RetinaNet Detection Model 2022 ['Defect Detection', 'Deep Learning', 'Steel Defect Detection', 'RetinaNet model', 'One-Stage Detector'] None Some surface defects make the weak quality of steel materials. To limit these defects, we advocate a onestage detector model RetinaNet among diverse detection algorithms in deep learning. There are several backbones in the RetinaNet model. We acknowledged two backbones, which are ResNet50 and VGG19. To validate our model, we compared and analyzed several traditional models, one-stage models like YOLO and SSD models and two-stage models like Faster-RCNN, EDDN, and Xception models, with simulations based on steel individual classes. We also performed the correlation of the time factor between one-stage and twostage models. Comparative analysis shows that the proposed model achieves excellent results on the dataset of the Northeastern University surface defect detection dataset. We would like to work on different backbones to check the efficiency of the model for real world, increasing the datasets through augmentation and focus on improving our limitation.\n",
      "반려동물용 자동 사료급식기의 비용효율적 사료 중량 예측을 위한 딥러닝 방법 2022 ['사료급식기', '컴퓨터 비전', '중량 예측', '딥러닝', '합성곱 신경망', 'pet feeder', 'computer vision', 'weight prediction', 'deep learning', 'convolutional neural network'] 최근 IoT 기술의 발달로 외출 중에도 반려동물에 급여하도록 자동 사료급식기가 유통되고 있다. 그러나 자동급식에서 중요한 중량을 측정하는 저울 방식은 쉽게 고장이 나고, 3D카메라 방식은 비용이 든다는 단점이 있으며, 2D카메라 방식 은중량측정의정확도가떨어진다. 특히사료가복합된경우중량측정문제는더욱어려워질수있다. 따라서본연구의 목적은 2D카메라를 사용하면서도 중량을 정확하게 추정할 수 있는 딥러닝 접근법을 제안하는 것이다. 이를 위해 다양한 합성곱 신경망을 이용하였으며, 그중 ResNet101 기반 모델이 3.06 gram의 평균 절대 오차와 3.40%의 평균 절대비 오차를 기록하며 가장 우수한 성능을 보였다. 본 연구의 결과로 사료와 같이 규격화된 물체의 중량을 확보가 용이한 2D 이미지를 통해서만 예측할 필요가 있을 경우 유용한 정보로 활용될 수 있다. None\n",
      "배터리 리드탭 압흔 오류 검출의 딥러닝 기법 적용 2022 ['배터리 리드탭', '압흔 오류 검출', '인공지능', '딥러닝', 'Faster R-CNN', '객체 탐지', 'Battery lead tab', 'Welding error detection', 'Artificial intelligence', 'Deep learning', 'Faster R-CNN', 'Object detection'] 자동차용 배터리 제조공정 가운데 하나인 Tab Welding 공정에서 생산된 제품의 샘플링 인장검사를 대체하기 위해 현재 비전검사기를 개발하여 사용하고 있다. 그러나, 비전검사는 검사 위치 오차 문제와 이를 개선하기 위해 발생하는 비용 문제를 가지고 있다. 이러한 문제점들을 해결하기 위해 최근 딥러닝 기술을 적용하는 사례들이 발생하고 있다. 본 논문도 그런 사례 중 하나로 기존 제품 검사에 딥러닝 기술 중 하나인 Faster R-CNN을 적용하여 그 유용성을 파악하고자 하였다. 기존 비전검사기를 통해 획득한 이미지들을 학습 데이터로 사용하여 Faster R-CNN ResNet101 V1 1024x1024 모델을 사용하여 학습하였다. 검사 기준인 미검률 0%, 과검률 10%의 기준으로 기존 비전검사와 Faster R-CNN 검사결과를 비교 분석하였다. 미검출률은 기존 비전검사에서 34.5%, Faster R-CNN 검사에서 0%였다. 과검출률은 기존 비전검사에서 100%, Faster R-CNN에서 6.9%였다. 결론적으로 자동차용 배터리 리드탭 암흔 오류 검출에 딥러닝 기술이 매우 유용함을 확인할 수 있었다. None\n",
      "Human activity recognition based on wrist PPG via the ensemble method 2022 ['HAR', 'Exercise', 'PPG', 'ECG', 'Classification', 'Ensemble', 'Machine learning', 'Transfer learning'] None Human activity recognition via Electrocardiography (ECG) and Photoplethysmography (PPG) is extensively researched. While ECG requires less filtering and is less prone to disturbance and artifacts, nonetheless, PPG is cheaper and widely available in smart devices, making it a desired alternative. In this study, we explore the employment of the ensemble method with several pre-trained machine learning models namely Resnet50V2, MobileNetV2, and Xception for the classification of wrist PPG data of human activity, in comparison to its ECG counterpart. The study produced promising results with a test classification accuracy of 88.91% and 94.28% for PPG and ECG, respectively.\n",
      "A Manually Captured and Modified Phone Screen Image Dataset for Widget Classification on CNNs 2022 ['Captured Image', 'CNN', 'Deep Learning Dataset', 'Image Classification', 'Object Detection', 'Widget'] None The applications and user interfaces (UIs) of smart mobile devices are constantly diversifying. For example,deep learning can be an innovative solution to classify widgets in screen images for increasing convenience.To this end, the present research leverages captured images and the ReDraw dataset to write deep learningdatasets for image classification purposes. First, as the validation for datasets using ResNet50 and EfficientNet,the experiments show that the dataset composed in this study is helpful for classification according to a widget'sfunctionality. An implementation for widget detection and classification on RetinaNet and EfficientNet is thenexecuted. Finally, the research suggests the Widg-C and Widg-D datasets—a deep learning dataset for identifyingthe widgets of smart devices—and implementing them for use with representative convolutional neuralnetwork models.\n",
      "심전도의 다양한 2차원 변환에 의한 합성곱 신경망기반 개인식별 2022 ['person identification', 'electrocardiogram', 'convolutional neural networks', 'time-frequency transform', 'Short-Time Fourier Transform', 'Fourier Synchrosqueezed Transform'] None In this paper, we propose personal identification method based on Convolutional Neural Networks (CNN) by various two-dimensional (2D) transform of Electrocardiogram (ECG) signals. For this purpose, various 2D time-frequency representation are peformed by Short-Time Fourier Transform (STFT), Fourier Synchrosqueezed Transform (FSST), and Wavelet Synchrosqueezed Transform (WSST) from one-dimensional ECG signals. The individual identification performance is achieved by transfer learning based on the pretrained GoogleNet and ResNet-101. The performance of experimental results are compared by the well-known PTB-ECG database.\n",
      "No-Reference Image Quality Assessment based on Quality Awareness Feature and Multi-task Training 2022 ['Deep Learning', 'No-Reference Image Quality Assessment', 'Multiple Task Learning', 'Score Prediction'] None The existing image quality assessment (IQA) datasets have a small number of samples. Some methods based on transfer learning or data augmentation cannot make good use of image quality-related features. A No Reference (NR)-IQA method based on multi-task training and quality awareness is proposed. First, single or multiple distortion types and levels are imposed on the original image, and different strategies are used to augment different types of distortion datasets. With the idea of weak supervision, we use the Full Reference (FR)-IQA methods to obtain the pseudo-score label of the generated image. Then, we combine the classification information of the distortion type, level, and the information of the image quality score. The ResNet50 network is trained in the pre-train stage on the augmented dataset to obtain more quality-aware pre-training weights. Finally, the fine-tuning stage training is performed on the target IQA dataset using the quality-aware weights to predicate the final prediction score. Various experiments designed on the synthetic distortions and authentic distortions datasets (LIVE, CSIQ, TID2013, LIVEC, KonIQ-10K) prove that the proposed method can utilize the image quality-related features better than the method using only single-task training. The extracted quality-aware features improve the accuracy of the model.\n",
      "Enhanced 3D Residual Network for Human Fall Detection in Video Surveillance 2022 ['Video surveillance', 'fall detection', 'deep learning', 'residual network', '3D CNN'] None In the public healthcare, a computational system that can automatically and efficiently detect and classify falls from a video sequence has significant potential. With the advancement of deep learning, which can extract temporal and spatial information, has become more widespread. However, traditional 3D CNNs that usually adopt shallow networks cannot obtain higher recognition accuracy than deeper networks. Additionally, some experiences of neural network show that the problem of gradient explosions occurs with increasing the network layers. As a result, an enhanced three-dimensional ResNet-based method for fall detection (3D-ERes-FD) is proposed to directly extract spatio-temporal features to address these issues. In our method, a 50-layer 3D residual network is used to deepen the network for improving fall recognition accuracy. Furthermore, enhanced residual units with four convolutional layers are developed to efficiently reduce the number of parameters and increase the depth of the network. According to the experimental results, the proposed method outperformed several state-of-the-art methods.\n",
      "CNN 기반 전이학습을 이용한 뼈 전이가 존재하는 뼈 스캔 영상 분류 2022 ['Deep Learning', 'Computer Vision', 'CNN', 'Transfer Learning', 'Medical Image', 'Bone Scan'] None Whole body bone scan is the most frequently performed nuclear medicine imaging to evaluate bone metastasis in cancer patients. We evaluated the performance of a VGG16-based transfer learning classifier for bone scan images in which metastatic bone lesion was present. A total of 1,000 bone scans in 1,000 cancer patients (500 patients with bone metastasis, 500 patients without bone metastasis) were evaluated. Bone scans were labeled with abnormal/normal for bone metastasis using medical reports and image review. Subsequently, gradient-weighted class activation maps (Grad-CAMs) were generated for explainable AI. The proposed model showed AUROC 0.96 and F1-Score 0.90, indicating that it outperforms to VGG16, ResNet50, Xception, DenseNet121 and InceptionV3. Grad-CAM visualized that the proposed model focuses on hot uptakes, which are indicating active bone lesions, for classification of whole body bone scan images with bone metastases.\n",
      "게이트심장혈액풀검사에서 딥러닝 기반 좌심실 영역 분할방법의 유용성 평가 2022 ['대한방사선과학회(구 대한방사선기술학회)'] None The Cardiac Gated Blood Pool (GBP) scintigram, a nuclear medicine imaging, calculates the left ventricular Ejection Fraction (EF) by segmenting the left ventricle from the heart. However, in order to accurately segment the substructure of the heart, specialized knowledge of cardiac anatomy is required, and depending on the expert s processing, there may be a problem in which the left ventricular EF is calculated differently. In this study, using the DeepLabV3 architecture, GBP images were trained on 93 training data with a ResNet-50 backbone. Afterwards, the trained model was applied to 23 separate test sets of GBP to evaluate the reproducibility of the region of interest and left ventricular EF. Pixel accuracy, dice coefficient, and IoU for the region of interest were 99.32±0.20, 94.65±1.45, 89.89±2.62(%) at the diastolic phase, and 99.26±0.34, 90.16±4.19, and 82.33±6.69(%) at the systolic phase, respectively. Left ventricular EF was calculated to be an average of 60.37±7.32% in the ROI set by humans and 58.68±7.22% in the ROI set by the deep learning segmentation model. (p<0.05) The automated segmentation method using deep learning presented in this study similarly predicts the average human-set ROI and left ventricular EF when a random GBP image is an input. If the automatic segmentation method is developed and applied to the functional examination method that needs to set ROI in the field of cardiac scintigram in nuclear medicine in the future, it is expected to greatly contribute to improving the efficiency and accuracy of processing and analysis by nuclear medicine specialists.\n",
      "합성곱 신경망을 이용한 정사사진 기반 균열 탐지 기법 2022 ['Ortho-image', 'UAV', 'Machine learning', 'Crack detection', 'CNN'] None Visual inspection methods have limitations, such as reflecting the subjective opinions of workers. Moreover, additional equipment is required when inspecting the high-rise buildings because the height is limited during the inspection. Various methods have been studied to detect concrete cracks due to the disadvantage of existing visual inspection. In this study, a crack detection technology was proposed, and the technology was objectively and accurately through AI. In this study, an efficient method was proposed that automatically detects concrete cracks by using a Convolutional Neural Network(CNN) with the Orthomosaic image, modeled with the help of UAV. The concrete cracks were predicted by three different CNN models: AlexNet, ResNet50, and ResNeXt. The models were verified by accuracy, recall, and F1 Score. The ResNeXt model had the high performance among the three models. Also, this study confirmed the reliability of the model designed by applying it to the experiment.\n",
      "필터 다양화를 통한 합성곱 신경망의 표현력 향상 2022 ['Deep learning', 'CNN', 'Feature Representation', 'Filter Diversity', 'Singular Value Decomposition (SVD) Entropy', 'Filter Spreading'] None This paper aims to improve the feature representation by diversifying CNN filters inspired by niche concept in evolution. The singular value decomposition (SVD) entropy based efficient metric for diversity is proposed In the proposed approach, filters are clustered by groups and they are calculated as differences from the center values within the groups, rather than by entire rank based comparison. This provides an effective method for increasing the substantial diversity of filters. Furthermore, the filters with low diversity are adjusted by the diversity spreading framework for better diversity in the reconstruction process. The improvement of the filter representation by performing experiments on CIFAR 10/100 data for VGG16, and ImageNet for ResNet34 is provided. Because there are no similar studies, we compare our results with respect to those of relatively relevant pruning methods in terms of classification performance accuracy as well as the pruned rates and flops.\n",
      "표정 분류에 기반한 감정 인식을 위한 열화상 데이터베이스의 유용성 평가 2022 ['thermal face image', 'facial expression classification', 'emotion recognition', 'CNN architecture', 'database performance'] None Facial expression is an important part of human communication and is an element that helps to understand other people’s intentions. Recently, as a complementary solution in the field of emotion recognition, interest in thermal imaging is increasing as an alternative means to compensate for the shortcomings of visible light imaging. In this paper, thermal image data was acquired by itself and a database was established in which only facial areas necessary for emotional recognition were extracted separately. Verification accuracy and learning time were analyzed using the existing CNN architecture to confirm whether the built database can be used to classify facial expressions for emotion recognition. As a result of analysis through CNN network, ResNet-18 showed a verification accuracy of up to 81.28%, and on average, it showed a verification accuracy of 68.13%. Through this, it was confirmed that the self-built thermal imaging database is useful for emotional recognition research.\n",
      "A Deep Learning-Based Compact Weighted Binary Classification Technique to Discriminate between Targets and Clutter in SAR Images 2022 ['Automatic Target Detection (ATD)', 'Deep Learning (DL)', 'Machine Learning (ML)', 'Synthetic Aperture Radar (SAR).'] None The proposed approach is a deep learning-based compact weighted binary classification (DL-CWBC) method to discriminate between targets and clutter in synthetic aperture radar (SAR) images. A new modified cross-entropy error function is proposed to improve the probability of detection by controlling the rate of false alarms (FAs). The unique feature of a CWBC algorithm is reducing the FA rate and maximizing the probability of target detection without missing any target. For pre-processing, targets and clutter are detected through a constant false alarm rate (CFAR) as a conventional detection algorithm. These are then manually divided into two classes. The classified targets and clutter were trained through a ResNet-101 network. There is a trade-off between the minimization of the FA rate and the maximization of the detection probability for targets of interest (TOIs). The weighted coefficient of the modified cross-entropy error function tries to maximize the performance of this trade-off. In addition, the proposed approach enables us not to miss any targets by an extreme distinction decision. Above all, the DL-CWBC algorithm performs very well despite its simplicity.\n",
      "관개용수로 CCTV 이미지를 이용한 CNN 딥러닝 이미지 모델 적용 2022 ['Image classification', 'image segmentation', 'CCTV images', 'irrigation canal'] None A more accurate understanding of the irrigation water supply is necessary for efficient agricultural water management. Although we measure water levelsin an irrigation canal using ultrasonic water level gauges, some errors occur due to malfunctions or the surrounding environment. This study aims toapply CNN (Convolutional Neural Network) Deep-learning-based image classification and segmentation models to the irrigation canal’s CCTV(Closed-Circuit Television) images. The CCTV images were acquired from the irrigation canal of the agricultural reservoir in Cheorwon-gun,Gangwon-do. We used the ResNet-50 model for the image classification model and the U-Net model for the image segmentation model. Using theNatural Breaks algorithm, we divided water level data into 2, 4, and 8 groups for image classification models. The classification models of 2, 4, and8 groups showed the accuracy of 1.000, 0.987, and 0.634, respectively. The image segmentation model showed a Dice score of 0.998 and predictedwater levels showed R2of 0.97 and MAE (Mean Absolute Error) of 0.02 m. The image classification models can be applied to the automaticgate-controller at four divisions of water levels. Also, the image segmentation model results can be applied to the alternative measurement for ultrasonicwater gauges. We expect that the results of this study can provide a more scientific and efficient approach for agricultural water management.\n",
      "다수 조명의 채널별 융합을 이용한 CNN 기반 머신 비전 분류기 2022 ['Semiconductor', 'Auto Visual Inspection', 'Deep Learning', 'CNN', '.'] None .\n",
      "금속 음영이 포함된 CBCT 영상에서 딥러닝을 이용한 해부학적 구조물의 다중 클래스 분할 방법 2022 ['Deep learning', 'Anatomical structure segmentation', 'Metal artifacts', 'U-Net', 'Tversky loss'] None In order to perform preoperative surgical planning, accurate segmentation of anatomical structures in cone-beam computed tomography (CBCT) images is required. However, this image segmentation is often impeded by metal artifacts, and it takes a lot of time due to morphological variability in patients. In this paper, we proposed a deep learning based automatic multi-class segmentation method for anatomical structures in CBCT images containing metal artifacts. Four U-Net based deep learning models were used for anatomical structure segmentation. Each deep learning model was constructed by changing the encoder of U-Net architecture to the backbones (DenseNet121, VGGNet16, ResNet101, and EfficienNetB4). For training and testing our method, we used 20744 CBCT images containing metal artifacts from 30 patient datasets. Experimental results show that the segmentation performances of the mandible, midfacial bone, mandibular canal, and maxillary sinus were achieved F1 scores of 0.912±0.070, 0.880±0.080, 0.687±0.265, and 0.954±0.063 using DenseNet121 with Tversky loss, respectively. Furthermore, our method was able to perform robust and accurate segmentation of anatomical structures in CBCT images containing metal artifacts.\n",
      "CNN을 이용한 Al 6061 압출재의 표면 결함 분류 연구 2022 ['Convolution Neural Network', 'Surface Defect', 'Aluminum alloy', 'Extrusion', 'Deep Learning', 'Data Augmentation'] None Convolution Neural Network(CNN) is a class of deep learning algorithms and can be used for image analysis. In particular, it has excellent performance in finding the pattern of images. Therefore, CNN is commonly applied for recognizing, learning and classifying images. In this study, the surface defect classification performance of Al 6061 extruded material using CNN-based algorithms were compared and evaluated. First, the data collection criteria were suggested and a total of 2,024 datasets were prepared. And they were randomly classified into 1,417 learning data and 607 evaluation data. After that, the size and quality of the training data set were improved using data augmentation techniques to increase the performance of deep learning. The CNN-based algorithms used in this study were VGGNet-16, VGGNet-19, ResNet-50 and DenseNet-121. The evaluation of the defect classification performance was made by comparing the accuracy, loss, and learning speed using verification data. The DenseNet-121 algorithm showed better performance than other algorithms with an accuracy of 99.13% and a loss value of 0.037. This was due to the structural characteristics of the DenseNet model, and the information loss was reduced by acquiring information from all previous layers for image identification in this algorithm. Based on the above results, the possibility of machine vision application of CNN-based model for the surface defect classification of Al extruded materials was also discussed.\n",
      "Cycle-accurate NPU 시뮬레이터 및 데이터 접근 방식에 따른 NPU 성능평가 2022 ['Neural Processing Unit', 'Convolutional Neural Network', 'Data Reuse', 'FIFO', 'Interleaved Memory'] None Currently, there are increasing demands for applying deep neural networks (DNNs) in the embedded domain such as classification and object detection. The DNN processing in embedded domain often requires custom hardware such as NPU for acceleration due to the constraints in power, performance, and area. Processing DNN models requires a large amount of data, and its seamless transfer to NPU is crucial for performance. In this paper, we developed a cycle-accurate NPU simulator to evaluate diverse NPU microarchitectures. In addition, we propose a novel technique for reducing the number of memory accesses when processing convolutional layers in convolutional neural networks (CNNs) on the NPU. The main idea is to reuse data with memory interleaving, which recycles the overlapping data between previous and current input windows. Data memory interleaving makes it possible to quickly read consecutive data in unaligned locations. We implemented the proposed technique to the cycle-accurate NPU simulator and measured the performance with LeNet-5, VGGNet-16, and ResNet-50. The experiment shows up to 2.08x speedup in processing one convolutional layer, compared to the baseline.\n",
      "Retinal Disease Identification using Upgraded CLAHE filter and Transfer Convolution Neural Network 2022 ['Retinal disease', 'Retinal fundus images', 'Convolution neural network (CNN)', '(CLAHE) filter'] None Retinal tissue plays a crucial part in human vision. Infections of retinal tissue and delayed treatment or untreated infection could lead to loss of vision. Additionally, the diagnosis is prone to errors when huge dataset is involved. Therefore, a fully automated model of identification of retinal disease is proposed to reduce human interaction while retaining its high accuracy classification results. This paper introduces an enhanced design of a fully automatic multi-class retina diseases prediction system to assist ophthalmologists in making speedy and accurate investigation. Retinal fundus images, which have been used in this study, were downloaded from the stare website (157 images from five classes: BDR, CRVO, CNV, PDR, and Normal). The five files were categorized according to their annotations conducted by the experienced specialists. The categorized images were first processed with the proposed upgraded contrast-limited adaptive histogram filter for image brightness enhancement, noise reduction, and intensity spectrum normalization. The proposed model was designed with transfer learning method and the fine-tuned pre-trained RESNET50. Eventually, the proposed framework was examined with performance evaluation parameters, recorded a classification rate with 100% sensitivity, 100% specificity, and 100% accuracy. The performance of the proposed model showed a magnificent superiority as compared to the state-of-the-art studies.\n",
      "Analysis of Weights and Feature Patterns in Popular 2D Deep Neural Networks Models for MRI Image Classification 2022 ['Deep Neural Network', 'Activation Channels', 'MRI', 'Fully Connected Layer', 'Weights Correlation.'] None A deep neural network (DNN) includes variables whose values keep on changing with the training process until it reaches the final point of convergence. These variables are the co-efficient of a polynomial expression to relate to the feature extraction process. In general, DNNs work in multiple ‘dimensions’ depending upon the number of channels and batches accounted for training. However, after the execution of feature extraction and before entering the SoftMax or other classifier, there is a conversion of features from multiple N-dimensions to a single vector form, where ‘N’ represents the number of activation channels. This usually happens in a Fully connected layer (FCL) or a dense layer. This reduced 2D feature is the subject of study for our analysis. For this, we have used the FCL, so the trained weights of this FCL will be used for the weight-class correlation analysis. The popular DNN models selected for our study are ResNet-101, VGG-19, and GoogleNet. These models’ weights are directly used for fine-tuning (with all trained weights initially transferred) and scratch trained (with no weights transferred). Then the comparison is done by plotting the graph of feature distribution and the final FCL weights.\n",
      "Representative Batch Normalization for Scene Text Recognition 2022 ['Scene text recognition', 'deep learning', 'Representative Batch Normalization', 'Feature representation', 'Feature enhancement'] None Scene text recognition has important application value and attracted the interest of plenty of researchers. At present, many methods have achieved good results, but most of the existing approaches attempt to improve the performance of scene text recognition from the image level. They have a good effect on reading regular scene texts. However, there are still many obstacles to recognizing text on low-quality images such as curved, occlusion, and blur. This exacerbates the difficulty of feature extraction because the image quality is uneven. In addition, the results of model testing are highly dependent on training data, so there is still room for improvement in scene text recognition methods. In this work, we present a natural scene text recognizer to improve the recognition performance from the feature level, which contains feature representation and feature enhancement. In terms of feature representation, we propose an efficient feature extractor combined with Representative Batch Normalization and ResNet. It reduces the dependence of the model on training data and improves the feature representation ability of different instances. In terms of feature enhancement, we use a feature enhancement network to expand the receptive field of feature maps, so that feature maps contain rich feature information. Enhanced feature representation capability helps to improve the recognition performance of the model. We conducted experiments on 7 benchmarks, which shows that this method is highly competitive in recognizing both regular and irregular texts. The method achieved top1 recognition accuracy on four benchmarks of IC03, IC13, IC15, and SVTP .\n",
      "1-D PE 어레이로 컨볼루션 연산을 수행하는 저전력 DCNN 가속기 2022 ['FPGA', 'Deep Convolutional Neural Network', 'Accelerator', 'Processing Element', 'Data Reuse'] None In this paper, we propose a novel method of performing convolutional operations on a 2-D Processing Element(PE) array. The conventional method [1] of mapping the convolutional operation using the 2-D PE array lacks flexibility and provides low utilization of PEs. However, by mapping a convolutional operation from a 2-D PE array to a 1-D PE array, the proposed method can increase the number and utilization of active PEs. Consequently, the throughput of the proposed Deep Convolutional Neural Network(DCNN) accelerator can be increased significantly.  Furthermore, the power consumption for the transmission of weights between PEs can be saved. Based on the simulation results, the performance of the proposed method provides approximately 4.55%, 13.7%, and 2.27% throughput gains for each of the convolutional layers of AlexNet, VGG16, and ResNet50 using the DCNN accelerator with a (weights size) x (output data size) 2-D PE array compared to the conventional method. Additionally the proposed method provides approximately 63.21%, 52.46%, and 39.23% power savings.\n",
      "ISAR 영상 기반 해상표적 식별을 위한 인공지능 연구 2022 ['Artificial Intelligence', 'Inverse Synthetic Aperture Radar', 'Radar Image', 'Maritime Target'] None Artificial intelligence is driving the Fourth Industrial Revolution and is in the spotlight as a general-purpose technology. As the data collection from the battlefield increases rapidly, the need to us artificial intelligence is increasing in the military, but it is still in its early stages. In order to identify maritime targets, Republic of Korea navy acquires images by ISAR(Inverse Synthetic Aperture Radar) of maritime patrol aircraft, and humans make out them. The radar image is displayed by synthesizing signals reflected from the target after radiating radar waves. In addition, day/night and all-weather observations are possible. In this study, an artificial intelligence is used to identify maritime targets based on radar images. Data of radar images of 24 maritime targets in Republic of Korea and North Korea acquired by ISAR were pre-processed, and an artificial intelligence algo- rithm(ResNet-50) was applied. The accuracy of maritime targets identification showed about 99%. Out of the 81 warship types, 75 types took less than 5 seconds, and 6 types took 15 to 163 seconds.\n",
      "샴 네트워크 기반의 적은 데이터셋을 이용한 유사한 형태의 특정 객체 인식 연구 2022 ['Object Recognition', 'Deep Learning', 'Few-shot Learning', 'Siamese Network', 'Convolutional Neural Network', '.'] None .\n",
      "Fundus Photograph Discrimination Using Transfer Learning over Limited Computing Power Environment 2022 ['AI', 'CNN', 'Fundus photograph', 'ImageNet', 'OPhthalmoloscopy', 'Transfer learning'] None In this paper, we demonstrates a reliable and efficient approach to detect eye-related disease with automated fundus screening using convolutional neural network (CNN) and transfer learning that counteracts to insufficient annotated data set and image domain shifts. The weight values learned from the data sets can be used as initial parameters for the other desired neural networks, and additional learning can be conducted on top of the pre-learned model, called transfer learning. It is a particularly useful method when the number of data sets is and small over limited computing power environment. Four different fundus image data sets, image domains such as ethnicity of the target and equipment which the fundus photograph was captured with, were used for the validation. The data sets were annotated by ophthalmologists as healthy, abnormal, or diabetic retinopathy. The ResNet-18 model, pre-trained with ImageNet data set of 1.2 million images of 1000 daily routine objects, were used for transfer learning. The pre-trained model were modified and additionally trained to learn features from the fundus images, and were validated with separate test sets. Given limited quantity of fundus photograph data set and various image domains, the deep learning models can yield robust ophthalmological performance in discriminating pathologies in the eyes. In spite of the simplicity, this study illustrates the capability of transfer learning and suggests pragmatic and practical approach to varied medical settings with fluctuating status of data maintenance and different image domains.\n",
      "Safety monitoring system of personal mobility driving using deep learning 2022 ['deep learning', 'personal mobility', 'short-time Fourier transform', 'wavelet transform', 'convolutional neural networks'] None Although the e-scooter sharing service market is growing as a representative last-mile mobility, the accident rate is increasing proportionally as the number of users increases. This study proposes a deep learning-based personal mobility driver monitoring system that detects inattentive driving by classifying vibration data transmitted to the e-scooter when the driver fails to concentrate on driving. First, the N-back task technique is used. The driver was stimulated by external visual and auditory factors to generate a cognitive load, and vibration data were collected through a six-axis sensor. Second, the generated vibration data were pre-processed using short-time Fourier transform and wavelet transform (WT) and then converted into an image (spectrogram). Third, four multimodal convolutional neural networks such as LeNet-5, VGG16, ResNet50, and DenseNet121 were constructed and their performance was compared to find the best architecture. Experimental results show that multimodal DenseNet121 with WT can accurately classify safe, slightly anxious, and very anxious driving conditions. The proposed model can be applied to real-time monitoring and warning systems for sharing service providers and used as a basis for insurance and legal action in the case of accidents.\n",
      "Multiple butterfly recognition based on deep residual learning and image analysis 2022 ['butterfly  identification', 'deep  residual  learning', 'image  recognition', 'multiple  species recognition'] None Insect recognition is crucial for taxonomy. It helps researchers to process tremendous and various ecology data. Most studies focus on fine-tuning the deep learning network or altering the algorithm to enhance the identification accuracy, and some useful tools have been generated with these methods. This study focuses on the influence of image data on the recognition model. The single data set source of the existing automated identification tools is relatively simple, and the competition-based data set released only focuses on evaluating the model at present.For the first time, this article integrates butterfly image data sets from multiple sources, covered illustrated books, and popular butterfly science websites. The image types include standard specimen images, illustrated book scan images and camera shots. In addition, these images included not only fixed poses, but also various other images of butterflies in natural poses. The size of these images is also various. The testing data set is new data that does not belong to the training set, which also verifies the generalizability of the model, indicating that in practical applications this model can identify new images. This testing method is a breakthrough compared to the previous work. We designed different data sets using the ResNet18 network to train a classifier, which achieves a validation accuracy of 86% in the end of the analysis.By adjusting the data sets, the accuracy changes as well. This study provides a method to recognize hundreds of butterfly species and analyzes the testing progress from the point of view of data. It is the first to combine butterflies from multiple countries in a single data set, with a recognition accuracy that outperforms previous experiments, to the best of our knowledge. We further analyze the testing results of butterfly recognition at the family and genus level. We perform two more experiments to demonstrate the model in the case of similar species or genus.\n",
      "Beta and Alpha Regularizers of Mish Activation Functions for Machine Learning Applications in Deep Neural Networks 2022 ['Neural Network', 'Machine Learning Applications', 'α and β Regularizers', 'Activation Function.'] None A very complex task in deep learning such as image classification must be solved with the help of neural networks and activation functions. The backpropagation algorithm advances backward from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged, as a result, the gradient descent never converges to the optimum. We propose a two-factor non-saturating activation functions known as Bea-Mish for machine learning applications in deep neural networks. Our method uses two factors, beta (β) and alpha (α), to normalize the area below the boundary in the Mish activation function and we regard these elements as Bea. Bea-Mish provide a clear understanding of the behaviors and conditions governing this regularization term can lead to a more principled approach for constructing better performing activation functions. We evaluate Bea-Mish results against Mish and Swish activation functions in various models and data sets. Empirical results show that our approach (Bea-Mish) outperforms native Mish using SqueezeNet backbone with an average precision (AP50val) of 2.51% in CIFAR-10 and top-1accuracy in ResNet-50 on ImageNet-1k. shows an improvement of 1.20%.\n",
      "디지털트윈 구축을 위한 3차원 공간정보 폐색영역 검출 및 복원 기술개발 2022 ['Digital Twin', '3D Modeling', 'Occlusion Area', 'Detection', 'In-paining'] 디지털트윈의 중요한 요소는 실제 객체에 대한 폐색영역이 없는 실감형 공간정보를 구축하고 제공하여야 한다. 하지만 3차원 공간정보 모델링에서는 촬영 각도와 촬영 시기로 인해 필연적으로 폐색영역이 발생한다. 이에 최신 기술인 인공지능 기술을 이용한 특정객체의 검출과 이를 제거하고 복원할 수 있는 기술을 구현하고자 한다. 연구에서는 인공지능 모델 중에서 ResNet 알고리즘을 사용하 폐색 유발 객체를 자동으로 검출하고, 텍스처링 영상에서 검출된 폐색영역을 삭제하여 특정 알고리즘을 적용하여 해당 건물의 텍스처링 영상에서 폐색영역을 복원하는 복원 기술을 개발하였다. 실험데이터는 건물 모델링에서 가장 많은 폐색을 유발하는 가로수를 대상으로 최신모델인 ResNet 학습모델을 사용하여 폐색영역을 유발하는 가로수 객체 데이터셋을 만들고 자동으로 검출하였으며 그 결과를 분석하였다. 연구의 결과로는 ResNet 알고리즘을 이용하여 비정형 데이터인 가로수를 검출할 수 있으며, DeepFillv2 알고리즘을 이용하여 가로수를 제거하고 인접 픽셀을 이용하여 상가와 아파트 텍스처링 영상을 복원하였다. An important element of a digital twin is to model and provide realistic spatial information without an occluded area for real objects. In 3D spatial information modeling, however, an occluded area inevitably occurs due to the shooting angle and shooting time. In this regard, this study implemented a technology that can detect a specific object using the latest technology, AI, and remove and in-paint it. In this study, the occlusion-causing object was automatically detected using the ResNet algorithm and the occlusion area in-painting in the texturing image. For the experimental data, the ResNet algorithm was applied to street trees that cause the most occlusion in building modeling to produce a street tree dataset and automatically detected and analyzed the results. Street trees, which are unstructured data, can be detected using the ResNet algorithm and removed using the DeepFillv2 algorithm, producing texturing images of shops and apartments that are restored using the adjacent pixels.\n",
      "개선된 Deep Residual Learning 및 물리 모델 데이터를 이용한 잡음 조건에서의 자동차 샤시 결함 진단 2022 ['Deep Learning(딥러닝)', 'Vehicle Chassis System(차량 샤시 시스템)', 'Physics Model(물리 모델)', 'ResNet(심층 잔차 신경망)', 'DenseNet(밀집 연결 합성곱 네크워크)', 'FFT(빠른 푸리에 변환)', 'Domain Adaption(도메인 적응)', 'SNR(신호 대 잡음비)'] None For autonomous vehicles, technology that monitors the state of the vehicle and detects a failure using sensor data is receiving increasing attention. The purpose of this study is to determine the type and location of faults of a vehicle chassis system under noisy conditions using acceleration data and deep learning. Because there is a limit in the acquisition of specific defect data from a real vehicle, normal and defect data were obtained using a vehicle physics model that considers various vehicle speeds, vehicle-to-vehicle variations and road changes. We proposed DNI-ResNet (DenseNet Inspired ResNet), which applied the advantages of DenseNet to ResNet, and used it to determine the type and location of defects occurring in the rubber of the vehicle chassis system. Additionally, the domain adaptation ability of the proposed method was verified with various vehicle speed and new types of defects.\n",
      "Deep learning-guided attenuation correction in the image domain for myocardial perfusion SPECT imaging 2022 ['SPECT', 'myocardial perfusion imaging', 'quantification', 'attenuation correction', 'deep learning'] None We investigate the accuracy of direct attenuation correction (AC) in the image domain for myocardial perfusion SPECT (single-photon emission computed tomography) imaging (MPI-SPECT) using residual (ResNet) and UNet deep convolutional neural networks. MPI-SPECT 99mTc-sestamibi images of 99 patients were retrospectively included. UNet and ResNet networks were trained using non-attenuation-corrected SPECT images as input, whereas CT-based attenuation-corrected (CT-AC) SPECT images served as reference. Chang’s calculated AC approach considering a uniform attenuation coefficient within the body contour was also implemented. Clinical and quantitative evaluations of the proposed methods were performed considering SPECT CT-AC images of 19 subjects (external validation set) as reference. Image-derived metrics, including the voxel-wise mean error (ME), mean absolute error, relative error, structural similarity index (SSI), and peak signal-to-noise ratio, as well as clinical relevant indices, such as total perfusion deficit (TPD), were utilized. Overall, AC SPECT images generated using the deep learning networks exhibited good agreement with SPECT CT-AC images, substantially outperforming Chang’s method. The ResNet and UNet models resulted in an ME of −6.99 ± 16.72 and −4.41 ± 11.8 and an SSI of 0.99 ± 0.04 and 0.98 ± 0.05, respectively. Chang’s approach led to ME and SSI of 25.52 ± 33.98 and 0.93 ± 0.09, respectively. Similarly, the clinical evaluation revealed a mean TPD of 12.78 ± 9.22% and 12.57 ± 8.93% for ResNet and UNet models, respectively, compared to 12.84 ± 8.63% obtained from SPECT CT-AC images. Conversely, Chang’s approach led to a mean TPD of 16.68 ± 11.24%. The deep learning AC methods have the potential to achieve reliable AC in MPI-SPECT imaging.\n",
      "Detection of fake news using deep learning CNN–RNN based methods 2022 ['Fake news detection', 'Deep learning', 'CNN', 'Bidirectional LSTM', 'ResNet'] None Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets.\n",
      "Preliminary study of artificial intelligence-based fuel-rod pattern analysis of low-quality tomographic image of fuel assembly 2022 ['Single-photon emission computed', 'tomography', 'Monte Carlo', 'Nuclear fuel assembly', 'Artificial intelligence', 'VGG', 'GoogLeNet', 'ResNet'] None Single-photon emission computed tomography is one of the reliable pin-by-pin verification techniques for spent-fuel assemblies. One of the challenges with this technique is to increase the total fuel assembly verification speed while maintaining high verification accuracy. The aim of the present study, therefore, was to develop an artificial intelligence (AI) algorithm-based tomographic image analysis technique for partial-defect verification of fuel assemblies. With the Monte Carlo (MC) simulation technique, a tomographic image dataset consisting of 511 fuel-rod patterns of a 3  3 fuel assembly was generated, and with these images, the VGG16, GoogLeNet, and ResNet models were trained. According to an evaluation of these models for different training dataset sizes, the ResNet model showed 100% pattern estimation accuracy. And, based on the different tomographic image qualities, all of the models showed almost 100% pattern estimation accuracy, even for low-quality images with unrecognizable fuel patterns.This study verified that an AI model can be effectively employed for accurate and fast partial-defect verification of fuel assemblies\n",
      "영상 인식을 위한 딥러닝 모델의 적대적 공격에 대한 백색 잡음 효과에 관한 연구 2022 ['Deep learning', 'Adversarial attack', 'FGSM attack', 'BIM attack', 'CW attack', 'White noise', 'Perturbation'] 본 논문에서는 영상 데이터에 대한 적대적 공격으로부터 생성된 적대적 예제로 인하여 발생할 수 있는 딥러닝 시스템의 오분류를 방어하기 위한 방법으로 분류기의 입력 영상에 백색 잡음을 가산하는 방법을 제안하였다. 제안된 방법은 적대적이든 적대적이지 않던 구분하지 않고 분류기의 입력 영상에 백색 잡음을 더하여 적대적 예제가 분류기에서 올바른 출력을 발생할 수 있도록 유도하는 것이다. 제안한 방법은 FGSM 공격, BIM 공격 및 CW 공격으로 생성된 적대적 예제에 대하여 서로 다른 레이어 수를 갖는 Resnet 모델에 적용하고 결과를 고찰하였다. 백색 잡음의 가산된 데이터의 경우 모든 Resnet 모델에서 인식률이 향상되었음을 관찰할 수 있다. 제안된 방법은 단순히 백색 잡음을 경험적인 방법으로 가산하고 결과를 관찰하였으나 에 대한 엄밀한 분석이 추가되는 경우 기존의 적대적 훈련 방법과 같이 비용과 시간이 많이 소요되는 적대적 공격에 대한 방어 기술을 제공할 수 있을 것으로 사료된다. In this paper we propose white noise adding method to prevent missclassification of deep learning system by adversarial attacks. The proposed method is that adding white noise to input image that is benign or adversarial example. The experimental results are showing that the proposed method is robustness to 3 adversarial attacks such as FGSM attack, BIN attack and CW attack. The recognition accuracies of Resnet model with 18, 34, 50 and 101 layers are enhanced when white noise is added to test data set while it does not affect to classification of benign test dataset. The proposed model is applicable to defense to adversarial attacks and replace to time- consuming and high expensive defense method against adversarial attacks such as adversarial training method and deep learning replacing method.\n",
      "뉴스 감성 분석을 이용한 딥러닝 기반 주가 예측에 대한 연구 2022 ['Stock Price Forecasting', 'LSTM', 'ResNet', 'Sentiment Analysis', 'Text Summarization', '주가 예측 모델', '텍스트 요약', 'LSTM', 'ResNet', '감정 분석'] 주가는 거래량, 종가 등과 같은 숫자 기반의 내부적인 요인뿐만 아니라 법, 유행 등 여러 외부요인에 의해 영향을 받는다. 수많은 요인이 주가에 영향을 미치기 때문에 단편적인 주식 데이터만을 이용한 정확한 주가 예측은 매우 어려운 일이다. 특히 기업의 가치는 실제 주식을 거래하는사람들의 인식에 영향을 많이 받기 때문에 특정 기업에 대한 감성 정보가 중요한 요인으로 여겨진다. 본 논문에서는 시간적 특성을 고려한 뉴스 데이터의 감성 분석을 이용한 딥러닝 기반 주가예측 모델을 제안하고자 한다. 주식과 뉴스 데이터, 서로 다른 특성을 가진 2개의 이종 데이터를시간 크기에 따라 통합하여 모델의 입력으로 사용하며, 시간 크기와 감성 지표가 주가 예측에 미치는 영향에 대해 최종적으로 비교 및 분석한다. 또한 우리는 기존 모델과의 비교 실험을 통해제안 모델의 정확성이 개선되었음을 검증한다. Stock prices are influenced by a number of external factors, such as laws and trends, as well as number-based internal factors such as trading volume and closing prices. Since many factors affect stock prices, it is very difficult to accurately predict stock prices using only fragmentary stock data. In particular, since the value of a company is greatly affected by the perception of people who actually trade stocks, emotional information about a specific company is considered an important factor. In this paper, we propose a deep learning-based stock price prediction model using sentiment analysis with news data considering temporal characteristics. Stock and news data, two heterogeneous data with different characteristics, are integrated according to time scale and used as input to the model, and the effect of time scale and sentiment index on stock price prediction is finally compared and analyzed. Also, we verify that the accuracy of the proposed model is improved through comparative experiments with existing models.\n",
      "딥러닝 기반 이미지 필터 간 한국인 표정 분류 성능 비교 2022 ['얼굴 표정 분류', 'CNN', 'VGG', 'ResNet', '가장자리 검출 필터', 'Facial Expression Classification', 'CNN', 'VGG', 'ResNet', 'Edge Detection Filter'] None None\n",
      "GAN을 활용한 인테리어 스타일 변환 모델에 관한 연구 2022 ['CycleGAN', '이미지 변환', '인테리어 스타일', '이미지 렌더링', 'CycleGAN', 'Image Translation', 'Interior Style', 'Image Rendering'] None Recently, demand for designing own space is increasing as the rapid growth of home furnishing market. However, there is a limitation that it is not easy to compare the style between before construction view and after view. This study aims to translate real image into another style with GAN model learned with interior images. To implement this, first we established style criteria and collected modern, natural, and classic style images, and experimented with ResNet, UNet, Gradient penalty concept to CycleGAN algorithm. As a result of training, model recognize common indoor image elements, such as floor, wall, and furniture, and suitable color, material was converted according to interior style. On the other hand, the form of furniture, ornaments, and detailed pattern expressions are difficult to be recognized by CycleGAN model, and the accuracy lacked. Although UNet converted images more radically than ResNet, it was more stained. The GAN algorithm allowed us to represent results within 2 seconds. Through this, it is possible to quickly and easily visualize and compare the front and after the interior space style to be constructed. Furthermore, this GAN will be available to use in the design rendering include interior.\n",
      "비정상심박 검출을 위해 영상화된 심전도 신호를 이용한 비교학습 기반 딥러닝 알고리즘 2022 ['심전도', '딥러닝', 'CNN', '템플릿 군', '비정상심박 검출', 'Electrocardiogram', 'Deep learning', 'Convolutional neural network', 'Template cluster', 'Abnormal beat detection'] 심전도 신호는 개인에 따라 형태와 특징이 다양하므로, 하나의 신경망으로는 분류하기가 어렵다. 주어진 데이터를 직접적으로 분류하는 것은 어려우나, 대응되는 정상 데이터가 있을 경우, 이를 비교하여 정상 및 비정상을 분류하는 것은 상대적으로 쉽고 정확하다. 본 논문에서는 템플릿 군을 이용하여 대표정상심박 정보를 획득하고, 이를 입력심박에 결합함으로써 심박을 분류한다. 결합된 심박을 영상화한 후, 학습 및 분류를 진행하여, 하나의 신경망으로도 다양한 레코드의 비정상심박을 검출이 가능하였다. 특히, GoogLeNet, ResNet, DarkNet 등 다양한 신경망에 대해서도 비교학습 기법을 적용한 결과, 모두 우수한 검출성능을 가졌으며, GoogLeNet의 경우 99.72%의 민감도로, 실험에 사용된 신경망 중 가장 우수한 성능을 가졌음을 확인하였다. Electrocardiogram (ECG) signal's shape and characteristic varies through each individual, so it is difficult to classify with one neural network. It is difficult to classify the given data directly, but if corresponding normal beat is given, it is relatively easy and accurate to classify the beat by comparing two beats. In this study, we classify the ECG signal by generating the reference normal beat through the template cluster, and combining with the input ECG signal. It is possible to detect abnormal beats of various individual’s records with one neural network by learning and classifying with the imaged ECG beats which are combined with corresponding reference normal beat. Especially, various neural networks, such as GoogLeNet, ResNet, and DarkNet, showed excellent performance when using the comparative learning. Also, we can confirmed that GoogLeNet has 99.72% sensitivity, which is the highest performance of the three neural networks.\n",
      "블랙 박스 모델의 출력값을 이용한 AI 모델 종류 추론 공격 2022 ['AI security', 'Privacy', 'Exploratory Attack', 'Inference Attack'] AI 기술이 여러 분야에 성공적으로 도입되는 추세이며, 서비스로 환경에 배포된 모델들은 지적 재산권과 데이터를 보호하기 위해 모델의 정보를 노출시키지 않는 블랙 박스 상태로 배포된다. 블랙 박스 환경에서 공격자들은 모델 출력을 이용해 학습에 쓰인 데이터나 파라미터를 훔치려고 한다. 본 논문은 딥러닝 모델을 대상으로 모델 종류에 대한 정보를 추론하는 공격이 없다는 점에서 착안하여, 모델의 구성 레이어 정보를 직접 알아내기 위해 모델의 종류를 추론하는 공격 방법을 제안한다. MNIST 데이터셋으로 학습된 ResNet, VGGNet, AlexNet과 간단한 컨볼루션 신경망 모델까지 네 가지 모델의 그레이 박스 및 블랙 박스 환경에서의 출력값을 이용해 모델의 종류가 추론될 수 있다는 것을 보였다. 또한 본 논문이 제안하는 방식인 대소 관계 피쳐를 딥러닝 모델에 함께 학습시킨 경우 블랙 박스 환경에서 약 83%의 정확도로 모델의 종류를 추론했으며, 그 결과를 통해 공격자에게 확률 벡터가 아닌 제한된 정보만 제공되는 상황에서도 모델 종류가 추론될 수 있음을 보였다. AI technology is being successfully introduced in many fields, and models deployed as a service are deployed with black box environment that does not expose the model's information to protect intellectual property rights and data. In a black box environment, attackers try to steal data or parameters used during training by using model output. This paper proposes a method of inferring the type of model to directly find out the composition of layer of the target model, based on the fact that there is no attack to infer the information about the type of model from the deep learning model. With ResNet, VGGNet, AlexNet, and simple convolutional neural network models trained with MNIST datasets, we show that the types of models can be inferred using the output values in the gray box and black box environments of the each model. In addition, we inferred the type of model with approximately 83% accuracy in the black box environment if we train the big and small relationship feature that proposed in this paper together, the results show that the model type can be infrerred even in situations where only partial information is given to attackers, not raw probability vectors.\n",
      "산림복원 대상 후보지 추출을 위한 딥러닝 접근법 2022 ['딥러닝', '분할', '산림복원', '항공사진', 'Aerial photo', 'Deep learning', 'Forest restoration', 'Segmentation'] 이미지 인식에 특화된 CNN (Convolutional Neural Network) 기반의 딥러닝 기법은 영상의 항목별 분류가 필요한 다양한 연구에 적용되고있다. 본 연구는 건물, 도로, 논, 밭, 산림, 나지의 6가지 항목을 산림복원 대상 후보지로 정의하고 CNN 기반의 산림복원 대상 후보지 추출 및분류의 최적 방법론을 탐색하였다. 6,640개의 데이터셋을 75:25의 비율로 훈련(4,980개) 및 검증(1,660개)로 구분하여 구축하고 학습에 활용하였다.모델별 정확도는 픽셀정확도(PA), 평균 교차 겹침 결합(Mean IoU)을 이용하여 평가하였다. 픽셀정확도는 90.6%, 평균 교차 겹침 결합은 80.8%로산정되어 Inception-Resnet-v2 모델이 세 모델 중 가장 산림복원 대상 후보지 추출에 뛰어난 정확도를 보였다. 이 결과는 기존의 산림복원 대상후보지 현장조사 혹은 항공사진을 활용한 조사에 비해 시공간적 이점을 가지며, 향후 산림복원 대상지 선정 자료로 적용 가능성이 있다고 판단된다. Many studies using aerial photography and deep learning are increasing for efficient monitoring of the forest resources. We defined six semantic classes of buildings, roads, paddy fields, fields, forests, and barren as forest restoration target sites and explored the optimal methodology for extracting and classifying target sites for forest restoration based on CNN. The datasets (6,640) were divided at a ratio of 75:25 into training (4,980) and validation datasets (1,660). The accuracy of each model was evaluated using pixel accuracy (PA) and Mean Intersection over union (Mean IoU). PA was calculated as 90.6% and Mean IoU was 80.8%, and the Inception-Resnet-v2 model showed excellent accuracy in extracting target sites for forest restoration among the three models. This result has a Spatio-temporal advantage over the existing field survey for forest restoration sites or surveys using aerial photographs by manually. This study will be able to contribute to the classification of forest restoration sites efficiently and support forest restoration.\n",
      "모노 카메라를 사용하는 머신 비전 시스템에서 비정형 결함을 검사하는 CNN을 훈련하기 위한 데이터 증식 방법 2022 ['Machine Vision(머신 비전)', 'Convolutional Neural Networks(합성곱 신경망)', 'Data Augmentation (데이터 증식)', 'Image Segmentation(이미지 영역 분할)'] 최근 합성곱 신경망(CNN)을 머신 비전에 적용함으로써 비정형 결함의 검사에서 우수한 검사 성능을 보이고 있다. 그러나 머신 비전 시스템에서 CNN을 훈련하기 위해 충분한 양의 데이터를 모으고 정리하는 것은 상당한 시간이 필요하다. 이 문제를 해결하기 위해 본 연구는 모노 카메라를 사용하는 머신 비전 시스템에서 CNN을 사용하여 비정형 결함을 검사할 때 사용할 수 있는 데이터 증식 방법을 제안한다. 임의의 패턴에서 비정형의 결함을 검출하기 위해 제작된 DAGM 2007 데이터 중 1번 하위 클래스 데이터를 실험 데이터로 사용하였다. 결함 검출을 위한 CNN은 Mask R-CNN ResNet 50을 사용하였다. 제안하는 방법을 통해 증식한 데이터로 훈련한 CNN이 원본 회색조 이미지로 검증을 수행했을 때 원본 데이터로 훈련한 CNN에 비해 Mask mAP@0.5:0.95 기준 평균 16.73%p 높은 61.38%의 정확도를 보이는 것을 확인했다. 본 연구에서 적용한 데이터 증식 방법을 통해 모노 카메라를 활용하는 머신 비전시스템에서 우수한 성능을 가진 CNN을 훈련할 수 있다. Applying convolutional neural networks (CNN) to machine vision has recently exhibited excellent performance in amorphous defect inspection. However, collecting and annotating sufficient amounts of data to train CNNs in machine vision systems take considerable time. In this study, a data augmentation method is proposed that can be used to inspect amorphous defects using CNNs in machine vision systems using mono cameras. Class 1 subdata of the DAGM 2007 dataset produced to detect defects in arbitrary patterns were used as experimental data. We trained Mask R-CNN ResNet 50 for defect inspection. Through the proposed method, we found that the CNN trained with the augmented data exhibited an average accuracy of 61.38%, which is 16.73%p higher based on Mask mAP@0.5:0.95 compared to the CNN trained with the original data when validation was performed with original grayscale images. By using the data augmentation method applied in this study, CNNs in the machine vision systems using mono cameras can achieve higher inspection performance.\n",
      "Detection and Localization of Coordinated State-and-Topology False Data Injection Attack by Multi-modal Learning 2022 ['Power system cyber security', 'False data injection attack', 'Topology attack', 'Multi-modal learning', 'Data-driven methods'] None False data injection attack (FDIA) is a typical cyber-attack targeting on power system state estimation. By inserting bias into the power system meter measurements, FDIA can cause errors in state estimation and consequently mislead power system operation. Recently, the coordinated state-and-topology FDIA was developed to falsify both the meter measurements and the topology information to conceal a cyber or physical attack. Conventional machine-learning-based detection methods against FDIA depend on the complete observability of power system topology, therefore cannot detect the coordinated state-andtopology FDIA. To address this challenge, we propose a multi-modal learning model based on Graph Auto-Encoder (GAE) and Residual Neural Networks (ResNet) to detect this type of FDIA. First, GAE is used to learn the compact representation of power system topologies. Then, the obtained topological representation is fused with the measurement to obtain the multi-modal features. Third, ResNet is trained as a multi-label classifi er to accept the fused features and generate an array, which can detect attack scenarios and identify the falsifi ed measurements/topologies by coordinated attacks. Comprehensive case studies using IEEE 9-bus, IEEE 57-bus, and IEEE 118-bus systems are presented. The simulation results show that, as compared to single feature methods, the proposed method is more eff ective in detecting coordinated state-and-topology attacks, and more robust in case of FDIA under unseen topological changes in power system operation\n",
      "Tutorial and applications of convolutional neural network models in image classification 2022 ['AdamW', 'convolution neural network', 'deep learning', 'ILSVRC'] None Image classification is a supervised learning problem in the machine learning area. We apply deep learning models to classify image data. In particular, we discuss the advantages of the various types of convolutional neural networks competed in the ImageNet large-scale visual recognition challenge (ILSVRC). First, we provide a review of the CNN models to be applied and explain the details of models to be employed. In general, we keep the core structure of the models in the same form proposed in ILSVRC. We investigate the models via four popular image data sets of various sizes. To compare the performance of the models, we adopt top-1 accuracy, top-5 accuracy, and f1-score as the measures of accuracy. We employ AdamW for an optimizer that is a fast algorithm and often yields precise learning. As a result, we show that the Inception-ResNet-v2 model has excellent performance, and the ResNet is robust to imbalanced data.\n",
      "딥러닝과 다양한 데이터 증강 기법을 활용한 주변국 군용기 기종 분류에 관한 연구 2022 ['Deep Learning(딥러닝)', 'ResNet(레스넷)', 'Fine-Grained Visual Classification(세밀한 이미지 분류)', 'Edge Detection(윤곽선 탐지)', 'Image Data Augmentation(이미지 데이터 증강)'] None The analysis of foreign aircraft appearing suddenly in air defense identification zones requires a lot of cost and time. This study aims to develop a pre-trained model that can identify neighboring military aircraft based on aircraft photographs available on the web and present a model that can determine which aircraft corresponds to based on aerial photographs taken by allies. The advantages of this model are to reduce the cost and time required for model classification by proposing a pre-trained model and to improve the performance of the classifier by data augmentation of edge-detected images, cropping, flipping and so on.\n",
      "병렬 병합 구조를 이용한 기상 데이터의 예측률 향상을 위한 딥러닝 모델 2022 ['deep neural network', 'long short term memory', 'convolution neural network', 'ResNet', 'Inception', '.'] 본 연구는 딥러닝 기본모델인 DNN, LSTM, BiLSTM, 1D-CNN 등으로, 예측의 성능을 향상하기 위해, 중간층을 병렬로 병합하는 구조를 제안하였다. 제안모델 1은 동일한 기본모델을 병렬 병합한 구조이고, 제안모델 2는 서로 다른 모델의 병렬 병합한 구조이다. 각 모델의 평가는 RMSE와 MAE로 10회 실험에 대한 평균값이다. 제안모델 1에서 가장 좋은 예측률을 보인 BiLSTM2의 RMSE는 0.064이다. 제안모델 2의 RMSE는 DC(DNN-CNN), LC(BiLSTM-CNN), DLC(DNN-BiLSTM-CNN) 등 모든 모델이 0.054였다. 이처럼 제안모델 2가 12.8%의 성능 향상을 보였다. 제안모델 2가 서로 다른 모델의 장점을 유지하면서 많은 파라메터로, 예측률을 향상할 수 있게 하는 모델임을 확인할 수 있었다. In this study, using deep learning basic models DNN, LSTM, BiLSTM, and 1D-CNN, to improve prediction performance, we proposed a structure in which hidden layers are merged in parallel. Proposed model 1 is a parallel merging structure of the same basic model, and Proposed model 2 is a parallel merging structure of different models. The evaluation of each model is the average value for 10 experiments with RMSE and MAE. The RMSE of BiLSTM2, which showed the best prediction rate in Proposed Model 1, is 0.064. The RMSE of Proposed Model 2 was 0.054 for all models including DC (DNN-CNN), LC (BiLSTM-CNN), and DLC (DNN-BiLSTM-CNN). As such, Proposed Model 2 showed a performance improvement of 12.8%. It was confirmed that Proposed Model 2 is a model that can improve the prediction rate by using many parameters while maintaining the advantages of different models.\n",
      "음성기반 대화형 서비스 키오스크 설계 및 구현 2022 ['Web', 'Voice recognition', 'Collaborative Filtering', 'Recommendation System', 'Dialogflow', 'REST API', 'ResNet', 'MobileNet'] 최근에 늘어가는 키오스크(KIOSK)의 수요에 따라 불편함을 호소하는 이용자가 많아졌다. 이에 음성 기반 대화형 서비스를 구현하여 손쉽게 메뉴 선택 및 주문을 가능하게 해주는 키오스크를 제작해 웹의 형태로 제공한다. Annyang API와SpeechSynthesis API를 바탕으로 음성 기능을 구현하고 Dialogflow를 통해 사용자의 의도를 파악하는 과정을 Rest API를 기반으로 구현하는 방법에 대해 논한다. 또한 협업 필터링을 기반으로 추천 시스템을 적용하여 기존 키오스크의 낮은 소비자 접근성을 개선하였고, 음성인식 서비스 이용 도중 발생하는 비말로 인한 감염을 예방하기 위해 서비스 이용 전 마스크 착용을 확인하는 기능을 제공한다. As the demand for kiosks increases, more users complain of discomfort. Accordingly, a kiosk that enables easy menu selectionand order by producing a voice-based interactive service is produced and provided in the form of a web. It implements voice functions based on the Annyang API and SpeechSynthesis API, and understands the user’s intention through Dialogflow. And discusshow to implement this process based on Rest API. In addition, the recommendation system is applied based on collaborative filteringto improve the low consumer accessibility of existing kiosks, and to prevent infection caused by droplets during the use of voicerecognition services, it provides the ability to check the wearing of masks before using the service.\n",
      "Healing Game using AI and Art Therapy 2022 ['Healing Game', 'Self-help-therapy', 'Word Cloud', 'Unity ML Agent', 'ResNet'] None Recently, AI technology is developing rapidly every day, and AI technology is being used in various fields. This paper produced a healing game that comforts the mind tired of human relationships and personal reasons in the COVID-19 situation by using AI technology in the art field. In the produced healing game, the effect of self-help-therapy may be obtained, and thus, it is expected that a user may obtain a healing effect in a daily use process through a healing game without the help of a therapist. By statistically analyzing the game review data, a healing game was created by accommodating the public's demands as a healing game, and users were able to obtain a self-help-therapy effect by making a simple storyline and a simple conversation that could interact with AI before the game began.\n",
      "Triplet Loss 기반 딥러닝 모델을 통한 유사 아동 그림 선별 알고리즘 2022 ['이미지 유사성', 'Triplet Loss', '인공지능', '딥러닝', 'CNN', '아동 그림분석', 'Image Similarity', 'Triplet Loss', 'Artificial Intelligence', 'Deep Learning', 'CNN', 'Child drawing analysis'] 본 논문은 유사 아동 그림 선별 알고리즘 생성을 위한 Triplet Loss 기반 딥러닝 모델 설계를 목적으로 한다. 아동 그림들 사이 유사성 측정을 위해서는 동일 클래스에 속하는 그림 간 특징 벡터의 거리는 가까워야 하고 다른 클래스 간 특징 벡터의 거리는 멀어져야 한다. 따라서, 본 연구에서는 클래스 수가 많아지는 경우에 이미지 유사성 측정에 이점을 지닌 Triplet Loss와 잔여 네트워크(ResNet)를 결합한 딥러닝 모델을 구축하여 유사 아동 그림 선별 알고리즘을 생성하였다. 결론적으로 본 모델을 활용한 유사 아동 그림 선별 알고리즘을 통해 대상 아동 그림과 다른 그림 간의 유사성을 측정하고 유사성이 높은 그림을 선별할 수 있다. None\n",
      "단일 파노라마 입력의 실내 공간 레이아웃 복원 모델 경량화 2022 ['Room Layout Estimation', 'Neural Architecture Search', 'Lightweight Deep Learning', '.'] None .\n",
      "Genetic Algorithm-Based Structure Reduction for Convolutional Neural Network 2022 ['Structure reduction', 'Convolutional neural network', 'Genetic algorithm', 'Knowledge distillation'] None Due to the heavy computational burden, deep models of embedded and mobile systems inevitably require network reduction with minimum performance degradation. Pruning method is mainly used to reduce the model by removing some fi lters only within the layer without changing the structure. Some methods for structural reduction of models are far from optimization.We propose a structure reduction method using a genetic algorithm to optimize the removal of reducible layers. Knowledge distillation is carried out to recover the resultant network. We evaluate our method for ResNet on two image classifi cation datasets, CIFAR-10 and CIFAR-100. Experiments show that our method performs a signifi cant improvement over other state-of-the-art methods.\n",
      "Classification of dog skin diseases using deep learning with images captured from multispectral imaging device 2022 ['Deep learning', 'Dog skin disease', 'Multispectral image', 'Dermatosis'] None Background Dog-associated infections are related to more than 70 human diseases. Given that the health diagnosis of a dog requires expertise of the veterinarian, an artificial intelligence model for detecting dog diseases could significantly reduce time and cost required for a diagnosis and efficiently maintain animal health.Objective We collected normal and multispectral images to develop classification model of each three dog skin diseases (bacterial dermatosis, fungal infection, and hypersensitivity allergic dermatosis). The single models (normal image- and multispectral image-based) and consensus models were developed used to four CNN model architecture (InceptionNet, ResNet, DenseNet, MobileNet) and select well-performed model.Results For single models, such as normal image- or multispectral image-based model, the best accuracies and Matthew’s correlation coefficients (MCCs) for validation data set were 0.80 and 0.64 for bacterial dermatosis, 0.70 and 0.36 for fungal infection, and 0.82 and 0.47 for hypersensitivity allergic dermatosis. For the consensus models, the best accuracies and MCCs for the validation set were 0.89 and 0.76 for the bacterial dermatosis data set, 0.87 and 0.63 for the fungal infection data set, and 0.87 and 0.63 for the hypersensitivity allergic dermatosis data set, respectively, which supported that the consensus models of each disease were more balanced and well-performed.Conclusions We developed consensus models for each skin disease for dogs by combining each best model developed with the normal and multispectral images, respectively. Since the normal images could be used to determine areas suspected of lesion of skin disease and additionally the multispectral images could help confirming skin redness of the area, the models achieved higher prediction accuracy with balanced performance between sensitivity and specificity.\n",
      "머신러닝 기반 금속외관 결함 검출 비교 분석 2022 ['금속 외관', '결함 검출', '머신러닝', '합성곱신경망', 'Metal Surface', 'Defect Detection', 'Machine Learning', 'Convolutional Neural Network'] 최근 스마트팩토리와 인공지능 기술의 수요 증가로 인해 다양한 분야에서 인공지능 기술을 적용하는 연구가 진행되고 있다. 결함 검사 분야에서도 인공지능 알고리즘을 도입하기 위한 노력을 기울이고 있다. 특히, 금속 외관의 결함을 검출하는 연구는 다른 소재(목재, 플라스틱, 섬유 등)의 결함을 검출하는 연구에 비해 많은 연구가 이루어지고 있다. 본 논문에서는 머신러닝 기법(서포터 벡터 머신(SVM: Support Vector Machine), 소프트맥스 회귀(Softmax Regression), 결정 트리(Decesion Tree))과 차원 축소 알고리즘(주성분 분석(PCA: Principal Component Analysis), 오토인코더(AutoEncoder))의 9가지 조합과 2가지 합성곱신경망(CNN: Convolution Neural Network) 기법(자체 알고리즘, ResNet)의 금속 외관의 결함 분류 성능 및 속도를 비교하고 분석하는 연구를 수행하고자 한다. 두 종류의 학습 데이터셋((i) 공용 데이터셋, (ii) 실측 데이터셋)에 대한 실험을 통해 각 데이터셋에 대한 성능 및 속도를 비교 분석하고, 가장 효율적인 알고리즘을 찾아낸다. None\n",
      "A Novel Transfer Learning-Based Algorithm for Detecting Violence Images 2022 ['Deep learning', 'Image classification', 'Pre-training', 'Violence images'] None Violence in the Internet era poses a new challenge to the current counter-riot work, and according to research and analysis, most of the violent incidents occurring are related to the dissemination of violence images. The use of the popular deep learning neural network to automatically analyze the massive amount of images on the Internet has become one of the important tools in the current counter-violence work. This paper focuses on the use of transfer learning techniques and the introduction of an attention mechanism to the residual network (ResNet) model for the classification and identification of violence images. Firstly, the feature elements of the violence images are identified and a targeted dataset is constructed; secondly, due to the small number of positive samples of violence images, pre-training and attention mechanisms are introduced to suggest improvements to the traditional residual network; finally, the improved model is trained and tested on the constructed dedicated dataset. The research results show that the improved network model can quickly and accurately identify violence images with an average accuracy rate of 92.20%, thus effectively reducing the cost of manual identification and providing decision support for combating rebel organization activities.\n",
      "Deep Learning for Weeds’ Growth Point Detection based on U-Net 2022 ['artificial intelligence', 'growth point', 'deep learning', 'semantic graphics', 'U-Net'] None Weeds bring disadvantages to crops since they can damage them, and a clean treatment with less pollution and contamination should be developed. Artificial intelligence gives new hope to agriculture to achieve smart farming. This study delivers an automated weeds growth point detection using deep learning. This study proposes a combination of semantic graphics for generating data annotation and U-Net with pre-trained deep learning as a backbone for locating the growth point of the weeds on the given field scene. The dataset was collected from an actual field. We measured the intersection over union, f1-score, precision, and recall to evaluate our method. Moreover, Mobilenet V2 was chosen as the backbone and compared with Resnet 34. The results showed that the proposed method was accurate enough to detect the growth point and handle the brightness variation. The best performance was achieved by Mobilenet V2 as a backbone with IoU 96.81%, precision 97.77%, recall 98.97%, and f1-score 97.30%.\n",
      "INTERIOR WIND NOISE PREDICTION AND VISUAL EXPLANATION SYSTEM FOR EXTERIOR VEHICLE DESIGN USING COMBINED CONVOLUTION NEURAL NETWORKS 2022 ['Wind noise prediction', 'Image regression', 'Convolutional neural networks (CNN)', 'Gradient-weighted class activation map (Grad-CAM)'] None An analytical model configuration, in addition to air pressure analysis and post-processing, was conducted to measure the interior wind noise by changing the exterior vehicular design. Although wind noise can be calculated accurately through the current process, it requires three to five days for each design. In this study, a convolutional neural network (CNN), which is a class of deep neural networks designed for processing image data, was applied to predict the wind noise with vehicle design images from four different views. Feature maps were extracted from the CNN models trained with images of each view and concatenated to flow through a sequence of fully connected (FC) layers to predict the wind noise. Moreover, visualization of the significant vehicle parts for wind noise prediction was provided using a gradient-weighted class activation map (Grad- CAM). Finally, we compared the performance of various CNN-based models, such as ResNet, DenseNet, and EfficientNet, in addition to the architecture of the FC layers. The proposed method can predict the wind noise using vehicle images from different views with a root-mean-square error (RMSE) value of 0.206, substantially reducing the time and cost required for interior wind noise estimation.\n",
      "Sex determination from lateral cephalometric radiographs using an automated deep learning convolutional neural network 2022 ['Sex Determination Analysis', 'Deep Learning', 'Radiography', 'Cephalometry', 'Cervical Vertebrae'] None Purpose: Despite the proliferation of numerous morphometric and anthropometric methods for sex identification based on linear, angular, and regional measurements of various parts of the body, these methods are subject to error due to the observer's knowledge and expertise. This study aimed to explore the possibility of automated sex determination using convolutional neural networks(CNNs) based on lateral cephalometric radiographs. Materials and Methods: Lateral cephalometric radiographs of 1,476 Iranian subjects (794 women and 682 men) from 18 to 49 years of age were included. Lateral cephalometric radiographs were considered as a network input and output layer including 2 classes(male and female). Eighty percent of the data was used as a training set and the rest as a test set. Hyperparameter tuning of each network was done after preprocessing and data augmentation steps. The predictive performance of different architectures (DenseNet, ResNet, and VGG) was evaluated based on their accuracy in test sets. Results: The CNN based on the DenseNet121 architecture, with an overall accuracy of 90%, had the best predictive power in sex determination. The prediction accuracy of this model was almost equal for men and women. Furthermore, with all architectures, the use of transfer learning improved predictive performance. Conclusion: The results confirmed that a CNN could predict a person's sex with high accuracy. This prediction was independent of human bias because feature extraction was done automatically. However, for more accurate sex determination on a wider scale, further studies with larger sample sizes are desirable.\n",
      "Sex determination from lateral cephalometric radiographs using an automated deep learning convolutional neural network 2022 ['Sex Determination Analysis', 'Deep Learning', 'Radiography', 'Cephalometry', 'Cervical Vertebrae'] None Purpose: Despite the proliferation of numerous morphometric and anthropometric methods for sex identification based on linear, angular, and regional measurements of various parts of the body, these methods are subject to error due to the observer’s knowledge and expertise. This study aimed to explore the possibility of automated sex determination using convolutional neural networks(CNNs) based on lateral cephalometric radiographs.Materials and Methods: Lateral cephalometric radiographs of 1,476 Iranian subjects (794 women and 682 men) from 18 to 49 years of age were included. Lateral cephalometric radiographs were considered as a network input and output layer including 2 classes(male and female). Eighty percent of the data was used as a training set and the rest as a test set. Hyperparameter tuning of each network was done after preprocessing and data augmentation steps.The predictive performance of different architectures (DenseNet, ResNet, and VGG) was evaluated based on their accuracy in test sets.Results: The CNN based on the DenseNet121 architecture, with an overall accuracy of 90%, had the best predictive power in sex determination. The prediction accuracy of this model was almost equal for men and women.Furthermore, with all architectures, the use of transfer learning improved predictive performance.Conclusion: The results confirmed that a CNN could predict a person’s sex with high accuracy. This prediction was independent of human bias because feature extraction was done automatically. However, for more accurate sex determination on a wider scale, further studies with larger sample sizes are desirable.\n",
      "객체 검출을 활용한 교육적 목적의 웹 기반 브레드보드 전기회로 분석 2022 ['전기전자 실험', '서비스', '브레드보드', '위치 검출', '위치 예측', 'Electrical and electronic testing', 'Service', 'Breadboard', 'Position detection', 'Position prediction'] None In an experiment where students learn electrical and electronic theory and apply it, students have difficulty in connecting circuits. To alleviate the above difficulties, we first propose a circuit analysis service. The proposed method detects an electric device using an object detection model in which electric devices connected to a breadboard are labeled with data. To verify this, we connect to the breadboard circuit and create a custom dataset through photographs. The proposed method is divided into two processes: electric device prediction and electric device position detection. The electric device prediction model was compared using five object detection models, and the Faster R-CNN model had the best prediction performance. The electrical device position detector extracts features from the object detection model through transition learning to predict two coordinates (x1, y1), (x2, y2). A comparison of each model confirmed that the ResNet model has good location detection performance. Through this, it was confirmed that the proposed method alleviates the difficulty of first-time students learning electric and electronic experiments.\n",
      "CNN 아키텍쳐 기반 인공 고관절 판독 모델 성능 비교 연구 2022 ['THRA', 'Medical image processing field', 'Patient information', 'Anonymization', 'Binarization work', 'CNN-based architecture'] 최근, 고관절 전치환술의 진행에 있어, 제조사마다 규격이 다른 인공관절의 특성으로 인해 과거에 삽입했던 제품과 호환되는 제품을 반드시 사용해야 하는 고질적인 문제점이 있다. 이런 문제를 해결하기 위해 집도의는 수술에 들어가기 전에 X-Ray이미지를 보고 경험치료를 바탕으로 가장 비슷한 인공 보철물을 준비한다. 이러한 준비 과정은 의사의 경험에 따라 오차가 발생할 위험이 있다. 이에 따라 전 세계의 많은 고관절 인공 보철물 제조사는 자사의 데이터를 활용하여 인공 보철물 제품군의 이미지 판독 시스템의 개발이 활발히 이루어지고 있는 추세이다. 따라서 본 연구는 의료영상처리분야에서 시각 정보에 기반한 고차원적 추상화를 통한 병변 진단 및 예후 예측에 뛰어나 성능을 보이는 CNN(Convolutional Neural Network)기반의 다양한 아키텍처 모델들의 성능을 비교한다. 연구방법은 다음과 같다. 국내 유일의 인공고관절 제조업체인 ㈜코렌텍에서 제공한 자사 제품이 사용된 X-Ray 이미지와 Kaggle 데이터셋 ‘Aseptic Loose Hip Implant X-Ray Database’을 활용한다. 이때 사용되는 X-Ray 이미지는 환자 정보에 대한 비식별화가 된 상태이다. 습득한 X-Ray 이미지는 인공 보철물이 나타난 부분을 뜯어내 배경을 제거한 후 이진화 작업, 특정 영역 외 객체 제거, 이미지 반전 등 데이터 전처리 작업을 거치고, 전처리된 데이터셋을 이진 분류모델에 맞게 변환한다. 이후 CNN기반의 다양한 아키텍처 모델에 적용하여 각 모델에 대한 성능을 비교한다. 성능비교에 사용한 CNN기반 모델은 CNN, VGG16, GoogLe Net/Inception Net, Xception Net, Mobile Net, ResNet 총 6개이며, 각 모델에 대한 특징과 연결점에 대해 설명한다. None\n",
      "2022년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                                                                                                    title  \\\n",
      "0                                                                                                                                              ResNet 기반 작물 생육단계 추정 모델 개발   \n",
      "1                                                                                                                                     스펙트럼 첨도를 이용한 ResNet 기반의 표적 분류 성능 분석   \n",
      "2                                                                          A ResNet based multiscale feature extraction for classifying multi-variate medical time series   \n",
      "3                                                                                       ResNet-Based Simulations for a Heat-Transfer Model Involving an Imperfect Contact   \n",
      "4                                                                       Rotate vector reducer design using resnet-based model and integration of discretized optimization   \n",
      "5                                                                                                                                               딥러닝을 이용한 직물의 결함 검출에 관한 연구   \n",
      "6                                                                                                                                               ArcFace를 사용한 경량 신원인식 네트워크   \n",
      "7                                                                                                                        비소세포폐암 환자의 재발 예측을 위한 흉부 CT 영상 패치 기반 CNN 분류 및 시각화   \n",
      "8                                                                                                                                     무선 단말기 Fingerprint 식별을 위한 딥러닝 구조 개발   \n",
      "9                                                         Development of an Artificial Intelligence-Based Support  Technology for Urethral and Ureteral Stricture Surgery   \n",
      "10                                                                                                                                          다양한 CNN모델을 사용한 컬러 콘택트렌즈 불량 검출   \n",
      "11                                                                                                                      심혈관 상태 식별을 위한 CNN Block 구조를 적용한 고성능 ECG 데이터 분석시스템   \n",
      "12                                                                                                                                               딥러닝 기반의 핵의학 폐검사 분류 모델 적용   \n",
      "13  Feasibility of Deep Learning-Based Analysis of Auscultation for Screening Significant Stenosis of Native Arteriovenous Fistula for Hemodialysis Requiring Angioplasty   \n",
      "14                                                                                A Study on the Optimal Artificial Intelligence Model for  Determination of Urolithiasis   \n",
      "15                                                                                                                                     새로운 반려견 등록방식 도입을 위한 안면 인식 성능 개선 연구   \n",
      "16                                                                                                                                                 딥러닝기반 토마토 병해 진단 서비스 연구   \n",
      "17                                                            Two-phase flow pattern online monitoring system based on convolutional neural network and transfer learning   \n",
      "18                                                                                                                                    안전벨트에 관성측정장치 장착을 통한 운전자의 호흡 상태 모니터링   \n",
      "19                                                                                                                                         딥러닝 기반의 반도체 패키지 다이면 스크래치 검출 방법   \n",
      "20                                                                                                                            설명 가능한 합성곱 신경망을 활용한 센서 기반의 시계열 데이터 분류 모델 제안   \n",
      "21                                                                                                                                            반려견 자동 품종 분류를 위한 전이학습 효과 분석   \n",
      "22                                                        Scaling Up Face Masks Classification Using a Deep Neural Network and Classical Method Inspired Hybrid Technique   \n",
      "23                                                                                                                                          합성곱 신경망 기반 분류 모델의 화재 예측 성능 분석   \n",
      "24                                                                                                                                     포인트 클라우드 및 투영 이미지를 이용한 다중 모달 형상 분류   \n",
      "25                                                                                                                                               재구성 가능한 모듈 기반 CNN 가속기 구현   \n",
      "26                                                                                             SVM on Top of Deep Networks for Covid-19 Detection from Chest X-ray Images   \n",
      "27                                                                                                                                               감정 분류를 이용한 표정 연습 보조 인공지능   \n",
      "28                                                                                                                                              발레 자세 교정을 위한 발레 동작 코칭 시스템   \n",
      "29                                                                                                                            1인 가구 환경에서 프라이버시 보호 영상을 활용한 위험 행동 인식에 관한 연구   \n",
      "30                                    The Evaluation of Deep Learning Using Convolutional Neural Network (CNN) Approach for Identifying Arabica and Robusta Coffee Plants   \n",
      "31                                                                                          Corneal Ulcer Region Detection With Semantic Segmentation Using Deep Learning   \n",
      "32                                                                                                                            영상 기반 Semantic Segmentation 알고리즘을 이용한 도로 추출   \n",
      "33                                                                                                                                  합성곱 신경망 및 영상처리 기법을 활용한피부 모공 등급 예측 시스템   \n",
      "34                                                                                                                         A new lightweight network based on MobileNetV3   \n",
      "35                                                                                                                                 언어장애인의 스마트스피커 접근성 향상을 위한 개인화된 음성 분류 기법   \n",
      "36                                                                                     ShortcutFusion++: Optimizing an End-to-End CNN Accelerator for High PE Utilization   \n",
      "37                                                                                                                                       위성 및 드론 영상을 이용한 해안쓰레기 모니터링 기법 개발   \n",
      "38                                                                                                                                               심층학습 알고리즘을 활용한 인접면 우식 탐지   \n",
      "39                                                                                                     Steel Surface Defect Detection using the RetinaNet Detection Model   \n",
      "40                                                                                                                              반려동물용 자동 사료급식기의 비용효율적 사료 중량 예측을 위한 딥러닝 방법   \n",
      "41                                                                                                                                            배터리 리드탭 압흔 오류 검출의 딥러닝 기법 적용   \n",
      "42                                                                                                  Human activity recognition based on wrist PPG via the ensemble method   \n",
      "43                                                                          A Manually Captured and Modified Phone Screen Image Dataset for Widget Classification on CNNs   \n",
      "44                                                                                                                                     심전도의 다양한 2차원 변환에 의한 합성곱 신경망기반 개인식별   \n",
      "45                                                                       No-Reference Image Quality Assessment based on Quality Awareness Feature and Multi-task Training   \n",
      "46                                                                                            Enhanced 3D Residual Network for Human Fall Detection in Video Surveillance   \n",
      "47                                                                                                                                 CNN 기반 전이학습을 이용한 뼈 전이가 존재하는 뼈 스캔 영상 분류   \n",
      "48                                                                                                                                게이트심장혈액풀검사에서 딥러닝 기반 좌심실 영역 분할방법의 유용성 평가   \n",
      "49                                                                                                                                          합성곱 신경망을 이용한 정사사진 기반 균열 탐지 기법   \n",
      "50                                                                                                                                             필터 다양화를 통한 합성곱 신경망의 표현력 향상   \n",
      "51                                                                                                                                표정 분류에 기반한 감정 인식을 위한 열화상 데이터베이스의 유용성 평가   \n",
      "52                                       A Deep Learning-Based Compact Weighted Binary Classification Technique to Discriminate between Targets and Clutter in SAR Images   \n",
      "53                                                                                                                                  관개용수로 CCTV 이미지를 이용한 CNN 딥러닝 이미지 모델 적용   \n",
      "54                                                                                                                                    다수 조명의 채널별 융합을 이용한 CNN 기반 머신 비전 분류기   \n",
      "55                                                                                                                   금속 음영이 포함된 CBCT 영상에서 딥러닝을 이용한 해부학적 구조물의 다중 클래스 분할 방법   \n",
      "56                                                                                                                                      CNN을 이용한 Al 6061 압출재의 표면 결함 분류 연구   \n",
      "57                                                                                                                      Cycle-accurate NPU 시뮬레이터 및 데이터 접근 방식에 따른 NPU 성능평가   \n",
      "58                                                                     Retinal Disease Identification using Upgraded CLAHE filter and Transfer Convolution Neural Network   \n",
      "59                                                        Analysis of Weights and Feature Patterns in Popular 2D Deep Neural Networks Models for MRI Image Classification   \n",
      "60                                                                                                          Representative Batch Normalization for Scene Text Recognition   \n",
      "61                                                                                                                                 1-D PE 어레이로 컨볼루션 연산을 수행하는 저전력 DCNN 가속기   \n",
      "62                                                                                                                                         ISAR 영상 기반 해상표적 식별을 위한 인공지능 연구   \n",
      "63                                                                                                                            샴 네트워크 기반의 적은 데이터셋을 이용한 유사한 형태의 특정 객체 인식 연구   \n",
      "64                                                                      Fundus Photograph Discrimination Using Transfer Learning over Limited Computing Power Environment   \n",
      "65                                                                                              Safety monitoring system of personal mobility driving using deep learning   \n",
      "66                                                                                      Multiple butterfly recognition based on deep residual learning and image analysis   \n",
      "67                                                     Beta and Alpha Regularizers of Mish Activation Functions for Machine Learning Applications in Deep Neural Networks   \n",
      "68                                                                                                                                디지털트윈 구축을 위한 3차원 공간정보 폐색영역 검출 및 복원 기술개발   \n",
      "69                                                                                                      개선된 Deep Residual Learning 및 물리 모델 데이터를 이용한 잡음 조건에서의 자동차 샤시 결함 진단   \n",
      "70                                                                 Deep learning-guided attenuation correction in the image domain for myocardial perfusion SPECT imaging   \n",
      "71                                                                                                       Detection of fake news using deep learning CNN–RNN based methods   \n",
      "72                                         Preliminary study of artificial intelligence-based fuel-rod pattern analysis of low-quality tomographic image of fuel assembly   \n",
      "73                                                                                                                           영상 인식을 위한 딥러닝 모델의 적대적 공격에 대한 백색 잡음 효과에 관한 연구   \n",
      "74                                                                                                                                      뉴스 감성 분석을 이용한 딥러닝 기반 주가 예측에 대한 연구   \n",
      "75                                                                                                                                        딥러닝 기반 이미지 필터 간 한국인 표정 분류 성능 비교   \n",
      "76                                                                                                                                         GAN을 활용한 인테리어 스타일 변환 모델에 관한 연구   \n",
      "77                                                                                                                         비정상심박 검출을 위해 영상화된 심전도 신호를 이용한 비교학습 기반 딥러닝 알고리즘   \n",
      "78                                                                                                                                      블랙 박스 모델의 출력값을 이용한 AI 모델 종류 추론 공격   \n",
      "79                                                                                                                                             산림복원 대상 후보지 추출을 위한 딥러닝 접근법   \n",
      "80                                                                                                           모노 카메라를 사용하는 머신 비전 시스템에서 비정형 결함을 검사하는 CNN을 훈련하기 위한 데이터 증식 방법   \n",
      "81                                                       Detection and Localization of Coordinated State-and-Topology False Data Injection Attack by Multi-modal Learning   \n",
      "82                                                                               Tutorial and applications of convolutional neural network models in image classification   \n",
      "83                                                                                                                           딥러닝과 다양한 데이터 증강 기법을 활용한 주변국 군용기 기종 분류에 관한 연구   \n",
      "84                                                                                                                                병렬 병합 구조를 이용한 기상 데이터의 예측률 향상을 위한 딥러닝 모델   \n",
      "85                                                                                                                                              음성기반 대화형 서비스 키오스크 설계 및 구현   \n",
      "86                                                                                                                                  Healing Game using AI and Art Therapy   \n",
      "87                                                                                                                            Triplet Loss 기반 딥러닝 모델을 통한 유사 아동 그림 선별 알고리즘   \n",
      "88                                                                                                                                       단일 파노라마 입력의 실내 공간 레이아웃 복원 모델 경량화   \n",
      "89                                                                                           Genetic Algorithm-Based Structure Reduction for Convolutional Neural Network   \n",
      "90                                                         Classification of dog skin diseases using deep learning with images captured from multispectral imaging device   \n",
      "91                                                                                                                                               머신러닝 기반 금속외관 결함 검출 비교 분석   \n",
      "92                                                                                                A Novel Transfer Learning-Based Algorithm for Detecting Violence Images   \n",
      "93                                                                                                         Deep Learning for Weeds’ Growth Point Detection based on U-Net   \n",
      "94                                    INTERIOR WIND NOISE PREDICTION AND VISUAL EXPLANATION SYSTEM FOR EXTERIOR VEHICLE DESIGN USING COMBINED CONVOLUTION NEURAL NETWORKS   \n",
      "95                                                 Sex determination from lateral cephalometric radiographs using an automated deep learning convolutional neural network   \n",
      "96                                                 Sex determination from lateral cephalometric radiographs using an automated deep learning convolutional neural network   \n",
      "97                                                                                                                                  객체 검출을 활용한 교육적 목적의 웹 기반 브레드보드 전기회로 분석   \n",
      "98                                                                                                                                      CNN 아키텍쳐 기반 인공 고관절 판독 모델 성능 비교 연구   \n",
      "\n",
      "    date  \\\n",
      "0   2022   \n",
      "1   2022   \n",
      "2   2022   \n",
      "3   2022   \n",
      "4   2022   \n",
      "5   2022   \n",
      "6   2022   \n",
      "7   2022   \n",
      "8   2022   \n",
      "9   2022   \n",
      "10  2022   \n",
      "11  2022   \n",
      "12  2022   \n",
      "13  2022   \n",
      "14  2022   \n",
      "15  2022   \n",
      "16  2022   \n",
      "17  2022   \n",
      "18  2022   \n",
      "19  2022   \n",
      "20  2022   \n",
      "21  2022   \n",
      "22  2022   \n",
      "23  2022   \n",
      "24  2022   \n",
      "25  2022   \n",
      "26  2022   \n",
      "27  2022   \n",
      "28  2022   \n",
      "29  2022   \n",
      "30  2022   \n",
      "31  2022   \n",
      "32  2022   \n",
      "33  2022   \n",
      "34  2022   \n",
      "35  2022   \n",
      "36  2022   \n",
      "37  2022   \n",
      "38  2022   \n",
      "39  2022   \n",
      "40  2022   \n",
      "41  2022   \n",
      "42  2022   \n",
      "43  2022   \n",
      "44  2022   \n",
      "45  2022   \n",
      "46  2022   \n",
      "47  2022   \n",
      "48  2022   \n",
      "49  2022   \n",
      "50  2022   \n",
      "51  2022   \n",
      "52  2022   \n",
      "53  2022   \n",
      "54  2022   \n",
      "55  2022   \n",
      "56  2022   \n",
      "57  2022   \n",
      "58  2022   \n",
      "59  2022   \n",
      "60  2022   \n",
      "61  2022   \n",
      "62  2022   \n",
      "63  2022   \n",
      "64  2022   \n",
      "65  2022   \n",
      "66  2022   \n",
      "67  2022   \n",
      "68  2022   \n",
      "69  2022   \n",
      "70  2022   \n",
      "71  2022   \n",
      "72  2022   \n",
      "73  2022   \n",
      "74  2022   \n",
      "75  2022   \n",
      "76  2022   \n",
      "77  2022   \n",
      "78  2022   \n",
      "79  2022   \n",
      "80  2022   \n",
      "81  2022   \n",
      "82  2022   \n",
      "83  2022   \n",
      "84  2022   \n",
      "85  2022   \n",
      "86  2022   \n",
      "87  2022   \n",
      "88  2022   \n",
      "89  2022   \n",
      "90  2022   \n",
      "91  2022   \n",
      "92  2022   \n",
      "93  2022   \n",
      "94  2022   \n",
      "95  2022   \n",
      "96  2022   \n",
      "97  2022   \n",
      "98  2022   \n",
      "\n",
      "                                                                                                                                                                                     keywords  \\\n",
      "0                                                                                   [딥러닝, 컴퓨터 비전, Convolution Neural Network, 작물 생육단계, Deep Learing, Computer Vision, CNN, Crop Growth Stage]   \n",
      "1                                                                                                  [Micro-Doppler, Characteristic vector, Spectral kurtosis, Target classification, ResNet34]   \n",
      "2                                                                                  [Multi-scale convolutional feature extraction methods, ResNet50 structure, Squeeze-and-Excitation Modules]   \n",
      "3                                                                                                      [Composite material, deep learning, heat transfer, Kapitza thermal resistance, ResNet]   \n",
      "4                                                                              [Sequential engineering, RV reducer, Design automation, Multidisciplinary optimization, Secondary development]   \n",
      "5                                                                                                                  [fabric defects, VGGNet, ResNet, Grad-CAM, 직물결함, VGGNet, ResNet, Grad-CAM]   \n",
      "6                                                                                                                                   [ArcFace, lightweight CNN, identity recognition, edge AI]   \n",
      "7   [Non-Small Cell Lung Cancer(NSCLC), Recurrence Prediction, Deep Learning, Classification, Ensemble Learning, Convolutional Neural Network(CNN)1, 비소세포폐암, 재발 예측, 딥러닝, 분류, 앙상블 학습, 합성곱 신경망]   \n",
      "8                                                                                                                      [DMR Fingerprint, ResNet-1D, fingerprinting feature, polar coordinate]   \n",
      "9                                                                        [Urethral stricture, Ureteral stricture, ResNet-50, Surgical support technology, Endoscope, Artificial intelligence]   \n",
      "10                                                               [컬러 콘택트렌즈, 딥러닝, ResNet, GoogLeNet, DenseNet, MobileNet, Contact Lens, Deep Learning, ResNet, GoogLeNet, DenseNet, MobileNet]   \n",
      "11                                                                                                       [CNN block structure, ECG data analysis scheme, ResNeXt, MIT-BIH database, F1-score]   \n",
      "12                                            [컨볼루션 신경망, 딥러닝, 폐 신티그라피, 분류-활성화 맵, 핵의학, Convolutional neural network, Deep learning, Lung scintigraphy, Class activation map, Nuclear medicine]   \n",
      "13                                                                                                          [Angioplasty, Deep learning, Arteriovenous fistula, Auscultation, Renal dialysis]   \n",
      "14                                                                                                          [Urolithiasis, Ureter stones, ResNet-50, Fast R-CNN, Surgical support technology]   \n",
      "15                                                                                                                   [Deep Learning, Face Recognition, Dog Recognition, ResNet, Triplet Loss]   \n",
      "16                                                                                                                 [딥러닝, 토마토병해충, 모바일넷, 레스넷, deep learning, tomato disease, MobileNet, ResNet]   \n",
      "17                                                 [Flow pattern, Online monitoring system, Artificial neural network (ANN), Convolutional neural network (CNN), Transfer learning, ResNet50]   \n",
      "18                        [운전자 모니터링 시스템, 생체신호 분석, 관성측정장치, 운전자 상태 분류, Driver Monitoring Systems, Bio-Signal Data Measurement, Inertial Measurement Unit, ResNet, Driver States Classification]   \n",
      "19                                                                                                             [Scratch Detection, Deep Learning, Image Processing, Semiconductor Package, .]   \n",
      "20                        [Sensor Data, Time Series Classification, Pattern Recognition, Deep Learning, eXplainable Artificial Intelligence(XAI), 센서 데이터, 시계열 데이터 분류, 패턴 인식, 딥러닝, 설명가능한 인공지능]   \n",
      "21                                                                                                                              [Deep Learning, Transfer Learning, Resnet, VGGNet, Dog Breed]   \n",
      "22                                                                                                                   [CNNs, Face masks, Machine learning, Multi-layer perceptron, ResNet-101]   \n",
      "23                                                                                     [Fire detection, Fire image classification, Convolutional neural network, Fire prediction performance]   \n",
      "24                                                                   [포인트 클라우드, 심층신경망, 딥러닝, 다중 모달, 형상 분류, Point Cloud, Deep Neural Network, Deep Learning, Multi-modal, Shape Classification]   \n",
      "25                                                                                                                         [CNN accelerator, module-based architecture, FPGA, Verilog-HDL, .]   \n",
      "26                                                                                                                            [Covid-19, X-ray image, Deep learning, Support vector machines]   \n",
      "27                                                [감정 분류, 표정 연습, 얼굴 이미지 처리, 자연어 처리, Emotion Classification, Facial Expression Practice, Facial Image Processing, Natural Language Processing]   \n",
      "28                                                               [발레, 심플 포즈, 오픈 포즈, 관절 좌표 추출, 특징 중요도 분석, Ballet, Simple pose, Open pose, Keypoints detection, Analysis of feature importance]   \n",
      "29                                                                                    [Deep Learning, Privacy, Action Recognition, YOLOv5, Single-person household, 딥러닝, 프라이버시, 행동 인식, 1인 가구]   \n",
      "30                                                                 [Leaf classification, Robusta coffee, Arabica coffee, Convolutional neural network, Coffee species, Precision agriculture]   \n",
      "31                                                                                                                                                                                         []   \n",
      "32                                                                        [Drone Image, Semantic Segmentation, Remote Sensing, Road Extraction, 드론 정사영상, Semantic Segmentation, 원격 탐사, 도로 추출]   \n",
      "33                                                                                                                                            [Skin, Pore, Image processing, CNN, Prediction]   \n",
      "34                                                                  [MobileNetV3, Real-time image classification, Lightweight network, Deep convolutional neural network, residual structure]   \n",
      "35                         [스마트스피커, 언어장애인, 장애인접근성, 개인화된 음성분류기법, 딥러닝, smart speaker, speech-impaired people, disabled accessibility, personalized speech classification scheme, deep learning]   \n",
      "36                                                                                                                 [CNN accelerator, Processing element, Hardware utilization, FPGA, YOLO-v3]   \n",
      "37                                                                                                                     [Sentinel-2, Drone, Multispectral image, Deep learning, Marine debris]   \n",
      "38                                                                                         [Artificial intelligence, Deep learning, Proximal caries, Primary teeth, Intraoral radiography, .]   \n",
      "39                                                                                             [Defect Detection, Deep Learning, Steel Defect Detection, RetinaNet model, One-Stage Detector]   \n",
      "40                                                          [사료급식기, 컴퓨터 비전, 중량 예측, 딥러닝, 합성곱 신경망, pet feeder, computer vision, weight prediction, deep learning, convolutional neural network]   \n",
      "41                     [배터리 리드탭, 압흔 오류 검출, 인공지능, 딥러닝, Faster R-CNN, 객체 탐지, Battery lead tab, Welding error detection, Artificial intelligence, Deep learning, Faster R-CNN, Object detection]   \n",
      "42                                                                                                   [HAR, Exercise, PPG, ECG, Classification, Ensemble, Machine learning, Transfer learning]   \n",
      "43                                                                                               [Captured Image, CNN, Deep Learning Dataset, Image Classification, Object Detection, Widget]   \n",
      "44                       [person identification, electrocardiogram, convolutional neural networks, time-frequency transform, Short-Time Fourier Transform, Fourier Synchrosqueezed Transform]   \n",
      "45                                                                                           [Deep Learning, No-Reference Image Quality Assessment, Multiple Task Learning, Score Prediction]   \n",
      "46                                                                                                              [Video surveillance, fall detection, deep learning, residual network, 3D CNN]   \n",
      "47                                                                                                         [Deep Learning, Computer Vision, CNN, Transfer Learning, Medical Image, Bone Scan]   \n",
      "48                                                                                                                                                                    [대한방사선과학회(구 대한방사선기술학회)]   \n",
      "49                                                                                                                                 [Ortho-image, UAV, Machine learning, Crack detection, CNN]   \n",
      "50                                                               [Deep learning, CNN, Feature Representation, Filter Diversity, Singular Value Decomposition (SVD) Entropy, Filter Spreading]   \n",
      "51                                                                        [thermal face image, facial expression classification, emotion recognition, CNN architecture, database performance]   \n",
      "52                                                                             [Automatic Target Detection (ATD), Deep Learning (DL), Machine Learning (ML), Synthetic Aperture Radar (SAR).]   \n",
      "53                                                                                                                  [Image classification, image segmentation, CCTV images, irrigation canal]   \n",
      "54                                                                                                                             [Semiconductor, Auto Visual Inspection, Deep Learning, CNN, .]   \n",
      "55                                                                                                   [Deep learning, Anatomical structure segmentation, Metal artifacts, U-Net, Tversky loss]   \n",
      "56                                                                                  [Convolution Neural Network, Surface Defect, Aluminum alloy, Extrusion, Deep Learning, Data Augmentation]   \n",
      "57                                                                                               [Neural Processing Unit, Convolutional Neural Network, Data Reuse, FIFO, Interleaved Memory]   \n",
      "58                                                                                                 [Retinal disease, Retinal fundus images, Convolution neural network (CNN), (CLAHE) filter]   \n",
      "59                                                                                               [Deep Neural Network, Activation Channels, MRI, Fully Connected Layer, Weights Correlation.]   \n",
      "60                                                                   [Scene text recognition, deep learning, Representative Batch Normalization, Feature representation, Feature enhancement]   \n",
      "61                                                                                                     [FPGA, Deep Convolutional Neural Network, Accelerator, Processing Element, Data Reuse]   \n",
      "62                                                                                                  [Artificial Intelligence, Inverse Synthetic Aperture Radar, Radar Image, Maritime Target]   \n",
      "63                                                                                   [Object Recognition, Deep Learning, Few-shot Learning, Siamese Network, Convolutional Neural Network, .]   \n",
      "64                                                                                                                [AI, CNN, Fundus photograph, ImageNet, OPhthalmoloscopy, Transfer learning]   \n",
      "65                                                                         [deep learning, personal mobility, short-time Fourier transform, wavelet transform, convolutional neural networks]   \n",
      "66                                                                                   [butterfly  identification, deep  residual  learning, image  recognition, multiple  species recognition]   \n",
      "67                                                                                                [Neural Network, Machine Learning Applications, α and β Regularizers, Activation Function.]   \n",
      "68                                                                                                                         [Digital Twin, 3D Modeling, Occlusion Area, Detection, In-paining]   \n",
      "69         [Deep Learning(딥러닝), Vehicle Chassis System(차량 샤시 시스템), Physics Model(물리 모델), ResNet(심층 잔차 신경망), DenseNet(밀집 연결 합성곱 네크워크), FFT(빠른 푸리에 변환), Domain Adaption(도메인 적응), SNR(신호 대 잡음비)]   \n",
      "70                                                                                               [SPECT, myocardial perfusion imaging, quantification, attenuation correction, deep learning]   \n",
      "71                                                                                                                      [Fake news detection, Deep learning, CNN, Bidirectional LSTM, ResNet]   \n",
      "72                                                         [Single-photon emission computed, tomography, Monte Carlo, Nuclear fuel assembly, Artificial intelligence, VGG, GoogLeNet, ResNet]   \n",
      "73                                                                                         [Deep learning, Adversarial attack, FGSM attack, BIM attack, CW attack, White noise, Perturbation]   \n",
      "74                                                                     [Stock Price Forecasting, LSTM, ResNet, Sentiment Analysis, Text Summarization, 주가 예측 모델, 텍스트 요약, LSTM, ResNet, 감정 분석]   \n",
      "75                                                                        [얼굴 표정 분류, CNN, VGG, ResNet, 가장자리 검출 필터, Facial Expression Classification, CNN, VGG, ResNet, Edge Detection Filter]   \n",
      "76                                                                                        [CycleGAN, 이미지 변환, 인테리어 스타일, 이미지 렌더링, CycleGAN, Image Translation, Interior Style, Image Rendering]   \n",
      "77                                                [심전도, 딥러닝, CNN, 템플릿 군, 비정상심박 검출, Electrocardiogram, Deep learning, Convolutional neural network, Template cluster, Abnormal beat detection]   \n",
      "78                                                                                                                               [AI security, Privacy, Exploratory Attack, Inference Attack]   \n",
      "79                                                                                                       [딥러닝, 분할, 산림복원, 항공사진, Aerial photo, Deep learning, Forest restoration, Segmentation]   \n",
      "80                                                                 [Machine Vision(머신 비전), Convolutional Neural Networks(합성곱 신경망), Data Augmentation (데이터 증식), Image Segmentation(이미지 영역 분할)]   \n",
      "81                                                                     [Power system cyber security, False data injection attack, Topology attack, Multi-modal learning, Data-driven methods]   \n",
      "82                                                                                                                                 [AdamW, convolution neural network, deep learning, ILSVRC]   \n",
      "83                                             [Deep Learning(딥러닝), ResNet(레스넷), Fine-Grained Visual Classification(세밀한 이미지 분류), Edge Detection(윤곽선 탐지), Image Data Augmentation(이미지 데이터 증강)]   \n",
      "84                                                                                            [deep neural network, long short term memory, convolution neural network, ResNet, Inception, .]   \n",
      "85                                                                          [Web, Voice recognition, Collaborative Filtering, Recommendation System, Dialogflow, REST API, ResNet, MobileNet]   \n",
      "86                                                                                                                      [Healing Game, Self-help-therapy, Word Cloud, Unity ML Agent, ResNet]   \n",
      "87                                      [이미지 유사성, Triplet Loss, 인공지능, 딥러닝, CNN, 아동 그림분석, Image Similarity, Triplet Loss, Artificial Intelligence, Deep Learning, CNN, Child drawing analysis]   \n",
      "88                                                                                                         [Room Layout Estimation, Neural Architecture Search, Lightweight Deep Learning, .]   \n",
      "89                                                                                             [Structure reduction, Convolutional neural network, Genetic algorithm, Knowledge distillation]   \n",
      "90                                                                                                                         [Deep learning, Dog skin disease, Multispectral image, Dermatosis]   \n",
      "91                                                                              [금속 외관, 결함 검출, 머신러닝, 합성곱신경망, Metal Surface, Defect Detection, Machine Learning, Convolutional Neural Network]   \n",
      "92                                                                                                                       [Deep learning, Image classification, Pre-training, Violence images]   \n",
      "93                                                                                                           [artificial intelligence, growth point, deep learning, semantic graphics, U-Net]   \n",
      "94                                                          [Wind noise prediction, Image regression, Convolutional neural networks (CNN), Gradient-weighted class activation map (Grad-CAM)]   \n",
      "95                                                                                                 [Sex Determination Analysis, Deep Learning, Radiography, Cephalometry, Cervical Vertebrae]   \n",
      "96                                                                                                 [Sex Determination Analysis, Deep Learning, Radiography, Cephalometry, Cervical Vertebrae]   \n",
      "97                                                       [전기전자 실험, 서비스, 브레드보드, 위치 검출, 위치 예측, Electrical and electronic testing, Service, Breadboard, Position detection, Position prediction]   \n",
      "98                                                                      [THRA, Medical image processing field, Patient information, Anonymization, Binarization work, CNN-based architecture]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      산업화 이후 가속화된 지구 온난화 현상으로 인해 기존환경 변화 및 이상기후 발생 빈도가 증가하고 있다. 농업은 기후변화에 매우 민감한 분야의 산업으로 지구 온난화는 작물의 생산량을 감소시키고 재배 지역이 변하는 등의 문제를 발생시킨다. 또한, 환경 변화는 작물의 생육 시기를 불규칙하게 만들어 숙련된 농사꾼들도 작물의 생육단계를 쉽게 추정할 수 없도록 만들어 여러 문제를 발생시킨다. 이에 본 논문에서는 작물의 생육단계를 추정하기 위한 CNN(Convolution Neural Network) 모델을 제안한다. 제안한 모델은 ResNet의 Pooling Layer를 수정한 모델로 ResNet, DenseNet 모델의 생육단계 추정보다 높은 성능 결과를 확인하였다.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                  미세 도플러 (micro-Doppler) 변조는 각 개체의 구분 및 각각의 움직임에 대한 미세한 운동 상태를 나타내는 표적 특징으로서, 표적을 인식하고 분류하는 기술에 활용되고 있다. 미세 도플러 주파수는 물체의 회전과 진동 등의 기본적인 운동 특징에 의한 도플러 주파수의 변조 형태로 나타나며, 이를 이용하면 높은 표적 인식 정확도로 표적을 추적하고 분류할 수 있다. 본 논문에서는 드론, 조류, 사람 표적에 따른 미세 운동 신호를 모델링하고, 미세 도플러 영상의 스펙트럼 첨도를 계산하여 표적의 미세 도플러 특징 벡터를 추출한다. 그리고 서로 다른 미세 운동을 하는 표적을 분류하기 위해 스펙트럼 첨도를 입력으로 하는 ResNet34 심층 신경망 네트워크를 적용한다. 모의실험을 통해 각 표적의 레이더 실측 데이터 입력 세트에 따른 ResNet34 알고리즘의 분류 성능을 분석한다. 모의실험 결과를 통해 제안하는 기법이 정확도, 정밀도, 재현도 측면에서 평균 95% 이상의 성능을 제시함으로써, 미세 도플러 영상을 이용하는 기존 기법보다 우수함을 보인다.   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                              섬유산업에서 생산된 직물의 결함을 식별하는 것은 품질관리를 위한 핵심적인 절차이다. 본 연구는 직물의 이미지를 분석하여 결함을 검출하는 모델을 만들고자 하였다. 연구에 사용된 모델은 딥러닝 기반의 VGGNet과 ResNet이었고, 두 모델의 결함 검출 성능을 비교하여 평가하였다. 정확도는 VGGNet 모델이 0.859, ResNet 모델이 0.893으로 ResNet 모델의 정확도가 더 높은 결과를 보여주었다. 추가적으로 딥러닝 모델이 직물의 이미지 내에서 결함으로 인식한 부분의 위치를 알아보기 위하여 XAI(eXplainable Artificial Intelligence)기법인 Grad-CAM 알고리즘을 사용하여 모델의 관심영역을 도출하였다. 그 결과 딥러닝 모델이 직물의 결함으로 인식한 부분이 육안으로도 실제 결함이 있는 것으로 확인되었다. 본 연구의 결과는 직물의 결함 검출에 있어서 딥러닝 기반의 인공지능을 활용함으로써 섬유의 생산과정에서 발생하는 시간과 비용을 줄일 수 있을 것으로 기대된다.   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "7                                                                                                                                                                                                                                                                                비소세포폐암(NSCLC)은 전체 폐암 중 85%의 높은 비중을 차지하며 사망률(22.7%)이 다른 암에 비해 현저히 높은 암으로비소세포폐암 환자의 수술 후 예후에 대한 예측은 매우 중요하다. 본 연구에서는 종양을 관심영역으로 갖는 비소세포폐암환자의 수술 전 흉부 CT 영상 패치의 종류를 종양 관련 정보에 따라 총 다섯 가지로 다양화하고, 이를 입력데이터로 갖는사전 학습 된 ResNet 과 EfficientNet CNN 네트워크를 사용하여 단일 모델과 간접 투표 방식을 이용한 앙상블 모델, 그리고3 개의 입력 채널을 활용한 앙상블 모델에서의 실험 결과 및 성능을 오분류의 사례와 Grad-CAM 시각화를 통해 비교분석한다. 실험 결과, 종양 주변부 패치를 학습한 ResNet152 단일 모델과 EfficientNet-b7 단일 모델은 각각 87.93%와81.03%의 정확도를 보였다. 또한 ResNet152 에서 총 3 개의 입력 채널에 각각 영상 패치, 종양 주변부 패치, 형상 집중 종양내부 패치를 넣어 앙상블 모델을 구성한 경우에는 정확도 87.93%를, EfficientNet-b7 에서 간접 투표 방식으로 영상 패치와종양 주변부 패치 학습 모델을 앙상블 한 경우에는 정확도 84.48%를 도출하며 안정적인 성능을 보였다.   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                           RF-Fingerprint 기술은 전송된 파형에서 송신기의 하드웨어 고유 특성을 추출하는 기술로써, 디바이스 보안분야에 매우 유용한 기술 중의 하나이다. 본 논문은 무선 단말기의 In-phase(I)와 Quadrature(Q) 값을 입력으로 동종무선 단말기 및 이기종 무선 단말기를 식별할 수 있는 fingerprint 특징을 추출하고 이를 식별할 수 있는 딥러닝 구조를제안한다. 동종/이기종 무선 단말기를 식별하기 위한 특징으로 I/Q를 극좌표로 변환한 후 크기 값을 시간축으로 배열한데이터를 무선 단말기의 fingerprinting 특징으로 제안하고 이를 식별하기 위해서 수정된 1차원 ResNet 모델을 제안한다. 실험을 위해서 동일 모델 10대의 두 종류 무선 단말기를 대상으로 제안한 딥러닝 구조의 성능을 분석한다. 제안한딥러닝 구조 및 fingerprint 특징의 성능 검증을 위해서 4000개의 데이터셋 중에서 20%인 800개 데이터셋을 이용하여성능 분석한 결과 약 99.5%의 식별 성능을 보였다.   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "10                                                                                                                                                                                                                                                                                                                                  4차 산업혁명과 함께 디지털 전환(Digital Transformation, DX) 기술이 중요해지고 있다. 이와 함께 인공지능을 통한 생산공정에서의 불량 검출 및 분류에 대한 연구가 활발히 이루어지고 있다. 본 논문에서는 다양한 CNN 모델을 사용하여 컬러 콘택트렌즈 생산공정에서 발생하는 불량 검출을 효과적으로 수행하는 모델을 선정하고자 하며, 이를 통해 생산 및 품질의 향상을 이루어, 자원의 낭비와 소비자의 안전을 확보하고자 한다. 이를 위해 컬러 콘택트렌즈 영상에 대한 전처리와 증강을 통해 학습 및 검증  데이터를 생성하였으며, RGB 및 HSV 채널 영상에 대해 ResNet101, GoogLeNet V2, GoogLeNet V4, DenseNet121, MobileNet의 CNN 기법을 활용하여 RGB와 HSV 채널별로 불량 탐지율 비교 분석하였다. 위 모델의 정확도는 순서대로 각각 89.74%, 84.46%, 95.43%, 82.80%, 89.74%로, RGB 채널의 GoogLeNet V4가 가장 높은 불량 검출 정확도를 얻었으며, 대부분의 모델에서 RGB 채널이 HSV 채널보다 더 좋은 결과를 얻어냄을 알 수 있었다.   \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                동물보호법 개정에 따라 반려견 등록이 의무화 되었음에도 불구하고, 현재 등록 방법의 불편함으로 등록율이 저조한 상태이다. 본논문에서는 새로운 등록 방법으로 검토되고 있는 반려견 안면 인식 기술에 대한 성능 개선 연구를 진행하였다. 딥러닝 학습을 통해,반려견의 안면 인식을 위한 임베딩 벡터를 생성하여 반려견 개체별로 식별하기 위한 방법을 실험하였다. 딥러닝 학습을 위한 반려견이미지 데이터셋을 구축하고, InceptionNet과 ResNet-50을 백본 네트워크로 사용하여 실험하였다. 삼중항 손실 방법으로 학습하였으며,안면 검증과 안면 식별로 나뉘어 실험하였다. ResNet-50 기반의 모델에서 최고 93.46%의 안면 검증 성능을 얻을 수 있었으며, 안면식별 시험에서는 rank-5에서 91.44%의 최고 성능을 각각 얻을 수 있었다. 본 논문에서 제시한 실험 방법과 결과는 반려견의 등록 여부 확인, 반려견 출입시설에서의 개체 확인 등 다양한 분야로 활용이 가능하다.   \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        토마토 작물은 병해에 노출이 쉽고 단시간에 퍼지므로 병해에 대한 늦은 조치로 인한 피해는 생산량과 매출에 직접적인 영향을 끼친다.  따라서, 토마토의 병해에 대해 누구나 현장에서 간편하고 정확하게 진단하여 조기 예방을 가능하게 하는 서비스가 요구된다. 본 논문에서는 사전에 ImageNet 전이 학습된 딥러닝 기반 모델을 적용하여 토마토의 9가지 병해 및 정상인 경우의 클래스를 분류하고 서비스를 제공하는 시스템을 구성한다. Plant Village 데이터 셋으로부터 토마토 병해 및 정상을 분류한 잎의 이미지 셋을 합성곱을 사용하여 조금 더 가벼운 신경망을 구축한 딥러닝 기반 CNN구조를 갖는 MobileNet, ResNet의 입력을 사용한다. 2가지 제안 모델의 학습을 통해 정확도와 학습속도가 빠른 MobileNet를 사용하여 빠르고 편리한 서비스를 제공할 수 있다.   \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                              사람의 건강 상태를 파악하는 것은 응급상황 예방 및 대처에 있어 매우 중요하다. 호흡 상태와 같은 생활 징후를 파악하는 것은 응급상황 발생 여부를 확인할 수 있는 주요한 요소로 작용한다. 이때, 운전자의 생체신호로부터 운전 중에 발생하는 응급상황을 모니터링할 수 있다면, 운전자의 제어권 상실로 인해 발생하는 교통사고에 대한 대처가 가능하다. 본 논문에서는 운전자의 응급상황을 모니터링하기 위해 운전자의 흉부 움직임을 측정하고, 측정된 데이터를 통해 운전자의 호흡 상태를 분류한다. 운전자의 호흡 상태는 관성측정장치를 사용하여 측정하고, Kalman filter를 통해 가속도와 자이로 센서값에 대한 노이즈를 제거한다. 이후, 운전자의 호흡 상태를 정확히 분류하기 위해 데이터를 이미지로 가시화하며, ResNet을 통한 학습을 진행한다. 각 이미지화된 데이터에 대한 분류 성능을 비교하고, 가장 높은 정확도를 보이는 관성측정장치의 데이터를 확인한다.   \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "20                                                                                                                                                                                                                                                                                                                                                  센서 데이터를 활용하여 설비의 이상 진단이 가능해졌다. 하지만 설비 이상에 대한 원인 분석은 미비한 실정이다. 본 연구에서는 센서 기반 시계열 데이터 분류 모델을 위한 해석가능한 합성곱 신경망 프레임워크를 제안한다. 연구에서 사용된 센서 기반 시계열 데이터는 실제 차량에 부착된 센서를 통해 수집되었고, 반도체의 웨이퍼 데이터는 공정 과정에서 수집되었다. 추가로 실제 기계 설비에서 수집된 주기 신호 데이터를 이용 하였으며, 충분한 학습을 위해 Data augmentation 방법론인 Scaling과 Jittering을 적용하였다. 또한, 본 연구에서는 3가지 합성곱 신경망 기반 모델들을 제안하고 각각의 성능을 비교하였다. 본 연구에서는 ResNet에 Jittering을 적용한 결과 정확도 95%, F1 점수 95%로 가장 뛰어난 성능을 보였으며, 기존 연구 대비 3%의 성능 향상을 보였다. 더 나아가 결과의 해석을 위한 XAI 방법론으로 Class Activation Map과 Layer Visualization을 제안하였으며, 센서 데이터 분류에 중요 영향을 끼치는 시계열 구간을 시각적으로 확인하였다.   \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                 본 연구에서는 화재 안전 향상을 위한 엣지 컴퓨팅(edge computing) 기반 화재감지시스템에 적용 가능한 합성곱 신경망 기반 이미지 분류 모델들인 MobileNetV2, ResNet101, EfficientNetB0를 이용하여 화재 예측 성능 해석을 수행하였다. 성능평가지표인 정확도, 재현율, 정밀도, F1-score와 혼동 행렬을 이용하여 화재 예측 성능을 비교 분석하였다. 또한 분류 모델의 경량화와 관련한 모델 용량 및 추론시간에 대한 비교 분석을 수행하였다. 비교 분석 결과로서 화재 예측 정확도는 EfficientNetB0 모델이 가장 높았으며 경량성 측면에서는 MobileNetV2가 가장 우수한 것으로 확인하였다. 더하여 화재와 유사한 특징을 갖는 비 화재 이미지인 빛과 연무에 대한 이미지 특성을 추가 학습한 결과, 경량성은 우수하나 예측 성능이 낮은 MobileNetV2의 화재 예측 정확도가 개선되는 것을 확인하였다.   \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                     포인트 클라우드의 형상을 분류하기 위해 심층신경망을 활용할 때, 점들의 좌푯값만을 활용하거나 연산 부담이 큰 3차원 렌더링을 통해 생성한 이미지를 활용하여 형상 분류를 수행한다. 본 연구에서는 좌푯 값과 해당 좌표를 활용해 생성한 투영 이미지를 함께 다중 모달로 심층신경망의 입력으로 활용하는 포인트 클라우드의 형상 분류 기법을 제안한다. 성능 향상 여부를 확인하고자, 좌푯값 기반으로 형상을 분류하는 PointNet과 이미지 기반 분류 모델인 ResNet-18을 조합하여 다중 모달 모델을 구성하고 ModelNet40 데이터셋에 대해서 투영 이미지의 여부 및 방향(등각면, 정면, 측면, 상면)에 따른 성능 평가를 수행하였다. 그 결과 측면 투영 이미지가 함께 고려될 때 가장 성능이 좋았으며, 정면 투영 이미지의 경우가 두 번째로 우수한 성능을 보였다. 이는 포인트 클라우드의 형상 분류에 있어서 좌푯값과 더불어 투영 이미지를 함께 입력으로 활용하는 것이 형상 분류에 효과적임을 뒷받침한다.   \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              본 논문에서는 CNN(Convolutional Neural Network)을 구성하는 주요 연산 모듈을 모듈 제어 명령어를 통해 구동함으로써 네트워크를 구현할 수 있는 CNN 가속기를 제안한다. 모듈 기반 CNN 가속기는 합성곱(Convolution), 풀링(Pooling) 등 CNN의 주요 연산 모듈로 구성되어 있으며, 프로세서에서 모듈 제어 명령어를 통해 네트워크 구성에 필요한 연산 모듈을 선택 및 내부 파라미터를 설정 할 수 있다. 본 논문에서 제안하는 모듈 기반 CNN 가속기를 사용하여 Xilinx SoC형 FPGA에 ResNet-18을 구현하였으며 CNN 프레임워크 모델인 PyTorch와 C 기반 검증 모델을 사용하여 출력 결과를 비교 검증하였다. 실험결과, CNN 가속기의 추론 결과는 92.87%의 정확도를 보였다.   \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "27                                                                                                                                                                                                                                                                                                                                                         본 연구에서는 감정을 표현하기 위한 표정 연습을 보조하는 인공지능을 개발하였다. 개발한 인공지능은 서술형 문장과 표정 이미지로 구성된 멀티모달 입력을 심층신경망에 사용하고 서술형 문장에서 예측되는 감정과 표정 이미지에서 예측되는 감정 사이의 유사도를 계산하여 출력하였다. 사용자는 서술형 문장으로 주어진 상황에 맞게 표정을 연습하고 인공지능은 서술형 문장과 사용자의 표정 사이의 유사도를 수치로 출력하여 피드백한다. 표정 이미지에서 감정을 예측하기 위해 ResNet34 구조를 사용하였으며 FER2013 공공데이터를 이용해 훈련하였다. 자연어인 서술형 문장에서 감정을 예측하기 위해 KoBERT 모델을 전이학습 하였으며 AIHub의 감정 분류를 위한 대화 음성 데이터 세트를 사용해 훈련하였다. 표정 이미지에서 감정을 예측하는 심층신경망은 65% 정확도를 달성하여 사람 수준의 감정 분류 능력을 보여주었다. 서술형 문장에서 감정을 예측하는 심층신경망은 90% 정확도를 달성하였다. 감정표현에 문제가 없는 일반인이 개발한 인공지능을 이용해 표정 연습 실험을 수행하여 개발한 인공지능의 성능을 검증하였다.   \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                발레는 몸의 가동 범위가 넓은 운동인 만큼 정확하지 않은 자세로 운동을 하게 되면 부상을 입을 수 있고, 운동 효과를 느끼기 어렵다는 문제점이 있다. 이에 사용자가 올바른 자세로 운동을 할 수 있도록 돕는 발레 코칭 시스템을 제안한다. 제안한 방법은 15개의 관절이 레이블링 된 발레 동작을 기반으로 ResNet 기반의 Simple pose 모델과 Open pose의 BODY-25 모델을 사용하여 관절의 좌표를 검출한다. 검출한 관절 좌표의 분포를 히스토그램의 분위 수 분석을 통해 분석했다. 6가지 분류기를 통해 3가지 발레 동작 분류를 4가지 성능 지표를 통해 분석한다. Gradient Boosting Classifier가 비교한 모델 중 가장 최적의 성능 획득했다. 특징 중요도 분석 결과, 발목 관절이 중요하게 사용됨을 확인했다. 또한 4가지 특징 추출 방법에 따른 모델의 성능을 분석했다. 그리고 Odds ratio를 구하여 동작 별 예측 성공 비율을 비교했다. 제안한 방법이 발레 동작 코칭을 할 수 있음을 확인했다.   \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                     최근 딥러닝 기술의 발달로 사람의 행동을 인식하는 연구가 진행 중에 있다. 본 논문에서는 딥러닝 기술을 활용하여 1인 가구 환경에서 발생할 수 있는 위험 행동을 인식하는 연구를 진행하였다. 1인 가구의 특성상 개인의 프라이버시 보호가 필요하다. 본 논문에서는 개인의 프라이버시 보호를 위해 가우시안 블러 필터가 적용된 프라이버시 보호 영상에서 사람의 위험 행동을 인식한다. 위험 행동 인식 방법은 객체 검출 모델인 YOLOv5 모델을 활용하여 영상에서 사람 객체 검출 및 전처리 방법을 적용한 후 행동 인식 모델의 입력값으로 활용하여 위험 행동을 인식한다. 실험에는 ResNet3D, I3D, SlowFast 모델을 사용하였고, 실험 결과 SlowFast 모델이 프라이버시 보호 영상에서 95.7%로 가장 높은 정확도를 달성하였다. 이를 통해 개인의 프라이버시를 보호하면서 1인 가구 환경에서 사람의 위험 행동을 인식하는 것이 가능하다.   \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "32                                                                                                                                                                               현대에는 급속한 산업화와 인구 증가로 인해 도시들이 더욱 복잡해지고 있다. 특히 도심은 택지개발, 재건축, 철거 등으로 인해 빠르게 변화하는 지역에 해당한다. 따라서 자율주행에 필요한 정밀도로지도와 같은 다양한 목적을 위해 빠른 정보 갱신이 필요하다. 우리나라의 경우 기존 지도 제작 과정을 통해 지도를 제작하면 정확한 공간정보를 생성할 수 있으나 대상 지역이 넓은 경우 시간과 비용이 많이 든다는 한계가 있다. 지도 요소 중 하나인 도로는 인류 문명을 위한 많은 다양한 자원을 제공하는 중추이자 필수적인 수단에 해당한다. 따라서 도로 정보를 정확하고 신속하게 갱신하는 것이 중요하다. 이 목표를 달성하기 위해 본 연구는 Semantic Segmentation 알고리즘인 LinkNet, D-LinkNet 및 NL-LinkNet을 사용하여 광주광역시 도시철도 2호선 공사 현장을 촬영한 드론 정사영상에서 도로를 추출한 다음 성능이 가장 높은 모델에 하이퍼 파라미터 최적화를 적용하였다. 그 결과, 사전 훈련된 ResNet-34를 Encoder로 사용한 LinkNet 모델이 85.125 mIoU를 달성했다. 향후 연구 방향으로 최신 Semantic Segmentation 알고리즘 또는 준지도 학습 기반 Semantic Segmentation 기법을 사용하는 연구의 결과와의 비교 분석이 수행될 것이다. 본 연구의 결과는 기존 지도 갱신 프로세스의 속도를 개선하는 데 도움을 줄 수 있을 것으로 예상된다.   \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                          본 논문은 사용자들에 의해 촬영된 피부이미지를 가공하여 데이터 세트를 구축하고, 제안한 영상처리 기법에 의해 모공 특징이미지를 생성하여, CNN(Convolution Neural Network) 모델 기반의 모공 상태 등급 예측 시스템을 구현한다. 본 논문에서 활용하는피부이미지 데이터 세트는, 피부미용 전문가의 육안 분류 기준에 근거하여, 모공 특징에 대한 등급을 라벨링 하였다. 제안한 영상처리 기법을 적용하여 피부이미지로 부터 모공 특징 이미지를 생성하고, 모공 특징 등급을 예측하는 CNN 모델의 학습을 진행하였다.제안한 CNN 모델에 의한 모공 특징은 전문가의 육안 분류 결과와 유사한 예측 결과를 얻었으며, 비교 모델(Resnet-50)에 의한 결과보다 적은 학습시간과 높은 예측결과를 얻었다. 본 논문의 본론에서는 제안한 영상처리 기법과 CNN 적용의 결과에 대해 서술하며, 결론에서는 제안한 방법에 대한 결과와 향후 연구방안에 대해 서술한다.   \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "35                                                                                                                                                                                                                                                                                                                                                     음성인식 기술과 인공지능 기술을 기반으로 한 스마트스피커의 보급으로 비장애인뿐만 아니라 시각장애인이나 지체장애인들도 홈 네트워크 서비스를 연동하여 주택의 전등이나 TV와 같은 가전제품을 음성을 통해 쉽게 제어할 수 있게 되어 삶의 질이 대폭 향상되었다. 하지만 언어장애인의 경우 조음장애나 구음장애 등으로 부정확한 발음을 하게 됨으로서 스마트스피커의 유용한 서비스를 사용하는 것이 불가능하다. 본 논문에서는 스마트스피커에서 제공되는 기능 중 일부 서비스를 대상으로 언어장애인이 이용할 수 있도록 개인화된 음성분류 기법을 제안한다. 본 논문에서는 소량의 데이터와 짧은 학습시간으로도 언어장애인이 구사하는 문장의 인식률과 정확도를 높여 스마트스피커가 제공하는 서비스를 실제로 이용할 수 있도록 하는 것이 목표이다. 본 논문에서는 ResNet18 모델을 fine tuning하고 데이터 증강과 one cycle learning rate 최적화 기법을 추가하여 적용하였으며, 실험을 통하여 30개의 스마트스피커 명령어 별로 10회 녹음한 후 3분 이내로 학습할 경우 음성분류 정확도가 95.2% 정도가 됨을 보였다.   \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                               이번 연구는 소아의 인접면 우식을 진단하는데 있어 사용하고 있는 구내방사선 사진에서 심층학습(deep learning) 알고리즘을 활용하여 치아우식을 진단하는 모델의 성능을 평가하고자 하였다.제1유구치와 제2유구치 사이의 인접면이 포함된 500개의 구내방사선 사진을 대상으로 연구를 시행하였다. 치아우식을 진단하는 모델의 학습에는 Resnet50 기반의 인공신경망 모델을 사용하였다. 평가자료군에서 진단모델의 정확도, 민감도, 특이도를 구하고, ROC 곡선을 얻어 AUC 값을 바탕으로 분류 모델의 성능을 평가하였다.학습 모델의 정확도는 0.84, 민감도는 0.74, 특이도는 0.94로 나타났으며 AUC는 0.86으로 나타났다.인공신경망을 기반으로 하는 소아의 구내방사선 사진에서의 인접면 우식의 진단 모델은 비교적 높은 정확도를 보여주었다. 심층학습 모델은 구내방사선 사진상에서 인접면 우식을 진단하는데 있어 향후 치과의사를 보조하는 진단 도구로서 활용될 수 있을 것이다.   \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              최근 IoT 기술의 발달로 외출 중에도 반려동물에 급여하도록 자동 사료급식기가 유통되고 있다. 그러나 자동급식에서 중요한 중량을 측정하는 저울 방식은 쉽게 고장이 나고, 3D카메라 방식은 비용이 든다는 단점이 있으며, 2D카메라 방식 은중량측정의정확도가떨어진다. 특히사료가복합된경우중량측정문제는더욱어려워질수있다. 따라서본연구의 목적은 2D카메라를 사용하면서도 중량을 정확하게 추정할 수 있는 딥러닝 접근법을 제안하는 것이다. 이를 위해 다양한 합성곱 신경망을 이용하였으며, 그중 ResNet101 기반 모델이 3.06 gram의 평균 절대 오차와 3.40%의 평균 절대비 오차를 기록하며 가장 우수한 성능을 보였다. 본 연구의 결과로 사료와 같이 규격화된 물체의 중량을 확보가 용이한 2D 이미지를 통해서만 예측할 필요가 있을 경우 유용한 정보로 활용될 수 있다.   \n",
      "41                                                                                                                                                                                                                                                                                                                                             자동차용 배터리 제조공정 가운데 하나인 Tab Welding 공정에서 생산된 제품의 샘플링 인장검사를 대체하기 위해 현재 비전검사기를 개발하여 사용하고 있다. 그러나, 비전검사는 검사 위치 오차 문제와 이를 개선하기 위해 발생하는 비용 문제를 가지고 있다. 이러한 문제점들을 해결하기 위해 최근 딥러닝 기술을 적용하는 사례들이 발생하고 있다. 본 논문도 그런 사례 중 하나로 기존 제품 검사에 딥러닝 기술 중 하나인 Faster R-CNN을 적용하여 그 유용성을 파악하고자 하였다. 기존 비전검사기를 통해 획득한 이미지들을 학습 데이터로 사용하여 Faster R-CNN ResNet101 V1 1024x1024 모델을 사용하여 학습하였다. 검사 기준인 미검률 0%, 과검률 10%의 기준으로 기존 비전검사와 Faster R-CNN 검사결과를 비교 분석하였다. 미검출률은 기존 비전검사에서 34.5%, Faster R-CNN 검사에서 0%였다. 과검출률은 기존 비전검사에서 100%, Faster R-CNN에서 6.9%였다. 결론적으로 자동차용 배터리 리드탭 암흔 오류 검출에 딥러닝 기술이 매우 유용함을 확인할 수 있었다.   \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "68                                                                                                                                                                                                                                                                                                                                                                 디지털트윈의 중요한 요소는 실제 객체에 대한 폐색영역이 없는 실감형 공간정보를 구축하고 제공하여야 한다. 하지만 3차원 공간정보 모델링에서는 촬영 각도와 촬영 시기로 인해 필연적으로 폐색영역이 발생한다. 이에 최신 기술인 인공지능 기술을 이용한 특정객체의 검출과 이를 제거하고 복원할 수 있는 기술을 구현하고자 한다. 연구에서는 인공지능 모델 중에서 ResNet 알고리즘을 사용하 폐색 유발 객체를 자동으로 검출하고, 텍스처링 영상에서 검출된 폐색영역을 삭제하여 특정 알고리즘을 적용하여 해당 건물의 텍스처링 영상에서 폐색영역을 복원하는 복원 기술을 개발하였다. 실험데이터는 건물 모델링에서 가장 많은 폐색을 유발하는 가로수를 대상으로 최신모델인 ResNet 학습모델을 사용하여 폐색영역을 유발하는 가로수 객체 데이터셋을 만들고 자동으로 검출하였으며 그 결과를 분석하였다. 연구의 결과로는 ResNet 알고리즘을 이용하여 비정형 데이터인 가로수를 검출할 수 있으며, DeepFillv2 알고리즘을 이용하여 가로수를 제거하고 인접 픽셀을 이용하여 상가와 아파트 텍스처링 영상을 복원하였다.   \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "73                                                                                                                                                                                                                                                                                                                                                                                                                       본 논문에서는 영상 데이터에 대한 적대적 공격으로부터 생성된 적대적 예제로 인하여 발생할 수 있는 딥러닝 시스템의 오분류를 방어하기 위한 방법으로 분류기의 입력 영상에 백색 잡음을 가산하는 방법을 제안하였다. 제안된 방법은 적대적이든 적대적이지 않던 구분하지 않고 분류기의 입력 영상에 백색 잡음을 더하여 적대적 예제가 분류기에서 올바른 출력을 발생할 수 있도록 유도하는 것이다. 제안한 방법은 FGSM 공격, BIM 공격 및 CW 공격으로 생성된 적대적 예제에 대하여 서로 다른 레이어 수를 갖는 Resnet 모델에 적용하고 결과를 고찰하였다. 백색 잡음의 가산된 데이터의 경우 모든 Resnet 모델에서 인식률이 향상되었음을 관찰할 수 있다. 제안된 방법은 단순히 백색 잡음을 경험적인 방법으로 가산하고 결과를 관찰하였으나 에 대한 엄밀한 분석이 추가되는 경우 기존의 적대적 훈련 방법과 같이 비용과 시간이 많이 소요되는 적대적 공격에 대한 방어 기술을 제공할 수 있을 것으로 사료된다.   \n",
      "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   주가는 거래량, 종가 등과 같은 숫자 기반의 내부적인 요인뿐만 아니라 법, 유행 등 여러 외부요인에 의해 영향을 받는다. 수많은 요인이 주가에 영향을 미치기 때문에 단편적인 주식 데이터만을 이용한 정확한 주가 예측은 매우 어려운 일이다. 특히 기업의 가치는 실제 주식을 거래하는사람들의 인식에 영향을 많이 받기 때문에 특정 기업에 대한 감성 정보가 중요한 요인으로 여겨진다. 본 논문에서는 시간적 특성을 고려한 뉴스 데이터의 감성 분석을 이용한 딥러닝 기반 주가예측 모델을 제안하고자 한다. 주식과 뉴스 데이터, 서로 다른 특성을 가진 2개의 이종 데이터를시간 크기에 따라 통합하여 모델의 입력으로 사용하며, 시간 크기와 감성 지표가 주가 예측에 미치는 영향에 대해 최종적으로 비교 및 분석한다. 또한 우리는 기존 모델과의 비교 실험을 통해제안 모델의 정확성이 개선되었음을 검증한다.   \n",
      "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    심전도 신호는 개인에 따라 형태와 특징이 다양하므로, 하나의 신경망으로는 분류하기가 어렵다. 주어진 데이터를 직접적으로 분류하는 것은 어려우나, 대응되는 정상 데이터가 있을 경우, 이를 비교하여 정상 및 비정상을 분류하는 것은 상대적으로 쉽고 정확하다. 본 논문에서는 템플릿 군을 이용하여 대표정상심박 정보를 획득하고, 이를 입력심박에 결합함으로써 심박을 분류한다. 결합된 심박을 영상화한 후, 학습 및 분류를 진행하여, 하나의 신경망으로도 다양한 레코드의 비정상심박을 검출이 가능하였다. 특히, GoogLeNet, ResNet, DarkNet 등 다양한 신경망에 대해서도 비교학습 기법을 적용한 결과, 모두 우수한 검출성능을 가졌으며, GoogLeNet의 경우 99.72%의 민감도로, 실험에 사용된 신경망 중 가장 우수한 성능을 가졌음을 확인하였다.   \n",
      "78                                                                                                                                                                                                                                                                                                                                                                            AI 기술이 여러 분야에 성공적으로 도입되는 추세이며, 서비스로 환경에 배포된 모델들은 지적 재산권과 데이터를 보호하기 위해 모델의 정보를 노출시키지 않는 블랙 박스 상태로 배포된다. 블랙 박스 환경에서 공격자들은 모델 출력을 이용해 학습에 쓰인 데이터나 파라미터를 훔치려고 한다. 본 논문은 딥러닝 모델을 대상으로 모델 종류에 대한 정보를 추론하는 공격이 없다는 점에서 착안하여, 모델의 구성 레이어 정보를 직접 알아내기 위해 모델의 종류를 추론하는 공격 방법을 제안한다. MNIST 데이터셋으로 학습된 ResNet, VGGNet, AlexNet과 간단한 컨볼루션 신경망 모델까지 네 가지 모델의 그레이 박스 및 블랙 박스 환경에서의 출력값을 이용해 모델의 종류가 추론될 수 있다는 것을 보였다. 또한 본 논문이 제안하는 방식인 대소 관계 피쳐를 딥러닝 모델에 함께 학습시킨 경우 블랙 박스 환경에서 약 83%의 정확도로 모델의 종류를 추론했으며, 그 결과를 통해 공격자에게 확률 벡터가 아닌 제한된 정보만 제공되는 상황에서도 모델 종류가 추론될 수 있음을 보였다.   \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                          이미지 인식에 특화된 CNN (Convolutional Neural Network) 기반의 딥러닝 기법은 영상의 항목별 분류가 필요한 다양한 연구에 적용되고있다. 본 연구는 건물, 도로, 논, 밭, 산림, 나지의 6가지 항목을 산림복원 대상 후보지로 정의하고 CNN 기반의 산림복원 대상 후보지 추출 및분류의 최적 방법론을 탐색하였다. 6,640개의 데이터셋을 75:25의 비율로 훈련(4,980개) 및 검증(1,660개)로 구분하여 구축하고 학습에 활용하였다.모델별 정확도는 픽셀정확도(PA), 평균 교차 겹침 결합(Mean IoU)을 이용하여 평가하였다. 픽셀정확도는 90.6%, 평균 교차 겹침 결합은 80.8%로산정되어 Inception-Resnet-v2 모델이 세 모델 중 가장 산림복원 대상 후보지 추출에 뛰어난 정확도를 보였다. 이 결과는 기존의 산림복원 대상후보지 현장조사 혹은 항공사진을 활용한 조사에 비해 시공간적 이점을 가지며, 향후 산림복원 대상지 선정 자료로 적용 가능성이 있다고 판단된다.   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                최근 합성곱 신경망(CNN)을 머신 비전에 적용함으로써 비정형 결함의 검사에서 우수한 검사 성능을 보이고 있다. 그러나 머신 비전 시스템에서 CNN을 훈련하기 위해 충분한 양의 데이터를 모으고 정리하는 것은 상당한 시간이 필요하다. 이 문제를 해결하기 위해 본 연구는 모노 카메라를 사용하는 머신 비전 시스템에서 CNN을 사용하여 비정형 결함을 검사할 때 사용할 수 있는 데이터 증식 방법을 제안한다. 임의의 패턴에서 비정형의 결함을 검출하기 위해 제작된 DAGM 2007 데이터 중 1번 하위 클래스 데이터를 실험 데이터로 사용하였다. 결함 검출을 위한 CNN은 Mask R-CNN ResNet 50을 사용하였다. 제안하는 방법을 통해 증식한 데이터로 훈련한 CNN이 원본 회색조 이미지로 검증을 수행했을 때 원본 데이터로 훈련한 CNN에 비해 Mask mAP@0.5:0.95 기준 평균 16.73%p 높은 61.38%의 정확도를 보이는 것을 확인했다. 본 연구에서 적용한 데이터 증식 방법을 통해 모노 카메라를 활용하는 머신 비전시스템에서 우수한 성능을 가진 CNN을 훈련할 수 있다.   \n",
      "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      본 연구는 딥러닝 기본모델인 DNN, LSTM, BiLSTM, 1D-CNN 등으로, 예측의 성능을 향상하기 위해, 중간층을 병렬로 병합하는 구조를 제안하였다. 제안모델 1은 동일한 기본모델을 병렬 병합한 구조이고, 제안모델 2는 서로 다른 모델의 병렬 병합한 구조이다. 각 모델의 평가는 RMSE와 MAE로 10회 실험에 대한 평균값이다. 제안모델 1에서 가장 좋은 예측률을 보인 BiLSTM2의 RMSE는 0.064이다. 제안모델 2의 RMSE는 DC(DNN-CNN), LC(BiLSTM-CNN), DLC(DNN-BiLSTM-CNN) 등 모든 모델이 0.054였다. 이처럼 제안모델 2가 12.8%의 성능 향상을 보였다. 제안모델 2가 서로 다른 모델의 장점을 유지하면서 많은 파라메터로, 예측률을 향상할 수 있게 하는 모델임을 확인할 수 있었다.   \n",
      "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              최근에 늘어가는 키오스크(KIOSK)의 수요에 따라 불편함을 호소하는 이용자가 많아졌다. 이에 음성 기반 대화형 서비스를 구현하여 손쉽게 메뉴 선택 및 주문을 가능하게 해주는 키오스크를 제작해 웹의 형태로 제공한다. Annyang API와SpeechSynthesis API를 바탕으로 음성 기능을 구현하고 Dialogflow를 통해 사용자의 의도를 파악하는 과정을 Rest API를 기반으로 구현하는 방법에 대해 논한다. 또한 협업 필터링을 기반으로 추천 시스템을 적용하여 기존 키오스크의 낮은 소비자 접근성을 개선하였고, 음성인식 서비스 이용 도중 발생하는 비말로 인한 감염을 예방하기 위해 서비스 이용 전 마스크 착용을 확인하는 기능을 제공한다.   \n",
      "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               본 논문은 유사 아동 그림 선별 알고리즘 생성을 위한 Triplet Loss 기반 딥러닝 모델 설계를 목적으로 한다. 아동 그림들 사이 유사성 측정을 위해서는 동일 클래스에 속하는 그림 간 특징 벡터의 거리는 가까워야 하고 다른 클래스 간 특징 벡터의 거리는 멀어져야 한다. 따라서, 본 연구에서는 클래스 수가 많아지는 경우에 이미지 유사성 측정에 이점을 지닌 Triplet Loss와 잔여 네트워크(ResNet)를 결합한 딥러닝 모델을 구축하여 유사 아동 그림 선별 알고리즘을 생성하였다. 결론적으로 본 모델을 활용한 유사 아동 그림 선별 알고리즘을 통해 대상 아동 그림과 다른 그림 간의 유사성을 측정하고 유사성이 높은 그림을 선별할 수 있다.   \n",
      "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "91                                                                                                                                                                                                                                                                                                                        최근 스마트팩토리와 인공지능 기술의 수요 증가로 인해 다양한 분야에서 인공지능 기술을 적용하는 연구가 진행되고 있다. 결함 검사 분야에서도 인공지능 알고리즘을 도입하기 위한 노력을 기울이고 있다. 특히, 금속 외관의 결함을 검출하는 연구는 다른 소재(목재, 플라스틱, 섬유 등)의 결함을 검출하는 연구에 비해 많은 연구가 이루어지고 있다. 본 논문에서는 머신러닝 기법(서포터 벡터 머신(SVM: Support Vector Machine), 소프트맥스 회귀(Softmax Regression), 결정 트리(Decesion Tree))과 차원 축소 알고리즘(주성분 분석(PCA: Principal Component Analysis), 오토인코더(AutoEncoder))의 9가지 조합과 2가지 합성곱신경망(CNN: Convolution Neural Network) 기법(자체 알고리즘, ResNet)의 금속 외관의 결함 분류 성능 및 속도를 비교하고 분석하는 연구를 수행하고자 한다. 두 종류의 학습 데이터셋((i) 공용 데이터셋, (ii) 실측 데이터셋)에 대한 실험을 통해 각 데이터셋에 대한 성능 및 속도를 비교 분석하고, 가장 효율적인 알고리즘을 찾아낸다.   \n",
      "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "98  최근, 고관절 전치환술의 진행에 있어, 제조사마다 규격이 다른 인공관절의 특성으로 인해 과거에 삽입했던 제품과 호환되는 제품을 반드시 사용해야 하는 고질적인 문제점이 있다. 이런 문제를 해결하기 위해 집도의는 수술에 들어가기 전에 X-Ray이미지를 보고 경험치료를 바탕으로 가장 비슷한 인공 보철물을 준비한다. 이러한 준비 과정은 의사의 경험에 따라 오차가 발생할 위험이 있다. 이에 따라 전 세계의 많은 고관절 인공 보철물 제조사는 자사의 데이터를 활용하여 인공 보철물 제품군의 이미지 판독 시스템의 개발이 활발히 이루어지고 있는 추세이다. 따라서 본 연구는 의료영상처리분야에서 시각 정보에 기반한 고차원적 추상화를 통한 병변 진단 및 예후 예측에 뛰어나 성능을 보이는 CNN(Convolutional Neural Network)기반의 다양한 아키텍처 모델들의 성능을 비교한다. 연구방법은 다음과 같다. 국내 유일의 인공고관절 제조업체인 ㈜코렌텍에서 제공한 자사 제품이 사용된 X-Ray 이미지와 Kaggle 데이터셋 ‘Aseptic Loose Hip Implant X-Ray Database’을 활용한다. 이때 사용되는 X-Ray 이미지는 환자 정보에 대한 비식별화가 된 상태이다. 습득한 X-Ray 이미지는 인공 보철물이 나타난 부분을 뜯어내 배경을 제거한 후 이진화 작업, 특정 영역 외 객체 제거, 이미지 반전 등 데이터 전처리 작업을 거치고, 전처리된 데이터셋을 이진 분류모델에 맞게 변환한다. 이후 CNN기반의 다양한 아키텍처 모델에 적용하여 각 모델에 대한 성능을 비교한다. 성능비교에 사용한 CNN기반 모델은 CNN, VGG16, GoogLe Net/Inception Net, Xception Net, Mobile Net, ResNet 총 6개이며, 각 모델에 대한 특징과 연결점에 대해 설명한다.   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Due to the accelerated global warming phenomenon after industrialization, the frequency of changes in the existing environment and abnormal climate is increasing. Agriculture is an industry that is very sensitive to climate change, and global warming causes problems such as reducing crop yields and changing growing regions. In addition, environmental changes make the growth period of crops irregular, making it difficult for even experienced farmers to easily estimate the growth stage of crops, thereby causing various problems. Therefore, in this paper, we propose a CNN model for estimating the growth stage of crops. The proposed model was a model that modified the pooling layer of ResNet, and confirmed the accuracy of higher performance than the growth stage estimation of the ResNet and DenseNet models.  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               None  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We construct a deep neural network model named ECGResNet. This model can diagnosis diseases based on 12-lead ECG data of eight common cardiovascular diseases with a high accuracy. We chose the 16 Blocks of ResNet50 as the main body of the model and added the Squeeze-and-Excitation module to learn the data information between channels adaptively. We modified the first convolutional layer of ResNet50 which has a convolutional kernel of 7 to a superposition of convolutional kernels of 8 and 16 as our feature extraction method. This way allows the model to focus on the overall trend of the ECG signal while also noticing subtle changes. The model further improves the accuracy of cardiovascular and cerebrovascular disease classification by using a fully connected layer that integrates factors such as gender and age. The ECGResNet model adds Dropout layers to both the residual block and SE module of ResNet50, further avoiding the phenomenon of model overfitting. The model was eventually trained using a five-fold cross-validation and Flooding training method, with an accuracy of 95% on the test set and an F1-score of 0.841.We design a new deep neural network, innovate a multi-scale feature extraction method, and apply the SE module to extract features of ECG data.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Simulating the heat transfer in a composite material is an important topic in material science. Difficulties arise from the fact that adjacent materials cannot match perfectly, resulting in discontinuity in the temperature variables. Although there have been several numerical methods for solving the heat-transfer problem in imperfect contact conditions, the methods known so far are complicated to implement, and the computational times are non-negligible. In this study, we developed a ResNet-type deep neural network for simulating a heat transfer model in a composite material. To train the neural network, we generated datasets by numerically solving the heat-transfer equations with Kapitza thermal resistance conditions. Because datasets involve various configurations of composite materials, our neural networks are robust to the shapes of material-material interfaces. Our algorithm can predict the thermal behavior in real time once the networks are trained. The performance of the proposed neural networks is documented, where the root mean square error (RMSE) and mean absolute error (MAE) are below 2.47E-6, and 7.00E-4, respectively.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The author present an artificial intelligent (AI)-based deep generative model that demonstrate how to generate design options of mechanical systems, which are not only suitable for specific working conditions but also optimized for engineering performance. In current study, (1) a structural generative residual netowork (SG-Resnet) model is developed to establish the non-linear mapping between the working conditions and the external dimensions of the reducer, the main hyperparameters influencing the prediction ability and learning rate of the SG-Resnet are analyzed. (2) The mixed population non dominated sorting genetic algorithm-II (MP-NSGA-II) is proposed, and used to obtain pareto optimal solutions of the internal dimensions of the reducer. Experiments are performed to validate the positive effect of the structural generative model on the stiffness of the reducer. This research provides a novel method for reducer design and lays a solid foundation for the development of sequential engineering software for integrated rotate vector (RV) reducer.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Identifying defects in textiles is a key procedure for quality control. This study attempted to create a model that detects defects by analyzing the images of the fabrics. The models used in the study were deep learning-based VGGNet and ResNet, and the defect detection performance of the two models was compared and evaluated. The accuracy of the VGGNet and the ResNet model was 0.859 and 0.893, respectively, which showed the higher accuracy of the ResNet. In addition, the region of attention of the model was derived by using the Grad-CAM algorithm, an eXplainable Artificial Intelligence (XAI) technique, to find out the location of the region that the deep learning model recognized as a defect in the fabric image. As a result, it was confirmed that the region recognized by the deep learning model as a defect in the fabric was actually defective even with the naked eyes. The results of this study are expected to reduce the time and cost incurred in the fabric production process by utilizing deep learning-based artificial intelligence in the defect detection of the textile industry.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this paper, we propose a lightweight identity recognition network combining ArcFace, a loss function with robust characteristics in the field of face recognition, and ResNet-18. In this paper, ResNet-18 is modified to a single-channel-based network to reduce the computational amount of the ResNet-18 network. And to compensate for the network performance degradation due to the decrease in the number of channels, ArcFace, which uses an angle margin-based loss function, was combined with a single-channel-based ResNet-18 network to compensate for the network performance degradation. The proposed network performed training and inference with 10,056 experimental data sets consisting of face photos of 33 people. As a result of the experiment, it was confirmed that the proposed ArcFace-based lightweight ResNet-18 improved processing speed by about 1.3 times compared to the existing ResNet-18. In addition, an inference accuracy of 96.9%, similar to that of the existing network ResNet-18, which is 97.6%, was derived.  \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Non-small cell lung cancer (NSCLC) accounts for a high proportion of 85% among all lung cancer and has a significantly higher mortality rate (22.7%) compared to other cancers. Therefore, it is very important to predict the prognosis after surgery in patients with non-small cell lung cancer. In this study, the types of preoperative chest CT image patches for non-small cell lung cancer patients with tumor as a region of interest are diversified into five types according to tumor-related information, and performance of single classifier model, ensemble classifier model with soft-voting method, and ensemble classifier model using 3 input channels for combination of three different patches using pre-trained ResNet and EfficientNet CNN networks are analyzed through misclassification cases and Grad-CAM visualization. As a result of the experiment, the ResNet152 single model and the EfficientNet-b7 single model trained on the peritumoral patch showed accuracy of 87.93% and 81.03%, respectively. In addition, ResNet152 ensemble model using the image, peritumoral, and shape-focused intratumoral patches which were placed in each input channels showed stable performance with an accuracy of 87.93%. Also, EfficientNet-b7 ensemble classifier model with soft-voting method using the image and peritumoral patches showed accuracy of 84.48%.  \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Radio frequency fingerprinting refers to a methodology that extracts hardware-specific characteristics of a transmitter that are unintentionally embedded in a transmitted waveform. In this paper, we put forward a fingerprinting feature and deep learning structure that can identify the same type of Digital Mobile Radio(DMR) by inputting the in-phase(I) and quadrature(Q). We proposes using the magnitude in polar coordinates of I/Q as RF fingerprinting feature and a modified ResNet-1D structure that can identify them. Experimental results show that our proposed modified ResNet-1D structure can achieve recognition accuracy of 99.5% on 20 DMR.  \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Purpose: This paper proposes a technological system that uses artificial intelligence to recognize and guide the operator to the exact stenosis area during endoscopic surgery in patients with urethral or ureteral strictures. The aim of this technological solution was to increase surgical efficiency.Methods: The proposed system utilizes the ResNet-50 algorithm, an artificial intelligence technology, and analyzes images entering the endoscope during surgery to detect the stenosis location accurately and provide intraoperative clinical assistance.The ResNet-50 algorithm was chosen to facilitate accurate detection of the stenosis site.Results: The high recognition accuracy of the system was confirmed by an average final sensitivity value of 0.96. Since sensitivity is a measure of the probability of a true-positive test, this finding confirms that the system provided accurate guidance to the stenosis area when used for support in actual surgery.Conclusions: The proposed method supports surgery for patients with urethral or ureteral strictures by applying the ResNet-50 algorithm. The system analyzes images entering the endoscope during surgery and accurately detects stenosis, thereby assisting in surgery. In future research, we intend to provide both conservative and flexible boundaries of the strictures.  \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The importance of Digital Transformation (DX) technology has increased with the Fourth Industrial Revolution. At the same time, research on defect detection and classification in the production process through artificial intelligence has been actively applied. In this paper, we select a model that effectively detects defects that occur in the production process of color contact lenses using various models, secure reducing resource waste and consumer safety by improving production and quality. For this purpose, data for training and validation were generated through preprocessing and augmentation of color contact lens images, using CNN technologies such as ResNet101, GoogLeNet V2, GoogLeNet V4, DenseNet121, MobileNet compared and analyzed the defect detection rate for each RGB channel and HSV channel. The accuracies of the above models are 89.74%, 84.46%, 95.43%, 82.80%, and 89.74% respectively, with GoogLeNet V4 on the RGB channel having the highest defect detection accuracy, and in most models, the RGB channel is higher than the HSV channel.  \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The purpose of this study is to apply a deep learning model that can distinguish lung perfusion and lung ventilation images in nuclear medicine, and to evaluate the image classification ability. Image data pre-processing was performed in the following order: image matrix size adjustment, min-max normalization, image center position adjustment, train/validation/test data set classification, and data augmentation. The convolutional neural network(CNN) structures of VGG-16, ResNet-18, Inception-ResNet-v2, and SE-ResNeXt-101 were used. For classification model evaluation, performance evaluation index of classification model, class activation map(CAM), and statistical image evaluation method were applied. As for the performance evaluation index of the classification model, SE-ResNeXt-101 and Inception-ResNet-v2 showed the highest performance with the same results. As a result of CAM, cardiac and right lung regions were highly activated in lung perfusion, and upper lung and neck regions were highly activated in lung ventilation. Statistical image evaluation showed a meaningful difference between SE-ResNeXt-101 and Inception-ResNet-v2. As a result of the study, the applicability of the CNN model for lung scintigraphy classification was confirmed. In the future, it is expected that it will be used as basic data for research on new artificial intelligence models and will help stable image management in clinical practice.  \n",
      "13                                                                                                                              Objective: To investigate the feasibility of using a deep learning-based analysis of auscultation data to predict significant stenosis of arteriovenous fistulas (AVF) in patients undergoing hemodialysis requiring percutaneous transluminal angioplasty (PTA).Materials and Methods: Forty patients (24 male and 16 female; median age, 62.5 years) with dysfunctional native AVF were prospectively recruited. Digital sounds from the AVF shunt were recorded using a wireless electronic stethoscope before (pre-PTA) and after PTA (post-PTA), and the audio files were subsequently converted to mel spectrograms, which were used to construct various deep convolutional neural network (DCNN) models (DenseNet201, EfficientNetB5, and ResNet50). The performance of these models for diagnosing ≥ 50% AVF stenosis was assessed and compared. The ground truth for the presence of ≥ 50% AVF stenosis was obtained using digital subtraction angiography. Gradient-weighted class activation mapping (Grad-CAM) was used to produce visual explanations for DCNN model decisions.Results: Eighty audio files were obtained from the 40 recruited patients and pooled for the study. Mel spectrograms of “pre-PTA” shunt sounds showed patterns corresponding to abnormal high-pitched bruits with systolic accentuation observed in patients with stenotic AVF. The ResNet50 and EfficientNetB5 models yielded an area under the receiver operating characteristic curve of 0.99 and 0.98, respectively, at optimized epochs for predicting ≥ 50% AVF stenosis. However, Grad- CAM heatmaps revealed that only ResNet50 highlighted areas relevant to AVF stenosis in the mel spectrogram.Conclusion: Mel spectrogram-based DCNN models, particularly ResNet50, successfully predicted the presence of significant AVF stenosis requiring PTA in this feasibility study and may potentially be used in AVF surveillance.  \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Purpose: This paper aims to develop a clinical decision support system (CDSS) that can help detect the stone that is most important to the diagnosis of urolithiasis. Among them, especially for the development of artificial intelligence (AI) models that support a final judgment in CDSS, we would like to study the optimal AI model by comparing and evaluating them.Methods: This paper proposes the optimal ureter stone detection model using various AI technologies. The use of AI technology compares and evaluates methods such as machine learning (support vector machine), deep learning (ResNet-50, Fast RCNN), and image processing (watershed) to find a more effective method for detecting ureter stones.Results: The final value of sensitivity, which is calculated using true positive (TP) and false negative and is a measure of the probability of TP results, showed high recognition accuracy, with an average value of 0.93 for ResNet-50. This finding confirmed that accurate guidance to the stones area was possible when the developed platform was used to support actual surgery.Conclusions: The general situation in the most effective way to the detection stone can be found. But a variety of variables may be slightly different the difference through the term could tell. Future works, on urological diseases, are diverse and the research will be expanded by customizing AI models specialized for those diseases.  \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Tomato crops are easy to expose to disease and spread in a short period of time, so late measures against disease are directly related to production and sales, which can cause damage. Therefore, there is a need for a service that enables early prevention by simply and accurately diagnosing tomato diseases in the field. In this paper, we construct a system that applies a deep learning-based model in which ImageNet transition is learned in advance to classify and serve nine classes of tomatoes for disease and normal cases. We use the input of MobileNet, ResNet, with a deep learning-based CNN structure that builds a lighter neural network using a composite product for the image set of leaves classifying tomato disease and normal from the Plant Village dataset. Through the learning of two proposed models, it is possible to provide fast and convenient services using MobileNet with high accuracy and learning speed.  \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Two-phase flow may almost exist in every branch of the energy industry. For the corresponding engineering design, it is very essential and crucial to monitor flow patterns and their transitions accurately.With the high-speed development and success of deep learning based on convolutional neural network (CNN), the study of flow pattern identification recently almost focused on this methodology. Additionally, the photographing technique has attractive implementation features as well, since it is normally considerably less expensive than other techniques. The development of such a two-phase flow pattern online monitoring system is the objective of this work, which seldom studied before. The ongoing preliminary engineering design (including hardware and software) of the system are introduced. The flow pattern identification method based on CNNs and transfer learning was discussed in detail. Several potential CNN candidates such as ALexNet, VggNet16 and ResNets were introduced and compared with each other based on a flow pattern dataset. According to the results, ResNet50 is the most promising CNN network for the system owing to its high precision, fast classification and strong robustness. This work can be a reference for the online monitoring system design in the energy system.  \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Monitoring a person\"s health status is a crucial factor to prevent and react to emergencies. Especially, identifying vital signs such as respiratory status is directly related to recognizing urgent situations. Therefore, we can initiatively respond to traffic accidents caused by the loss of control/consciousness if the emergencies can be detected based on a person’s vital reaction while driving a car. To solve this issue, this paper proposes a methodology to monitor the emergencies in driver’s health condition by measuring the dynamic movement of a chest and then classifying the respiratory conditions. First, the driver’s respiratory status is measured by a seat belt equipped with an inertial measurement unit (IMU) and the measured data is denoised by applying the Kalman filter. To classify respiratory status, next, each IMU data is visualized as an image, and then the image is used to train the ResNet. Finally, the classification accuracies under various combinations of visualization approaches and IMU data types are compared to scrutinize the combination yielding the best classification accuracy. The result from this study allows us to design a driver respiratory monitoring system with the IMU installed seat belt.  \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 .  \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Sensor data can provide fault diagnosis for equipment. However, the cause analysis for fault results of equipment is not often provided. In this study, we propose an explainable convolutional neural network framework for the sensor-based time series classification model. We used sensor-based time series dataset, acquired from vehicles equipped with sensors, and the Wafer dataset, acquired from manufacturing process. Moreover, we used Cycle Signal dataset, acquired from real world mechanical equipment, and for Data augmentation methods, scaling and jittering were used to train our deep learning models. In addition, our proposed classification models are convolutional neural network based models, FCN, 1D-CNN, and ResNet, to compare evaluations for each model. Our experimental results show that the ResNet provides promising results in the context of time series classification with accuracy and F1 Score reaching 95%, improved by 3% compared to the previous study. Furthermore, we propose XAI methods, Class Activation Map and Layer Visualization, to interpret the experiment result. XAI methods can visualize the time series interval that shows important factors for sensor data classification.  \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Compared to the continuously increasing dog population and industry size in Korea, systematic analysis of related data and research on breed classification methods are very insufficient. In this paper, an automatic breed classification method is proposed using deep learning technology for 14 major dog breeds domestically raised. To do this, dog images are collected for deep learning training and a dataset is built, and a breed classification algorithm is created by performing transfer learning based on VGG-16 and Resnet-34 as backbone networks. In order to check the transfer learning effect of the two models on dog images, we compared the use of pre-trained weights and the experiment of updating the weights. When fine tuning was performed based on VGG-16 backbone network, in the final model, the accuracy of Top 1 was about 89% and that of Top 3 was about 94%, respectively. The domestic dog breed classification method and data construction proposed in this paper have the potential to be used for various application purposes, such as classification of abandoned and lost dog breeds in animal protection centers or utilization in pet-feed industry.  \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Classification of persons wearing and not wearing face masks in images has emerged as a new computer vision problem during the COVID-19 pandemic. In order to address this problem and scale up the research in this domain, in this paper a hybrid technique by employing ResNet-101 and multi-layer perceptron (MLP) classifier has been proposed. The proposed technique is tested and validated on a self-created face masks classification dataset and a standard dataset. On self-created dataset, the proposed technique achieved a classification accuracy of 97.3%. To embrace the proposed technique, six other state-of-the-art CNN feature extractors with six other classical machine learning classifiers have been tested and compared with the proposed technique. The proposed technique achieved better classification accuracy and 1-6% higher precision, recall, and F1 score as compared to other tested deep feature extractors and machine learning classifiers.  \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this study, fire prediction performance was analyzed using convolutional neural network (CNN)-based classification models such as MobileNetV2, ResNet101, and EfficientNetB0 applicable to an edge computing-based fire detection system for improving fire safety. The fire prediction performance was evaluated using the performance evaluation measures including accuracy, recall, precision, F1-score, and the confusion matrix. The model size and inference time were assessed in terms of the light-weight classification model for the practical deployment and use. The analysis results confirmed that the EfficientNetB0 model had the highest fire prediction accuracy, and the MobileNetV2 was the best light-weight classification model. Notably, additionally learning the image features about light and haze images having similar features with those of the fire images improved the fire prediction accuracy of the light-weight MobileNetV2 model.  \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           A deep neural network is used to classify the shape of the point cloud using only the coordinate values of the points or the rendered image. In this study, we proposed a point cloud shape classification technique using the coordinate values and the projection image generated using the coordinates as an input to a multi-modal deep neural network. To verify the performance improvement, the multi-modal model built using PointNet, which classifies the shapes based on coordinate values, and ResNet-18, an image-based classification model, was evaluated for the shape classification performance on a ModelNet40 dataset. The result showed that the performance was the best when the side projection image was additionally considered and the second best when the front projection image was considered. This supports the idea that the shape classification performance of the point cloud can be improved using the coordinate values and its projection image as the input to the deep neural network in a multi-modal manner.  \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In this paper, we propose a CNN(convolution neural network) accelerator that can implement a network by driving main computation modules constituting the CNN through module control commands. The module-based CNN accelerator consists of CNN's main computation modules such as convolution and pooling, and the processor can select the computation module required for network configuration and set internal parameters through module control commands. In this paper, ResNet-18 was implemented on a Xilinx SoC-type FPGA using the proposed module-based CNN accelerator, and the output results were compared and verified using PyTorch, a CNN framework model, and a C-based verification model. As a result of the experiment, the inference result of the CNN accelerator showed an accuracy of 92.87%.  \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In this study, we propose training a support vector machine (SVM) model on top of deep networks for detecting Covid-19 from chest X-ray images. We started by gathering a real chest X-ray image dataset, including positive Covid-19, normal cases, and other lung diseases not caused by Covid-19. Instead of training deep networks from scratch, we fine-tuned recent pre-trained deep network models, such as DenseNet121, MobileNet v2, Inception v3, Xception, ResNet50, VGG16, and VGG19, to classify chest X-ray images into one of three classes (Covid-19, normal, and other lung). We propose training an SVM model on top of deep networks to perform a nonlinear combination of deep network outputs, improving classification over any single deep network. The empirical test results on the real chest X-ray image dataset show that deep network models, with an exception of ResNet50 with 82.44%, provide an accuracy of at least 92% on the test set. The proposed SVM on top of the deep network achieved the highest accuracy of 96.16%.  \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In this study, an artificial intelligence(AI) was developed to help with facial expression practice in order to express emotions. The developed AI used multimodal inputs consisting of sentences and facial images for deep neural networks (DNNs). The DNNs calculated similarities between the emotions predicted by the sentences and the emotions predicted by facial images. The user practiced facial expressions based on the situation given by sentences, and the AI provided the user with numerical feedback based on the similarity between the emotion predicted by sentence and the emotion predicted by facial expression. ResNet34 structure was trained on FER2013 public data to predict emotions from facial images. To predict emotions in sentences, KoBERT model was trained in transfer learning manner using the conversational speech dataset for emotion classification opened to the public by AIHub. The DNN that predicts emotions from the facial images demonstrated 65% accuracy, which is comparable to human emotional classification ability. The DNN that predicts emotions from the sentences achieved 90% accuracy. The performance of the developed AI was evaluated through experiments with changing facial expressions in which an ordinary person was participated.  \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Since ballet is an exercise with a wide range of movement, if you exercise with an incorrect posture, you may get injured and it is difficult to feel the effect of the exercise. We propose a ballet coaching system that helps users to exercise with correct posture. The proposed method detects joint coordinates using the ResNet-based Simple pose model and the Open pose BODY-25 model based on ballet motions labeled with 15 joints. Analysis of the quantiles of the histogram was performed on the distribution of the detected joint coordinates. Using 6 classifiers, 3 ballet motion classifications are analyzed through 4 performance indicators. Gradient Boosting Classifier obtained the most optimal performance among the compared models. As a result of the feature importance analysis, it was confirmed that the ankle joint is important. We also analyzed the performance of the model according to four feature extraction method. And the Odds ratio was obtained and the prediction success rates for each motion were compared. The proposed method can coach ballet movements.  \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Recently, with the development of deep learning technology, research on recognizing human behavior is in progress. In this paper, a study was conducted to recognize risky behaviors that may occur in a single-person household environment using deep learning technology. Due to the nature of single-person households, personal privacy protection is necessary. In this paper, we recognize human dangerous behavior in privacy protection video with Gaussian blur filters for privacy protection of individuals. The dangerous behavior recognition method uses the YOLOv5 model to detect and preprocess human object from video, and then uses it as an input value for the behavior recognition model to recognize dangerous behavior. The experiments used ResNet3D, I3D, and SlowFast models, and the experimental results show that the SlowFast model achieved the highest accuracy of 95.7% in privacy-protected video. Through this, it is possible to recognize human dangerous behavior in a single-person household environment while protecting individual privacy.  \n",
      "30                                                                                                                                                                                                                                                                                                                                                                         Purpose Arabica and Robusta coffee plants are physically distinctive as manifested in their leaves, leaf shape, color, and size.However, for ordinary people or those who have just begun their business in coffee cultivation, identifying the type of coffee plant can be challenging. In this study, we incorporated and evaluated deep learning technology to identify the types of coffee based on leaf image identification.Methods In this study, we designed a deep learning architecture and compared it with the well-known approaches, including LeNet, AlexNet, ResNet-50, and GoogleNet. A total of 19,980 image datasets were split into training and testing data, consisting of 15,984 images and 3,996 images, respectively.Results The hyperparameters were taken into account where the use of 100 epoch and 0.0001 learning rate provided the highest accuracy. In addition, 10-fold cross-validation and ROC were used for evaluating the proposed architectures. The results show that the developed convolutional neural network (CNN) generated the highest accuracy of 97.67% compared to LeNet, AlexNet, ResNet-50, and GoogleNet with an accuracy rate of 97.20%, 95.10%, 72.35%, and 82,16%, respectively.Conclusions The modified-CNN algorithm had satisfactory accuracy in identifying different types of coffee. The underlying principles of such classification draw specific attention to the leaf shape, size, and color of Arabica and Robusta coffee. For future works, it is a potential method that can be used to rapidly identify diverse varieties of Robusta and Arabica coffee plants based on leaf tissue and above canopy characteristics.  \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Traditional methods of measuring corneal ulcers were difficult to present objective basis for diagnosis because of the subjective judgment of the medical staff through photographs taken with special equipment. In this paper, we propose a method to detect the ulcer area on a pixel basis in corneal ulcer images using a semantic segmentation model. In order to solve this problem, we performed the experiment to detect the ulcer area based on the DeepLab model which has the highest performance in semantic segmentation model. For the experiment, the training and test data were selected and the backbone network of DeepLab model which set as Xception and ResNet, respectively were evaluated and compared the performances. We used Dice similarity coefficient and IoU value as an indicator to evaluate the performances. Experimental results show that when 'crop & resized' images are added to the dataset, it segment the ulcer area with an average accuracy about 93% of Dice similarity coefficient on the DeepLab model with ResNet101 as the backbone network. This study shows that the semantic segmentation model used for object detection also has an ability to make significant results when classifying objects with irregular shapes such as corneal ulcers. Ultimately, we will perform the extension of datasets and experiment with adaptive learning methods through future studies so that they can be implemented in real medical diagnosis environment.  \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Cities are becoming more complex due to rapid industrialization and population growth in modern times. In particular, urban areas are rapidly changing due to housing site development, reconstruction, and demolition. Thus accurate road information is necessary for various purposes, such as High Definition Map for autonomous car driving. In the case of the Republic of Korea, accurate spatial information can be generated by making a map through the existing map production process. However, targeting a large area is limited due to time and money. Road, one of the map elements, is a hub and essential means of transportation that provides many different resources for human civilization. Therefore, it is essential to update road information accurately and quickly. This study uses Semantic Segmentation algorithms Such as LinkNet, D-LinkNet, and NL-LinkNet to extract roads from drone images and then apply hyperparameter optimization to models with the highest performance. As a result, the LinkNet model using pre-trained ResNet-34 as the encoder achieved 85.125 mIoU. Subsequent studies should focus on comparing the results of this study with those of studies using state-of-the-art object detection algorithms or semi-supervised learning-based Semantic Segmentation techniques. The results of this study can be applied to improve the speed of the existing map update process.  \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In this paper, we propose a prediction system for skin pore labeling based on a CNN(Convolution NeuralNetwork) model, where a data set is constructed by processing skin images taken by users, and a pore featureimage is generated by the proposed image processing algorithm. The skin image data set was labeled for porecharacteristics based on the visual classification criteria of skin beauty experts. The proposed image processingalgorithm was applied to generate pore feature images from skin images and to train a CNN model that predictspore feature ratings. The prediction results with pore features by the proposed CNN model is similar to expertsvisual classification results, where less learning time and higher prediction results were obtained than the resultsby the comparison model (Resnet-50). In this paper, we describe the proposed image processing algorithm andCNN model, the results of the prediction system and future research plans.  \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The MobileNetV3 is specially designed for mobile devices with limited memory and computing power. To reduce the network parameters and improve the network inference speed, a new lightweight network is proposed based on MobileNetV3. Firstly, to reduce the computation of residual blocks, a partial residual structure is designed by dividing the input feature maps into two parts. The designed partial residual structure is used to replace the residual block in MobileNetV3. Secondly, a dual-path feature extraction structure is designed to further reduce the computation of MobileNetV3. Different convolution kernel sizes are used in the two paths to extract feature maps with different sizes. Besides, a transition layer is also designed for fusing features to reduce the influence of the new structure on accuracy. The CIFAR-100 dataset and Image Net dataset are used to test the performance of the proposed partial residual structure. The ResNet based on the proposed partial residual structure has smaller parameters and FLOPs than the original ResNet. The performance of improved MobileNetV3 is tested on CIFAR-10, CIFAR-100 and ImageNet image classification task dataset. Comparing MobileNetV3, GhostNet and MobileNetV2, the improved MobileNetV3 has smaller parameters and FLOPs. Besides, the improved MobileNetV3 is also tested on CPU and Raspberry Pi. It is faster than other networks  \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         With the spread of smart speakers based on voice recognition technology and deep learning technology, not only non-disabled people, but also the blind or physically handicapped can easily control home appliances such as lights and TVs through voice by linking home network services. This has greatly improved the quality of life. However, in the case of speech-impaired people, it is impossible to use the useful services of the smart speaker because they have inaccurate pronunciation due to articulation or speech disorders. In this paper, we propose a personalized voice classification technique for the speech-impaired to use for some of the functions provided by the smart speaker. The goal of this paper is to increase the recognition rate and accuracy of sentences spoken by speech-impaired people even with a small amount of data and a short learning time so that the service provided by the smart speaker can be actually used. In this paper, data augmentation and one cycle learning rate optimization technique were applied while fine-tuning ResNet18 model. Through an experiment, after recording 10 times for each 30 smart speaker commands, and learning within 3 minutes, the speech classification recognition rate was about 95.2%.  \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ShorcutFusion [1] is an end-to-end framework that effectively maps many well-known deep neural networks (DNNs), such as MobileNet-v2, EfficientNet-B0, ResNet-50, and YOLO-v3, to a generic CNN accelerator on FPGA. Nevertheless, its processing elements are not fully utilized when supporting various networks, leading to relatively low hardware utilization (e.g., 68.42% for YOLO-v3). This study aimed to enhance the performance of ShortcutFusion and introduce ShortcutFusion++ by proposing two simple but effective techniques for eliminating unnecessary stalls in conventional design. First, the prefetching scheme was re-designed to avoid bubble cycles when feeding data to the PE array. Second, the output buffer was reconstructed to pipeline the operations of PEs and the process of writing output feature maps to off-chip memory. The experimental results show that ShortcutFusion++ achieves a PE utilization of 80.95% for the wellknown object detection network YOLO-v3, outperforming its baseline by 12.53%.  \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 This study proposes a marine debris monitoring methods using satellite and drone multispectral images. A multi-layer perceptron (MLP) model was applied to detect marine debris using Sentinel-2 satellite image. And for the detection of marine debris using drone multispectral images, performance evaluation and comparison of U-Net, DeepLabv3+ (ResNet50) and DeepLabv3+ (Inceptionv3) among deep learning models were performed (mIoU 0.68). As a result of marine debris detection using satellite image, the F1-Score was 0.97. Marine debris detection using drone multispectral images was performed on vegetative debris and plastics. As a result of detection, when DeepLabv3+ (Inceptionv3) was used, the most model accuracy, mean intersection over union (mIoU), was 0.68. Vegetative debris showed an F1-Score of 0.93 and IoU of 0.86, while plastics showed low performance with an F1-Score of 0.5 and IoU of 0.33. However, the F1-Score of the spectral index applied to generate plastic mask images was 0.81, which was higher than the plastics detection performance of DeepLabv3+ (Inceptionv3), and it was confirmed that plastics monitoring using the spectral index was possible. The marine debris monitoring technique proposed in this study can be used to establish a plan for marine debris collection and treatment as well as to provide quantitative data on marine debris generation.  \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Some surface defects make the weak quality of steel materials. To limit these defects, we advocate a onestage detector model RetinaNet among diverse detection algorithms in deep learning. There are several backbones in the RetinaNet model. We acknowledged two backbones, which are ResNet50 and VGG19. To validate our model, we compared and analyzed several traditional models, one-stage models like YOLO and SSD models and two-stage models like Faster-RCNN, EDDN, and Xception models, with simulations based on steel individual classes. We also performed the correlation of the time factor between one-stage and twostage models. Comparative analysis shows that the proposed model achieves excellent results on the dataset of the Northeastern University surface defect detection dataset. We would like to work on different backbones to check the efficiency of the model for real world, increasing the datasets through augmentation and focus on improving our limitation.  \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Human activity recognition via Electrocardiography (ECG) and Photoplethysmography (PPG) is extensively researched. While ECG requires less filtering and is less prone to disturbance and artifacts, nonetheless, PPG is cheaper and widely available in smart devices, making it a desired alternative. In this study, we explore the employment of the ensemble method with several pre-trained machine learning models namely Resnet50V2, MobileNetV2, and Xception for the classification of wrist PPG data of human activity, in comparison to its ECG counterpart. The study produced promising results with a test classification accuracy of 88.91% and 94.28% for PPG and ECG, respectively.  \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The applications and user interfaces (UIs) of smart mobile devices are constantly diversifying. For example,deep learning can be an innovative solution to classify widgets in screen images for increasing convenience.To this end, the present research leverages captured images and the ReDraw dataset to write deep learningdatasets for image classification purposes. First, as the validation for datasets using ResNet50 and EfficientNet,the experiments show that the dataset composed in this study is helpful for classification according to a widget'sfunctionality. An implementation for widget detection and classification on RetinaNet and EfficientNet is thenexecuted. Finally, the research suggests the Widg-C and Widg-D datasets—a deep learning dataset for identifyingthe widgets of smart devices—and implementing them for use with representative convolutional neuralnetwork models.  \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In this paper, we propose personal identification method based on Convolutional Neural Networks (CNN) by various two-dimensional (2D) transform of Electrocardiogram (ECG) signals. For this purpose, various 2D time-frequency representation are peformed by Short-Time Fourier Transform (STFT), Fourier Synchrosqueezed Transform (FSST), and Wavelet Synchrosqueezed Transform (WSST) from one-dimensional ECG signals. The individual identification performance is achieved by transfer learning based on the pretrained GoogleNet and ResNet-101. The performance of experimental results are compared by the well-known PTB-ECG database.  \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The existing image quality assessment (IQA) datasets have a small number of samples. Some methods based on transfer learning or data augmentation cannot make good use of image quality-related features. A No Reference (NR)-IQA method based on multi-task training and quality awareness is proposed. First, single or multiple distortion types and levels are imposed on the original image, and different strategies are used to augment different types of distortion datasets. With the idea of weak supervision, we use the Full Reference (FR)-IQA methods to obtain the pseudo-score label of the generated image. Then, we combine the classification information of the distortion type, level, and the information of the image quality score. The ResNet50 network is trained in the pre-train stage on the augmented dataset to obtain more quality-aware pre-training weights. Finally, the fine-tuning stage training is performed on the target IQA dataset using the quality-aware weights to predicate the final prediction score. Various experiments designed on the synthetic distortions and authentic distortions datasets (LIVE, CSIQ, TID2013, LIVEC, KonIQ-10K) prove that the proposed method can utilize the image quality-related features better than the method using only single-task training. The extracted quality-aware features improve the accuracy of the model.  \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In the public healthcare, a computational system that can automatically and efficiently detect and classify falls from a video sequence has significant potential. With the advancement of deep learning, which can extract temporal and spatial information, has become more widespread. However, traditional 3D CNNs that usually adopt shallow networks cannot obtain higher recognition accuracy than deeper networks. Additionally, some experiences of neural network show that the problem of gradient explosions occurs with increasing the network layers. As a result, an enhanced three-dimensional ResNet-based method for fall detection (3D-ERes-FD) is proposed to directly extract spatio-temporal features to address these issues. In our method, a 50-layer 3D residual network is used to deepen the network for improving fall recognition accuracy. Furthermore, enhanced residual units with four convolutional layers are developed to efficiently reduce the number of parameters and increase the depth of the network. According to the experimental results, the proposed method outperformed several state-of-the-art methods.  \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Whole body bone scan is the most frequently performed nuclear medicine imaging to evaluate bone metastasis in cancer patients. We evaluated the performance of a VGG16-based transfer learning classifier for bone scan images in which metastatic bone lesion was present. A total of 1,000 bone scans in 1,000 cancer patients (500 patients with bone metastasis, 500 patients without bone metastasis) were evaluated. Bone scans were labeled with abnormal/normal for bone metastasis using medical reports and image review. Subsequently, gradient-weighted class activation maps (Grad-CAMs) were generated for explainable AI. The proposed model showed AUROC 0.96 and F1-Score 0.90, indicating that it outperforms to VGG16, ResNet50, Xception, DenseNet121 and InceptionV3. Grad-CAM visualized that the proposed model focuses on hot uptakes, which are indicating active bone lesions, for classification of whole body bone scan images with bone metastases.  \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                        The Cardiac Gated Blood Pool (GBP) scintigram, a nuclear medicine imaging, calculates the left ventricular Ejection Fraction (EF) by segmenting the left ventricle from the heart. However, in order to accurately segment the substructure of the heart, specialized knowledge of cardiac anatomy is required, and depending on the expert s processing, there may be a problem in which the left ventricular EF is calculated differently. In this study, using the DeepLabV3 architecture, GBP images were trained on 93 training data with a ResNet-50 backbone. Afterwards, the trained model was applied to 23 separate test sets of GBP to evaluate the reproducibility of the region of interest and left ventricular EF. Pixel accuracy, dice coefficient, and IoU for the region of interest were 99.32±0.20, 94.65±1.45, 89.89±2.62(%) at the diastolic phase, and 99.26±0.34, 90.16±4.19, and 82.33±6.69(%) at the systolic phase, respectively. Left ventricular EF was calculated to be an average of 60.37±7.32% in the ROI set by humans and 58.68±7.22% in the ROI set by the deep learning segmentation model. (p<0.05) The automated segmentation method using deep learning presented in this study similarly predicts the average human-set ROI and left ventricular EF when a random GBP image is an input. If the automatic segmentation method is developed and applied to the functional examination method that needs to set ROI in the field of cardiac scintigram in nuclear medicine in the future, it is expected to greatly contribute to improving the efficiency and accuracy of processing and analysis by nuclear medicine specialists.  \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Visual inspection methods have limitations, such as reflecting the subjective opinions of workers. Moreover, additional equipment is required when inspecting the high-rise buildings because the height is limited during the inspection. Various methods have been studied to detect concrete cracks due to the disadvantage of existing visual inspection. In this study, a crack detection technology was proposed, and the technology was objectively and accurately through AI. In this study, an efficient method was proposed that automatically detects concrete cracks by using a Convolutional Neural Network(CNN) with the Orthomosaic image, modeled with the help of UAV. The concrete cracks were predicted by three different CNN models: AlexNet, ResNet50, and ResNeXt. The models were verified by accuracy, recall, and F1 Score. The ResNeXt model had the high performance among the three models. Also, this study confirmed the reliability of the model designed by applying it to the experiment.  \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         This paper aims to improve the feature representation by diversifying CNN filters inspired by niche concept in evolution. The singular value decomposition (SVD) entropy based efficient metric for diversity is proposed In the proposed approach, filters are clustered by groups and they are calculated as differences from the center values within the groups, rather than by entire rank based comparison. This provides an effective method for increasing the substantial diversity of filters. Furthermore, the filters with low diversity are adjusted by the diversity spreading framework for better diversity in the reconstruction process. The improvement of the filter representation by performing experiments on CIFAR 10/100 data for VGG16, and ImageNet for ResNet34 is provided. Because there are no similar studies, we compare our results with respect to those of relatively relevant pruning methods in terms of classification performance accuracy as well as the pruned rates and flops.  \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Facial expression is an important part of human communication and is an element that helps to understand other people’s intentions. Recently, as a complementary solution in the field of emotion recognition, interest in thermal imaging is increasing as an alternative means to compensate for the shortcomings of visible light imaging. In this paper, thermal image data was acquired by itself and a database was established in which only facial areas necessary for emotional recognition were extracted separately. Verification accuracy and learning time were analyzed using the existing CNN architecture to confirm whether the built database can be used to classify facial expressions for emotion recognition. As a result of analysis through CNN network, ResNet-18 showed a verification accuracy of up to 81.28%, and on average, it showed a verification accuracy of 68.13%. Through this, it was confirmed that the self-built thermal imaging database is useful for emotional recognition research.  \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The proposed approach is a deep learning-based compact weighted binary classification (DL-CWBC) method to discriminate between targets and clutter in synthetic aperture radar (SAR) images. A new modified cross-entropy error function is proposed to improve the probability of detection by controlling the rate of false alarms (FAs). The unique feature of a CWBC algorithm is reducing the FA rate and maximizing the probability of target detection without missing any target. For pre-processing, targets and clutter are detected through a constant false alarm rate (CFAR) as a conventional detection algorithm. These are then manually divided into two classes. The classified targets and clutter were trained through a ResNet-101 network. There is a trade-off between the minimization of the FA rate and the maximization of the detection probability for targets of interest (TOIs). The weighted coefficient of the modified cross-entropy error function tries to maximize the performance of this trade-off. In addition, the proposed approach enables us not to miss any targets by an extreme distinction decision. Above all, the DL-CWBC algorithm performs very well despite its simplicity.  \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          A more accurate understanding of the irrigation water supply is necessary for efficient agricultural water management. Although we measure water levelsin an irrigation canal using ultrasonic water level gauges, some errors occur due to malfunctions or the surrounding environment. This study aims toapply CNN (Convolutional Neural Network) Deep-learning-based image classification and segmentation models to the irrigation canal’s CCTV(Closed-Circuit Television) images. The CCTV images were acquired from the irrigation canal of the agricultural reservoir in Cheorwon-gun,Gangwon-do. We used the ResNet-50 model for the image classification model and the U-Net model for the image segmentation model. Using theNatural Breaks algorithm, we divided water level data into 2, 4, and 8 groups for image classification models. The classification models of 2, 4, and8 groups showed the accuracy of 1.000, 0.987, and 0.634, respectively. The image segmentation model showed a Dice score of 0.998 and predictedwater levels showed R2of 0.97 and MAE (Mean Absolute Error) of 0.02 m. The image classification models can be applied to the automaticgate-controller at four divisions of water levels. Also, the image segmentation model results can be applied to the alternative measurement for ultrasonicwater gauges. We expect that the results of this study can provide a more scientific and efficient approach for agricultural water management.  \n",
      "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 .  \n",
      "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 In order to perform preoperative surgical planning, accurate segmentation of anatomical structures in cone-beam computed tomography (CBCT) images is required. However, this image segmentation is often impeded by metal artifacts, and it takes a lot of time due to morphological variability in patients. In this paper, we proposed a deep learning based automatic multi-class segmentation method for anatomical structures in CBCT images containing metal artifacts. Four U-Net based deep learning models were used for anatomical structure segmentation. Each deep learning model was constructed by changing the encoder of U-Net architecture to the backbones (DenseNet121, VGGNet16, ResNet101, and EfficienNetB4). For training and testing our method, we used 20744 CBCT images containing metal artifacts from 30 patient datasets. Experimental results show that the segmentation performances of the mandible, midfacial bone, mandibular canal, and maxillary sinus were achieved F1 scores of 0.912±0.070, 0.880±0.080, 0.687±0.265, and 0.954±0.063 using DenseNet121 with Tversky loss, respectively. Furthermore, our method was able to perform robust and accurate segmentation of anatomical structures in CBCT images containing metal artifacts.  \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Convolution Neural Network(CNN) is a class of deep learning algorithms and can be used for image analysis. In particular, it has excellent performance in finding the pattern of images. Therefore, CNN is commonly applied for recognizing, learning and classifying images. In this study, the surface defect classification performance of Al 6061 extruded material using CNN-based algorithms were compared and evaluated. First, the data collection criteria were suggested and a total of 2,024 datasets were prepared. And they were randomly classified into 1,417 learning data and 607 evaluation data. After that, the size and quality of the training data set were improved using data augmentation techniques to increase the performance of deep learning. The CNN-based algorithms used in this study were VGGNet-16, VGGNet-19, ResNet-50 and DenseNet-121. The evaluation of the defect classification performance was made by comparing the accuracy, loss, and learning speed using verification data. The DenseNet-121 algorithm showed better performance than other algorithms with an accuracy of 99.13% and a loss value of 0.037. This was due to the structural characteristics of the DenseNet model, and the information loss was reduced by acquiring information from all previous layers for image identification in this algorithm. Based on the above results, the possibility of machine vision application of CNN-based model for the surface defect classification of Al extruded materials was also discussed.  \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Currently, there are increasing demands for applying deep neural networks (DNNs) in the embedded domain such as classification and object detection. The DNN processing in embedded domain often requires custom hardware such as NPU for acceleration due to the constraints in power, performance, and area. Processing DNN models requires a large amount of data, and its seamless transfer to NPU is crucial for performance. In this paper, we developed a cycle-accurate NPU simulator to evaluate diverse NPU microarchitectures. In addition, we propose a novel technique for reducing the number of memory accesses when processing convolutional layers in convolutional neural networks (CNNs) on the NPU. The main idea is to reuse data with memory interleaving, which recycles the overlapping data between previous and current input windows. Data memory interleaving makes it possible to quickly read consecutive data in unaligned locations. We implemented the proposed technique to the cycle-accurate NPU simulator and measured the performance with LeNet-5, VGGNet-16, and ResNet-50. The experiment shows up to 2.08x speedup in processing one convolutional layer, compared to the baseline.  \n",
      "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Retinal tissue plays a crucial part in human vision. Infections of retinal tissue and delayed treatment or untreated infection could lead to loss of vision. Additionally, the diagnosis is prone to errors when huge dataset is involved. Therefore, a fully automated model of identification of retinal disease is proposed to reduce human interaction while retaining its high accuracy classification results. This paper introduces an enhanced design of a fully automatic multi-class retina diseases prediction system to assist ophthalmologists in making speedy and accurate investigation. Retinal fundus images, which have been used in this study, were downloaded from the stare website (157 images from five classes: BDR, CRVO, CNV, PDR, and Normal). The five files were categorized according to their annotations conducted by the experienced specialists. The categorized images were first processed with the proposed upgraded contrast-limited adaptive histogram filter for image brightness enhancement, noise reduction, and intensity spectrum normalization. The proposed model was designed with transfer learning method and the fine-tuned pre-trained RESNET50. Eventually, the proposed framework was examined with performance evaluation parameters, recorded a classification rate with 100% sensitivity, 100% specificity, and 100% accuracy. The performance of the proposed model showed a magnificent superiority as compared to the state-of-the-art studies.  \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    A deep neural network (DNN) includes variables whose values keep on changing with the training process until it reaches the final point of convergence. These variables are the co-efficient of a polynomial expression to relate to the feature extraction process. In general, DNNs work in multiple ‘dimensions’ depending upon the number of channels and batches accounted for training. However, after the execution of feature extraction and before entering the SoftMax or other classifier, there is a conversion of features from multiple N-dimensions to a single vector form, where ‘N’ represents the number of activation channels. This usually happens in a Fully connected layer (FCL) or a dense layer. This reduced 2D feature is the subject of study for our analysis. For this, we have used the FCL, so the trained weights of this FCL will be used for the weight-class correlation analysis. The popular DNN models selected for our study are ResNet-101, VGG-19, and GoogleNet. These models’ weights are directly used for fine-tuning (with all trained weights initially transferred) and scratch trained (with no weights transferred). Then the comparison is done by plotting the graph of feature distribution and the final FCL weights.  \n",
      "60                                                                                                                                                                                                                                                                                                                                   Scene text recognition has important application value and attracted the interest of plenty of researchers. At present, many methods have achieved good results, but most of the existing approaches attempt to improve the performance of scene text recognition from the image level. They have a good effect on reading regular scene texts. However, there are still many obstacles to recognizing text on low-quality images such as curved, occlusion, and blur. This exacerbates the difficulty of feature extraction because the image quality is uneven. In addition, the results of model testing are highly dependent on training data, so there is still room for improvement in scene text recognition methods. In this work, we present a natural scene text recognizer to improve the recognition performance from the feature level, which contains feature representation and feature enhancement. In terms of feature representation, we propose an efficient feature extractor combined with Representative Batch Normalization and ResNet. It reduces the dependence of the model on training data and improves the feature representation ability of different instances. In terms of feature enhancement, we use a feature enhancement network to expand the receptive field of feature maps, so that feature maps contain rich feature information. Enhanced feature representation capability helps to improve the recognition performance of the model. We conducted experiments on 7 benchmarks, which shows that this method is highly competitive in recognizing both regular and irregular texts. The method achieved top1 recognition accuracy on four benchmarks of IC03, IC13, IC15, and SVTP .  \n",
      "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this paper, we propose a novel method of performing convolutional operations on a 2-D Processing Element(PE) array. The conventional method [1] of mapping the convolutional operation using the 2-D PE array lacks flexibility and provides low utilization of PEs. However, by mapping a convolutional operation from a 2-D PE array to a 1-D PE array, the proposed method can increase the number and utilization of active PEs. Consequently, the throughput of the proposed Deep Convolutional Neural Network(DCNN) accelerator can be increased significantly.  Furthermore, the power consumption for the transmission of weights between PEs can be saved. Based on the simulation results, the performance of the proposed method provides approximately 4.55%, 13.7%, and 2.27% throughput gains for each of the convolutional layers of AlexNet, VGG16, and ResNet50 using the DCNN accelerator with a (weights size) x (output data size) 2-D PE array compared to the conventional method. Additionally the proposed method provides approximately 63.21%, 52.46%, and 39.23% power savings.  \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Artificial intelligence is driving the Fourth Industrial Revolution and is in the spotlight as a general-purpose technology. As the data collection from the battlefield increases rapidly, the need to us artificial intelligence is increasing in the military, but it is still in its early stages. In order to identify maritime targets, Republic of Korea navy acquires images by ISAR(Inverse Synthetic Aperture Radar) of maritime patrol aircraft, and humans make out them. The radar image is displayed by synthesizing signals reflected from the target after radiating radar waves. In addition, day/night and all-weather observations are possible. In this study, an artificial intelligence is used to identify maritime targets based on radar images. Data of radar images of 24 maritime targets in Republic of Korea and North Korea acquired by ISAR were pre-processed, and an artificial intelligence algo- rithm(ResNet-50) was applied. The accuracy of maritime targets identification showed about 99%. Out of the 81 warship types, 75 types took less than 5 seconds, and 6 types took 15 to 163 seconds.  \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 .  \n",
      "64                                                                                                                                                                                                                                                                                                                                                                                                           In this paper, we demonstrates a reliable and efficient approach to detect eye-related disease with automated fundus screening using convolutional neural network (CNN) and transfer learning that counteracts to insufficient annotated data set and image domain shifts. The weight values learned from the data sets can be used as initial parameters for the other desired neural networks, and additional learning can be conducted on top of the pre-learned model, called transfer learning. It is a particularly useful method when the number of data sets is and small over limited computing power environment. Four different fundus image data sets, image domains such as ethnicity of the target and equipment which the fundus photograph was captured with, were used for the validation. The data sets were annotated by ophthalmologists as healthy, abnormal, or diabetic retinopathy. The ResNet-18 model, pre-trained with ImageNet data set of 1.2 million images of 1000 daily routine objects, were used for transfer learning. The pre-trained model were modified and additionally trained to learn features from the fundus images, and were validated with separate test sets. Given limited quantity of fundus photograph data set and various image domains, the deep learning models can yield robust ophthalmological performance in discriminating pathologies in the eyes. In spite of the simplicity, this study illustrates the capability of transfer learning and suggests pragmatic and practical approach to varied medical settings with fluctuating status of data maintenance and different image domains.  \n",
      "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Although the e-scooter sharing service market is growing as a representative last-mile mobility, the accident rate is increasing proportionally as the number of users increases. This study proposes a deep learning-based personal mobility driver monitoring system that detects inattentive driving by classifying vibration data transmitted to the e-scooter when the driver fails to concentrate on driving. First, the N-back task technique is used. The driver was stimulated by external visual and auditory factors to generate a cognitive load, and vibration data were collected through a six-axis sensor. Second, the generated vibration data were pre-processed using short-time Fourier transform and wavelet transform (WT) and then converted into an image (spectrogram). Third, four multimodal convolutional neural networks such as LeNet-5, VGG16, ResNet50, and DenseNet121 were constructed and their performance was compared to find the best architecture. Experimental results show that multimodal DenseNet121 with WT can accurately classify safe, slightly anxious, and very anxious driving conditions. The proposed model can be applied to real-time monitoring and warning systems for sharing service providers and used as a basis for insurance and legal action in the case of accidents.  \n",
      "66  Insect recognition is crucial for taxonomy. It helps researchers to process tremendous and various ecology data. Most studies focus on fine-tuning the deep learning network or altering the algorithm to enhance the identification accuracy, and some useful tools have been generated with these methods. This study focuses on the influence of image data on the recognition model. The single data set source of the existing automated identification tools is relatively simple, and the competition-based data set released only focuses on evaluating the model at present.For the first time, this article integrates butterfly image data sets from multiple sources, covered illustrated books, and popular butterfly science websites. The image types include standard specimen images, illustrated book scan images and camera shots. In addition, these images included not only fixed poses, but also various other images of butterflies in natural poses. The size of these images is also various. The testing data set is new data that does not belong to the training set, which also verifies the generalizability of the model, indicating that in practical applications this model can identify new images. This testing method is a breakthrough compared to the previous work. We designed different data sets using the ResNet18 network to train a classifier, which achieves a validation accuracy of 86% in the end of the analysis.By adjusting the data sets, the accuracy changes as well. This study provides a method to recognize hundreds of butterfly species and analyzes the testing progress from the point of view of data. It is the first to combine butterflies from multiple countries in a single data set, with a recognition accuracy that outperforms previous experiments, to the best of our knowledge. We further analyze the testing results of butterfly recognition at the family and genus level. We perform two more experiments to demonstrate the model in the case of similar species or genus.  \n",
      "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      A very complex task in deep learning such as image classification must be solved with the help of neural networks and activation functions. The backpropagation algorithm advances backward from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged, as a result, the gradient descent never converges to the optimum. We propose a two-factor non-saturating activation functions known as Bea-Mish for machine learning applications in deep neural networks. Our method uses two factors, beta (β) and alpha (α), to normalize the area below the boundary in the Mish activation function and we regard these elements as Bea. Bea-Mish provide a clear understanding of the behaviors and conditions governing this regularization term can lead to a more principled approach for constructing better performing activation functions. We evaluate Bea-Mish results against Mish and Swish activation functions in various models and data sets. Empirical results show that our approach (Bea-Mish) outperforms native Mish using SqueezeNet backbone with an average precision (AP50val) of 2.51% in CIFAR-10 and top-1accuracy in ResNet-50 on ImageNet-1k. shows an improvement of 1.20%.  \n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       An important element of a digital twin is to model and provide realistic spatial information without an occluded area for real objects. In 3D spatial information modeling, however, an occluded area inevitably occurs due to the shooting angle and shooting time. In this regard, this study implemented a technology that can detect a specific object using the latest technology, AI, and remove and in-paint it. In this study, the occlusion-causing object was automatically detected using the ResNet algorithm and the occlusion area in-painting in the texturing image. For the experimental data, the ResNet algorithm was applied to street trees that cause the most occlusion in building modeling to produce a street tree dataset and automatically detected and analyzed the results. Street trees, which are unstructured data, can be detected using the ResNet algorithm and removed using the DeepFillv2 algorithm, producing texturing images of shops and apartments that are restored using the adjacent pixels.  \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     For autonomous vehicles, technology that monitors the state of the vehicle and detects a failure using sensor data is receiving increasing attention. The purpose of this study is to determine the type and location of faults of a vehicle chassis system under noisy conditions using acceleration data and deep learning. Because there is a limit in the acquisition of specific defect data from a real vehicle, normal and defect data were obtained using a vehicle physics model that considers various vehicle speeds, vehicle-to-vehicle variations and road changes. We proposed DNI-ResNet (DenseNet Inspired ResNet), which applied the advantages of DenseNet to ResNet, and used it to determine the type and location of defects occurring in the rubber of the vehicle chassis system. Additionally, the domain adaptation ability of the proposed method was verified with various vehicle speed and new types of defects.  \n",
      "70                                                                                                                                                                                                            We investigate the accuracy of direct attenuation correction (AC) in the image domain for myocardial perfusion SPECT (single-photon emission computed tomography) imaging (MPI-SPECT) using residual (ResNet) and UNet deep convolutional neural networks. MPI-SPECT 99mTc-sestamibi images of 99 patients were retrospectively included. UNet and ResNet networks were trained using non-attenuation-corrected SPECT images as input, whereas CT-based attenuation-corrected (CT-AC) SPECT images served as reference. Chang’s calculated AC approach considering a uniform attenuation coefficient within the body contour was also implemented. Clinical and quantitative evaluations of the proposed methods were performed considering SPECT CT-AC images of 19 subjects (external validation set) as reference. Image-derived metrics, including the voxel-wise mean error (ME), mean absolute error, relative error, structural similarity index (SSI), and peak signal-to-noise ratio, as well as clinical relevant indices, such as total perfusion deficit (TPD), were utilized. Overall, AC SPECT images generated using the deep learning networks exhibited good agreement with SPECT CT-AC images, substantially outperforming Chang’s method. The ResNet and UNet models resulted in an ME of −6.99 ± 16.72 and −4.41 ± 11.8 and an SSI of 0.99 ± 0.04 and 0.98 ± 0.05, respectively. Chang’s approach led to ME and SSI of 25.52 ± 33.98 and 0.93 ± 0.09, respectively. Similarly, the clinical evaluation revealed a mean TPD of 12.78 ± 9.22% and 12.57 ± 8.93% for ResNet and UNet models, respectively, compared to 12.84 ± 8.63% obtained from SPECT CT-AC images. Conversely, Chang’s approach led to a mean TPD of 16.68 ± 11.24%. The deep learning AC methods have the potential to achieve reliable AC in MPI-SPECT imaging.  \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets.  \n",
      "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Single-photon emission computed tomography is one of the reliable pin-by-pin verification techniques for spent-fuel assemblies. One of the challenges with this technique is to increase the total fuel assembly verification speed while maintaining high verification accuracy. The aim of the present study, therefore, was to develop an artificial intelligence (AI) algorithm-based tomographic image analysis technique for partial-defect verification of fuel assemblies. With the Monte Carlo (MC) simulation technique, a tomographic image dataset consisting of 511 fuel-rod patterns of a 3  3 fuel assembly was generated, and with these images, the VGG16, GoogLeNet, and ResNet models were trained. According to an evaluation of these models for different training dataset sizes, the ResNet model showed 100% pattern estimation accuracy. And, based on the different tomographic image qualities, all of the models showed almost 100% pattern estimation accuracy, even for low-quality images with unrecognizable fuel patterns.This study verified that an AI model can be effectively employed for accurate and fast partial-defect verification of fuel assemblies  \n",
      "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            In this paper we propose white noise adding method to prevent missclassification of deep learning system by adversarial attacks. The proposed method is that adding white noise to input image that is benign or adversarial example. The experimental results are showing that the proposed method is robustness to 3 adversarial attacks such as FGSM attack, BIN attack and CW attack. The recognition accuracies of Resnet model with 18, 34, 50 and 101 layers are enhanced when white noise is added to test data set while it does not affect to classification of benign test dataset. The proposed model is applicable to defense to adversarial attacks and replace to time- consuming and high expensive defense method against adversarial attacks such as adversarial training method and deep learning replacing method.  \n",
      "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Stock prices are influenced by a number of external factors, such as laws and trends, as well as number-based internal factors such as trading volume and closing prices. Since many factors affect stock prices, it is very difficult to accurately predict stock prices using only fragmentary stock data. In particular, since the value of a company is greatly affected by the perception of people who actually trade stocks, emotional information about a specific company is considered an important factor. In this paper, we propose a deep learning-based stock price prediction model using sentiment analysis with news data considering temporal characteristics. Stock and news data, two heterogeneous data with different characteristics, are integrated according to time scale and used as input to the model, and the effect of time scale and sentiment index on stock price prediction is finally compared and analyzed. Also, we verify that the accuracy of the proposed model is improved through comparative experiments with existing models.  \n",
      "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Recently, demand for designing own space is increasing as the rapid growth of home furnishing market. However, there is a limitation that it is not easy to compare the style between before construction view and after view. This study aims to translate real image into another style with GAN model learned with interior images. To implement this, first we established style criteria and collected modern, natural, and classic style images, and experimented with ResNet, UNet, Gradient penalty concept to CycleGAN algorithm. As a result of training, model recognize common indoor image elements, such as floor, wall, and furniture, and suitable color, material was converted according to interior style. On the other hand, the form of furniture, ornaments, and detailed pattern expressions are difficult to be recognized by CycleGAN model, and the accuracy lacked. Although UNet converted images more radically than ResNet, it was more stained. The GAN algorithm allowed us to represent results within 2 seconds. Through this, it is possible to quickly and easily visualize and compare the front and after the interior space style to be constructed. Furthermore, this GAN will be available to use in the design rendering include interior.  \n",
      "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Electrocardiogram (ECG) signal's shape and characteristic varies through each individual, so it is difficult to classify with one neural network. It is difficult to classify the given data directly, but if corresponding normal beat is given, it is relatively easy and accurate to classify the beat by comparing two beats. In this study, we classify the ECG signal by generating the reference normal beat through the template cluster, and combining with the input ECG signal. It is possible to detect abnormal beats of various individual’s records with one neural network by learning and classifying with the imaged ECG beats which are combined with corresponding reference normal beat. Especially, various neural networks, such as GoogLeNet, ResNet, and DarkNet, showed excellent performance when using the comparative learning. Also, we can confirmed that GoogLeNet has 99.72% sensitivity, which is the highest performance of the three neural networks.  \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    AI technology is being successfully introduced in many fields, and models deployed as a service are deployed with black box environment that does not expose the model's information to protect intellectual property rights and data. In a black box environment, attackers try to steal data or parameters used during training by using model output. This paper proposes a method of inferring the type of model to directly find out the composition of layer of the target model, based on the fact that there is no attack to infer the information about the type of model from the deep learning model. With ResNet, VGGNet, AlexNet, and simple convolutional neural network models trained with MNIST datasets, we show that the types of models can be inferred using the output values in the gray box and black box environments of the each model. In addition, we inferred the type of model with approximately 83% accuracy in the black box environment if we train the big and small relationship feature that proposed in this paper together, the results show that the model type can be infrerred even in situations where only partial information is given to attackers, not raw probability vectors.  \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Many studies using aerial photography and deep learning are increasing for efficient monitoring of the forest resources. We defined six semantic classes of buildings, roads, paddy fields, fields, forests, and barren as forest restoration target sites and explored the optimal methodology for extracting and classifying target sites for forest restoration based on CNN. The datasets (6,640) were divided at a ratio of 75:25 into training (4,980) and validation datasets (1,660). The accuracy of each model was evaluated using pixel accuracy (PA) and Mean Intersection over union (Mean IoU). PA was calculated as 90.6% and Mean IoU was 80.8%, and the Inception-Resnet-v2 model showed excellent accuracy in extracting target sites for forest restoration among the three models. This result has a Spatio-temporal advantage over the existing field survey for forest restoration sites or surveys using aerial photographs by manually. This study will be able to contribute to the classification of forest restoration sites efficiently and support forest restoration.  \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Applying convolutional neural networks (CNN) to machine vision has recently exhibited excellent performance in amorphous defect inspection. However, collecting and annotating sufficient amounts of data to train CNNs in machine vision systems take considerable time. In this study, a data augmentation method is proposed that can be used to inspect amorphous defects using CNNs in machine vision systems using mono cameras. Class 1 subdata of the DAGM 2007 dataset produced to detect defects in arbitrary patterns were used as experimental data. We trained Mask R-CNN ResNet 50 for defect inspection. Through the proposed method, we found that the CNN trained with the augmented data exhibited an average accuracy of 61.38%, which is 16.73%p higher based on Mask mAP@0.5:0.95 compared to the CNN trained with the original data when validation was performed with original grayscale images. By using the data augmentation method applied in this study, CNNs in the machine vision systems using mono cameras can achieve higher inspection performance.  \n",
      "81                                                                                                                                                                                                                                                                                                                                                                                                                                                 False data injection attack (FDIA) is a typical cyber-attack targeting on power system state estimation. By inserting bias into the power system meter measurements, FDIA can cause errors in state estimation and consequently mislead power system operation. Recently, the coordinated state-and-topology FDIA was developed to falsify both the meter measurements and the topology information to conceal a cyber or physical attack. Conventional machine-learning-based detection methods against FDIA depend on the complete observability of power system topology, therefore cannot detect the coordinated state-andtopology FDIA. To address this challenge, we propose a multi-modal learning model based on Graph Auto-Encoder (GAE) and Residual Neural Networks (ResNet) to detect this type of FDIA. First, GAE is used to learn the compact representation of power system topologies. Then, the obtained topological representation is fused with the measurement to obtain the multi-modal features. Third, ResNet is trained as a multi-label classifi er to accept the fused features and generate an array, which can detect attack scenarios and identify the falsifi ed measurements/topologies by coordinated attacks. Comprehensive case studies using IEEE 9-bus, IEEE 57-bus, and IEEE 118-bus systems are presented. The simulation results show that, as compared to single feature methods, the proposed method is more eff ective in detecting coordinated state-and-topology attacks, and more robust in case of FDIA under unseen topological changes in power system operation  \n",
      "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Image classification is a supervised learning problem in the machine learning area. We apply deep learning models to classify image data. In particular, we discuss the advantages of the various types of convolutional neural networks competed in the ImageNet large-scale visual recognition challenge (ILSVRC). First, we provide a review of the CNN models to be applied and explain the details of models to be employed. In general, we keep the core structure of the models in the same form proposed in ILSVRC. We investigate the models via four popular image data sets of various sizes. To compare the performance of the models, we adopt top-1 accuracy, top-5 accuracy, and f1-score as the measures of accuracy. We employ AdamW for an optimizer that is a fast algorithm and often yields precise learning. As a result, we show that the Inception-ResNet-v2 model has excellent performance, and the ResNet is robust to imbalanced data.  \n",
      "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The analysis of foreign aircraft appearing suddenly in air defense identification zones requires a lot of cost and time. This study aims to develop a pre-trained model that can identify neighboring military aircraft based on aircraft photographs available on the web and present a model that can determine which aircraft corresponds to based on aerial photographs taken by allies. The advantages of this model are to reduce the cost and time required for model classification by proposing a pre-trained model and to improve the performance of the classifier by data augmentation of edge-detected images, cropping, flipping and so on.  \n",
      "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In this study, using deep learning basic models DNN, LSTM, BiLSTM, and 1D-CNN, to improve prediction performance, we proposed a structure in which hidden layers are merged in parallel. Proposed model 1 is a parallel merging structure of the same basic model, and Proposed model 2 is a parallel merging structure of different models. The evaluation of each model is the average value for 10 experiments with RMSE and MAE. The RMSE of BiLSTM2, which showed the best prediction rate in Proposed Model 1, is 0.064. The RMSE of Proposed Model 2 was 0.054 for all models including DC (DNN-CNN), LC (BiLSTM-CNN), and DLC (DNN-BiLSTM-CNN). As such, Proposed Model 2 showed a performance improvement of 12.8%. It was confirmed that Proposed Model 2 is a model that can improve the prediction rate by using many parameters while maintaining the advantages of different models.  \n",
      "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 As the demand for kiosks increases, more users complain of discomfort. Accordingly, a kiosk that enables easy menu selectionand order by producing a voice-based interactive service is produced and provided in the form of a web. It implements voice functions based on the Annyang API and SpeechSynthesis API, and understands the user’s intention through Dialogflow. And discusshow to implement this process based on Rest API. In addition, the recommendation system is applied based on collaborative filteringto improve the low consumer accessibility of existing kiosks, and to prevent infection caused by droplets during the use of voicerecognition services, it provides the ability to check the wearing of masks before using the service.  \n",
      "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Recently, AI technology is developing rapidly every day, and AI technology is being used in various fields. This paper produced a healing game that comforts the mind tired of human relationships and personal reasons in the COVID-19 situation by using AI technology in the art field. In the produced healing game, the effect of self-help-therapy may be obtained, and thus, it is expected that a user may obtain a healing effect in a daily use process through a healing game without the help of a therapist. By statistically analyzing the game review data, a healing game was created by accommodating the public's demands as a healing game, and users were able to obtain a self-help-therapy effect by making a simple storyline and a simple conversation that could interact with AI before the game began.  \n",
      "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 .  \n",
      "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Due to the heavy computational burden, deep models of embedded and mobile systems inevitably require network reduction with minimum performance degradation. Pruning method is mainly used to reduce the model by removing some fi lters only within the layer without changing the structure. Some methods for structural reduction of models are far from optimization.We propose a structure reduction method using a genetic algorithm to optimize the removal of reducible layers. Knowledge distillation is carried out to recover the resultant network. We evaluate our method for ResNet on two image classifi cation datasets, CIFAR-10 and CIFAR-100. Experiments show that our method performs a signifi cant improvement over other state-of-the-art methods.  \n",
      "90                                                                                                    Background Dog-associated infections are related to more than 70 human diseases. Given that the health diagnosis of a dog requires expertise of the veterinarian, an artificial intelligence model for detecting dog diseases could significantly reduce time and cost required for a diagnosis and efficiently maintain animal health.Objective We collected normal and multispectral images to develop classification model of each three dog skin diseases (bacterial dermatosis, fungal infection, and hypersensitivity allergic dermatosis). The single models (normal image- and multispectral image-based) and consensus models were developed used to four CNN model architecture (InceptionNet, ResNet, DenseNet, MobileNet) and select well-performed model.Results For single models, such as normal image- or multispectral image-based model, the best accuracies and Matthew’s correlation coefficients (MCCs) for validation data set were 0.80 and 0.64 for bacterial dermatosis, 0.70 and 0.36 for fungal infection, and 0.82 and 0.47 for hypersensitivity allergic dermatosis. For the consensus models, the best accuracies and MCCs for the validation set were 0.89 and 0.76 for the bacterial dermatosis data set, 0.87 and 0.63 for the fungal infection data set, and 0.87 and 0.63 for the hypersensitivity allergic dermatosis data set, respectively, which supported that the consensus models of each disease were more balanced and well-performed.Conclusions We developed consensus models for each skin disease for dogs by combining each best model developed with the normal and multispectral images, respectively. Since the normal images could be used to determine areas suspected of lesion of skin disease and additionally the multispectral images could help confirming skin redness of the area, the models achieved higher prediction accuracy with balanced performance between sensitivity and specificity.  \n",
      "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Violence in the Internet era poses a new challenge to the current counter-riot work, and according to research and analysis, most of the violent incidents occurring are related to the dissemination of violence images. The use of the popular deep learning neural network to automatically analyze the massive amount of images on the Internet has become one of the important tools in the current counter-violence work. This paper focuses on the use of transfer learning techniques and the introduction of an attention mechanism to the residual network (ResNet) model for the classification and identification of violence images. Firstly, the feature elements of the violence images are identified and a targeted dataset is constructed; secondly, due to the small number of positive samples of violence images, pre-training and attention mechanisms are introduced to suggest improvements to the traditional residual network; finally, the improved model is trained and tested on the constructed dedicated dataset. The research results show that the improved network model can quickly and accurately identify violence images with an average accuracy rate of 92.20%, thus effectively reducing the cost of manual identification and providing decision support for combating rebel organization activities.  \n",
      "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Weeds bring disadvantages to crops since they can damage them, and a clean treatment with less pollution and contamination should be developed. Artificial intelligence gives new hope to agriculture to achieve smart farming. This study delivers an automated weeds growth point detection using deep learning. This study proposes a combination of semantic graphics for generating data annotation and U-Net with pre-trained deep learning as a backbone for locating the growth point of the weeds on the given field scene. The dataset was collected from an actual field. We measured the intersection over union, f1-score, precision, and recall to evaluate our method. Moreover, Mobilenet V2 was chosen as the backbone and compared with Resnet 34. The results showed that the proposed method was accurate enough to detect the growth point and handle the brightness variation. The best performance was achieved by Mobilenet V2 as a backbone with IoU 96.81%, precision 97.77%, recall 98.97%, and f1-score 97.30%.  \n",
      "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       An analytical model configuration, in addition to air pressure analysis and post-processing, was conducted to measure the interior wind noise by changing the exterior vehicular design. Although wind noise can be calculated accurately through the current process, it requires three to five days for each design. In this study, a convolutional neural network (CNN), which is a class of deep neural networks designed for processing image data, was applied to predict the wind noise with vehicle design images from four different views. Feature maps were extracted from the CNN models trained with images of each view and concatenated to flow through a sequence of fully connected (FC) layers to predict the wind noise. Moreover, visualization of the significant vehicle parts for wind noise prediction was provided using a gradient-weighted class activation map (Grad- CAM). Finally, we compared the performance of various CNN-based models, such as ResNet, DenseNet, and EfficientNet, in addition to the architecture of the FC layers. The proposed method can predict the wind noise using vehicle images from different views with a root-mean-square error (RMSE) value of 0.206, substantially reducing the time and cost required for interior wind noise estimation.  \n",
      "95                                                                                                                                                                                                                                                                                                                                                       Purpose: Despite the proliferation of numerous morphometric and anthropometric methods for sex identification based on linear, angular, and regional measurements of various parts of the body, these methods are subject to error due to the observer's knowledge and expertise. This study aimed to explore the possibility of automated sex determination using convolutional neural networks(CNNs) based on lateral cephalometric radiographs. Materials and Methods: Lateral cephalometric radiographs of 1,476 Iranian subjects (794 women and 682 men) from 18 to 49 years of age were included. Lateral cephalometric radiographs were considered as a network input and output layer including 2 classes(male and female). Eighty percent of the data was used as a training set and the rest as a test set. Hyperparameter tuning of each network was done after preprocessing and data augmentation steps. The predictive performance of different architectures (DenseNet, ResNet, and VGG) was evaluated based on their accuracy in test sets. Results: The CNN based on the DenseNet121 architecture, with an overall accuracy of 90%, had the best predictive power in sex determination. The prediction accuracy of this model was almost equal for men and women. Furthermore, with all architectures, the use of transfer learning improved predictive performance. Conclusion: The results confirmed that a CNN could predict a person's sex with high accuracy. This prediction was independent of human bias because feature extraction was done automatically. However, for more accurate sex determination on a wider scale, further studies with larger sample sizes are desirable.  \n",
      "96                                                                                                                                                                                                                                                                                                                                                            Purpose: Despite the proliferation of numerous morphometric and anthropometric methods for sex identification based on linear, angular, and regional measurements of various parts of the body, these methods are subject to error due to the observer’s knowledge and expertise. This study aimed to explore the possibility of automated sex determination using convolutional neural networks(CNNs) based on lateral cephalometric radiographs.Materials and Methods: Lateral cephalometric radiographs of 1,476 Iranian subjects (794 women and 682 men) from 18 to 49 years of age were included. Lateral cephalometric radiographs were considered as a network input and output layer including 2 classes(male and female). Eighty percent of the data was used as a training set and the rest as a test set. Hyperparameter tuning of each network was done after preprocessing and data augmentation steps.The predictive performance of different architectures (DenseNet, ResNet, and VGG) was evaluated based on their accuracy in test sets.Results: The CNN based on the DenseNet121 architecture, with an overall accuracy of 90%, had the best predictive power in sex determination. The prediction accuracy of this model was almost equal for men and women.Furthermore, with all architectures, the use of transfer learning improved predictive performance.Conclusion: The results confirmed that a CNN could predict a person’s sex with high accuracy. This prediction was independent of human bias because feature extraction was done automatically. However, for more accurate sex determination on a wider scale, further studies with larger sample sizes are desirable.  \n",
      "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In an experiment where students learn electrical and electronic theory and apply it, students have difficulty in connecting circuits. To alleviate the above difficulties, we first propose a circuit analysis service. The proposed method detects an electric device using an object detection model in which electric devices connected to a breadboard are labeled with data. To verify this, we connect to the breadboard circuit and create a custom dataset through photographs. The proposed method is divided into two processes: electric device prediction and electric device position detection. The electric device prediction model was compared using five object detection models, and the Faster R-CNN model had the best prediction performance. The electrical device position detector extracts features from the object detection model through transition learning to predict two coordinates (x1, y1), (x2, y2). A comparison of each model confirmed that the ResNet model has good location detection performance. Through this, it was confirmed that the proposed method alleviates the difficulty of first-time students learning electric and electronic experiments.  \n",
      "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              None  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                                                  keyword  count\n",
      "36                                          Deep Learning     17\n",
      "20                                                 ResNet     17\n",
      "0                                                     딥러닝     14\n",
      "70                                          Deep learning     14\n",
      "6                                                     CNN     14\n",
      "17                                          deep learning     10\n",
      "105                                                     .      7\n",
      "69                           Convolutional neural network      5\n",
      "58                                              MobileNet      4\n",
      "54                                Artificial intelligence      4\n",
      "92                                      Transfer learning      3\n",
      "27                                                 VGGNet      3\n",
      "279                          Convolutional Neural Network      3\n",
      "56                                              GoogLeNet      3\n",
      "83                                           Triplet Loss      3\n",
      "119                                      Machine learning      3\n",
      "135                                                  FPGA      3\n",
      "343                                                   VGG      3\n",
      "325                                    Deep Learning(딥러닝)      2\n",
      "114                                     Transfer Learning      2\n",
      "272                                                 U-Net      2\n",
      "217                                          Faster R-CNN      2\n",
      "216                                                  인공지능      2\n",
      "44                                                합성곱 신경망      2\n",
      "171                                 Semantic Segmentation      2\n",
      "204                                      Defect Detection      2\n",
      "199                                   Multispectral image      2\n",
      "51                                              ResNet-50      2\n",
      "52                            Surgical support technology      2\n",
      "280                                            Data Reuse      2\n",
      "37                                         Classification      2\n",
      "2                              Convolution Neural Network      2\n",
      "57                                               DenseNet      2\n",
      "158                                               Privacy      2\n",
      "361                                              CycleGAN      2\n",
      "130                                   Deep Neural Network      2\n",
      "264                                  Image classification      2\n",
      "440                                           Radiography      2\n",
      "439                            Sex Determination Analysis      2\n",
      "5                                         Computer Vision      2\n",
      "1                                                  컴퓨터 비전      2\n",
      "28                                               Grad-CAM      2\n",
      "442                                    Cervical Vertebrae      2\n",
      "441                                          Cephalometry      2\n",
      "298                               Artificial Intelligence      2\n",
      "351                                                  LSTM      2\n",
      "234                         convolutional neural networks      2\n",
      "133                                       CNN accelerator      2\n",
      "393                            convolution neural network      2\n",
      "308                                      OPhthalmoloscopy      1\n",
      "319                                  Activation Function.      1\n",
      "307                                              ImageNet      1\n",
      "313                              deep  residual  learning      1\n",
      "318                                  α and β Regularizers      1\n",
      "321                                           3D Modeling      1\n",
      "317                         Machine Learning Applications      1\n",
      "316                                        Neural Network      1\n",
      "309                                     personal mobility      1\n",
      "315                         multiple  species recognition      1\n",
      "310                          short-time Fourier transform      1\n",
      "311                                     wavelet transform      1\n",
      "314                                    image  recognition      1\n",
      "312                             butterfly  identification      1\n",
      "320                                          Digital Twin      1\n",
      "252                                      Filter Diversity      1\n",
      "322                                        Occlusion Area      1\n",
      "323                                             Detection      1\n",
      "342                                 Nuclear fuel assembly      1\n",
      "341                                           Monte Carlo      1\n",
      "340                                            tomography      1\n",
      "339                       Single-photon emission computed      1\n",
      "338                                    Bidirectional LSTM      1\n",
      "337                                   Fake news detection      1\n",
      "336                                attenuation correction      1\n",
      "335                                        quantification      1\n",
      "334                          myocardial perfusion imaging      1\n",
      "333                                                 SPECT      1\n",
      "332                                         SNR(신호 대 잡음비)      1\n",
      "331                               Domain Adaption(도메인 적응)      1\n",
      "330                                        FFT(빠른 푸리에 변환)      1\n",
      "329                              DenseNet(밀집 연결 합성곱 네크워크)      1\n",
      "328                                     ResNet(심층 잔차 신경망)      1\n",
      "305                                                    AI      1\n",
      "327                                  Physics Model(물리 모델)      1\n",
      "326                     Vehicle Chassis System(차량 샤시 시스템)      1\n",
      "324                                            In-paining      1\n",
      "306                                     Fundus photograph      1\n",
      "292                    Representative Batch Normalization      1\n",
      "304                                       Siamese Network      1\n",
      "265                                    image segmentation      1\n",
      "275                                        Aluminum alloy      1\n",
      "274                                        Surface Defect      1\n",
      "273                                          Tversky loss      1\n",
      "271                                       Metal artifacts      1\n",
      "270                     Anatomical structure segmentation      1\n",
      "250                                       Crack detection      1\n",
      "269                                Auto Visual Inspection      1\n",
      "268                                         Semiconductor      1\n",
      "267                                      irrigation canal      1\n",
      "266                                           CCTV images      1\n",
      "263                       Synthetic Aperture Radar (SAR).      1\n",
      "277                                     Data Augmentation      1\n",
      "262                                 Machine Learning (ML)      1\n",
      "261                                    Deep Learning (DL)      1\n",
      "260                      Automatic Target Detection (ATD)      1\n",
      "251                                Feature Representation      1\n",
      "259                                  database performance      1\n",
      "258                                      CNN architecture      1\n",
      "257                                   emotion recognition      1\n",
      "256                      facial expression classification      1\n",
      "255                                    thermal face image      1\n",
      "254                                      Filter Spreading      1\n",
      "276                                             Extrusion      1\n",
      "278                                Neural Processing Unit      1\n",
      "303                                     Few-shot Learning      1\n",
      "291                                Scene text recognition      1\n",
      "302                                    Object Recognition      1\n",
      "301                                       Maritime Target      1\n",
      "300                                           Radar Image      1\n",
      "299                      Inverse Synthetic Aperture Radar      1\n",
      "297                                    Processing Element      1\n",
      "296                                           Accelerator      1\n",
      "295                     Deep Convolutional Neural Network      1\n",
      "294                                   Feature enhancement      1\n",
      "293                                Feature representation      1\n",
      "253            Singular Value Decomposition (SVD) Entropy      1\n",
      "290                                  Weights Correlation.      1\n",
      "281                                                  FIFO      1\n",
      "249                                                   UAV      1\n",
      "345                                           FGSM attack      1\n",
      "289                                 Fully Connected Layer      1\n",
      "288                                                   MRI      1\n",
      "287                                   Activation Channels      1\n",
      "286                                        (CLAHE) filter      1\n",
      "285                      Convolution neural network (CNN)      1\n",
      "284                                 Retinal fundus images      1\n",
      "283                                       Retinal disease      1\n",
      "282                                    Interleaved Memory      1\n",
      "344                                    Adversarial attack      1\n",
      "359                      Facial Expression Classification      1\n",
      "346                                            BIM attack      1\n",
      "428                                         Metal Surface      1\n",
      "426                                                  머신러닝      1\n",
      "425                                                 결함 검출      1\n",
      "424                                                 금속 외관      1\n",
      "423                                            Dermatosis      1\n",
      "422                                      Dog skin disease      1\n",
      "421                                Knowledge distillation      1\n",
      "420                                     Genetic algorithm      1\n",
      "419                                   Structure reduction      1\n",
      "418                             Lightweight Deep Learning      1\n",
      "417                            Neural Architecture Search      1\n",
      "416                                Room Layout Estimation      1\n",
      "415                                Child drawing analysis      1\n",
      "414                                      Image Similarity      1\n",
      "413                                               아동 그림분석      1\n",
      "412                                               이미지 유사성      1\n",
      "411                                        Unity ML Agent      1\n",
      "410                                            Word Cloud      1\n",
      "409                                     Self-help-therapy      1\n",
      "408                                          Healing Game      1\n",
      "407                                              REST API      1\n",
      "406                                            Dialogflow      1\n",
      "405                                 Recommendation System      1\n",
      "404                               Collaborative Filtering      1\n",
      "427                                                합성곱신경망      1\n",
      "429                                      Machine Learning      1\n",
      "347                                             CW attack      1\n",
      "430                                          Pre-training      1\n",
      "457                                     Binarization work      1\n",
      "456                                         Anonymization      1\n",
      "455                                   Patient information      1\n",
      "454                        Medical image processing field      1\n",
      "453                                                  THRA      1\n",
      "452                                   Position prediction      1\n",
      "451                                    Position detection      1\n",
      "450                                            Breadboard      1\n",
      "449                                               Service      1\n",
      "448                     Electrical and electronic testing      1\n",
      "447                                                 위치 예측      1\n",
      "446                                                 위치 검출      1\n",
      "445                                                 브레드보드      1\n",
      "444                                                   서비스      1\n",
      "443                                               전기전자 실험      1\n",
      "438     Gradient-weighted class activation map (Grad-CAM)      1\n",
      "437                   Convolutional neural networks (CNN)      1\n",
      "436                                      Image regression      1\n",
      "435                                 Wind noise prediction      1\n",
      "434                                     semantic graphics      1\n",
      "433                                          growth point      1\n",
      "432                               artificial intelligence      1\n",
      "431                                       Violence images      1\n",
      "403                                     Voice recognition      1\n",
      "402                                                   Web      1\n",
      "401                                             Inception      1\n",
      "400                                long short term memory      1\n",
      "372                                      Template cluster      1\n",
      "371                                     Electrocardiogram      1\n",
      "370                                              비정상심박 검출      1\n",
      "369                                                 템플릿 군      1\n",
      "368                                                   심전도      1\n",
      "367                                       Image Rendering      1\n",
      "366                                        Interior Style      1\n",
      "365                                     Image Translation      1\n",
      "364                                               이미지 렌더링      1\n",
      "363                                              인테리어 스타일      1\n",
      "362                                                이미지 변환      1\n",
      "360                                 Edge Detection Filter      1\n",
      "247                                 대한방사선과학회(구 대한방사선기술학회)      1\n",
      "358                                            가장자리 검출 필터      1\n",
      "357                                              얼굴 표정 분류      1\n",
      "356                                                 감정 분석      1\n",
      "355                                                텍스트 요약      1\n",
      "354                                              주가 예측 모델      1\n",
      "353                                    Text Summarization      1\n",
      "352                                    Sentiment Analysis      1\n",
      "350                               Stock Price Forecasting      1\n",
      "349                                          Perturbation      1\n",
      "348                                           White noise      1\n",
      "373                               Abnormal beat detection      1\n",
      "374                                           AI security      1\n",
      "375                                    Exploratory Attack      1\n",
      "388                           False data injection attack      1\n",
      "399                                   deep neural network      1\n",
      "398                   Image Data Augmentation(이미지 데이터 증강)      1\n",
      "397                                Edge Detection(윤곽선 탐지)      1\n",
      "396        Fine-Grained Visual Classification(세밀한 이미지 분류)      1\n",
      "395                                           ResNet(레스넷)      1\n",
      "394                                                ILSVRC      1\n",
      "392                                                 AdamW      1\n",
      "391                                   Data-driven methods      1\n",
      "390                                  Multi-modal learning      1\n",
      "389                                       Topology attack      1\n",
      "387                           Power system cyber security      1\n",
      "376                                      Inference Attack      1\n",
      "386                         Image Segmentation(이미지 영역 분할)      1\n",
      "385                            Data Augmentation (데이터 증식)      1\n",
      "384                Convolutional Neural Networks(합성곱 신경망)      1\n",
      "383                                 Machine Vision(머신 비전)      1\n",
      "382                                          Segmentation      1\n",
      "381                                    Forest restoration      1\n",
      "380                                          Aerial photo      1\n",
      "379                                                  항공사진      1\n",
      "378                                                  산림복원      1\n",
      "377                                                    분할      1\n",
      "248                                           Ortho-image      1\n",
      "229                                  Image Classification      1\n",
      "246                                             Bone Scan      1\n",
      "99                            Bio-Signal Data Measurement      1\n",
      "97                                              운전자 상태 분류      1\n",
      "96                                                 관성측정장치      1\n",
      "95                                                생체신호 분석      1\n",
      "94                                           운전자 모니터링 시스템      1\n",
      "93                                               ResNet50      1\n",
      "91                     Convolutional neural network (CNN)      1\n",
      "90                        Artificial neural network (ANN)      1\n",
      "89                               Online monitoring system      1\n",
      "88                                           Flow pattern      1\n",
      "87                                         tomato disease      1\n",
      "86                                                    레스넷      1\n",
      "85                                                   모바일넷      1\n",
      "84                                                 토마토병해충      1\n",
      "82                                        Dog Recognition      1\n",
      "81                                       Face Recognition      1\n",
      "80                                             Fast R-CNN      1\n",
      "79                                          Ureter stones      1\n",
      "78                                           Urolithiasis      1\n",
      "77                                         Renal dialysis      1\n",
      "76                                           Auscultation      1\n",
      "75                                  Arteriovenous fistula      1\n",
      "74                                            Angioplasty      1\n",
      "73                                       Nuclear medicine      1\n",
      "98                              Driver Monitoring Systems      1\n",
      "100                             Inertial Measurement Unit      1\n",
      "245                                         Medical Image      1\n",
      "101                          Driver States Classification      1\n",
      "127                                                 다중 모달      1\n",
      "126                                                 심층신경망      1\n",
      "125                                              포인트 클라우드      1\n",
      "124                           Fire prediction performance      1\n",
      "123                             Fire image classification      1\n",
      "122                                        Fire detection      1\n",
      "121                                            ResNet-101      1\n",
      "120                                Multi-layer perceptron      1\n",
      "118                                            Face masks      1\n",
      "117                                                  CNNs      1\n",
      "116                                             Dog Breed      1\n",
      "115                                                Resnet      1\n",
      "113                                            설명가능한 인공지능      1\n",
      "112                                                 패턴 인식      1\n",
      "111                                            시계열 데이터 분류      1\n",
      "110                                                센서 데이터      1\n",
      "109              eXplainable Artificial Intelligence(XAI)      1\n",
      "108                                   Pattern Recognition      1\n",
      "107                            Time Series Classification      1\n",
      "106                                           Sensor Data      1\n",
      "104                                 Semiconductor Package      1\n",
      "103                                      Image Processing      1\n",
      "102                                     Scratch Detection      1\n",
      "72                                   Class activation map      1\n",
      "71                                      Lung scintigraphy      1\n",
      "68                                                    핵의학      1\n",
      "67                                               분류-활성화 맵      1\n",
      "31                                        lightweight CNN      1\n",
      "30                                                ArcFace      1\n",
      "29                                                   직물결함      1\n",
      "26                                         fabric defects      1\n",
      "25                                  Secondary development      1\n",
      "24                         Multidisciplinary optimization      1\n",
      "23                                      Design automation      1\n",
      "22                                             RV reducer      1\n",
      "21                                 Sequential engineering      1\n",
      "19                             Kapitza thermal resistance      1\n",
      "18                                          heat transfer      1\n",
      "16                                     Composite material      1\n",
      "15                         Squeeze-and-Excitation Modules      1\n",
      "14                                     ResNet50 structure      1\n",
      "13   Multi-scale convolutional feature extraction methods      1\n",
      "12                                               ResNet34      1\n",
      "11                                  Target classification      1\n",
      "10                                      Spectral kurtosis      1\n",
      "9                                   Characteristic vector      1\n",
      "8                                           Micro-Doppler      1\n",
      "7                                       Crop Growth Stage      1\n",
      "4                                            Deep Learing      1\n",
      "3                                                 작물 생육단계      1\n",
      "32                                   identity recognition      1\n",
      "33                                                edge AI      1\n",
      "34                      Non-Small Cell Lung Cancer(NSCLC)      1\n",
      "50                                     Ureteral stricture      1\n",
      "66                                                폐 신티그라피      1\n",
      "65                                               컨볼루션 신경망      1\n",
      "64                                               F1-score      1\n",
      "63                                       MIT-BIH database      1\n",
      "62                                                ResNeXt      1\n",
      "61                               ECG data analysis scheme      1\n",
      "60                                    CNN block structure      1\n",
      "59                                           Contact Lens      1\n",
      "55                                               컬러 콘택트렌즈      1\n",
      "53                                              Endoscope      1\n",
      "49                                     Urethral stricture      1\n",
      "35                                  Recurrence Prediction      1\n",
      "48                                       polar coordinate      1\n",
      "47                                 fingerprinting feature      1\n",
      "46                                              ResNet-1D      1\n",
      "45                                        DMR Fingerprint      1\n",
      "43                                                 앙상블 학습      1\n",
      "42                                                     분류      1\n",
      "41                                                  재발 예측      1\n",
      "40                                                 비소세포폐암      1\n",
      "39                     Convolutional Neural Network(CNN)1      1\n",
      "38                                      Ensemble Learning      1\n",
      "128                                                 형상 분류      1\n",
      "129                                           Point Cloud      1\n",
      "131                                           Multi-modal      1\n",
      "188                                                장애인접근성      1\n",
      "214                                               배터리 리드탭      1\n",
      "213                          convolutional neural network      1\n",
      "212                                     weight prediction      1\n",
      "211                                       computer vision      1\n",
      "210                                            pet feeder      1\n",
      "209                                                 중량 예측      1\n",
      "208                                                 사료급식기      1\n",
      "207                                    One-Stage Detector      1\n",
      "206                                       RetinaNet model      1\n",
      "205                                Steel Defect Detection      1\n",
      "203                                 Intraoral radiography      1\n",
      "202                                         Primary teeth      1\n",
      "201                                       Proximal caries      1\n",
      "200                                         Marine debris      1\n",
      "198                                                 Drone      1\n",
      "197                                            Sentinel-2      1\n",
      "196                                               YOLO-v3      1\n",
      "195                                  Hardware utilization      1\n",
      "194                                    Processing element      1\n",
      "193             personalized speech classification scheme      1\n",
      "192                                disabled accessibility      1\n",
      "191                                speech-impaired people      1\n",
      "190                                         smart speaker      1\n",
      "215                                              압흔 오류 검출      1\n",
      "218                                                 객체 탐지      1\n",
      "219                                      Battery lead tab      1\n",
      "233                                     electrocardiogram      1\n",
      "244                                                3D CNN      1\n",
      "243                                      residual network      1\n",
      "242                                        fall detection      1\n",
      "241                                    Video surveillance      1\n",
      "240                                      Score Prediction      1\n",
      "239                                Multiple Task Learning      1\n",
      "238                 No-Reference Image Quality Assessment      1\n",
      "237                     Fourier Synchrosqueezed Transform      1\n",
      "236                          Short-Time Fourier Transform      1\n",
      "235                              time-frequency transform      1\n",
      "232                                 person identification      1\n",
      "220                               Welding error detection      1\n",
      "231                                                Widget      1\n",
      "230                                      Object Detection      1\n",
      "228                                 Deep Learning Dataset      1\n",
      "227                                        Captured Image      1\n",
      "226                                              Ensemble      1\n",
      "225                                                   ECG      1\n",
      "224                                                   PPG      1\n",
      "223                                              Exercise      1\n",
      "222                                                   HAR      1\n",
      "221                                      Object detection      1\n",
      "189                                           개인화된 음성분류기법      1\n",
      "187                                                 언어장애인      1\n",
      "132                                  Shape Classification      1\n",
      "186                                                스마트스피커      1\n",
      "157                        Analysis of feature importance      1\n",
      "156                                   Keypoints detection      1\n",
      "155                                             Open pose      1\n",
      "154                                           Simple pose      1\n",
      "153                                                Ballet      1\n",
      "152                                             특징 중요도 분석      1\n",
      "151                                              관절 좌표 추출      1\n",
      "150                                                 오픈 포즈      1\n",
      "149                                                 심플 포즈      1\n",
      "148                                                    발레      1\n",
      "147                           Natural Language Processing      1\n",
      "146                               Facial Image Processing      1\n",
      "145                            Facial Expression Practice      1\n",
      "144                                Emotion Classification      1\n",
      "143                                                자연어 처리      1\n",
      "142                                             얼굴 이미지 처리      1\n",
      "141                                                 표정 연습      1\n",
      "140                                                 감정 분류      1\n",
      "139                               Support vector machines      1\n",
      "138                                           X-ray image      1\n",
      "137                                              Covid-19      1\n",
      "136                                           Verilog-HDL      1\n",
      "134                             module-based architecture      1\n",
      "159                                    Action Recognition      1\n",
      "160                                                YOLOv5      1\n",
      "161                               Single-person household      1\n",
      "175                                                 원격 탐사      1\n",
      "185                                    residual structure      1\n",
      "184                     Deep convolutional neural network      1\n",
      "183                                   Lightweight network      1\n",
      "182                        Real-time image classification      1\n",
      "181                                           MobileNetV3      1\n",
      "180                                            Prediction      1\n",
      "179                                      Image processing      1\n",
      "178                                                  Pore      1\n",
      "177                                                  Skin      1\n",
      "176                                                 도로 추출      1\n",
      "174                                               드론 정사영상      1\n",
      "162                                                 프라이버시      1\n",
      "173                                       Road Extraction      1\n",
      "172                                        Remote Sensing      1\n",
      "170                                           Drone Image      1\n",
      "169                                 Precision agriculture      1\n",
      "168                                        Coffee species      1\n",
      "167                                        Arabica coffee      1\n",
      "166                                        Robusta coffee      1\n",
      "165                                   Leaf classification      1\n",
      "164                                                 1인 가구      1\n",
      "163                                                 행동 인식      1\n",
      "458                                CNN-based architecture      1\n",
      "데이터가 저장되었습니다: resnet_2022_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2022_academic_riss.csv\n",
      "영상신호를 입력으로 하는 3D ResNet기반 유아 행동 인식 기법 2023 ['유아 행동 인식', '딥러닝', 'ResNet', '영상신호', 'Children behavior recognition', 'deep learning', 'ResNet', 'video input'] 본 연구에서는 다수의 유아가 등장하는 영상 내의 행동을 인식하기 위하여 딥러닝 기반의 유아 행동 인식 기술을 개발하였다. 유아들의 경우 동일한 행동이라도 표현과 방법이 다양하여 다양한 종류의 입력에 강건하게 분석될 수 있는 딥러닝 모델에 대한 개발이 필요하다. 본 연구에서는 입력 신호를 딥러닝의 입력에 맞도록 처리하고 3D ResNet을 사용하여 행동 인식 알고리즘을 제안하였다. 50명의 유아를 대상으로 13개 행동을 수행하는 영상 자료를 수집하였으며, 실험결과 13개의 행동 인식에 평균 72.21% 정확도를 보였다. 행동 중 서 있기 90.74%, 밀고 당기기 88.89%, 앉기 90.74%의 행동 인식률을 보였다. 향후 본 연구 결과물을 통해 일상생활에서 유아들의 행동 패턴을 자동으로 분석하고 서비스하는 연구에 활용될 수 있다. None\n",
      "흉부 X선 영상을 이용한 작은 층수 ResNet 기반 폐렴 진단 모델의 성능 평가 2023 ['딥러닝', '흉부 X선 영상', '폐렴 진단', '학습 가능 파라미터', 'Residual 블록', 'Deep learning', 'Chest X-ray image', 'Pneumonia detection', 'Trainable parameter', 'Residual block'] None None\n",
      "SwinResNet: Swin Transformer와 ResNet 융합을 통한 Volumetric 의료 영상 분할 2023 ['Convolutional neural network', 'Medical image segmentation', 'ResNet', 'Swin Trans- former'] None Volumetric medical image segmentation is critical in diagnosing diseases and planning subse- quent treatment. The convolutional neural network (CNN)-based U-Net was proposed for con- ducting accurate and robust medical image segmentation since the skip connection of U-Net and deep feature representation significantly improved its performance. However, since CNN-based models mainly focus on local and low-level features, they cannot extract global and high-level features effectively. Meanwhile, the Vision Transformer developed in natural language process- ing is proposed to improve image classification performance by splitting an input image into patches and conducting linear embeddings of the patches, which can extract global features. However, the Vision Transformer has difficulty in handling detailed and low-level features. This study proposes SwinResNet which can effectively conduct volumetric medical image segmenta- tion by fusing the Swin Transformer and CNN models. The combination can take advantage of both models and complement each other. Swin Transformer and ResNet are used as encoders, and the receptive field blocks and aggregation modules are applied to the multi-level features extracted from both encoders. Comprehensive evaluation shows that the proposed approach out- performs well-known previous studies.\n",
      "밀 종자 품종 및 물성 분류를 위한 ResNet50 모델 기반의 이미지 분석 2023 ['Classification', 'Image Processing', 'ResNet50', 'Variety', 'Wheat'] 밀은 대표적인 식량 작물 중 하나이지만 최근 국내 밀 자급률은 1%에 불과하다. 밀의 자급량을 높이기 위해서는 밀 종자의 품종순도를 높여 고품질의 가공물을 얻을 수 있도록 품질 관리를 해야 한다. 본 연구에서는 밀의 품질을 자동으로 판정하는 기술을 마련하기 위하여 딥러닝 알고리즘을 활용하여 밀의 품종과 경질, 연질 여부를 분류하는 모델을 개발하고자 하였다. 우리나라 주요 보급 품종인 금강, 백강, 새금강, 조경, 황금알에 대하여 개발한 이미지 획득 시스템을 이용하여 총 21,256개의 밀 종자 낱알 이미지를 획득하였다. 획득한 이미지에서 낱알의 장축, 단축 길이와 RGB 각각의 평균 색상 값을 계산해 품종, 경도별로 비교하였다. 또한, ResNet50 모델을 이용하여 밀 종자 5품종과 경질, 연질을 분류하는 모델을 개발하였다. 그 결과, 학습, 검증, 테스트 그룹의 분류 정확도는 각각 98.17%, 96.68%, 96.40%를 나타냈다. 테스트 그룹의 혼동행렬을 확인한 결과, 대부분 성공적으로 분류가 이루어졌고 동일 품종에 대해서는 경질, 연질이 100% 정확도로 분류되는 것을 확인하였다. 이를 통해 딥러닝 알고리즘을 이용하여 밀 종자 품종 및 경질, 연질 여부를 판별할 수 있을 것으로 판단된다. Wheat is one of the major food crops, but the domestic self-sufficiency rate is only 1%. To increase self-sufficiency, it is essential to manage the quality of wheat seeds by classifying the seed quality grades. This study used deep learning algorithms to classify wheat varieties and distinguish hard and soft wheat. Five wheat varieties used in the South were employed, and 21,256 images of individual wheat seeds were acquired using an image acquisition system. The length of the axes and the average values in the RGB channels were then calculated. Furthermore, the ResNet50 architecture was used to classify the five wheat varieties and hardness. The results revealed high classification performances for the training, validation, and test groups, with rates of 98.17%, 96.68%, and 96.40%, respectively. The confusion matrix of the test group indicated successful classification, and hardness was classified with 100% accuracy for the same varieties. Deep learning algorithms enable the determination of wheat seed varieties and hardness.\n",
      "ResNet/SVM 기반 GNSS 재밍 식별 기법 2023 ['GNSS', 'Jamming', 'Classification', 'Transfer Learning', 'ResNet', 'SVM'] None None\n",
      "3D Object Generation and Renderer System based on VAE ResNet-GAN 2023 ['variational autoencoder', 'generative adversarial network', 'residual learning', 'generation', 'reconstruction', 'voxel.'] None We present a method for generating 3D structures and rendering objects by combining VAE (Variational Autoencoder) and GAN (Generative Adversarial Network). This approach focuses on generating and rendering 3D models with improved quality using residual learning as the learning method for the encoder. We deep stack the encoder layers to accurately reflect the features of the image and apply residual blocks to solve the problems of deep layers to improve the encoder performance. This solves the problems of gradient vanishing and exploding, which are problems when constructing a deep neural network, and creates a 3D model of improved quality. To accurately extract image features, we construct deep layers of the encoder model and apply the residual function to learning to model with more detailed information. The generated model has more detailed voxels for more accurate representation, is rendered by adding materials and lighting, and is finally converted into a mesh model. 3D models have excellent visual quality and accuracy, making them useful in various fields such as virtual reality, game development, and metaverse.\n",
      "Improving the Cyber Security over Banking Sector by Detecting the Malicious Attacks Using the Wrapper Stepwise Resnet Classifier 2023 ['Cyber security', 'Malicious attacks', 'Cyber-physical systems', 'banking sector', 'Hierarchical network feature extraction', 'Wrapper stepwise ResNet'] None With the advancement of information technology, criminals employ multiple cyberspaces to promote cybercrime. To combat cybercrime and cyber dangers, banks and financial institutions use artificial intelligence (AI). AI technologies assist the banking sector to develop and grow in many ways. Transparency and explanation of AI's ability are required to preserve trust. Deep learning protects client behavior and interest data. Deep learning techniques may anticipate cyber-attack behavior, allowing for secure banking transactions. This proposed approach is based on a user-centric design that safeguards people's private data over banking. Here, initially, the attack data can be generated over banking transactions. Routing is done for the configuration of the nodes. Then, the obtained data can be preprocessed for removing the errors. Followed by hierarchical network feature extraction can be used to identify the abnormal features related to the attack. Finally, the user data can be protected and the malicious attack in the transmission route can be identified by using the Wrapper stepwise ResNet classifier. The proposed work outperforms other techniques in terms of attack detection and accuracy, and the findings are depicted in the graphical format by employing the Python tool.\n",
      "Reversible Multipurpose Watermarking Algorithm Using ResNet and Perceptual Hashing 2023 ['Deep Residual Network', 'Multipurpose Watermarking', 'Perceptual Hashing', 'Reversible Visible Watermarking'] None To effectively track the illegal use of digital images and maintain the security of digital image communicationon the Internet, this paper proposes a reversible multipurpose image watermarking algorithm based on a deepresidual network (ResNet) and perceptual hashing (also called MWR). The algorithm first combines perceptualimage hashing to generate a digital fingerprint that depends on the user’s identity information and imagecharacteristics. Then it embeds the removable visible watermark and digital fingerprint in two different regionsof the orthogonal separation of the image. The embedding strength of the digital fingerprint is computed usingResNet. Because of the embedding of the removable visible watermark, the conflict between the copyrightnotice and the user’s browsing is balanced. Moreover, image authentication and traitor tracking are realizedthrough digital fingerprint insertion. The experiments show that the scheme has good visual transparency andwatermark visibility. The use of chaotic mapping in the visible watermark insertion process enhances thesecurity of the multipurpose watermark scheme, and unauthorized users without correct keys cannot effectivelyremove the visible watermark.\n",
      "다양한 CNN 모델을 이용한 얼굴 영상의 나이 인식 연구 2023 ['Facial age estimation', 'CNN', 'AlexNet', 'VGG', 'ResNet'] 얼굴 영상으로부터 나이를 인식하는 기술의 응용분야가 증가함에 따라 이에 대한 연구가활발히 진행되고 있다. 얼굴 영상으로부터 나이를 인식하기 위해서는 나이를 표현하는 특징을추출하고, 추출된 특징으로 나이를 정확하게 분류하는 기술이 필요하다. 최근 영상 인식분야에서 다양한 CNN 기반 딥러닝 모델이 적용되어 성능이 크게 개선되고 있으며, 얼굴 나이인식 분야에서도 성능 개선을 위해 다양한 CNN 기반 딥러닝 모델이 적용되고 있다. 본논문에서는 다양한 CNN 기반 딥러닝 모델의 얼굴 나이 인식 성능을 비교하는 연구를 수행하였다.영상 인식 분야에서 많이 활용되고 있는 AlexNet, VGG-16, VGG-19, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152를 활용하여 얼굴 나이 인식을 위한 모델을 구성하고 성능을비교하였다. 실험 결과에서 ResNet-34를 이용한 얼굴 나이 인식 모델의 성능이 가장 우수하다는것을 확인하였다. There is a growing interest in facial age estimation because many applications require age estimation techniques from facial images. In order to estimate the exact age of a face, a technique for extracting aging features from a face image and classifying the age according to the extracted features is required.Recently, the performance of various CNN-based deep learning models has been greatly improved in the image recognition field, and various CNN-based deep learning models are being used to improve performance in the field of facial age estimation. In this paper, age estimation performance was compared by learning facial features based on various CNN-based models such as AlexNet, VGG-16, VGG-19, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152. As a result of experiment, it was confirmed that the performance of the facial age estimation models using ResNet-34 was the best.\n",
      "위내시경 영상에서의 위 병변 자동 검출 모델 개발을 위한 RetinaNet 기반 backbone 네트워크에 따른 학습 성능 비교 2023 ['Gastroscopy image', 'gastric lesion', 'detection', 'deep learning', 'RetinaNet', '위내시경 영상', '위 병변', '검출', '딥러닝', '레티나넷'] 본 연구에서는 위내시경 검사 시에 보조 시스템으로 활용할 수 있도록 RetinaNet 네트워크를 사용하여 위내시경 영상에서의 위 병변의 위치를 자동으로 검출하는 모델을 개발하였다. 위암은 한국이나 일본 등의 아시아권에서 대부분 발생한다. 그러나 위내시경 검사는 동시에 진단이나 치료할 수 있으며, 조기 발견 시 치료 성공확률이 매우 높다. 그러나 실시간으로 진행되는 검사 특성상 숙련도나 경험이 결과에 영향을 주며, 업무의 피로도 상승과 집중력 하락으로 인해 검사의 정확도가 낮아지게 된다. RetinaNet 기반의 backbone 네트워크로 ResNet50, ResNet152, EfficientNetB0, EfficientNetB4 네트워크를 사용하여 학습한 모델의 검출 성능을 확인하고, 각 모델 간의 성능을 비교하였다. RetinaNet 기반 backbone 네트워크별 모델들의 평균 민감도(FP/images)는 ResNet50 73.72%(0.0489), ResNet152 78.26%(0.0458), EfficinetNetB0 79.67%(0.3268), EfficientNetB4 62.66%(0.0448)를 보였다. EfficientNetB0 네트워크는 가장 높은 민감도를 나타냈으나 FP/images가 매우 높게 나타나 두 성능치를 모두 만족하는 네트워크는 ResNet152였다. Gastric cancer occurs mostly in Asian countries such as Korea and Japan. Gastroscopy allows diagnosis and treatment of gastric cancer at the same time, and the probability of successful treatment is very high at early detection. However, due to the nature of the inspection which progresses in real time, proficiency and experience of the clinician affect the results, and the accuracy of the inspectioncan decrease due to increased work fatigue and decreased concentration. In this study, we developed a model that automatically detects the regions of gastric lesion in gastroscopic images using the RetinaNet network so that it can be used as an auxiliary system during gastroscopy. We confirmed the detection performance of models trained using ResNet50, ResNet152, EfficientNetB0, and EfficientNetB4 networks in a RetinaNet-based backbone network, and compared the performance between each model. The average sensitivities (FP/images) of RetinaNet-based backbone network-models were 73.72% (0.0489) for ResNet50, 78.26% (0.0458) for ResNet152, 79.67% (0.3268) for EfficinetNetB0, and 79.67% (0.3268) for EfficientNetB4. The EfficientNetB0 network showed the highest sensitivity, but the FP/images were very high, so the network satisfying both performance values was ResNet152.\n",
      "심층 네트워크 모델에 기반한 어선 횡동요 시계열 예측 2023 ['fishing boat', 'capsizing accident', 'deep learning model', 'Xception', 'ResNet50', 'CRNN', '어선', '전복 사고', '딥러닝 모델', 'Xception', 'ResNet50', 'CRNN'] 통계에 따르면 어선의 전복 사고는 전체 전복 사고의 절반 이상을 차지한다. 이는 미숙한 조업, 기상 악화, 정비 미흡 등 다양한 원인으로 발생할 수 있다. 업계 규모와 영향도, 기술 복잡성, 지역적 다양성 등으로 인해 어선은 상선에 비해 상대적으로 연구가 부족한 실정이다. 본 연구에서는 이미지 기반 딥러닝 모델을 활용하여 어선의 횡동요 시계열을 예측하고자 한다. 이미지 기반 딥러닝은 시계열의 다양한 패턴을 학습하여 높은 성능을 낼 수 있다. 이를 위해 Xception, ResNet50, CRNN의 3가지의 이미지 기반 딥러닝 모델을 활용하였다. Xception과 ResNet50은 각각 177, 184개의 층으로 구성되어 있으며 이에 반해 CRNN은 22개의 비교적 얇은 층으로 구성되어 있다. 실험 결과 Xception 딥러닝 모델이 가장 낮은 0.04291의 sMAPE와 0.0198의 RMSE를 기록하였다. ResNet50과 CRNN은 각각 0.0217, 0.022의 RMSE를 기록하였다. 이를 통해 상대적으로 층이 더 깊은 모델의 정확도가 높음을 확인할 수 있다. Fishing boat capsizing accidents account for more than half of all capsize accidents. These can occur for a variety of reasons, including inexperienced operation, bad weather, and poor maintenance. Due to the size and influence of the industry, technological complexity, and regional diversity, fishing ships are relatively under-researched compared to commercial ships. This study aimed to predict the rolling motion time series of fishing boats using an image-based deep learning model. Image-based deep learning can achieve high performance by learning various patterns in a time series. Three image-based deep learning models were used for this purpose: Xception, ResNet50, and CRNN. Xception and ResNet50 are composed of 177 and 184 layers, respectively, while CRNN is composed of 22 relatively thin layers. The experimental results showed that the Xception deep learning model recorded the lowest Symmetric mean absolute percentage error(sMAPE) of 0.04291 and Root Mean Squared Error(RMSE) of 0.0198. ResNet50 and CRNN recorded an RMSE of 0.0217 and 0.022, respectively. This confirms that the models with relatively deeper layers had higher accuracy.\n",
      "인공지능 기반 화자 식별 기술의 불공정성 분석 2023 ['Speaker Identification', 'Biased dataset', 'AI Fairness', 'CNN', 'VoxCeleb1'] Covid-19으로 인한 디지털화는 인공지능 기반의 음성인식 기술을 급속하게 발전시켰다. 그러나 이 기술은 데이터셋이 일부 집단에 편향될 경우 인종 및 성차별과 같은 불공정한 사회적 문제를 초래하고 인공지능 서비스의 신뢰성과 보안성을 열화시키는 요인이 된다. 본 연구에서는 대표적인 인공지능의 CNN(Convolutional Neural Network) 모델인 VGGNet(Visual Geometry Group Network), ResNet(Residual neural Network), MobileNet을 활용한 편향된 데이터 환경에서 정확도에 기반한 불공정성을 비교 및 분석한다. 실험 결과에 따르면 Top1-accuracy에서 ResNet34가 여성과 남성이 91%, 89.9%로 가장 높은 정확도를 보였고, 성별 간 정확도 차는 ResNet18이 1.8%로 가장 작았다. 모델별 성별 간의 정확도 차이는 서비스 이용 시 남녀 간의 서비스 품질에 대한 차이와 불공정한 결과를 야기한다. Digitalization due to COVID-19 has rapidly developed artificial intelligence-based voice recognition technology. However, this technology causes unfair social problems, such as race and gender discrimination if datasets are biased against some groups, and degrades the reliability and security of artificial intelligence services. In this work, we compare and analyze accuracy-based unfairness in biased data environments using VGGNet (Visual Geometry Group Network), ResNet (Residual Neural Network), and MobileNet, which are representative CNN (Convolutional Neural Network) models of artificial intelligence. Experimental results show that ResNet34 showed the highest accuracy for women and men at 91% and 89.9% in Top1-accuracy, while ResNet18 showed the slightest accuracy difference between genders at 1.8%. The difference in accuracy between genders by model causes differences in service quality and unfair results between men and women when using the service.\n",
      "단시간 수중음향 신호를 활용한 합성곱 신경망 기반의 선박 소음 탐지 2023 ['Underwater acoustics', 'Shipping noise', 'Deep learning', 'Convolutional neural network (CNN)'] 해양에서 인간에 의해 발생하는 대부분의 소음은 어업 및 상업 운송과 관련된 선박 방사소음이 주요한 원인이다. 최근 선박소음을 자동으로 탐지하기 위한 방법으로 딥러닝 기술이 활용되고 있다. 본 연구에서는 1분 단위로 분할한 선박소음 신호 기반의 스펙트로그램 이미지를 합성곱 신경망 기반 학습을 수행하여 근거리 선박소음 및 배경소음을 자동으로 탐지하는 연구를 수행하였다. 현재까지 많이 사용되고 있는 합성곱 신경망 모델인 Inception-V3, ResNet-50, VGG-16와 본 연구에서 제안한 모델을 이용하여 1분 단위의 선박소음을 학습 및 평가를 수행하였다. 분석 결과 F1 점수는 모델별로 각각 Inception-V3 97.42%, ResNet-50 98.42%, VGG-16 98.16%, 제안된 모델은 97.88%로 나타나 선박소음을 탐지함에 있어 준수한 성능이 나타났다. 이 때, 제안된 모델은 F1 점수가 가장 높게 나타난 ResNet-50 모델에 비해 약 1/8의 적은 파라미터로 동등한 탐지 성능을 보이는 것을 확인할 수 있었다. 추후에는 다양한 선박소음 및 선박자동식별장치(AIS, Automatic Identification System) 자료를 동시에 활용하여 원거리 선박소음 또한 자동으로 탐지가 가능할 것으로 판단된다. Most of the noise generated by humans in the ocean is ship radiated noise caused to fishing and commercial shipping. Recently, deep learning technology has been used to detect shipping noise. In this study, the convolutional neural network is trained by a shipping noise spectrogram divided into 1-minute units to detect a near distance ship. Inception-V3, ResNet-50, VGG-16 and the proposed model were used to learn and evaluate 1-minute shipping noise. As a result, the F1 scores were 97.42%, 98.42%, 98.16% and 97.88% for Inception-V3, ResNet-50, VGG-16 and the proposed model, respectively. These models showed satisfactory performance in detecting shipping noise. It was confirmed that the proposed model showed equivalent detection performance with about 1/8 parameters compared to ResNet-50. For future works, it is expected that it will be possible to detect long-distance shipping noise by using additional noise data and AIS(Automatic Identification System).\n",
      "딥러닝을 이용한 의류 이미지의 텍스타일 소재 분류 2023 ['Clothing Textile', 'Material Classification', 'Image Deep Learning', 'ResNet', 'Vision Transformer', '의류 텍스타일', '소재 분류', '이미지 딥러닝', 'ResNet', 'Vision Transformer'] None As online transactions increase, the image of clothing has a great influence on consumer purchasing decisions. The importance of image information for clothing materials has been emphasized, and it is important for the fashion industry to analyze clothing images and grasp the materials used. Textile materials used for clothing are difficult to identify with the naked eye, and much time and cost are consumed in sorting. This study aims to classify the materials of textiles from clothing images based on deep learning algorithms. Classifying materials can help reduce clothing production costs, increase the efficiency of the manufacturing process, and contribute to the service of recommending products of specific materials to consumers. We used machine vision-based deep learning algorithms ResNet and Vision Transformer to classify clothing images. A total of 760,949 images were collected and preprocessed to detect abnormal images. Finally, a total of 167,299 clothing images, 19 textile labels and 20 fabric labels were used. We used ResNet and Vision Transformer to classify clothing materials and compared the performance of the algorithms with the Top-k Accuracy Score metric. As a result of comparing the performance, the Vision Transformer algorithm outperforms ResNet.\n",
      "템플릿 매칭 및 딥러닝 모델을 이용한 공정 결함 탐지 방법 2023 ['결함 탐지', '템플릿 매칭', '딥러닝', '공장 자동화', 'ResNet34', 'Defect detection', 'Template Matching', 'Deep learning', 'Factory automation', 'ResNet34'] None None\n",
      "전이학습을 이용한 UNet 기반 건물 추출 딥러닝 모델의 학습률에 따른 성능 향상 분석 2023 ['Semantic building segmentation', 'UNet', 'VGG19', 'ResNet50', '의미론적 영상 분할'] None In recent times, semantic image segmentation methods using deep learning models have been widely used for monitoring changes in surface attributes using remote sensing imagery. To enhance the performance of various UNet-based deep learning models, including the prominent UNet model, it is imperative to have a sufficiently large training dataset. However, enlarging the training dataset not only escalates the hardware requirements for processing but also significantly increases the time required for training. To address these issues, transfer learning is used as an effective approach, enabling performance improvement of models even in the absence of massive training datasets. In this paper we present three transfer learning models, UNet-ResNet50, UNet-VGG19, and CBAM-DRUNet-VGG19, which are combined with the representative pretrained models of VGG19 model and ResNet50 model. We applied these models to building extraction tasks and analyzed the accuracy improvements resulting from the application of transfer learning. Considering the substantial impact of learning rate on the performance of deep learning models, we also analyzed performance variations of each model based on different learning rate settings. We employed three datasets, namely Kompsat-3A dataset, WHU dataset, and INRIA dataset for evaluating the performance of building extraction results. The average accuracy improvements for the three dataset types, in comparison to the UNet model, were 5.1% for the UNet-ResNet50 model, while both UNet-VGG19 and CBAM-DRUNet-VGG19 models achieved a 7.2% improvement.\n",
      "백본 네트워크에 따른 사람 속성 검출 모델의 성능 변화 분석 2023 ['Pedestrian attribute recognition', 'Deep neural networks', 'Backbone networks', 'Resnet', 'Swin'] None Recently, with the development of deep learning technology, research on pedestrian attribute recognition technology using deep neural networks has been actively conducted. Existing pedestrian attribute recognition techniques can be obtained in such a way as global-based, regional-area-based, visual attention-based, sequential prediction-based, and newly designed loss function-based, depending on how pedestrian attributes are detected. It is known that the performance of these pedestrian attribute recognition technologies varies greatly depending on the type of backbone network that constitutes the deep neural networks model. Therefore, in this paper, several backbone networks are applied to the baseline pedestrian attribute recognition model and the performance changes of the model are analyzed. In this paper, the analysis is conducted using Resnet34, Resnet50, Resnet101, Swin-tiny, and Swinv2-tiny, which are representative backbone networks used in the fields of image classification, object detection, etc. Furthermore, this paper analyzes the change in time complexity when inferencing each backbone network using a CPU and a GPU.\n",
      "명함 이미지 회전 판단을 위한 딥러닝 모델 비교 2023 ['명함', '이미지 회전', '인공 신경망', '스마트 프린팅 시스템', 'business cards', 'image rotation', 'artificial neural networks', 'smart printing system'] 고객이 온라인으로 요청한 명함을 자동으로 명함을 인쇄하는 스마트 명함 인쇄 시스템이 활성화되고 있다. 이때, 문제는 고객이 시스템에 제출한 명함이 비정상일 수 있다는 것이다. 본 논문에서는 인공 지능 기술을 도입하여 명함의 이미지가 비정상적으로 회전됐는지 여부를 판정하는 문제를 다룬다. 명함은 0도, 90도, 180도, 270도 회전한다고 가정하였다. 특별한 인공신경망을 설계하지 않고 기존의 VGG, ResNet, DenseNet 인공신경망을 적용하여 실험하였는데 모든 신경망이 97% 정도의 정확도로 이미지 회전을 분별할 수 있었다. DenseNet161은 97.9%의 정확도를 보였고 ResNet34도 97.2%의 정밀도를 보였다. 이는 문제가 단순할 경우, 복잡한 인공신경망이 아니어도 충분히 좋은 결과를 낼 수 있음을 시사한다. A smart business card printing system that automatically prints business cards requested by customers online is being activated. What matters is that the business card submitted by the customer to the system may be abnormal. This paper deals with the problem of determining whether the image of a business card has been abnormally rotated by adopting artificial intelligence technology. It is assumed that the business card rotates 0 degrees, 90 degrees, 180 degrees, and 270 degrees. Experiments were conducted by applying existing VGG, ResNet, and DenseNet artificial neural networks without designing special artificial neural networks, and they were able to distinguish image rotation with an accuracy of about 97%. DenseNet161 showed 97.9% accuracy and ResNet34 also showed 97.2% precision. This illustrates that if the problem is simple, it can produce sufficiently good results even if the neural network is not a complex one.\n",
      "MEMS 라이다 센서를 활용한 심층학습 기반 조적벽체 결함 인식 기술 2023 ['Deep Learning', 'MEMS LiDAR', '3D Laser Scanner', 'Masonry Wall', 'Defect Classification', '심층학습', 'MEMS 라이다', '3D 레이저 스캐너', '조적벽체', '결함 인식'] 건축물의 유지관리 및 안전점검은 대부분 점검자의 육안으로 진행하여 많은 시간과 인력이 소모된다는 문제점이 있다. 이를 보완하기 위해 영상처리기술 및 인공지능을 활용한 결함 인식 기술 개발이 활발하게 진행되고 있다. 하지만 기존의 영상처리 기법은 카메라를 통해 얻은 이미지를 분석하는 방식으로 주변 환경에 따라 성능이 변하는 한계가 있다. 최근 이를 해결하기 위해 3D 레이저 스캐닝 센서를 이용한 결함인식 방법을 개발하였으나 장치의 가격이 비싸 활발한 활용이 어렵다는 단점이 있다. 이에 본 연구는 기존 스캐닝 장치보다 가격이 저렴하고 신뢰할만한 성능을 보이고 있는 MEMS 라이다 센서를 이용해 조적벽체의 결함을 인식할 수 있는 기술을 개발하였다. 해당 연구는 조적벽체를 대상으로 하였으며, 실험실 환경에서 여러 종류의 결함을 가진 시험체를 제작하여 데이터를 획득하였다. 조적벽체 결한 인식 방법으로 인공지능을 활용한 연구에서 많이 사용하고 있는 ResNet-50과 VGG16 모델을 사용하여 결함을 인식하였으며, 성능평가 결과 ResNet-50은 98.75%, VGG16은 96.88%의 정확도를 보여주었다. 해당 연구 결과는 모바일 3D 레이저 스캐닝 장치와 결합하여 조적벽체의 실시간 결함 인식 기술 개발에 활용될 수 있을 것으로 판단된다. Most of the maintenance and safety inspections of buildings are performed with visual assessment of the inspector, which consumes a lot oftime and cost. With the development of computer vision and digital technologies such as 3D Laser scanners, automatic defect recognitionusing image processing and artificial intelligence has been widely studied. Current approach is largely relying on the image obtained from thecamera and the recognition performance could be varied depending on the surrounding environment. Recently, studies using 3D Laser scannerare being conducted to solve these problems. However, terrestrial laser scanners are expensive, so it is difficult to apply at the constructionsite. Therefore, this study proposed a method that can recognize masonry wall defects using a Microelectromechanical systems based LightDetection and Ranging sensor that having much lower price and reliable performance. This study was performed using masonry wallstructures and data were collected from samples having various types of defects in a laboratory environment. Masonry wall defects wererecognized using ResNet-50 and VGG16 models, which are widely used in previous studies. As a result of the classification, ResNet-50 andVGG16 achieved 98.75% and 96.88% accuracy, respectively. The results of this study can be utilized in the development of real-time defectrecognition method for a masonry wall at construction sites.\n",
      "AI-Based Vehicle Damage Repair Price Estimation System 2023 ['차량 손상', '객체 탐지', '차량 제작 및 모델 분류', '이미지 분류', 'Vehicle Damage', 'Object Detection', 'Vehicle Make and Model Classification', 'ResNet50', 'Image Classification'] None Artificial intelligence-based estimation of repair costs for damaged vehicles is an emerging field that relies on artificial intelligence and computer vision systems to automatically generate accurate cost estimates. This area of research is becoming increasingly important owing to its potential to streamline the automotive repair industry, enhance overall transparency, improve the accuracy of cost estimation, and expedite insurance claims processing. This paper proposes the identification of the make and model of a vehicle, classification of the damaged vehicle type, and estimation of repair costs based on prices from various vehicle manufacturers. The proposed method for achieving state-of-the-art performance and time-saving in this system is through the use of ResNet50 and transfer learning. We propose a vehicle make and model classification module as well as a damaged vehicle classification module based on ResNet50 and transfer learning to improve the accuracy of the results. The accuracy of vehicle make and model classification module is 88%, which is approximately 11% higher than that of other studies. The accuracy of damaged vehicle classification module in this study is 86%, which is 67% higher than that of other studies.\n",
      "Transfer Learning for Effective Urolithiasis Detection 2023 ['Urolithiasis', 'Urinary Calculi', 'Deep learning', 'Machine learning', 'Artificial intelligence'] None Purpose: Urolithiasis is a common disease that can cause acute pain and complications. The objective of this study was to develop a deep learning model utilizing transfer learning for the rapid and accurate detection of urinary tract stones. By employing this method, we aim to improve the efficiency of medical staff and contribute to the progress of deep learning-based medical image diagnostic technology.Methods: The ResNet50 model was employed to develop feature extractors for detecting urinary tract stones. Transfer learning was applied by utilizing the weights of pretrained models as initial values, and the models were fine-tuned with the provided data. The model’s performance was evaluated using accuracy, precision-recall, and receiver operating characteristic curve metrics.Results: The ResNet-50-based deep learning model demonstrated high accuracy and sensitivity, outperforming traditional methods. Specifically, it enabled a rapid diagnosis of the presence or absence of urinary tract stones, thereby assisting doctors in their decision-making process.Conclusions: This research makes a meaningful contribution by accelerating the clinical implementation of urinary tract stone detection technology utilizing ResNet-50. The deep learning model can swiftly identify the presence or absence of urinary tract stones, thereby enhancing the efficiency of medical staff. We expect that this study will contribute to the advancement of medical imaging diagnostic technology based on deep learning.\n",
      "딥러닝 기반 토마토 성숙도 판별 시스템 2023 ['HSV', 'RGB', 'ResNet-50', 'Deep Learning', 'Ripe degree', 'Color', 'Noise cancellation'] None Currently, farmers are showing problems such as aging and rising labor costs, and the development of smart agriculture that combines cutting-edge technologies such as AI, video analysis, and big data is essential to solve these problems. In this study, a deep learning-based tomato maturity determination system was studied for the development of smart agriculture. In order to preprocess tomato images, colors were extracted for each image with RGB and HSV color models, and the extracted images were preprocessed into images that were easier to learn through noise removal using Gaussian filters and data normalization through Standard Scaler. The pretreated tomato image learned a tomato maturity discrimination image according to color using a deep learning model called ResNet-50 and a tomato maturity discrimination model was obtained. If a tomato maturity determination model is stored separately and then combined with a camera to take an image, the rating of the captured tomato can be immediately checked, and the captured image is also stored in the database for future model updates. It is expected to contribute to the great development of smart agriculture by using this tomato maturity determination system to prevent damage that may occur due to missed harvest time and to produce a system that can determine maturity by using the system.\n",
      "Localization of lung abnormalities on chest X-rays using self-supervised equivariant attention 2023 ['Self-supervised equivariant attention', 'ResNet50', 'Siamese network', 'Weak supervision', 'Pixel correlation module', 'Self-attention', 'CAM'] None Chest X-Ray (CXR) images provide most anatomical details and the abnormalities on a 2D plane. Therefore, a 2D view of the 3D anatomy is sometimes sufficient for the initial diagnosis. However, close to fourteen commonly occurring diseases are sometimes difficult to identify by visually inspecting the images. Therefore, there is a drift toward developing computer-aided assistive systems to help radiologists. This paper proposes a deep learning model for the classification and localization of chest diseases by using image-level annotations. The model consists of a modified Resnet50 backbone for extracting feature corpus from the images, a classifier, and a pixel correlation module (PCM). During PCM training, the network is a weight-shared siamese architecture where the first branch applies the affine transform to the image before feeding to the network, while the second applies the same transform to the network output. The method was evaluated on CXR from the clinical center in the ratio of 70:20 for training and testing. The model was developed and tested using the cloud computing platform Google Colaboratory (NVidia Tesla P100 GPU, 16 GB of RAM). A radiologist subjectively validated the results. Our model trained with the configurations mentioned in this paper outperformed benchmark results.\n",
      "딥러닝 기법을 이용한 농업용저수지 CCTV 영상 기반의 수위계측 방법 개발 2023 ['CCTV', 'deep  learning', 'ResNet-50', 'water  level'] None This study aimed to evaluate the performance of water level classification from CCTV images in agricultural facilities such as reservoirs. Recently, theCCTV system, widely used for facility monitor or disaster detection, can automatically detect and identify people and objects from the images bydeveloping new technologies such as a deep learning system. Accordingly, we applied the ResNet-50 deep learning system based on ConvolutionalNeural Network and analyzed the water level of the agricultural reservoir from CCTV images obtained from TOMS (Total Operation ManagementSystem) of the Korea Rural Community Corporation. As a result, the accuracy of water level detection was improved by excluding night and rainfallCCTV images and applying measures. For example, the error rate significantly decreased from 24.39 % to 1.43 % in the Bakseok reservoir. We believethat the utilization of CCTVs should be further improved when calculating the amount of water supply and establishing a supply plan according tothe integrated water management policy.\n",
      "CT 정도관리에서 ACR 팬텀을 이용한 딥러닝 모델 적용에 관한 연구 2023 ['딥러닝', '컴퓨터단층촬영', 'ACR 팬텀', '정도관리', 'Deep learning', 'ResNet18', 'Computed tomography', 'ACR phantom', 'Quality control'] None This study aimed to implement a deep learning model that can perform quantitative quality control through ACTS software used for quantitative evaluation of ACR phantom in CT quality control and evaluate its usefulness. By changing the scanning conditions, images of three modules of the ACR phantom's slice thickness (ST), low contrast resolution (LC), and high contrast resolution (HC) were obtained and classified as ACTS software. The deep learning model used ResNet18, implementing three models in which ST, HC, and LC were learned with epoch 50 and an integrated model in which three modules were learned with Epoch 10, 30, and 50 at once. The performance of each model was evaluated through Accuracy and Loss. When comparing and evaluating the accuracy and loss function values of the deep learning models by ST, LC, and HC modules, the Accuracy and Loss of the HC model were the best with 100% and 0.0081, and in the integrated model according to the Epoch value, Accuracy and Loss with epoch 50 were the best with 96.29% and 0.1856. This paper showed that quantitative quality control is possible through a deep learning model, and it can be used as a basis and evidence for applying deep learning to the CT quality control.\n",
      "시계열 이미지 데이터 기반 상품추천을 위한 CNN 모델 성능 비교 연구 2023 ['CNN-based product recommendation model', 'Time series image data', 'AlexNet', 'VGG16', 'ResNet50', 'MobileNet'] None In the modern world, advances in information technology have led to the expansion of e-commerce, making it important for automated recommendation systems to efficiently gather the flood of information and data to present consumers with their favorite products and services. Various techniques are used to improve the accuracy of product recommendation in existing e-commerce. Among them, there are chronic problems that use RNN, a multi classification-based product recommendation model. RNN is a deep learning model suitable for time series classification tasks, but it suffers from issues such as gradient vanishing and gradient exploding. To Compensate for these issues, CNN models are often used to effectively detect local patterns through kernels. In this study, we compare the performance of recommendation models based on an architecture that generates product recommendation models by training CNN models with time series data through three different imaging encodings: GAF, MTF and RP. In our experiments, we split the 540,000 published transaction dataset into train and test. The splitted data is constructed as time series data and zero-padded to equalize the size of the model’s input image. We train AlexNet, VGG16, ResNet50, and MobileNet models on images generated by the three imaging algorithms and compare their product recommendation accuracy with the performance of existing RNN recommendation models. We can see that the CNN models perform better than the LSTM. When imaged with the GAF algorithm and trained on the MobileNet model, the highest recommendation accuracy was achieved, and the learning time was also shortened, improving efficiency. Future research will include the advancement of imaging algorithms to improve the performance of product recommendation models and the development of CNN models optimized for time series image data.\n",
      "Classification of Short Circuit Marks in Electric Fire Case with Transfer Learning and Fine-Tuning the Convolutional Neural Network Models 2023 ['Electric fire', 'Short-circuit', 'Convolutional neural network', 'VGG16', 'VGG19', 'Resnet50', 'InceptionV3', 'Xception'] None One of the most essential substances for detecting electric fire is electric fire short-circuit marks. The traces of which can be found before and after the electric fire as the short circuit occurs. There are different kinds of electric fire short circuit marks, for instance, grounded, primary, and secondary molten marks these are categorized into the different types of short-circuit marks primary short circuit marks appear before the electric fire occurrence, and secondary short circuit marks appear after an electric fire to identify and classify them is crucial and time-consuming steps and procedures are needed for that purpose in this study we have used five convolutional neural network models such as VGG16, VGG19, Xception, InceptionV3, and Resnet50 to classify the short-circuit marks image data. Furthermore, according to our experiment on dataset among these five models, the best result was of VGG16 because the model performed well without any overfitting problems when we trained the sets of electric fire short circuit image data by applying the data augmentation, transfer learning, and fine-tuning techniques. The validation accuracy result of the VGG16 model at 50 epochs was 92.7% with a validation loss rate of 0.2.\n",
      "예술 작품 아티스트 분류의 정확도 향상을 위한 CNN 구조 최적화 2023 ['컴퓨터 비전', '합성곱 신경망', '예술 작품 아티스트 분류', '미세 조정', 'ResNet50', 'Computer Vision', 'CNN', 'Artwork Classification', 'Fine-tuning', 'ResNet50'] None Metaverse is a modern new technology that is advancing quickly. The goal of this study is to investigate this technique from the perspective of computer vision as well as general perspective. A thorough analysis of computer vision related Metaverse topics has been done in this study. Its history, method, architecture, benefits, and drawbacks are all covered. The Metaverse's future and the steps that must be taken to adapt to this technology are described. The concepts of Mixed Reality (MR), Augmented Reality (AR), Extended Reality (XR) and Virtual Reality (VR) are briefly discussed. The role of computer vision and its application, advantages and disadvantages and the future research areas are discussed.\n",
      "단락흔 및 열흔 판별을 위한 CNN 기반 알고리즘의 모델별 성능 비교 분석에 관한 연구 2023 ['1차 단락흔', '2차 단락흔', '열흔', 'Inception v3', 'Googlenet', 'Vgg16', 'Resnet50', 'Primary Arc-bead', 'Secondary Arc-bead', 'Molten mark Inception v3', 'Googlenet', 'Vgg16', 'Resnet50'] None None\n",
      "아크페이스에 지식 증류를 적용한 얼굴인식 2023 ['AI', 'Face Recognition', 'ArcFace', 'Knowledge Distillation', 'Deep Learning'] None This paper studied the model lightening and speed improvement of face recognition. To this end, we propose a method of training with MobileNetV2 through knowledge distillation as a student model and based on ArcFace and ResNet50 as a teacher model. ArcFace is face recognition model using Additive Angular Margin Loss. The main objective was to compare the performance of the student model against that of the teacher model in terms of similarity results. The findings indicated that the student model outperformed the teacher model in terms of similarity results. These results suggest that the MobileNetV2-based student model can achieve comparable face recognition performance to the more complex ResNet50-based teacher model, while being computationally more efficient.\n",
      "딥 러닝 분류 모델을 이용한 직하방과 경사각 영상 기반의 벼 출수기 판별 2023 ['Rice heading date', 'CNN', 'Deep learning', 'Classification model', 'Image processing', 'Paddy field monitoring'] 벼의 출수기를 추정하는 것은 농업생산성과 관련된 중요한 과정 중 하나이지만 세계적인 이상기후의 증가로 벼의 출수기를 추정하는 것이 어려워지고 있다. 본 연구에서는 CNN 분류모델을 사용하여 다양한 영상 데이터에서 벼의 출수기를 추정하려고 시도하였다. 드론과 타워형 영상관측장치 그리고 일반 RGB 카메라로 촬영된 직하방과 경사각 영상을 수집하였다. 수집 한 영상은 CNN 모델의 입력데이터로 사용하기 위해서 전처리를 진행하였고, 사용된 CNN 아키텍처는 이미지 분류 모델에서 일반적으로 사용되는 ResNet50, InceptionV3 그리고 VGG19 를 사용하였다. 각각의 아키텍처는 모델의 종류, 영상의 유형과 관계없이 0.98 이상의 정확도를 나타내었다. 또한 CNN 분류 모델이 영상의 어떤 특징을 보고 분류하였는지 시각적으로 확인하기 위해서 Grad-CAM 을 사용하였다. Grad-CAM 결과 CNN 분류 모델은 벼의 출수를 이삭의 형태에 높은 가중치를 두어 분류 하는 것을 확인하였다. 다음으로 작성된 모델이 실제 논 포장 모니터링 이미지에서 벼의 출수기를 정확하게 추정하는지 확인하였다. 각각 다른 지역 4 개의 벼 포장에서 벼의 출수 기를 약 하루정도의 차이로 추정하는 것을 확인하였다. 이 방법을 통해서 다양한 논 포장의 모니터링 이미지를 활용하여 자동적이고 정량적으로 벼의 출수기를 추정 할 수 있다고 판단된다. Estimating the rice heading date is one of the most crucial agricultural tasks related to productivity. However, due to abnormal climates around the world, it is becoming increasingly challenging to estimate the rice heading date. Therefore, a more objective classification method for estimating the rice heading date is needed than the existing methods. This study, we aimed to classify the rice heading stage from various images using a CNN classification model. We collected top-view images taken from a drone and a phenotyping tower, as well as slanted-view images captured with a RGB camera. The collected images underwent preprocessing to prepare them as input data for the CNN model. The CNN architectures employed were ResNet50, InceptionV3, and VGG19, which are commonly used in image classification models. The accuracy of the models all showed an accuracy of 0.98 or higher regardless of each architecture and type of image. We also used Grad-CAM to visually check which features of the image the model looked at and classified. Then verified our model accurately measure the rice heading date in paddy fields. The rice heading date was estimated to be approximately one day apart on average in the four paddy fields. This method suggests that the water head can be estimated automatically and quantitatively when estimating the rice heading date from various paddy field monitoring images.\n",
      "상추잎 너비와 길이 예측을 위한 합성곱 신경망 모델 비교 2023 ['data augmentation', 'lettuce imaging', 'plant growth', 'transfer learning', 'vertical farming', '데이터 증강', '상추이미지', '수직농장', '식물 생장', '전이학습'] 식물의 잎의 크기나 면적을 아는 것은 생장을 예측하고 실내농장의 생산성의 향상에 중요한 요소이다. 본 연구에서는 상추 잎 사진을 이용해 엽장과 엽폭을 예측할 수 있는 CNN기반모델을 연구하였다. 데이터의 한계와 과적합 문제를 극복하기 위해 콜백 함수를 적용하고, 모델의 일반화 능력을 향상시키기 위해 K겹 교차 검증을 사용했다. 또한 데이터 증강을 통한 학습데이터의 다양성을 높이기 위해 image generator를 사용하였다. 모델 성능을 비교하기 위해 VGG16, Resnet152, NASNetMobile 등 사전학습된 모델을 이용하였다. 그 결과너비 예측에서 R2 값 0.9436, RMSE 0.5659를 기록한 NASNetMobile이가장 높은 성능을 보였으며 길이 예측에서는 R2 값이 0.9537, RMSE가 0.8713로 나타났다. 최종 모델에는NASNetMobile 아키텍처, RMSprop 옵티마이저, MSE 손실 함수, ELU 활성화함수가 사용되었다. 모델의 학습 시간은Epoch당 평균 73분이 소요되었으며, 상추 잎 사진 한 장을 처리하는 데 평균 0.29초가 걸렸다. 본 연구는 실내 농장에서 식물의 엽장과 엽폭을 예측하는 CNN 기반 모델을 개발하였고이를 통해 단순한 이미지 촬영만으로도 식물의 생장 상태를신속하고 정확하게 평가할 수 있을 것으로 기대된다. 또한 그결과는 실시간 양액 조절 등의 적절한 농작업 조치를 하는데 활용됨으로써 농장의 생산성 향상과 자원 효율성을 향상시키는데 기여할 것이다. Determining the size or area of a plant's leaves is an important factor in predicting plant growth and improving the productivity of indoor farms. In this study, we developed a convolutional neural network (CNN)-based model to accurately predict the length and width of lettuce leaves using photographs of the leaves. A callback function was applied to overcome data limitations and overfitting problems, and K-fold cross-validation was used to improve the generalization ability of the model. In addition, ImageDataGenerator function was used to increase the diversity of training data through data augmentation. To compare model performance, we evaluated pre-trained models such as VGG16, Resnet152, and NASNetMobile. As a result, NASNetMobile showed the highest performance, especially in width prediction, with an R_squared value of 0.9436, and RMSE of 0.5659. In length prediction, the R_squared value was 0.9537, and RMSE of 0.8713. The optimized model adopted the NASNetMobile architecture, the RMSprop optimization tool, the MSE loss functions, and the ELU activation functions. The training time of the model averaged 73 minutes per Epoch, and it took the model an average of 0.29 seconds to process a single lettuce leaf photo. In this study, we developed a CNN-based model to predict the leaf length and leaf width of plants in indoor farms, which is expected to enable rapid and accurate assessment of plant growth status by simply taking images. It is also expected to contribute to increasing the productivity and resource efficiency of farms by taking appropriate agricultural measures such as adjusting nutrient solution in real time.\n",
      "컴퓨터 단층촬영 영상에서 3번 요추부 슬라이스 검출을 위한 최적화 기반 딥러닝 모델 2023 ['Medical image', 'Optimization', 'Computerized Tomography Data', 'Artificial intelligence'] 본 논문에서는 근감소증의 발병 여부와 정도를 확인하기 위해 3번 요추부 (L3) CT 영상을 검출하는 딥러닝 모델을 제안하는 것이다. 또한, CT 데이터 내에 L3 레벨과 L3 레벨이 아닌 부분의 데이터 불균형으로 인한 성능 저하의 문제점을 오버샘플링 비율과 클래스 가중치를 설계변수로 하는 최적화 기법을 제시하고자 한다. 모델 학습 및 검증을 위하여 강릉아산병원에 내원한 전립선암 환자 104명, 방광암 환자 46명의 총 150명의 전신 CT 영상이 활용되었다. 딥러닝 모델은 ResNet50을 활용하였으며, 최적화기법의 설계변수로는 모델 하이퍼파라미터 5종과 데이터 증강비율 및 클래스 가중치로 선정하였다. 제안하는 최적화 기반의 L3 레벨 추출 모델은 대조군 (하이퍼파라미터 5종만을 최적화한 모델)과 비교하여 중간 L3 오차가 약  1.0 슬라이스 감소한 것을 확인할 수 있었다. 본 연구결과를 통하여 정확한 L3 슬라이스 검출이 가능하며, 추가적으로 데이터 증강을 통한 오버 샘플링과 클래스 가중치 조절을 통해 데이터 불균형 문제를 효과적으로 해결할 수 있는 가능성을 제시할 수 있다. In this paper, we propose a deep learning model to detect lumbar 3 (L3) CT images to determine the occurrence and degree of sarcopenia. In addition, we would like to propose an optimization technique that uses oversampling ratio and class weight as design parameters to address the problem of performance degradation due to data imbalance between L3 level and non-L3 level portions of CT data. In order to train and test the model, a total of 150 whole-body CT images of 104 prostate cancer patients and 46 bladder cancer patients who visited Gangneung Asan Medical Center were used. The deep learning model used ResNet50, and the design parameters of the optimization technique were selected as six types of model hyperparameters, data augmentation ratio, and class weight. It was confirmed that the proposed optimization-based L3 level extraction model reduced the median L3 error by about 1.0 slices compared to the control model (a model that optimized only 5 types of hyperparameters). Through the results of this study, accurate L3 slice detection was possible, and additionally, we were able to present the possibility of effectively solving the data imbalance problem through oversampling through data augmentation and class weight adjustment.\n",
      "범용 AI 컴파일러의 비공개 NPU 코드생성을 위한 공통 인터페이스 설계 및 검증 2023 ['Nueral processing unit', 'Compiler', 'Deep learning'] 본 논문에서는 NPU(Neural Processing Units)의 제조사별 비공개 백엔드 컴파일러와 범용 AI 컴파일러를 연결할 수 있는 공통 인터페이스를 제안한다. 이를 통해 다양한 AI 모델에 대한 지원과 기업 자산 보호가 가능하다. 제안된 인터페이스는 ONNX 표준을 따르며, ETRI의 NEST-C 컴파일러와 오픈엣지의 ENLIGHT 컴파일러를 해당 인터페이스로 통합했다. 실험 결과, 통합된 컴파일러는 ENLIGHT만을 사용한 것과 Resnet50과 MobileNetV2 두 종류의 모델에 대해서 100% 결과가 일치했다. 따라서 제안된 인터페이스는 NPU 백엔드 컴파일러의 범용성을 향상하면서도 비공개성을 유지할 수 있는 유용한 방안을 제공한다. This paper proposes a common interface for connecting proprietary back-end compilers of NPU (Neural Processing Units) manufacturers with general-purpose AI compilers, enabling support for various AI models while protecting corporate assets. The proposed interface adheres to the ONNX standard and integrates ETRI's NEST-C compiler with OPENEDGES's ENLIGHT compiler through this interface. Experimental results showed that the integrated compiler achieved 100% result consistency for two types of models, ResNet50 and MobileNetV2, compared to using ENLIGHT alone. Therefore, the proposed interface offers a valuable solution for enhancing the versatility of NPU back-end compilers while maintaining their proprietary nature.\n",
      "X-ray 및 초음파 영상을 활용한 고관절 이형성증 진단을  위한 특징점 검출 딥러닝 모델 비교 연구 2023 ['영유아 고관절 이형성증', '초음파', 'X-ray', '딥러닝', '특징점 검', 'Developmental Dysplasia of Hip (DDH)', 'Ultrasound', 'X-ray', 'Deep-learning', 'Keypoint detection'] 고관절 이형성증(Developmental Dysplasia of Hip, DDH)은 영유아 성장기에 흔히 발생하는 병리학적 상태로, 영유아의 성장을 방해하고 잠재적인 합병증을 유발하는 원인 중 하나이며 이를 조기에 발견하고 치료하는 것은 매우 중요하다. 기존의 DDH 진단 방법으로는 촉진법과 X-ray 또는 초음파 영상 기반 고관절에서의 특징점 검출을 이용한 진단 방법이 있지만 특징점 검출 시 객관성과 생산성에 제한점이 존재한다. 본 연구에서는 X-ray 및 초음파 영상을이용한 딥러닝 모델 기반 특징점 검출 방법을 제시하고, 다양한 딥러닝 모델을 이용하여 특징점 검출의 성능을 비교분석하였다. 또한, 부족한 의료 데이터를 보완하는 방법인 다양한 데이터 증강 기법을 제시하고 비교 평가하였다. 본연구에서는 Residual Network 152(ResNet152) 및 Simple & Complex augmentation 기법을 적용하였을 때 가장 높은 특징점 검출 성능을 보여주었으며, X-ray 영상에서 평균 Object Keypoint Similarity(OKS)가 약 95.33 %, 초음파영상에서는 약 81.21 %로 각각 측정되었다. 이러한 결과는 고관절 초음파 및 X-ray 영상에서 딥러닝 모델을 적용함으로써 DDH 진단 시 특징점 검출에 관한 객관성과 생산성을 향상시킬 수 있음을 보여준다. Developmental Dysplasia of the Hip (DDH) is a pathological condition commonly occurring during the growth phase of infants. It acts as one of the factors that can disrupt an infant's growth and trigger potential complications. Therefore, it is critically important to detect and treat this condition early. The traditional diagnostic methods for DDH involve palpation techniques and diagnosis methods based on the detection of keypoints in the hip joint using X-ray or ultrasound imaging. However, there exist limitations in objectivity and productivity during keypoint detection in the hip joint. This study proposes a deep learning model-based keypoint detection method using X-ray and ultrasound imaging and analyzes the performance of keypoint detection using various deep learning models. Additionally, the study introduces and evaluates various data augmentation techniques to compensate the lack of medical data. This research demonstrated the highest keypoint detection performance when applying the residual network 152 (ResNet152) model with simple & complex augmentation techniques, with average Object Keypoint Similarity (OKS) of approximately 95.33 % and 81.21 % in X-ray and ultrasound images, respectively. These results demonstrate that the application of deep learning models to ultrasound and X-ray images to detect the keypoints in the hip joint could enhance the objectivity and productivity in DDH diagnosis.\n",
      "도로 침수 탐지를 위한 딥러닝 모델 구현 및 성능 비교 2023 ['road flooding prediction', 'road flooding detection', 'deep learning', 'pre-trained models', 'CNN network layer', '.'] None Existing road flooding systems using a single sensor give an alarm when the water level reaches a certain value, making it difficult to determine road flooding and take first action. Therefore, in this paper, 8 models based on CNN were implemented to develop a real-time road flooding system using CCTV, and their performance was compared through learning and verification. Each learning model was trained with a batch size of 16 and 120 epochs, and as a result of the experiment, the deep learning models showed an average accuracy of 90%. In particular, in terms of accuracy, the ShuffleNet V1, SqueezeNet, and ResNet-50 models performed best in order. However, for real-time road flood detection and prediction, an appropriate number of parameters and short inference time are required for each model. Assuming that each CCTV is analyzed once every 10 seconds, it was analyzed that the ResNet-50 model could accommodate up to 800 CCTVs.\n",
      "농업용 저수지 CCTV 영상자료 기반 수위 인식 모델 적용성 검토 2023 ['Machine learning', 'Water level recognition', 'Reservoir', 'CCTV', 'Image processing', '기계학습', '수위 인식', '저수지', 'CCTV', '이미지 처리'] 농업용 저수지는 농업용수 공급에 있어서 매우 중요한 생산기반시설로, 우리나라 농업용수의 60% 정도를 공급하고 있다. 다만, 여러 문제로 인해 농업용수의 효율적인 공급에 어려움이 발생하고 있으며, 효과적인 공급 및 관리 체계 구현을 위한 정확한 실시간 저수위 혹은 저수량 추정이 필요하다. 본 연구에서는 영상정보를 활용한 딥러닝 기반 농업용 저수지 수위 인식 모델을 제안하였다. 개발한 모델은 (1) CCTV 영상정보 자료 수집 및 분석, (2) U-Net 이미지 분할 방법을 통한 입력 자료 생성, 그리고 (3) CNN과 ResNet 모델을 통한 수위 인식 세 단계로 구성된다. 모델은 두 농업용 저수지(G저수지와 M저수지)의 영상자료와 저수위 시계열자료를 활용하여 구현하였다. 적용 결과 이미지 분할 모델의 성능은 매우 우수한 것으로 나타났으며, 수위 인식 모델의 경우 수위 분류 계급구간에 따라 성능이 상이한 것으로 나타났다. 특히 영상자료의 픽셀 변동이 클수록 정확도 80% 이상이 확보 가능한 것으로 확인되었으나, 그렇지 않은 경우, 정확도가 50% 수준인 것으로 나타났다. 본 연구에서 개발한 모델은 향후 이미지 자료가 추가로 확보될 경우, 그 활용도 및 정확도가 더 높아질 것으로 기대한다. The agricultural reservoir is a critical water supply system in South Korea, providing approximately 60% of the agricultural water demand. However, the reservoir faces several issues that jeopardize its efficient operation and management. To address this issues, we propose a novel deep-learning-based water level recognition model that uses CCTV image data to accurately estimate water levels in agricultural reservoirs. The model consists of three main parts: (1) dataset construction, (2) image segmentation using the U-Net algorithm, and (3) CCTV-based water level recognition using either CNN or ResNet. The model has been applied to two reservoirs G-reservoir and M-reservoir with observed CCTV image and water level time series data. The results show that the performance of the image segmentation model is superior, while the performance of the water level recognition model varies from 50 to 80% depending on water level classification criteria (i.e., classification guideline) and complexity of image data (i.e., variability of the image pixels). The performance of the model can be improved if more numbers of data can be collected.\n",
      "기계학습을 이용한 스마트 공장 자료의 불량 분류 모형 개발 2023 ['스마트 공장', '제조 데이터', '빅데이터', '기계학습', '딥러닝', 'Smart factory', 'Manufacturing data', 'Big data', 'Machine learning', 'Deep learning'] 정보기술의 발전으로 인해 현대사회의 다양한 분야에서 ICT 기술과의 융합이 가속화되면서 제조업 분야에서도 인공지능과 자동화 기술을 활용한 스마트 공장이 등장하였다. 스마트 공장은 실시간으로 자료를 수집하고 이를 분석하여 최적의 의사결정을 진행함으로써 생산 과정의 문제점을 개선하고 생산성과 효율성을 향상시키는 것을 목표로 한다. 본 연구에서는 스마트 공장에서 수집된 자료에 기계학습과 딥러닝 모형을 적용하여 제조공정의 생산성과 효율성을 향상시킬 수 있는 프레임워크를 구축하고자 한다. 먼저 생산 과정에서 발생하는 온도와 압력에 관련된 공정 환경 자료를 기계학습 방법인 로지스틱 회귀, 랜덤 포레스트, 그래디언트 부스티드 트리, 지지벡터기계를 사용하여 불량을 1차적으로 탐지한다. 다음으로 용접을 마치고 난 후 촬영된 제품의 용접 이미지 자료에 딥러닝 기법을 적용하여 불량을 탐지한다. 이를 위해 AlexNet, VGG-16, ResNet과 같은 합성곱 신경망 기반 모형을 사용하였다. 이후 각 자료에 대해 정확도, 정밀도, 재현율 등의 성능평가지표를 사용하여 구현된 모형들의 성능을 비교하고, 각 자료에 대해 가장 우수한 성능을 보이는 모형을 최종 모형으로 선택하였다. 공정 환경 및 이미지 자료에서 선택된 최적의 모형은 높은 정확도로 불량을 탐지해 낼 수 있었으며 이를 실제 제조공정에 적용하여 자동화된 불량 탐지 시스템을 구축한다면 공정의 생산성과 효율성을 크게 향상시킬 수 있을 것이라 기대된다. The rapid convergence of ICT (Information and Communication Technology) with various fields in modern society has led to the emergence of smart factories in the manufacturing industry. These factories leverage artificial intelligence and automation technology to enhance productivity and efficiency by collecting real-time data and making optimal decisions through analysis. In this study, we aimed to develop machine learning and deep learning models to improve manufacturing processes in smart factories. Firstly, we implemented a model using logistic regression, random forest, gradient boosted trees, and support vector machine to classify defects based on process environment data, including temperature and pressure. Next, we applied convolutional neural network models such as AlexNet, VGG-16, and ResNet to classify defective welding images captured after the welding process. We evaluated the performance of these models using metrics like accuracy, precision, and recall for each dataset and selected the top-performing model as the final choice.\n",
      "GPS 및 딥러닝을 이용한 스마트 시티 투어 모바일 애플리케이션 2023 ['스마트 투어', '위치 기반 서비스', '딥러닝', '모바일 애플리케이션', '이미지 분류', 'Smart tourism', 'Location-based services', 'Deep learning', 'Mobile applications', 'Image classification'] 본 논문에서는 GPS 및 딥러닝 기반의 스마트 시티 투어 모바일 애플리케이션을 제안한다. 제안된 애플리케이션은 딥러닝을 이용하여 랜드마크 이미지 인식 기능을 제공하고 위치기반 서비스를 통해 현재 사용자의 위치에 기반하여 주변 관광지 정보를 제공한다. 이미지 인식 서비스는 사용자가 애플리케이션에 랜드마크 이미지를 업로드하면 딥러닝 모델을 통해 이미지를 식별하고, 해당 랜드마크의 세부 정보를 제공한다. 랜드마크 이미지는 ResNet-D 모델을 사용하여 서울시에 있는 13개의 랜드마크에 대해 학습을 진행하여 최종 모델의 평균 분류 정확도는 약 0.957, 평균 F1-Score는 약 0.938로 좋은 성능을 얻었다. 또한 애플리케이션 내에서 위치기반 서비스를 통해 사용자의 현재 위치로부터 가까운 랜드마크의 정보를 알 수 있으며, 각 랜드마크의 정보를 다른 사용자에게 공유할 수 있다. 본 애플리케이션을 통해 여행 중이거나 여행을 계획 중인 외국인들에게 수도 서울에 대한 정보를 신속하게 제공하여 여행에 도움을 줄 수 있다. In this study, we propose a smart city tour mobile application based on GPS and deep learning. The proposed application provides the landmark image recognition service using deep learning and provides information on nearby tourist attractions based on the current user’s location through location-based services. The image recognition service identifies the image through a deep learning model when a user uploads a landmark image to the application and provides a detailed description of the landmark. We trained landmark images using the ResNet-D model on 13 landmarks in Seoul and achieved good performance of the final model with average classification accuracy and F1-score of around 0.957 and 0.938, respectively. In addition, location-based services within the application allow users to know information about landmarks close to their current location and share the information with other users. Foreigners who are traveling or planning to travel can easily obtain information about the capital Seoul with this application to aid their travel.\n",
      "Evaluation of Deep Learning Model for Scoliosis Pre-Screening Using Preprocessed Chest X-ray Images 2023 ['Scoliosis', 'Chest X-ray', 'Deep learning model', 'Preprocessed image', 'Data augmentation'] None Scoliosis is a three-dimensional deformation of the spine that is a deformity induced by physical or disease-related causes as the spine is rotated abnormally. Early detection has a significant influence on the possibility of nonsurgical treatment. To train a deep learning model with preprocessed images and to evaluate the results with and without data augmentation to enable the diagnosis of scoliosis based only on a chest X-ray image. The preprocessed images in which only the spine, rib contours, and some hard tissues were left from the original chest image, were used for learning along with the original images, and three CNN(Convolutional Neural Networks) models (VGG16, ResNet152, and EfficientNet) were selected to proceed with training. The results obtained by training with the preprocessed images showed a superior accuracy to those obtained by training with the original image. When the scoliosis image was added through data augmentation, the accuracy was further improved, ultimately achieving a classification accuracy of 93.56% with the ResNet152 model using test data. Through supplementation with future research, the method proposed herein is expected to allow the early diagnosis of scoliosis as well as cost reduction by reducing the burden of additional radiographic imaging for disease detection.\n",
      "준지도 학습과 전이 학습을 이용한 선로 체결 장치 결함 검출 2023 ['Fastener', 'Semi-supervised', 'Pretrained', 'Cost', '선로 체결 장치', '준지도 학습', '전이 학습', '비용'] 오늘날 인공지능 산업이 발전함에 따라 여러 분야에 걸쳐 인공지능을 통한 자동화 및 최적화가 이루어지고 있다. 국내의 철도 분야 또한 지도 학습을 이용한 레일의 결함을 검출하는 연구들을 확인할 수 있다. 그러나 철도에는 레일만이 아닌 다른 구조물들이 존재하며 그중 선로 체결 장치는 레일을 다른 구조물에 결합시켜주는 역할을 하는 장치로 안전사고의 예방을 위해서 주기적인 점검이 필요하다. 본 논문에는 선로 체결 장치의 데이터를 이용하여 준지도 학습(semi-supervised learning)과 전이 학습(transfer learning)을 이용한 분류기를 학습시켜 선로 안전 점검에 사용되는 비용을 줄이는 방안을 제안한다. 사용된 네트워크는 Resnet50이며 imagenet으로 선행 학습된 모델이다. 레이블이 없는 데이터에서 무작위로 데이터를 선정 후 레이블을 부여한 뒤 이를 통해 모델을 학습한다. 학습된 모델의 이용하여 남은 데이터를 예측 후 예측한 데이터 중 클래스 별 확률이 가장 높은 데이터를 정해진 크기만큼 훈련용 데이터에 추가하는 방식을 채택하였다. 추가적으로 초기의 레이블된 데이터의 크기가 끼치는 영향력을 확인해보기 위한 실험을 진행하였다. 실험 결과 최대 92%의 정확도를 얻을 수 있었으며 이는 지도 학습 대비 5% 내외의 성능 차이를 가진다. 이는 제안한 방안을 통해 추가적인 레이블링 과정 없이 비교적 적은 레이블을 이용하여 분류기의 성능을 기존보다 향상시킬 수 있을 것으로 예상된다. Recently, according to development of artificial intelligence, a wide range of industry being automatic and optimized. Also we can find out some research of using supervised learning for deteceting defect of railway in domestic rail industry. However, there are structures other than rails on the track, and the fastener is a device that binds the rail to other structures, and periodic inspections are required to prevent safety accidents. In this paper, we present a method of reducing cost for labeling using semi-supervised and transfer model trained on rail fastener data. We use Resnet50 as the backbone network pretrained on ImageNet. At first we randomly take training data from unlabeled data and then labeled that data to train model.  After predict unlabeled data by trained model, we adopted a method of adding the data with the highest probability for each class to the training data by a predetermined size.  Futhermore, we also conducted some experiments to investigate the influence of the number of initially labeled data. As a result of the experiment, model reaches 92% accuracy which has a performance difference of around 5% compared to supervised learning. This is expected to improve the performance of the classifier by using relatively few labels without additional labeling processes through the proposed method.\n",
      "그래프 트랜스포머 기반 농가 사과 품질 이미지의 그래프 표현 학습 연구 2023 ['딥 러닝', '그래프 표현 학습', '사과 품질 분류', '랜덤워크 위치 인코딩', 'Deep learning', 'Graph representation learning', 'Apple Quality Classification', 'Randomwalk positional encoding'] 최근 농가의 사과 품질 선별 작업에서 인적자원의 한계를 극복하기 위해 합성곱 신경망(CNN) 기반 시스템이 개발되고 있다. 그러나 합성곱 신경망은 동일한 크기의 이미지만을 입력받기 때문에 샘플링 등의 전처리 과정이 요구될 수 있으며, 과도 샘플링의 경우 화질 저하, 블러링 등 원본 이미지의 정보손실 문제가 발생한다. 본 논문에서는 위 문제를 최소화하기 위하여, 원본 이미지의 패치 기반 그래프를 생성하고 그래프 트랜스포머 모델의 랜덤워크 기반 위치 인코딩 방법을 제안한다. 위 방법은 랜덤워크 알고리즘 기반 위치정보가 없는 패치들의 위치 임베딩 정보를 지속적으로 학습하고, 기존 그래프 트랜스포머의 자가 주의집중 기법을 통해 유익한 노드정보들을 집계함으로써 최적의 그래프 구조를 찾는다. 따라서 무작위 노드 순서의 새로운 그래프 구조와 이미지의 객체 위치에 따른 임의의 그래프 구조에서도 강건한 성질을 가지며, 좋은 성능을 보여준다. 5가지 사과 품질 데이터셋으로 실험하였을 때, 다른 GNN 모델보다 최소 1.3%에서 최대 4.7%의 학습 정확도가 높았으며, ResNet18 모델의 23.52M보다 약 15% 적은 3.59M의 파라미터 수를 보유하여 연산량 절감에 따른 빠른 추론 속도를 보이며 그 효과를 증명한다. Recently, a convolutional neural network (CNN) based system is being developed to overcome the limitations of human resources in the apple quality classification of farmhouse. However, since convolutional neural networks receive only images of the same size, preprocessing such as sampling may be required, and in the case of oversampling, information loss of the original image such as image quality degradation and blurring occurs. In this paper, in order to minimize the above problem, to generate a image patch based graph of an original image and propose a random walk-based positional encoding method to apply the graph transformer model. The above method continuously learns the position embedding information of patches which don`t have a positional information based on the random walk algorithm, and finds the optimal graph structure by aggregating useful node information through the self-attention technique of graph transformer model. Therefore, it is robust and shows good performance even in a new graph structure of random node order and an arbitrary graph structure according to the location of an object in an image. As a result, when experimented with 5 apple quality datasets, the learning accuracy was higher than other GNN models by a minimum of 1.3% to a maximum of 4.7%, and the number of parameters was 3.59M, which was about 15% less than the 23.52M of the ResNet18 model. Therefore, it shows fast reasoning speed according to the reduction of the amount of computation and proves the effect.\n",
      "초소형 IoT 장치에 구현 가능한 딥러닝 양자화 기술 분석 2023 ['Internet of Things', 'Deep Learning', 'Quantization', 'Model Training', 'Experimental Configuration', '사물인터넷', '딥러닝', '양자화', '모델 훈련', '실험 구성'] 많은 연산량을 가진 딥러닝은 초소형 IoT 장치나 모바일 장치에 구현하기가 어렵다. 최근에는 이러한 장치에서도 딥러닝을 구현할 수 있도록 모델의 연산량을 줄이는 딥러닝 경량화 기술이 소개되었다. 양자화는 연속적인 분포를가지는 파라미터 값들을 고정된 비트의 이산 값으로 표현하여 모델의 메모리 및 크기 등을 줄여 효율적으로 사용할수 있는 경량화 기법이다. 그러나 양자화로 인한 이산 값 표현으로 인해 모델의 정확도가 낮아지게 된다. 본 논문에서는정확도를 개선할 수 있는 다양한 양자화 기술을 소개한다. 먼저 기존 양자화 기술 중 APoT와 EWGS를 선택하여 동일한 환경에서 실험을 통해 결과를 비교 분석하였다. 선택된 기술은 ResNet모델에서 CIFAR-10 또는 CIFAR-100 데이터 세트로 훈련되고 테스트 되었다. 실험 결과 분석을 통해 기존 양자화 기술의 문제점을 파악하고 향후 연구에 대한방향성을 제시하였다. Deep learning with large amount of computations is difficult to implement on micro-sized IoT devices or moblie devices. Recently, lightweight deep learning technologies have been introduced to make sure that deep learning can be implemented even on small devices by reducing the amount of computation of the model. Quantization is one of lightweight techniques that can be efficiently used to reduce the memory and size of the model by expressing parameter  values  with  continuous distribution as discrete values of fixed bits. However, the accuracy of the model is reduced due to discrete  value  representation  in  quantization.  In  this  paper,  we  introduce  various  quantization techniques to correct the accuracy. We selected APoT and EWGS from existing quantization techniques, and comparatively analyzed the results through experimentations The selected techniques were trained and tested with CIFAR-10 or CIFAR-100 datasets in the ResNet model. We found out problems with them through experimental results analysis and presented directions for future research.\n",
      "Dog-Species Classification through CycleGAN and Standard Data Augmentation 2023 ['CycleGAN', 'Data Augmentation', 'DNN', 'GAN', 'Image Classification'] None In the image field, data augmentation refers to increasing the amount of data through an editing method suchas rotating or cropping a photo. In this study, a generative adversarial network (GAN) image was created usingCycleGAN, and various colors of dogs were reflected through data augmentation. In particular, dog data fromthe Stanford Dogs Dataset and Oxford-IIIT Pet Dataset were used, and 10 breeds of dog, corresponding to 300images each, were selected. Subsequently, a GAN image was generated using CycleGAN, and four learninggroups were established: 2,000 original photos (group I); 2,000 original photos + 1,000 GAN images (groupII); 3,000 original photos (group III); and 3,000 original photos + 1,000 GAN images (group IV). The amountof data in each learning group was augmented using existing data augmentation methods such as rotating,cropping, erasing, and distorting. The augmented photo data were used to train the MobileNet_v3_Large,ResNet-152, InceptionResNet_v2, and NASNet_Large frameworks to evaluate the classification accuracy andloss. The top-3 accuracy for each deep neural network model was as follows: MobileNet_v3_Large of 86.4%(group I), 85.4% (group II), 90.4% (group III), and 89.2% (group IV); ResNet-152 of 82.4% (group I), 83.7%(group II), 84.7% (group III), and 84.9% (group IV); InceptionResNet_v2 of 90.7% (group I), 88.4% (groupII), 93.3% (group III), and 93.1% (group IV); and NASNet_Large of 85% (group I), 88.1% (group II), 91.8%(group III), and 92% (group IV). The InceptionResNet_v2 model exhibited the highest image classificationaccuracy, and the NASNet_Large model exhibited the highest increase in the accuracy owing to dataaugmentation.\n",
      "Biometric identification of Black Bengal goat: unique iris pattern matching system vs deep learning approach 2023 ['Biometric Identification', 'Black Bengal Goat', 'Deep Learning', 'Goat Identification', 'Iris Image', 'Iris Pattern Matching'] None Objective: Iris pattern recognition system is well developed and practiced in human, however, there is a scarcity of information on application of iris recognition system in animals at the field conditions where the major challenge is to capture a high-quality iris image from a constantly moving non-cooperative animal even when restrained properly. The aim of the study was to validate and identify Black Bengal goat biometrically to improve animal management in its traceability system.Methods: Forty-nine healthy, disease free, 3 months±6 days old female Black Bengal goats were randomly selected at the farmer’s field. Eye images were captured from the left eye of an individual goat at 3, 6, 9, and 12 months of age using a specialized camera made for human iris scanning. iGoat software was used for matching the same individual goats at 3, 6, 9, and 12 months of ages. Resnet152V2 deep learning algorithm was further applied on same image sets to predict matching percentages using only captured eye images without extracting their iris features.Results: The matching threshold computed within and between goats was 55%. The accuracies of template matching of goats at 3, 6, 9, and 12 months of ages were recorded as 81.63%, 90.24%, 44.44%, and 16.66%, respectively. As the accuracies of matching the goats at 9 and 12 months of ages were low and below the minimum threshold matching percentage, this process of iris pattern matching was not acceptable. The validation accuracies of resnet152V2 deep learning model were found 82.49%, 92.68%, 77.17%, and 87.76% for identification of goat at 3, 6, 9, and 12 months of ages, respectively after training the model.Conclusion: This study strongly supported that deep learning method using eye images could be used as a signature for biometric identification of an individual goat.\n",
      "PartitionTuner: An operator scheduler for deep-learning compilers supporting multiple heterogeneous processing units 2023 ['deep neural network', 'deep-learning compiler', 'parallel processing', 'partitioning'] None Recently, embedded systems, such as mobile platforms, have multiple processing units that can operate in parallel, such as centralized processing units (CPUs) and neural processing units (NPUs). We can use deep-learning compilers to generate machine code optimized for these embedded systems from a deep neural network (DNN). However, the deep-learning compilers proposed so far generate codes that sequentially execute DNN operators on a single processing unit or parallel codes for graphic processing units (GPUs). In this study, we propose PartitionTuner, an operator scheduler for deep-learning compilers that supports multiple heterogeneous PUs including CPUs and NPUs. PartitionTuner can generate an operator-scheduling plan that uses all available PUs simultaneously to minimize overall DNN inference time. Operator scheduling is based on the analysis of DNN architecture and the performance profiles of individual and group operators measured on heterogeneous processing units. By the experiments for seven DNNs, PartitionTuner generates scheduling plans that perform 5.03% better than a static type-based operator-scheduling technique for SqueezeNet. In addition, PartitionTuner outperforms recent profiling-based operator-scheduling techniques for ResNet50, ResNet18, and SqueezeNet by 7.18%, 5.36%, and 2.73%, respectively.\n",
      "CT 정도관리를  위한  인공지능  모델  적용에  관한  연구 2023 ['CT', 'Artificial intelligence', 'Quality control', 'AAPM CT phantom', 'Quantitative evaluation'] None CT is a medical device that acquires medical images based on Attenuation coefficient of human organs related to X-rays. In addition, using this theory, it can acquire sagittal and coronal planes and 3D images of the human body. Then, CT is essential device for universal diagnostic test. But Exposure of CT scan is so high that it is regulated and managed with special medical equipment. As the special medical equipment, CT must implement quality control.In detail of quality control, Spatial resolution of existing phantom imaging tests, Contrast resolution and clinical image evaluation are qualitative tests. These tests are not objective, so the reliability of the CT undermine trust. Therefore, by applying an artificial intelligence classification model, we wanted to confirm the possibility of quantitative eval- uation of the qualitative evaluation part of the phantom test. We used intelligence classification models (VGG19, DenseNet201, EfficientNet B2, inception_resnet_v2, ResNet50V2, and Xception). And the fine-tuning process used for learning was additionally performed. As a result, in all classification models, the accuracy of spatial resolution was 0.9562 or higher, the precision was 0.9535, the recall was 1, the loss value was 0.1774, and the learning time was from a maximum of 14 minutes to a minimum of 8 minutes and 10 seconds. Through the experimental results, it was concluded that the artificial intelligence model can be applied to CT implements quality control in spatial res- olution and contrast resolution.\n",
      "포장층 이상구간에서 획득한 열화상 이미지 해석을 위한 CNN 알고리즘의 적용성 평가 2023 ['CNN', 'Infrared camera', 'Res Net 101', 'Squeeze Net'] None The presence of abnormalities in the subgrade of roads poses safety risks to users and results in significant maintenance costs. In this study, we aimed to experimentally evaluate the temperature distributions in abnormal areas of subgrade materials using infrared cameras and analyze the data with machine learning techniques. The experimental site was configured as a cubic shape measuring 50 cm in width, length, and depth, with abnormal areas designated for water and air. Concrete blocks covered the upper part of the site to simulate the pavement layer. Temperature distribution was monitored over 23 h, from 4 PM to 3 PM the following day, resulting in image data and numerical temperature values extracted from the middle of the abnormal area. The temperature difference between the maximum and minimum values measured 34.8°C for water, 34.2°C for air, and 28.6°C for the original subgrade. To classify conditions in the measured images, we employed the image analysis method of a convolutional neural network (CNN), utilizing ResNet-101 and SqueezeNet networks. The classification accuracies of ResNet-101 for water, air, and the original subgrade were 70%, 50%, and 80%, respectively. SqueezeNet achieved classification accuracies of 60% for water, 30% for air, and 70% for the original subgrade. This study highlights the effectiveness of CNN algorithms in analyzing subgrade properties and predicting subsurface conditions.\n",
      "과수화상병 판별을 위한 AI 모델 신뢰성 평가 연구 2023 ['Trustworthiness', 'Grad-CAM', 'XAI', 'ResNet50V2', 'InceptionV3', 'Xception', 'Fireblight'] None None\n",
      "CNN 기술을 적용한 침수탐지 학습모델 개발 2023 ['Flooded Road', 'Deep Learning', 'Image Classification', 'CNN'] None This paper developed a training model to classify normal roads and flooded roads using artificial intelligence technology. We expanded the diversity of learning data using various data augmentation techniques and implemented a model that shows good performance in various environments. Transfer learning was performed using the CNN-based Resnet152v2 model as a pre-learning model. During the model learning process, the performance of the final model was improved through various parameter tuning and optimization processes. Learning was implemented in Python using Google Colab NVIDIA Tesla T4 GPU, and the test results showed that flooding situations were detected with very high accuracy in the test dataset.\n",
      "Identification of seed coat sculptures using deep learning 2023 ['Allium seed coat deep learning image recognition seed coat sculpture'] None Seed coat sculptures, including anticlinal and periclinal walls, are of great taxonomic importance. In this study, we identified seed coat patterns of Allium seeds using five deep learning methods namely, CNN, AlexNet, GoogleNet, ResNet50, and VGG16 for the first time. Selected images of seed coat patterns from over 100 Allium species reported in previously published literature and data from our samples were classified into seven types of anticlinal (irregular curved, irregular curved to nearly straight, straight, S, U, UO, and omega) and five types of periclinal walls (granule, small verruca, large verruca, marginal verruca, and verrucate verruca). The results revealed that GoogleNet and VGG16 achieved the highest classifi cation accuracy of 90.4% for the anticlinal wall, and VGG16 achieved the highest classification accuracy of 98.1% for the periclinal wall. Moreover, more than three, four, and five methods were combined, and their performance was investigated. Combining more than three methods was the most advantageous. The models achieved a suitable anticlinal wall classification using GoogleNet and periclinal wall classification using VGG16. In conclusion, using the machine-based method, we studied the seed coat of species of Allium on our own samples to see if the results of the machine-based method match with the human based classification.\n",
      "A study on the effectiveness of intermediate features in deep learning on facial expression recognition 2023 ['Intermediate Feature', 'Artificial Intelligence', 'Facial Expression Recognition'] None The purpose of this study is to evaluate the impact of intermediate features on FER performance. To achieve this objective, intermediate features were extracted from the input images at specific layers (FM1~FM4) of the pre-trained network (Resnet-18). These extracted intermediate features and original images were used as inputs to the vision transformer (ViT), and the FER performance was compared. As a result, when using a single image as input, using intermediate features extracted from FM2 yielded the best performance (training accuracy: 94.35%, testing accuracy: 75.51%). When using the original image as input, the training accuracy was 91.32% and the testing accuracy was 74.68%. However, when combining the original image with intermediate features as input, the best FER performance was achieved by combining the original image with FM2, FM3, and FM4 (training accuracy: 97.88%, testing accuracy: 79.21%). These results imply that incorporating intermediate features alongside the original image can lead to superior performance. The findings can be referenced and utilized when designing the preprocessing stages of a deep learning model in FER. By considering the effectiveness of using intermediate features, practitioners can make informed decisions to enhance the performance of FER systems.\n",
      "딥러닝과 XGBoost를 이용한 뇌졸중전조증상진단 애플리케이션 2023 ['뇌졸중 진단', '입술 특징추출', '딥러닝', '의미론적 영상 분할', 'XGBoost', 'Stroke', 'Lips Feature Extraction', 'Deep Learning', 'Semantic Segmentation', 'XGBoost'] None In this paper, after segmenting the lip area in the facial image, using the featurepoints of the lip, it is determined whether there is a precursor symptom of stroke.UNet and FCN, which are semantic image segmentation models, were used forsegmentation of the lip region, and at this moment, VGGNet16, ResNet101, andDenseNet121 were used as backbone networks. As a result of the experiment, themIoU of UNet using DenseNet121 was the highest at 92.5%. And, as a result oflearning with XGBoost using the feature map of the lip area and diagnosing astroke, it showed 98.8% accuracy. As a result of comparison with the existingstroke diagnosis method, the accuracy improved by 7.74~10.8%.\n",
      "파편 탐지 성능 향상을 위한 딥러닝 초해상도화 효과 분석 2023 None None The Arena Fragmentation Test(AFT) is designed to analyze warhead performance by measuring fragmentation data. In order to evaluate the results of the AFT, a set of AFT images are captured by high-speed cameras. To detect objects in the AFT image set, ResNet-50 based Faster R-CNN is used as a detection model. However, because of the low resolution of the AFT image set, a detection model has shown low performance. To enhance the performance of the detection model, Super-resolution(SR) methods are used to increase the AFT image set resolution. To this end, The Bicubic method and three SR models: ZSSR, EDSR, and SwinIR are used. The use of SR images results in an increase in the performance of the detection model. While the increase in the number of pixels representing a fragment flame in the AFT images improves the Recall performance of the detection model, the number of pixels representing noise also increases, leading to a slight decreases in Precision performance. Consequently, the F1 score is increased by up to 9 %, demonstrating the effectiveness of SR in enhancing the performance of the detection model.\n",
      "전이 학습과 데이터 증강을 이용한 너구리와 라쿤 분류 2023 ['딥러닝', '머신러닝', '전이 학습', '데이터 증강', '동물 분류', 'Deep learning', 'Machine Learning', 'Transfer Learning', 'Data Augmentation', 'Animal Classification'] 최근 인간의 활동 범위가 증가함에 따라 외래종의 유입이 잦아지고 있고 환경에 적응하지 못해 유기된 외래종 중 2020년부터 유해 지정 동물로 지정된 라쿤이 문제가 되고 있다. 라쿤은 국내 토종 너구리와 크기나 생김새가 유사하여 일반적으로 포획하는데 있어서 구분이 필요하다. 이를 해결하기 위해서 이미지 분류에 특화된 CNN 딥러닝 모델인 VGG19, ResNet152V2, InceptionV3, InceptionResNet, NASNet을 사용한다. 학습에 사용할 파라미터는 많은 양의 데이터인 ImageNet으로 미리 학습된 파라미터를 전이 학습하여 이용한다. 너구리와 라쿤 데이터셋에서 동물의 외형적인 특징으로 분류하기 위해서 이미지를 회색조로 변환한 후 밝기를 정규화하였으며, 조정된 데이터셋에 충분한 학습을 위한 데이터를 만들기 위해 좌우 반전, 회전, 확대/축소, 이동을 이용하여 증강 기법을 적용하였다. 증강하지 않은 데이터셋은 FCL을 1층으로, 증강된 데이터셋은 4층 으로 구성하여 진행하였다. 여러 가지 증강된 데이터셋의 정확도를 비교한 결과, 증강을 많이 할수록 성능이 증가함을 확인하였다. None\n",
      "Self-Gated Rectified Linear Unit for Performance Improvement of Deep Neural Networks 2023 ['Image classificationActivation functionDeep neural networkAccuracyTime complexity'] None This technical paper proposes an activation function, self-gated rectified linear unit (SGReLU), to achieve high classification accuracy, low loss, and low computational time. Vanishing gradient problem, dying ReLU, noise vulnerability are also resolved in our proposed SGReLU function. SGReLU’s performance is evaluated on MNIST, Fashion-MNIST, and Imagenet datasets and compared with seven highly effective activation functions. We obtained that the proposed SGReLU outperformed other activation functions in most cases in VGG16, Inception v3, and ResNet50. In VGG16 and Inception v3, it achieved an accuracy of 90.87% and 95.01%, respectively, exceeding other functions with the second-fastest computing time in these networks.\n",
      "딥러닝 기반의 실시간 상품 진열 상황 추정 시스템 2023 ['딥러닝', '상품 전시', 'Deep Learning', 'Plan-o-gram', 'Product Display', 'YOLO'] None In this paper, we developed a store management system that detects the display status of products in real time and manages them to comply with the plan-o-gram using real-time product recognition and display status estimation technology based on deep learning. To achieve this, we input store shelf images, localize each product using the YOLOv8 deep learning model trained with the SKU110K data set, and generate an image feature vector using an em-bedder that fine-tunes the ResNet-50 model, combining the pre-registered reference image feature vector and Compare and identify and classify products. The plan-o-gram compliance control algorithm has been supplemented so that the plan-o-gram compliance status generated through sequence alignment of the modified Needleman-Wunsch(NW) algorithm can be utilized in actual store situations. Previously, there were many errors by judging the four states: exact match(MT), missing item(MI), added item in the correct position(ME), and added or misplaced item or empty space(NM). Accordingly, in this paper, we subdivide NM into 5 states (addition, deletion, location change, remote placement, and position shift), expand it to a total of 8 states, and present an algorithm that can handle various characteristic cases.\n",
      "CNN과 학습 가능한 상관필터를 결합한  객체 추적 알고리즘 2023 ['Computer Vision', 'Object Tracking', 'Correlation Filter', 'Convolution Neural Networks', 'Deep Learning'] None Object tracking is considered a challenging problem due to various environmental changes contained in the video sequence. Object tracking is the use of information given in the first frame to estimate the area and trajectory of a target object in a video sequence. In this paper, we propose an algorithm for object tracking using multi-scale feature maps of Resnet-50 and correlation filters. To accommodate changes in object scale, we create an appearance model using different sized feature maps extracted from each block in the network. In order to maintain the robustness of the appearance model, it is adaptively updated according to the peak value of the response map when occlusion occurs due to obstacles. Experiments result using the OTB2015 dataset showed that the proposed algorithm achieved competitive results on several image attribution such as low resolution, scale variation, occlusion and out of view.\n",
      "데이터 재사용을 지원하는 HLS 기반 효율적인 합성곱 가속기의 설계 2023 ['CNN acceleration', 'high-level synthesis', 'FPGA', 'data reuse', 'CNN 가속기', 'high-level synthesis', 'FPGA', '데이터 재사용'] 본 논문에서는 HLS(High Level Synthesis)를 사용하여 개발, 검증이 쉬우면서 확장 가능한 CNN(Convolution Neural Network) 가속기를 설계하였다. DRAM 접근량을 줄이기 위한 타일 버퍼와 2차원 PE 배열을 가지는 weight stationary 가속기 구조를 설계하고, HLS의 디렉티브을 통해 PE 및 태스크 병렬성을 효율적으로 활용하는 가속기를 구현하였다. 한편, DRAM 접근을 생략할 수 있는 경우, 동적으로 타일 버퍼에서 데이터 재사용할 수 있도록 HLS 라이브러리의 Stream(FIFO)을 이용하여 구현하였다. 이를 통해 32×32 PE에서 13.7% 가속하여, Xilinx Alveo U200 FPGA에서 ResNet50 추론을 49 ms까지 가속하였다. Vitis HLS를 통해 PE의 개수를 손쉽게 확장하여, 서버 수준인 64×64 PE까지 835 GOPS의 성능 및 35.3 GOPS/W의 전력효율을 보이는 것을 확인하였다. None\n",
      "A dual-experience pool deep reinforcement learning method and its application in fault diagnosis of rolling bearing with unbalanced data 2023 ['· Deep reinforcement learning · Dual-experience pool · Unbalanced data · Rolling bearing · Fault diagnosis'] None A dual-experience pool deep reinforcement learning (DEPDRL) model is proposed for rolling bearing fault diagnosis with unbalanced data. In this method, a dualexperience pool structure is designed to store the sample data of majority and minority classes.A parallel double residual network model is established to extract deep features of the majority and minority input samples, respectively. In the process of training, the proposed balanced cross-sampling technique is used to randomly select samples from dual-experience pool in a certain proportion to realize the training of a double residual network model. We show the effectiveness of our method on three standard data sets, and compared with Resnet18, DCNN, DQN and DQNimb methods, the results show that DEPDRL has the best performance. Finally, with wavelet time-frequency graph as input, DEPDRL is applied to rolling bearing fault diagnosis with unbalanced test data. The results show that on a variety of unbalanced data sets, both the diagnostic accuracy and the G-means value of the DEPDRL are more than 5 % higher than other algorithms, which fully indicates that the DEPDRL has a very high fault diagnosis ability of rolling bearing with unbalanced data.\n",
      "심층신경망을 이용한 스마트 양식장용 사료 공급 시점 감지 시스템 구현 2023 ['Smart fish farm', 'LabVIEW', 'Automatic detection', 'Deep neural network', 'Classification'] None In traditional fish farming way, the workers have to observe all of the pools every time and every day to feed at the right timing. This method causes tremendous stress on workers and wastes time. To solve this problem, we implemented an automatic detection system for feeding time using deep neural network. The detection system consists of two steps: classification of the presence or absence of feed and checking DO (Dissolved Oxygen) of the pool. For the classification, the pretrained ResNet18 model and transfer learning with custom dataset are used. DO is obtained from the DO sensor in the pool through HTTP in real time. For better accuracy, the next step, checking DO proceeds when the result of the classification is absence of feed several times in a row. DO is checked if it is higher than a DO reference value that is set by the workers. These actions are performed automatically in the UI programs developed with LabVIEW.\n",
      "SAM Optimizer를 통한 위내시경 이미지 분류 CADx의 성능 향상 연구 2023 ['CADx', 'Gastric Diagnosis', 'Classification', 'Convolution Neural Network', 'Deep learning', 'Vision Transformer'] None Gastric cancer has a high incidence in East Asians, and the risk increases over time. Often, gastric cancer presents no early symptoms, leading to missed treatments. Consequently, in Korea, support is provided to individuals over 40 years of age who undergo gastroscopy. However, as the number of gastroscopy patients increases, doctors' fatigue rises, becoming a factor that can lead to misdiagnosis. Therefore, this paper proposes a CADx (Computer-Aided Diagnosis) system for gastric lesion classification based on ConvNeXt and ViT (Vision Transformer), applying the SAM (Sharpness Aware Minimization) optimizer. ConvNeXt is a network that achieves high performance by incorporating techniques from Swin Transformer and the latest advancements, with ResNet-50 as the base model. ViT divides the image into smaller patches and uses these patches as input to the Transformer. This allows for learning relationships between patches and ultimately leads to image classification. To address the issue of limited data in medical images, the gastric abnormal dataset was augmented using the AutoAugment policy. The SAM Optimizer is an optimization technique that detects and minimizes the \"sharpness\" of the loss function that may occur during the deep learning model's learning process. Using this method, the sensitivity of classifying abnormal and normal gastroscopy images in ConvNeXt increased from 0.7167 to 0.9583 for the original dataset and from 0.7583 to 0.9833 for the augmented dataset. ViT exhibited a significant decrease from 0.9500 to 0.7750 in the original dataset but increased from 0.9500 to 0.9583 in the augmented dataset. This demonstrates that the SAM Optimizer can effectively enhance CADx performance.\n",
      "내시경 영상에서의 딥러닝 기반 상부 위장관 랜드마크 식별 2023 ['deep learning', 'artificial intelligence', 'endoscopic image'] None Accurate identification of landmarks is critical for effective diagnosis and treatment in endoscopy, particularly in the upper gastrointestinal tract. However, there are many similar structures inside the stomach, and it might be difficult to accurately locate landmarks in camera images because of other factors such as air bubbles and the narrow field of view of wired endoscopic images. This study presents a comparative analysis experiment conducted with a model that can identify anatomical landmarks of the upper gastrointestinal tract with high accuracy through small-scale data-based local augmentation. We used five classes captured by esophagogastroduodenoscopy criterion, preprocessed medical image data to address the class imbalance, and compared the accuracies of ResNet50, MobileNetV2, and DensNet265 models. We used a dataset comprising 2,546 images of patients who underwent upper gastrointestinal endoscopy at Yonsei Severance Hospital. We augmented 4,632 images and evenly distributed them across five classes. Our results indicate that this is the most accurate model for improving diagnosis and treatment in upper gastrointestinal endoscopy. The ReseNet50 model achieved the highest accuracy at 74.88%, followed by the MobileNetV2 model at 78.91% and DensNet265 at 84.70%.\n",
      "뇌졸중 동물 모델의 분할 및 분류를 위한 방사광 영상 딥러닝 모델 분석 2023 ['Stroke', 'Synchrotron Radiation Imaging', 'Biomedical', 'CNN', 'Deep Learning'] None Stroke causes muscle dysfunction in lower limb depending on the brain damage. Therefore, it is important to identify the degree of muscle damaged and perform rehabilitation training for appropriate time of treatment. However, there is a limitation in that analysis using existing imaging techniques and artificial intelligence cannot analyze disease mechanisms.This paper aims to develop an AI GUI system using SRI to acquire damaged muscle regions, segment them into fiber and space areas, and classify them. For the segmentation, Attention U-Net performed best accuracy 95.32%. For the classification, ResNet50 with Attention U-Net performed best accuracy 99.07%. As a result of this, we designated the best performing network as suitable for stroke animal models. As an auxiliary tool for diagnosing the degree of stroke muscle damage in clinical practice, we constructed a system to analyze the degree of stroke fiber distribution on SRI images using pixel intensity values to show the results. Through this study, it is system that uses deep learning in the stroke animal model can be applied as a basic study for objective muscle tissue evaluation.\n",
      "엑스선 이미지 기반 결핵 예측 시스템을 위한 데이터 증강 전처리 조합법 2023 ['딥러닝', '헬스케어', '데이터 증강', 'X-선 이미지', '웹 프레임워크', 'Deep Learning', 'Health Care', 'Data Augmentation', 'X-ray Image', 'Web Framework'] 최근 헬스케어 및 의료 분야에서 컨볼루션 신경망(CNN) 모델을 활용한 X-선 이미지 분석 연구가 활발히 이루어지고 있다. X 선은 폐렴, 결핵, 유방암 등 다양한 흉부 질환을 비침습적으로 검사하는 주요 도구로서, 비용 효율적이며 많은 검사가 가능하다는장점을 갖는다. 하지만 기존의 X-선 이미지 기반 질환 진단 인공지능 모델은 복잡한 아키텍처를 가지며 많은 파라미터와 대량의데이터가 필요한 한계를 가진다. 본 연구에서는 이러한 문제를 해결하기 위해 적은 양의 X-선 이미지 데이터로 높은 결핵 예측 정확도를 달성할 수 있는 효과적인 전처리 조합법을 탐색하고 제안한다. 본 연구에서는 대표적인 데이터 증강법인 Random Erase,Random Flip, Random Augmentation을 함께 사용하는 전처리 조합법을 제안하였으며, 이를 ResNet50 모델과 EfficientNet-b0 모델에 적용하여 각 평균 11%, 9%의 성능 향상을 확인할 수 있었다. 또한, 전처리 조합이 적용된 모델을 쉽게 사용할 수 있도록 웹 프레임워크를 개발하였다. 사용자는 웹 프레임워크를 통해 입력 이미지를 수정할 수 있으며, 인공지능 결핵 판별 결과를 얻을 수 있다. None\n",
      "딥러닝 기반 농경지 속성분류를 위한 TIF 이미지와 ECW 이미지 간 정확도 비교 연구 2023 ['AI', 'deep  learning', 'agricultural  land', 'FarmMap'] None In this study, We conduct a comparative study of deep learning-based classification of agricultural field attributes using Tagged Image File (TIF) andEnhanced Compression Wavelet (ECW) images. The goal is to interpret and classify the attributes of agricultural fields by analyzing the differencesbetween these two image formats. “FarmMap,” initiated by the Ministry of Agriculture, Food and Rural Affairs in 2014, serves as the first digital mapof agricultural land in South Korea. It comprises attributes such as paddy, field, orchard, agricultural facility and ginseng cultivation areas. For thepurpose of comparing deep learning-based agricultural attribute classification, we consider the location and class information of objects, as well as theattribute information of FarmMap. We utilize the ResNet-50 instance segmentation model, which is suitable for this task, to conduct simulatedexperiments. The comparison of agricultural attribute classification between the two images is measured in terms of accuracy. The experimental resultsindicate that the accuracy of TIF images is 90.44%, while that of ECW images is 91.72%. The ECW image model demonstrates approximately 1.28%higher accuracy. However, statistical validation, specifically Wilcoxon rank-sum tests, did not reveal a significant difference in accuracy between thetwo images.\n",
      "Deep Learning-Based Feature Extraction from Whole-Body PET/CT Employing Maximum Intensity Projection Images: Preliminary Results of Lung Cancer Data 2023 ['Deep learning · PET/CT · Maximum intensity projection · Convolutional neural network · FDG'] None Purpose Deep learning (DL) has been widely used in various medical imaging analyses. Because of the difficulty in processingvolume data, it is difficult to train a DL model as an end-to-end approach using PET volume as an input for variouspurposes including diagnostic classification. We suggest an approach employing two maximum intensity projection (MIP)images generated by whole-body FDG PET volume to employ pre-trained models based on 2-D images.Methods As a retrospective, proof-of-concept study, 562 [18F]FDG PET/CT images and clinicopathological factors oflung cancer patients were collected. MIP images of anterior and lateral views were used as inputs, and image features wereextracted by a pre-trained convolutional neural network (CNN) model, ResNet-50. The relationship between the imageswas depicted on a parametric 2-D axes map using t-distributed stochastic neighborhood embedding (t-SNE), with clinicopathologicalfactors.Results A DL-based feature map extracted by two MIP images was embedded by t-SNE. According to the visualizationof the t-SNE map, PET images were clustered by clinicopathological features. The representative difference between theclusters of PET patterns according to the posture of a patient was visually identified. This map showed a pattern of clusteringaccording to various clinicopathological factors including sex as well as tumor staging.Conclusion A 2-D image-based pre-trained model could extract image patterns of whole-body FDG PET volume by usinganterior and lateral views of MIP images bypassing the direct use of 3-D PET volume that requires large datasets andresources. We suggest that this approach could be implemented as a backbone model for various applications for wholebodyPET image analyses.\n",
      "인공지능 의료 영상인식 기술을 활용한 유방암 영상 진단 기법 연구 2023 ['유방암', '이미지 분류', '이미지 분할', '데이터 증강', 'Breast cancer', 'Image recognition', 'segmentation', 'Classification'] 본 논문은 인공지능 의료영상인식 기술을 활용하여 유방암 진단을 하는 기법에 대해 연구하였다. 이를 위해 맘모그래피 이미지, 초음파 이미지, 조직병리 이미지를 사용하여, 이미지 분류 기법과 이미지 분할 기법을 통해 유방암 분류와 해당 환부 위치를 추론하는 과정의 정확도 향상을 위한 전략들에 대해 연구하였다. 즉, 성능 최적화를 위해 여러 이미지 분류 기술 및 이미지 분할 기법들과 관련 손실함수들 중에 각 의료영상 데이터 별 최적의기술을 선별하였고, 해당 성능 최적화를 위한 데이터 증강 기법을 제시하였다. 제시된 방법들을 통해 분석한 결과, 필터 기반의 데이터 증강 기술을 활용하면 이미지 분류 기술에서는 ResNet50이 가장 좋은 성능을, 이미지 분할기술로는 맘모그래피 영상과 초음파 영상 모두 UNet 기술이 가장 좋은 성능을 나타내었다. 해당 기술을 적용한결과, 맘모그래피 영상에서의 이미지 분할 작업에서는 33.3%, 초음파 영상에서의 이미지 분할 작업에서는 29.9%, 조직병리 이미지에서의 이미지분류 작업에서는 22.8%의 성능 향상을 나타내었다. None\n",
      "A Taekwondo Poomsae Movement Classification Model Learned Under Various Conditions 2023 ['Pose Estimation', 'Machine Learning', 'Deep Learning', 'Convolutional Neural Network', 'Deep Convolutional Neural Network', '자세 분류', '머신러닝', '딥러닝', '합성곱 신경망', '심층 합성곱 신경망'] 태권도 겨루기의 전자호구, 축구의 VAR 등 스포츠에서 기술 발전이 고도화되고 있다. 하지만태권도 품새는 사람이 직접 자세를 눈으로 보고 판단하며 지도하기 때문에 때로는 대회의 현장에서 판정시비가 일어난다. 본 연구는 인공지능을 이용하여 태권도 동작을 더 정확하게 판단하고평가할 수 있는 인공지능 모델을 제안한다. 본 연구에서는 촬영 및 수집한 데이터를 전처리한 후학습, 테스트, 검증 세트로 분리한다. 분리한 데이터를 각 모델과 조건을 적용하여 학습한 후 비교하여 가장 좋은 성능의 모델을 제시한다. 각 조건의 모델은 정확도, Precision, Recall, F1-Score, 학습 소요 시간, Top-n error의 값을 비교하였고 그 결과 ResNet50과 Adam을 사용한 조건에서 학습한 모델의 성능이 가장 우수한 것으로 나타났다. 본 연구에서 제시한 모델을 활용하여 교육 현장이나 대회 등 다양한 방면에서 활용할 수 있을 것으로 기대한다. None\n",
      "뇌성마비 환자의 자세 불균형 탐지를 위한 스마트폰 동영상 기반 보행 분석 시스템 2023 ['Gait analysis', 'Postural imbalance detection', 'Human pose estimation', 'Tilt correction', 'Temporal smoothing'] None Gait analysis is an important tool in the clinical management of cerebral palsy, allowing for the assessment of condition severity, identification of potential gait abnormalities, planning and evaluation of interventions, and providing a baseline for future comparisons. However, traditional methods of gait analysis are costly and time-consuming, leading to a need for a more convenient and continuous method. This paper proposes a method for analyzing the posture of cerebral palsy patients using only smartphone videos and deep learning models, including a ResNet-based image tilt correction, AlphaPose for human pose estimation, and SmoothNet for temporal smoothing. The indicators employed in medical practice, such as the imbalance angles of shoulder and pelvis and the joint angles of spine-thighs, knees and ankles, were precisely examined. The proposed system surpassed pose estimation alone, reducing the mean absolute error for imbalance angles in frontal videos from 4.196° to 2.971° and for joint angles in sagittal videos from 5.889° to 5.442°.\n",
      "Automatically Diagnosing Skull Fractures Using an Object Detection Method and Deep Learning Algorithm in Plain Radiography Images 2023 ['Deep learning · Artificial intelligence · Radiography · Skull fractures · Traumatic brain injury'] None Objective : Deep learning is a machine learning approach based on artificial neural network training, and object detection algorithm using deep learning is used as the most powerful tool in image analysis. We analyzed and evaluated the diagnostic performance of a deep learning algorithm to identify skull fractures in plain radiographic images and investigated its clinical applicability.Methods : A total of 2026 plain radiographic images of the skull (fracture, 991; normal, 1035) were obtained from 741 patients. The RetinaNet architecture was used as a deep learning model. Precision, recall, and average precision were measured to evaluate the deep learning algorithm’s diagnostic performance.Results : In ResNet-152, the average precision for intersection over union (IOU) 0.1, 0.3, and 0.5, were 0.7240, 0.6698, and 0.3687, respectively. When the intersection over union (IOU) and confidence threshold were 0.1, the precision was 0.7292, and the recall was 0.7650. When the IOU threshold was 0.1, and the confidence threshold was 0.6, the true and false rates were 82.9% and 17.1%, respectively. There were significant differences in the true/false and false-positive/false-negative ratios between the anteriorposterior, towne, and both lateral views (p=0.032 and p=0.003). Objects detected in false positives had vascular grooves and suture lines. In false negatives, the detection performance of the diastatic fractures, fractures crossing the suture line, and fractures around the vascular grooves and orbit was poor.Conclusion : The object detection algorithm applied with deep learning is expected to be a valuable tool in diagnosing skull fractures.\n",
      "A Defect Detection Algorithm of Denim Fabric Based on Cascading Feature Extraction Architecture 2023 ['Cascading Feature Extraction Architecture', 'Denim Defect Detection', 'ImageNet', 'Robustness', 'Transfer Learning'] None Defect detection is one of the key factors in fabric quality control. To improve the speed and accuracy of denimfabric defect detection, this paper proposes a defect detection algorithm based on cascading feature extractionarchitecture. Firstly, this paper extracts these weight parameters of the pre-trained VGG16 model on the largedataset ImageNet and uses its portability to train the defect detection classifier and the defect recognitionclassifier respectively. Secondly, retraining and adjusting partial weight parameters of the convolution layerwere retrained and adjusted from of these two training models on the high-definition fabric defect dataset. Thelast step is merging these two models to get the defect detection algorithm based on cascading architecture.Then there are two comparative experiments between this improved defect detection algorithm and otherfeature extraction methods, such as VGG16, ResNet-50, and Xception. The results of experiments show thatthe defect detection accuracy of this defect detection algorithm can reach 94.3% and the speed is also increasedby 1–3 percentage points.\n",
      "데이터 증강을 위한 클래스 활성화 맵  기반 Random Erasing 2023 ['Data Augmentation', 'Class Activation Map', 'Random Erasing', 'CNN', 'Generalization'] None Random erasing offers various levels of occlusion for data augmentation. However, due to its uniform distribution of random selection, it sometimes occludes regions that are unrelated to the object of interest. In this paper, we propose a novel method that utilizes Gradient Weighted Class Activation Mapping (Grad-CAM) for estimating the location of the object of interest and selectively erasing the surrounding areas. By utilizing Grad-CAM, we improve random erasing for CNN models without requiring additional modules or architectural changes. We generate Grad-CAM after the intermediate epochs where CNN models have sufficient representational power for the training data. The hyperparameter that restrict the erasing to the vicinity of the object is set based on Grad-CAM, and experiments were conducted accordingly. As a result of our experiments, we observed a 0.33% decrease in error-rate for image classification tasks using ResNet-20 on the CIFAR-10 dataset.\n",
      "Determination of the stage and grade  of periodontitis according to the  current classification of periodontal  and peri-implant diseases and  conditions (2018) using machine  learning algorithms 2023 ['Classification', 'Deep learning', 'Machine learning', 'Periodontitis'] None Purpose: The current Classification of Periodontal and Peri-Implant Diseases and Conditions, published and disseminated in 2018, involves some difficulties and causes diagnostic conflicts due to its criteria, especially for inexperienced clinicians. The aim of this study was to design a decision system based on machine learning algorithms by using clinical measurements and radiographic images in order to determine and facilitate the staging and grading of periodontitis.Methods: In the first part of this study, machine learning models were created using the Python programming language based on clinical data from 144 individuals who presented to the Department of Periodontology, Faculty of Dentistry, Süleyman Demirel University. In the second part, panoramic radiographic images were processed and classification was carried out with deep learning algorithms.Results: Using clinical data, the accuracy of staging with the tree algorithm reached 97.2%, while the random forest and k-nearest neighbor algorithms reached 98.6% accuracy. The best staging accuracy for processing panoramic radiographic images was provided by a hybrid network model algorithm combining the proposed ResNet50 architecture and the support vector machine algorithm. For this, the images were preprocessed, and high success was obtained, with a classification accuracy of 88.2% for staging. However, in general, it was observed that the radiographic images provided a low level of success, in terms of accuracy, for modeling the grading of periodontitis.Conclusions: The machine learning-based decision system presented herein can facilitate periodontal diagnoses despite its current limitations. Further studies are planned to optimize the algorithm and improve the results.\n",
      "딥러닝을 이용한 농경지 팜맵 판독 적용 방안 2023 ['Deep Learning', 'Agricultural Land', 'FarmMap', 'Aviation Image', '딥러닝', '농경지', '팜맵', '항공영상'] None The Ministry of Agriculture, Food and Rural Affairs established the FarmMap, an digital map of agricultural land. In this study, using deep learning, we suggest the application of farm map reading to farmland such as paddy fields, fields, ginseng, fruit trees, facilities, and uncultivated land. The farm map is used as spatial information for planting status and drone operation by digitizing agricultural land in the real world using aerial and satellite images. A reading manual has been prepared and updated every year by demarcating the boundaries of agricultural land and reading the attributes. Human reading of agricultural land differs depending on reading ability and experience, and reading errors are difficult to verify in reality because of budget limitations. The farmmap has location information and class information of the corresponding object in the image of 5 types of farmland properties, so the suitable AI technique was tested with ResNet50, an instance segmentation model. The results of attribute reading of agricultural land using deep learning and attribute reading by humans were compared. If technology is developed by focusing on attribute reading that shows different results in the future, it is expected that it will play a big role in reducing attribute errors and improving the accuracy of digital map of agricultural land.\n",
      "CNN 기반 딥러닝 모델을 통한 폐암 컴퓨터 보조 진단 시스템 개발 2023 ['CADx', 'Cancer Diagnosis', 'Classification', 'Convolution Neural Network', 'Deep learning', 'Lung cancer', 'Malignant Tumor'] None Lung cancer ranked second in Korea domestic cancer incidence in 2020 and second in death rate. Lung cancer often has no early symptoms, so patients often miss the time of treatment. Accordingly, in Korea, lung cancer has been included in the national cancer screening since 2019. However, among misdiagnosis cases, lung cancer had the highest misdiagnosis rate, and the accuracy of screening may vary depending on the medical specialist's skill level and fatigue. Accordingly, this paper proposed a lung cancer CADx(Computer-Aided Diagnosis) system based on EfficientNetV2-L and ConvNeXt-B. EfficientNetV2 is a model that can have high classification performance with a small number of parameters using the Training-Aware NAS (Neural Architecture Search) method. ConvNeXt is a network that achieves higher performance than ViT(Vision Transformer) by combining the latest techniques with ResNet-50 as a base model. Medical imaging generally suffers from a data shortage problem. Therefore, we augmented the lung cancer dataset using AutoAugment using the ImageNet augmentation policy. Through this method, the sensitivity in classifying malignant(lung cancer) and normal improved from 0.8354 to 0.9638 in EfficientNetV2 and from 0.9796 to 0.9963 in ConvNeXt.AUC (Area Under the ROC Curve) also improved from 0.9967 to 0.9974 for EfficientNetV2 and from 0.9973 to 1.0000 for ConvNeXt. Additionally, noise that may generally occur in CT images was added and compared through Gaussian noise.EfficientNetV2's Sensitivity was 0.7417 in the original model and 0.8954 in the model to which AutoAugment was applied, representing a decrease of 9.37% and 6.84%, respectively. In contrast, ConvNeXt exhibited a Sensitivity of 0.9796 in the original model and 0.9963 in the model to which AutoAugment was applied, showing no decrease in performance. This led to the development of a CADx system that demonstrates excellent performance.\n",
      "저비용 비전 시스템과 의미론적 이미지분할을 이용한 포장 패치 자동  탐지 시스템 2023 ['Pavement patch detection', 'U-net', 'Automated pavement monitoring system', 'Inverse perspective mapping'] None Local road management agencies, which require frequent road surface inspections to maintain a safe driving environment, face difficulties in manually assessing road conditions. Moreover, stopping on the shoulder of a high-speed highway to inspect the road surface is extremely dangerous for inspectors. To overcome these challenges, there have been many efforts to automate road surface image acquisition, assessment, and analysis. However, existing methods mainly focus on detecting damage such as potholes and cracks, and have limitations in monitoring the condition of road surfaces, such as the need for repairs or the status of maintenance. This study proposed an automatic pavement patch detection method incorporating image processing and deep neural network image segmentation to provide a global view of the road surface. The panoramic image of the road is obtained by converting the video images captured by the omnidirectional camera of vehicles into top-view images using inverse perspective and affixing the images together. Next, the U-Net-architecture-based deep neural network image segmentation technique is applied to detect patches. Testing using actual highway driving videos revealed performances of 0.827 and 0.821 in the U-Net model based on ResNet152 and EfficientNetb7 backbones.\n",
      "CenterNet Based on Diagonal Half-length and Center Angle Regression for Object Detection 2023 ['Object detection', 'CenterNet', 'Prediction stability', 'Accuracy consistency', 'Convergence speed'] None CenterNet, a novel object detection algorithm without anchor based on key points, regards the object as a single center point for prediction and directly regresses the object’s height and width. However, because the objects have different sizes, directly regressing their height and width will make the model difficult to converge and lose the intrinsic relationship between object’s width and height, thereby reducing the stability of the model and the consistency of prediction accuracy. For this problem, we proposed an algorithm based on the regression of the diagonal half-length and the center angle, which significantly compresses the solution space of the regression components and enhances the intrinsic relationship between the decoded components. First, encode the object’s width and height into the diagonal half-length and the center angle, where the center angle is the angle between the diagonal and the vertical centreline. Secondly, the predicted diagonal half-length and center angle are decoded into two length components. Finally, the position of the object bounding box can be accurately obtained by combining the corresponding center point coordinates. Experiments show that, when using CenterNet as the improved baseline and resnet50 as the Backbone, the improved model achieved 81.6% and 79.7% mAP on the VOC 2007 and 2012 test sets, respectively. When using Hourglass-104 as the Backbone, the improved model achieved 43.3% mAP on the COCO 2017 test sets. Compared with CenterNet, the improved model has a faster convergence rate and significantly improved the stability and prediction accuracy.\n",
      "딥러닝을 이용한 음식 이미지 분류 기술 개발 2023 ['Deep Learning', 'ResNet', 'Food Image Classification', '딥러닝', 'ResNet', '음식 이미지 분류'] 본 연구는 20대와 한국인을 대상으로 한 건강관리 애플리케이션의 음식 이미지 분류 모델을 개선하는 것을 목표로 진행되었다. AI Hub에서 546,194개의 이미지를 수집하여 175개의 음식 클래스를 구성하였으며, ResNet 인공지능 모델을 학습하고 검증하였다. 추가적으로, 실제 촬영한 음식 이미지에 대한 인식 정확도가 상대적으로 낮게 나타나는 원인에 대해 고찰하고, 이를 해결하기 위한 방안으로 모델 성능을 최적화를 위한 다양한 방법을 분석하였다. This study was conducted with the aim of improving the food image classification model of a health care application targeting Koreans in their twenties. 546,194 images were collected from the Public Data Portal and AI Hub, and 175 food classes were constructed. The ResNet artificial intelligence model was trained and validated. Additionally, we deeply investigated the reasons for the relatively lower recognition accuracy of the actual food images, and we attempted various methods to optimize the model’s performance as a solution.\n",
      "Medical Diagonosis System Using AI Evolution Algorithms - CNN Based Chest X-ray Classification - 2023 ['Medical diagnosis', 'CNN', 'ResNet', 'Pneumonia', 'Transfer learning', 'Chest x-ray'] None The application of artificial intelligence techniques, specifically evolution algorithms in medical diagnosis has shown promising potential to enhance the accuracy and efficiency of disease identification. These days the trend is evolutional algorithms be on the turn CNN((Conversion Neural Network) algorithms. As a result of that this paper research applied to medical devices using CNN technology is being actively carried out around the world. In this study, chest x-ray image classification is performed using CNN-based ResNet as a medical image reading assistance technology. In this study, we will classify pneumonia. A total of 5,863 x-ray images are used, and the data is divided into pneumonia and normal, and consists of 3,875 pneumonia image data and 1,341 normal image data. Shortcut is connected to a cnn composed of convolutional layers, pooling layers, and fully connected layers to improve performance using fine tuning on CNN-ResNet, which has been learned to minimize residual. In this paper, only the use of the pre-learning model was considered as fine tuning, but the batch size and learning rate also affect the learning of the model. It is expected that further research to find the appropriate proportions will allow the performance of the model to be maintained more stable.\n",
      "Comparative Analysis of Swin Transformer and Residual Neural Network for Pneumonia Classification 2023 ['Swin Transformer', 'ResNet', '딥러닝 모델', '폐렴 감지', '흉부 엑스레이', 'Swin Transformer', 'ResNet', 'Deep learning models', 'Pneumonia detection', 'Chest X-ray'] None PPneumonia is a respiratory infectious disease that causes fluids to fill the lungs. It is considered one of the leading causes of infection-related deaths in children and seniors worldwide. Clinicians usually use chest X-ray images to diagnose pneumonia. However, pneumonia is prone to be misdiagnosed because it overlaps with cold and flu, causing severe and critical medical complications. Consequently, alternative supportive diagnostic methods are needed to minimize human errors and assist clinicians. Several attempts have used artificial intelligence systems, mainly in deep learning methods, to assist clinicians in early pneumonia diagnoses. However, further studies are required to consolidate the use of deep learning as an assistant tool to diagnose pneumonia accurately. In this study, we examine the Swin Transformer and the Residual Neural Network’s performance in classifying pneumonia and healthy chest X-ray images using the Guangzhou Women and Children’s Medical Center dataset and the COVID19, Pneumonia and Normal Chest X-ray Posteroanterior dataset. The experiment results demonstrate that the Swin Transformer achieves an accuracy of 98.9% in the Chest X-ray images dataset and 92.35% in the COVID19, Pneumonia and Normal Chest X-ray Posteroanterior dataset, while the Residual Neural Network achieves an accuracy of 97.9% and 88.8% respectively in classifying pneumonia. These results indicate that the Swin Transformer outperforms the Residual Neural Network as a tool for assisting clinicians in diagnosing pneumonia. Thus, the Swin Transformer may help in early decision-making, leading to treatment initiation and improving patient's health.\n",
      "A Study on the Development of Deep Learning Algorithm for Determining External Quality of Welded Parts Using Transfer Learning 2023 ['Weld quality', 'Weld appearance', 'Deep learning', 'CNN (Convolution Neural Network)', 'ResNet'] None Recently, deep learning has been applied to various welding techniques, such as laser welding, gas metal arc welding (GMAW), and resistance spot welding, and research on automation and quality prediction is being conducted.Even for GMAW, many researchers have attempted to predict quality through X-ray, current, and voltage measurements. If judgment in real time is not necessary, it is most effective to judge the quality of a welded part using an exterior image. Therefore, in this study, a welded appearance quality judgment model was analyzed using image deep learning. Welding defects were classified into pores, overlaps, craters, melting of the base material, cracks, and undercuts, and were divided into 7 categories including normal ones. In constructing the deep-learning model, transfer learning was performed using existing networks, such as ResNet and AlexNet. To improve the accuracy of deep learning, tests were performed while the optimization technique, maximum number of epochs, and minibatch size were changed. It was confirmed that the accuracy of weld defect prediction improved as the minibatch size increased, and the stochastic gradient descent model had the highest accuracy. Increasing the number of data for learning should make the technique of using images to judge the quality of GMAW welds using the proposed model more widely applicable.\n",
      "Research on Damage Identification of Buried Pipeline Based on Fiber Optic Vibration Signal 2023 ['Distributed fiber optic vibration sensing', 'Pattern recognition', 'Residual network'] None Pipelines play an important role in urban water supply and drainage, oil and gas transmission, etc.This paper presents a technique for pattern recognition of fiber optic vibration signals collected by a distributed vibration sensing (DVS) system using a deep learning residual network (ResNet). The optical fiber is laid on the pipeline, and the signal is collected by the DVS system and converted into a 64 × 64 single-channel grayscale image. The grayscale image is input into the ResNet to extract features, and finally the K-nearest-neighbors (KNN) algorithm is used to achieve the classification and recognition of pipeline damage.\n",
      "다중모드 특징융합 기반 배관계 누출판별 앙상블 모델 연구 2023 ['deep learning', 'multi-mode feature fusion', 'ensemble', 'pipe leak detection', 'weight redistribution', '.'] 본 논문은 플랜트 배관계의 노후화 문제로 인해 발생하는 미세 누출을 탐지하기 위한 다중모드 특징융합을 이용한 가중치 재분배 앙상블 모델을 제안한다. 수집된 데이터는 2D 주파수 패턴 특징과 2D RMS 패턴 특징으로 변환되었으며, 여러 센서에서 추출된 다양한 도메인 특징들이 서로 결합되어 볼륨 특징으로 구성하였다. 실험을 위해 ResNet 기반의 단일 모델을 설계하여 다양한 볼륨 특징들을 이용한 앙상블 구조를 조합하였다. 또한, 다수의 예측 모델을 결합하는 과정에서 발생할 수 있는 성능 불균형 문제를 해결하기 위해, 소프트맥스 함수를 기반으로 한 가중치 재분배를 적용하였다. 실험 결과, 주파수 및 RMS 볼륨 특징을 활용한 센서별 앙상블 모델이 98.91%라는 가장 높은 분류 정확도를 제공하는 것을 실험적으로 관찰할 수 있었다. This paper proposes a weight redistribution ensemble model using multi-mode feature fusion for detecting micro leaks arising from aging problems in the power plant piping systems. The collected data was transformed into 2D frequency and RMS pattern features, and various domain features extracted from multiple sensors were combined to form volume features. For the experiment, a single model based on ResNet was designed, and an ensemble structure using various volume features was composed. Additionally, in order to resolve the performance imbalance problem that could arise in the process of combining multiple predictive models, we applied a softmax function-based weight redistribution. Experimental results showed that the ensemble model for each sensor, using frequency and RMS volume features, provided the highest classification accuracy of 98.91%.\n",
      "이미지 인식 기반 국내 자생종 소나무 종 분류 2023 ['Pine Tree', 'Image Classification', 'CNN', 'Data Labeling', 'Feature Importance'] 국내 소나무과 나무는 국내 자생 침엽수의 절반을 차지할 만큼 종 다양성이 가장 높다. 소나무가 국내 산림의 60% 이상을 차지한 적도 있었지만, 현재는 재선충, 산불 등으로 그 비율이 25%로 감소하였다. 국내 자생종 소나무의 종 분류를 위해 국립생물자원관에서 보유 중인 표준 이미지 데이터셋을 사용하여 3종의 소나무를 분류하였다. 또한, 성능이 검증된 ResNet 50과 같은 사전 학습모델을 변형하는 대신 이미지 인식 모델을 직접 구현하여 종 분류를 진행 85%의 정확도를 나타내었고, 이후, 인식 과정에서 도출되는 특징들을 판별하였다. 이미지 크기 및 증식이 분류에 미치는 영향을 파악하고자 이미지 크기 및 이미지 증식을 통한 성능 평가를 수행 이미지 크기는 약 3% 그리고 이미지 증식의 경우는 약 6.4% 성능 향상을 가져왔다. 또한, 데이터 레이블링 방식의 기계학습 모델과도 분류 성능을 비교 정리하였다. 이미지 인식 과정에서 종 분류에 가중치가 높게 반영된 특징을 추출하였으며, 이를 레이블링 기법에 사용된 특징 중요도와 비교, 레이블링 기법과 이미지 인식 기반에서 종 분류에 가장 유효하게 나타난 특징은 유사한 것으로 나타났다. Pine trees in Korea are the tree species with the highest diversity, accounting for half of the native coniferous trees. Pine trees once accounted for more than 60% of the nations forests, but the proportion has decreased to 25% due to infestations and wildfires. To classify Koreas native pine trees, we use the standard image dataset held by the National Institute of Biological Resources to classify three species. Instead of modifying a pre-trained model like ResNet 50, which has been proven to perform well, we instead directly implement an image recognition model to perform species classification, which showed an accuracy of 85%, and then determined the features derived from the recognition process. To understand the impact of image size and growth on classification, we evaluated performance of the model through image size and image growth. The performance improvement was about 3% for image size and 6.4% for image growth. We also compare its performance with a machine learning model based on data labeling. We extract the most weighted features for species classification from the image recognition process and compare them with feature importance used in the labeling method, finding that the most effective features for species classification in the labeling method and image recognition are similar.\n",
      "모델 내부/외부 특징량 상관 학습을 통한 지식 증류 2023 ['Knowledge distillation', 'model compression', 'transformer', 'correlation learning', 'Image classification', '지식 증류', '모델 압축', '트랜스포머', '상관관계 학습', '이미지 분류'] 본 논문에서는 이종 모델의 특징맵 간 상관관계인 외부적 상관관계와 동종 모델 내부 특징맵 간 상관관계인 내부적 상관관계를 활용하여 교사 모델로부터 학생 모델로 지식을 전이하는 Internal/External Knowledge Distillation (IEKD)를 제안한다. 두 상관관계를 모두 활용하기 위하여 특징맵을 시퀀스 형태로 변환하고, 트랜스포머를 통해 내부적/외부적 상관관계를 고려하여 지식 증류에 적합한 새로운 특징맵을 추출한다. 추출된 특징맵을 증류함으로써 내부적 상관관계와 외부적 상관관계를 함께 학습할 수 있다. 또한 추출된 특징맵을 활용하여 feature matching을 수행함으로써 학생 모델의 정확도 향상을 도모한다. 제안한 지식 증류 방법의 효과를 증명하기 위해, CIFAR-100 데이터 셋에서 “ResNet-32×4/VGG-8” 교사/학생 모델 조합으로 최신 지식 증류 방법보다 향상된 76.23% Top-1 이미지 분류 정확도를 달성하였다. In this paper, we propose an Internal/External Knowledge Distillation (IEKD), which utilizes both external correlations between feature maps of heterogeneous models and internal correlations between feature maps of the same model for transferring knowledge from a teacher model to a student model. To achieve this, we transform feature maps into a sequence format and extract new feature maps suitable for knowledge distillation by considering internal and external correlations through a transformer. We can learn both internal and external correlations by distilling the extracted feature maps and improve the accuracy of the student model by utilizing the extracted feature maps with feature matching. To demonstrate the effectiveness of our proposed knowledge distillation method, we achieved 76.23% Top-1 image classification accuracy on the CIFAR-100 dataset with the “ResNet-32×4/VGG-8” teacher and student combination and outperformed the state-of-the-art KD methods.\n",
      "열화상 영상을 활용한 CNN-Transformer 네트워크의 공장 설비 이상 진단 방법 2023 ['CNN', 'Transformer', 'Thermal imaging', 'Abnormality diagnosis', 'Factory facility'] 본 논문에서는 열화상 영상을 이용한 공장 설비 이상 진단에 최적화된 딥러닝 알고리즘을 제안한다. 이를 위하여 대조도 향상 알고리즘으로 열화상 영상의 대조도를 명확하게 변환하여 가장자리 정보를 강화하여 준다. 그 후에 Convolution Neural Network(CNN)와 Transformer Network 각각의 장점만을 이용하여 개발된 CvT(Convolutional vision Transformer)를 열화상 영상 기반의 고장 설비 이상 진단에 적합하게 수정한 modified CvT 개발을 통하여 공장 설비의 이상을 진단한다. AI Hub에서 제공되는 열화상 영상 중에서 공장 설비의 정상 및 이상 영상들을 추출하여 실험을 진행하였으며, 이를 통하여 기존 컴퓨터 비전 분야에서 보편적으로 사용되고 있는 CNN 기반의 ResNet, EfficientNet 그리고 Transformer 기반의 ViT(Vision Transformer), SwinT(Swin Transformer)보다 높은 정확도인 98.79%의 우수한 성능을 확인하였다. 결론적으로 CNN과 Transformer 융합 네트워크를 활용하였을 때 다른 열화상 영상을 이용한 공장 설비 이상 진단 알고리즘보다 우수한 성능을 보여준다는 것을 확인하였다. In this paper, we propose a deep learning algorithm optimized for diagnosing factory facility abnormalities using thermal imaging. For this purpose, the contrast of the thermal image is clearly converted with the contrast enhancement algorithm to enhance the edge information. After that, the Convolution Vision Transformer (CvT) developed using only the advantages of Convolution Neural Network (CNN) and Transformer Network is modified to suit the diagnosis of thermal image-based failure facility abnormalities. Experiments were conducted by extracting normal and abnormal images of factory facilities from the thermal image provided by AI Hub. Through this, we confirmed the excellent performance of 98.79% which is higher accuracy than CNN-based ResNet, EfficientNet, and Transformer-based Vision Transformer (ViT), SwinT (Swin Transformer), which are commonly used in the existing computer vision field. In conclusion, it was confirmed that when using the CNN and Transformer fusion network, it shows better performance than the factory facility failure diagnosis algorithm using other thermal imaging images.\n",
      "수경재배 환경에서 머신러닝 기반 불량 모종 진단을 위한 증강 데이터 활용 연구 2023 ['hydroponic conditions', 'anti-caner leaf lettuce', 'machine learning', 'deep learning', 'data augmentation', '.'] None The need for big data analysis and artificial intelligence for smart agriculture is continuously requested. It is essential to collect and label large amounts of quality data for artificial intelligence research, but there is a relative lack of data in the hydroponic cultivation area. In this paper the performance of growth diagnostic ML model with original anti-cancer leaf lettuce data set was checked. Then ML model development and model performance improvement experiment was proceeded with augmented data. First, ML model learning and testing was conducted using DCGAN data only. As a result, the accuracy of ResNet was 61.2, DenseNet was 62.4. And model performance improvement experiment was proceeded by adding augmented data to the original data. As a result, the accuracy of ResNet increase from 86.5 to 88.2 and DenseNet from 92.9 to 94.7. In these experiments the possibility of using augmented data and its influence are studied for developing and improving the performance of diagnostic ML model.\n",
      "학습 데이터가 없는 모델 탈취 방법에 대한 분석 2023 None 딥뉴럴네트워크 모델의 취약점으로 모델 탈취 방법이 있다. 이 방법은 대상 모델에 대하여 여러번의 반복된 쿼리를 통해서 유사 모델을 생성하여 대상 모델의 예측값과 동일하게 내는 유사 모델을 생성하는 것이다. 본 연구에서, 학습 데이터가 없이 대상 모델을 탈취하는 방법에 대해서 분석을 하였다. 생성 모델을 이용하여 입력 데이터를 생성하고 대상 모델과 유사 모델의 예측값이 서로 가까워지도록 손실함수를 정의하여 유사 모델을 생성한다. 이 방법에서 대상 모델의 입력 데이터에 대한 각 클래스의 logit(로직) 값을 이용하여 경사하강법으로 유사 모델이 그것과 유사하도록 학습하는 과정을 갖는다. 실험 환경으로 pytorch 머신러닝 라이브러리를 이용하였으며, 데이터셋으로 CIFAR10과 SVHN을 사용하였다. 대상 모델로 ResNet 모델을 이용하였다. 실험 결과로써, 모델 탈취방법은 CIFAR10에 대해서 86.18%이고 SVHN에 대해서 96.02% 정확도로 대상 모델과 유사한 예측값을 내는 유사 모델을 생성하는 것을 볼 수가 있었다. 추가적으로 모델 탈취 방법에 대한 고려사항와 한계점에 대한 고찰도 분석하였다. In this study, we analyzed how to steal the target model without training data. Input data is generated using the generative model, and a similar model is created by defining a loss function so that the predicted values of the target model and the similar model are close to each other. At this time, the target model has a process of learning so that the similar model is similar to it by gradient descent using the logit (logic) value of each class for the input data. The tensorflow machine learning library was used as an experimental environment, and CIFAR10 and SVHN were used as datasets. A similar model was created using the ResNet model as a target model. As a result of the experiment, it was found that the model stealing method generated a similar model with an accuracy of 86.18% for CIFAR10 and 96.02% for SVHN, producing similar predicted values to the target model. In addition, considerations on the model stealing method, military use, and limitations were also analyzed.\n",
      "Indoor Environment Drone Detection through DBSCAN and Deep Learning 2023 ['DBSCAN', 'Deep learning', 'FMCW radar', 'Object detection', 'UAV'] None In an era marked by the increasing use of drones and the growing demand for indoor surveillance, the development of a robust application for detecting and tracking both drones and humans within indoor spaces becomes imperative. This study presents an innovative application that uses FMCW radar to detect human and drone motions from the cloud point. At the outset, the DBSCAN (Density-based Spatial Clustering of Applications with Noise) algorithm is utilized to categorize cloud points into distinct groups, each representing the objects present in the tracking area. Notably, this algorithm demonstrates remarkable efficiency, particularly in clustering drone point clouds, achieving an impressive accuracy of up to 92.8%. Subsequently, the clusters are discerned and classified into either humans or drones by employing a deep learning model. A trio of models, including Deep Neural Network (DNN), Residual Network (ResNet), and Long Short-Term Memory (LSTM), are applied, and the outcomes reveal that the ResNet model achieves the highest accuracy. It attains an impressive 98.62% accuracy for identifying drone clusters and a noteworthy 96.75% accuracy for human clusters.\n",
      "합성곱 신경 회로망 모델을 활용한 흑색종 피부암 진단 2023 ['피부암', '흑색종', 'Skin Cancer', 'Melanoma', 'VGG-16', 'ResNet 50', 'DenseNet 121', 'Inception V3', 'WideResnet50'] None None\n",
      "합리적 가격결정을 위한 전이학습모델기반 아보카도 분류 및 출하 예측 시스템 2023 ['딥러닝', '전이 학습', 'Deep Learning', 'Transfer Learning', 'ImageNet', 'VGG16', 'ResNet', 'DenseNet'] 타임지가 선정한 슈퍼푸드이며, 후숙 과일 중 하나인 아보카도는 현지가격과 국내 유통 가격이 크게 차이가 나는 식품 중 하나이다. 이러한 아보카도의 분류과정을 자동화한다면 다양한 분야에서 인건비를 줄여 가격을 낮출 수 있을 것이다. 본 논문에서는 아보카도의 데이터셋을 크롤링을 통하여 제작하고, 딥러닝 기반 전이학습모델을 다수 사용하여, 최적의 분류모델을 만드는 것을 목표로 한다. 실험은 제작한 데이터셋에서 분리한 데이터셋에서 딥러닝 기반 전이학습모델에 직접 대입하고, 해당 모델의 하이퍼 파라미터를 Fine-tuning하며 진행하였다. 제작된 모델은 아보카도의 이미지를 입력하였을 때, 해당 아보카도의 익은 정도를 99% 이상의 정확도로 분류하였으며, 아보카도 생산 및 유통가정의 인력감소 및 정확성을 높일 수 있는 데이터셋 및 알고리즘을 제안한다. Avocado, a superfood selected by Time magazine and one of the late ripening fruits, is one of the foods with a big difference between local prices and domestic distribution prices. If this sorting process of avocados is automated, it will be possible to lower prices by reducing labor costs in various fields. In this paper, we aim to create an optimal classification model by creating an avocado dataset through crawling and using a number of deep learning-based transfer learning models. Experiments were conducted by directly substituting a deep learning-based transfer learning model from a dataset separated from the produced dataset and fine-tuning the hyperparameters of the model. When an avocado image is input, the model classifies the ripeness of the avocado with an accuracy of over 99%, and proposes a dataset and algorithm that can reduce manpower and increase accuracy in avocado production and distribution households.\n",
      "기술 분석과 환경요소를 이용한 주가 예측률 향상을 위한 딥러닝 병렬 모델 2023 ['stock forecast', 'deep neural network', 'parallel model', '1D-CNN', 'ResNet', '.'] 본 연구는 주가 데이터, 기술 분석 데이터, 환경요소 데이터를 이용하여, 주가예측을 위한 딥러닝 병렬 모델을 제안하였다. 예측을 위한 데이터 셋은 3개로 나누었으며, 데이터 셋 1은 시가, 고가, 저가, 종가, 거래량이며, 데이터 셋 2는 기술 분석 데이터를 추가하였으며, 데이터 셋 3은 주가에 영향을 줄 수 있는 환율, 전산업생산지수를 추가하였다. 딥러닝 모델은 기본 모델로서 DNN, LSTM, 1D-CNN 모델과 병렬 모델로서 DNN 모델을 기본으로 1D-CNN을 병합한 DCNN 모델과 LSTM을 병합한 DLSTM 모델을 제안하였다. 실험 결과, DNN과 CNN 보다는 LSTM과 BiLSTM 모델의 성능이 높았으며, 특히 병렬모델인 DLSTM 모델이 가장 성능이 좋았다. 병렬 모델인 DLSTM 모델에 대한 데이터 셋 1의 RMSE는 0.0091, 데이터 셋 2의 RMSE는 0.0080, 데이터 셋 3의 RMSE는 0.0071로서 모든 데이터가 합쳐진 데이터 셋 3의 성능이 가장 좋았다. This study proposed a deep learning parallel model for stock price prediction using stock price data, technical analysis data, and environmental factor data. The data set for prediction was divided into three, data set 1 is the opening price, high price, low price, closing price, and trading volume, data set 2 added technical analysis data, and data set 3 is the exchange rate that can affect the stock price. the overall industrial production index was added. The deep learning model proposed DNN, LSTM, and 1D-CNN models as basic models, and a DCNN model that merged 1D-CNN based on the DNN model as a parallel model, and a DLSTM model that merged LSTM as a parallel model. As a result of the experiment, the performance of LSTM and BiLSTM models was higher than that of DNN and CNN, and in particular, the DLSTM model, a parallel model, performed the best. For the DLSTM model, which is a parallel model, the RMSE of data set 1 was 0.0091, the RMSE of data set 2 was 0.0080, and the RMSE of data set 3 was 0.0071. Data set 3, which combined all data, had the best performance.\n",
      "기상 요소 데이터를 이용한 단기 강수량예측 향상을 위한 딥러닝 병렬 모델 2023 ['precipitation forecast', 'deep neural network', 'parallel model', '1D-CNN', 'ResNet', '.'] 강수량예측은 누적 강수량보다는 짧은 시간에 얼마나 많은 비가 집중적으로 내렸는지 알려주는 시간당 강수량에 따라서 피해 정도가 달라진다. 본 연구는 딥러닝 기본모델인 DNN, LSTM, BiLSTM, 1D-CNN 모델과 성능 향상을 위해 기본모델을 병합한 병렬 구조를 이용하여 단기 강수예측을 하였다. 병렬모델은 DNN 모델과 1D-CNN을 병합한 DCNN과 DNN 모델과 LSTM을 병합한 DLSTM 모델이다. 데이터셋은 강수일 만을 구축한 데이터셋, 6월부터 9월까지의 데이터셋, 5월부터 10월끼지의 데이터셋 등 3개의 데이터셋을 이용하였으며, 각 데이터셋에 대해서 세부적으로 7개의 데이터셋으로 구분하여, 총 21개의 데이터셋에 대하여 강수량을 예측하고 비교 평가하였다. 실험 결과, 세 번째 데이터셋이 가장 예측 결과가 좋았다. 특히, 5 번째 세부 데이터셋인 DLSTM 병렬 모델의 RMSE가 0.25로서, 다른 모델보다 10배 정도 월등히 예측 결과가 좋았다. In precipitation forecasting, the degree of damage varies according to the amount of precipitation per hour, which tells how much rain has fallen intensively in a short period of time, rather than the cumulative amount of precipitation. In this study, short-term precipitation prediction was performed using deep learning basic models such as DNN, LSTM, BiLSTM, and 1D-CNN models and a parallel structure merging the basic models to improve performance. Parallel models include DCNN, which combines DNN and 1D-CNN, and DLSTM, which combines DNN and LSTM. As for the data set, three data sets were used: a data set built with only rainy days, a data set from June to September, and a data set from May to October. Each data set was divided into seven data sets in detail. The precipitation was predicted and compared for a total of 21 data sets. As a result of the experiment, the third data set had the best prediction results. In particular, the RMSE of the 5th detailed data set, the DLSTM   parallel model, was 0.25, which was about 10 times better than other models.\n",
      "차량 단말기 기반 돌발상황 검지 알고리즘 개발 2023 ['Highway', 'Emergency situations', 'Detection', 'Scene classification', 'Artificial intelligence', 'Vehicle terminal', '고속도로', '돌발상황', '검지', '상황분류', '인공지능', '차량단말기'] 전방 낙하물과 같은 돌발상황이 발생했을 때 신속하고 적절한 정보 제공은 도로 위 이용자들의 편의를 가져다주고 2차 교통사고 또한 효과적으로 줄일 수 있다. 도로 상의 돌발상황은현재 국내에서 루프 검지기나 CCTV 등 ITS 기반 검지 체계를 사용하여 주로 검지하고 있다. 이러한 방식은 검지기의 검지 구간에서의 도로 위 데이터만을 얻을 수 있다. 때문에, 기존 ITS 기반 검지체계의 공간적 음영구간에서 돌발상황을 찾아내기 위하여 새로운 검지 수단이 필요하다. 이에 본 연구에서는 차량 내 설치된 단말기에서 촬영된 영상으로부터 돌발상황을 검지및 분류하는 ResNet 기반 알고리즘을 제안한다. 국내 고속도로 전방 주행영상을 수집하였고, 돌발상황 유형을 클래스로 정의하여 각 데이터를 라벨링한 후, 제안한 알고리즘으로 데이터를학습시켰다. 학습 결과, 개발한 알고리즘은 데이터 수가 상대적으로 적었던 일부 클래스를 제외하고 정의한 돌발상황 클래스에 대하여 높은 검지율을 보였다. None\n",
      "An Intercomparison of Deep‑Learning Methods for Super‑Resolution Bias‑Correction (SRBC) of Indian Summer Monsoon Rainfall (ISMR) Using CORDEX‑SA Simulations 2023 ['Regional climate · Indian monsoon · Climate change · Deep learning'] None The Indian Summer Monsoon Rainfall (ISMR) plays a significant role in India’s agriculture and economy. Our understandingof the climate dynamics of the Indian summer monsoon has been enriched with general circulation models (GCMs)and regional climate models (RCMs). Systematic bias associated with these numerical simulations, however, needs to becorrected before we can obtain accurate or reliable projections of the future. Therefore, this study applies two state-of-theartdeep-learning (DL)-based super-resolution bias correction (SRBC) methods, viz. Autoencoder-Decoder (ACDC) and adeeper network Residual Neural Network (ResNet) to perform spatial downscaling and bias-correction on high-resolutionCORDEX-SA climatic simulations of precipitation. To do so, we obtained eight meteorological variables from CORDEXSARCM simulations along with a digital elevation model at a spatial resolution of 0.25°×0.25° as input. Indian MonsoonData Assimilation and Analysis, precipitation reanalysis re-grided to 0.05°×0.05° spatial resolution is chosen as output forthe training period 1979–2005. To evaluate the DL algorithms, the RCP 2.6 scenario of CORDEX-SA future simulationsfor the period 2006–2020 is chosen. Moreover, we also conducted a performance assessment of the representation of mean,variability, extreme, and frequency of rainfall associated with ISMR. The results of the experiments show that the DL methodResNet a highly efficient in (i) improving the spatial resolution of the climatic simulations from 0.25°×0.25° to 0.05°×0.05°,(ii) reducing the systematic biases of the extreme rainfall of ISMR from 21.18 mm to -7.86 mm, and (iii) providing a robustbias-corrected climate simulation of ISMR for future climate mitigation and adaptation studies.\n",
      "콘볼루션 신경망의 학습 성능 강화를 위한 그레디언트의 복합적 분석 및 활용 방법 2023 ['convolutional neural network', 'optimization', 'first-order optimization', 'gradient descent'] None A convolutional neural network(CNN) is a deep neural network which is composed of many layers with convolution filters and sub-sampling operations. Because such complex structure of CNNs makes its effective train difficult, it is necessary to study more complicated and stable methods to train the CNNs than the existing ones. Therefore, in this paper, we introduce various methods utilizing gradients complicatedly and propose a new optimizer for CNNs, called CGAU-CNN, based on the methods. In the practical experiments, we trained two CNN models, i.e, ResNet and DenseNet, by utilizing CGAU-CNN and evaluated its image classification accuracy. As a result, we found that CGAU-CNN could train them with faster convergence and better accuracy than the existing optimizers such as Adam.\n",
      "압축센싱 수신기를 이용한 무선 주파수 지문식별 시스템 구현 2023 ['compressed sensing', 'radio frequency fingerprint identification', 'convolutional neural network', '.'] None The spectrum dominance strategy, which has recently been emphasized in the field of electronic warfare, requires pervasive spectrum awareness. However, the ability to continuously monitor all critical portions of the spectrum and identify specific threats, which is a key requirement for pervasive spectrum awareness, is still weak. In this paper, we implemented a radio frequency fingerprint identification system and presented the performance of two key functions for pervasive spectrum awareness. The implemented system applies a compressed sensing receiving technique that can simultaneously detect radio frequency signals existing in different frequency bands for continuous monitoring of the spectrum. To implement a specific emitter identification function, the 2-D ResNet model based on the convolutional neural network was converted into a 1-D model and applied to radio frequency signal identification.\n",
      "딥러닝을 활용한 콘텐츠 내 텍스트 폰트  저작권 검출 모델 연구 2023 ['Deep-Learning', 'Hangul Font', 'Classification', 'Segmentation', 'Text Detection'] None Despite the diverse forms and designs in which fonts are widely utilized in our daily lives, including videos, print materials, products, websites, and mobile sites, numerous copyright issues continue to arise. Despite efforts to address these concerns and take measures for improvement, a variety of issues persist. This research serves as a preliminary study to enhance this situation. The goal was to implement a detection model for fonts used in texts within challenging media such as images and videos, aiming to mitigate font copyright issues that arise across various mediums. As a preliminary step, a model for recognizing fonts used within videos, image, pdf was developed. The model consists of two primary components: a text detection and background removal model within images, and a font recognition model used within the text. The text detection model was refined through various approaches such as image processing and deep learning to identify the optimal model. For the font recognition model, comparisons were made between CNN and ResNet models to select the most suitable one. As a result, an integrated 2-stage model was constructed. Validation was performed using arbitrary video data, revealing a top-1 rate is 76% and a top-5 rate is 94%.\n",
      "Cross‑domain health status assessment of three‑phase inverters using improved DANN 2023 ['Health status assessment', 'Transfer learning', 'Deep residual network', 'Domain adversarial neural network'] None Information and large number of fault labels are required to achieve intelligent health status assessment of three-phase inverters. However, the current signals of inverters cannot be sufficiently collected since open-circuit faults (OCFs) occur briefly, which makes it difficult to determine the OCF mode of the various power switches. A transfer learning model that effectively uses a small amount of sample data to achieve domain adaptation is proposed to address this problem. First, collected fault-sensitive signals are subjected to a continuous wavelet transform (CWT) to obtain two-dimensional image data with more abundant fault feature information. Second, the source domain and target domain features are projected into the same feature space through a domain adversarial neural network (DANN) to achieve multi-domain feature extraction and adaptation. Then, in the feature extraction module of the DANN, the deep residual network (Resnet) structure is used to replace the typical convolutional neural network (CNN) structure. Finally, an intelligent diagnosis network is used to identify the health status of the inverter samples under variable conditions. Experimental results show that the proposed model can accurately and effectively realize the cross-domain health assessment of three-phase inverters in the case of small samples. The accuracy of the proposed model is better than that of other classical transfer learning models.\n",
      "딥러닝 기반 의미론적 분할 기법을 통한 건물 자동추출 연구: 모델의 가중치 경중과 전이학습에 따른 정확도 변화 중심으로 2023 ['Semantic Segmentation', 'Building Detection', 'Weight of the Model', 'Transfer Learning', '의미론적 분할', '건물 탐지', '모델 가중치', '전이학습'] None Building objects are an essential spatial information source that can be used in fields such as 3D modeling, urban expansion, and environmental analysis. They are one of the geographical features for which continuous information construction is essential but are not easy to construct automatically. As a solution to this problem, methods have been proposed to develop new heavy neural networks or utilize transfer learning, but there are still limitations. This study conducted an experiment to determine the models classification performance according to the weight and the possibility of using the transfer learning technique using ImageNet weights in remote sensing. For this purpose, AiHubs land cover map learning dataset was used, and U-Net and Deeplab V3+ classification models using MobileNet and ResNet as backbone neural networks were utilized. As a result of the experiment, the classification accuracy was found to be highest when transfer learning was not performed with the MobileNet-based U-Net model (f1-score: 0.8483). Additionally, visually, it was confirmed that the model learned from scratch rather than transfer learning depicted the building closer to the ground truth. This means that a variety of methods can be used to perform transfer learning without the need to limit the neural network, and it suggests that if there is an amount of data at the level provided by AiHub, a model with a certain level of classification accuracy can be created.\n",
      "섬유 제조 장비 최적화를 위한 딥러닝 기반 결함 분류 시스템 구축 2023 ['딥러닝', '합성곱 신경망', '다중 클래스', '이미지 분류', '결함 검사', '섬유', 'Deep learning', 'CNN', 'Multi Class', 'Image classification', 'Defect inspection', 'Textile'] None In this paper, we propose a process of increasing productivity by applying a deep learning-based defect detection and classification system to the prepreg fiber manufacturing process, which is in high demand in the field of producing composite materials. In order to apply it to toe prepreg manufacturing equipment that requires a solution due to the occurrence of a large amount of defects in various conditions, the optimal environment was first established by selecting cameras and lights necessary for defect detection and classification model production. In addition, data necessary for the production of multiple classification models were collected and labeled according to normal and defective conditions. The multi-classification model is made based on CNN and applies pre-learning models such as VGGNet, MobileNet, ResNet, etc. to compare performance and identify improvement directions with accuracy and loss graphs. Data augmentation and dropout techniques were applied to identify and improve overfitting problems as major problems. In order to evaluate the performance of the model, a performance evaluation was conducted using the confusion matrix as a performance indicator, and the performance of more than 99% was confirmed. In addition, it checks the classification results for images acquired in real time by applying them to the actual process to check whether the discrimination values are accurately derived.\n",
      "An effective deep learning model for ship detection from satellite images 2023 ['Maritime monitoring · Optical satellite imagery · CNN · Remote sensing · Marine traffic'] None Detecting ships from satellite images is a challenging task in the domain of remote sensing. It is very important for security, traffic management and to avoid smuggling etc. SAR (Synthetic Aperture Radar) is mostly used technology for Maritime monitoring but now researchers are increasingly studying Optical Satellite Images based technologies. Image processing and Computer Vision techniques were previously used to detect ships. In this work, Convolutional Neural Network based approach is used to detect ships from the satellite imagery. Several Deep Learning models have been used and tested for this kind of task. We used state of art model Inception-Resnet that is pre trained on Image-Net dataset. We used the dataset \"Ships in Satellite Imagery\" to detect the presence of ships in an image. The dataset is publicly available on Kaggle.The results indicate adoption of transfer learning and data augmentation yields a successful detection of ships with an accuracy of more than 99%. Similarly, exploring different deep learning models for this task provide results with high accuracy for less training time.\n",
      "Related-key Neural Distinguisher on Block Ciphers SPECK-32/64, HIGHT and GOST 2023 ['Related-key Attack', 'Neural Cryptanalysis', 'Distinguisher', 'Deep Learning', 'Lightweight Block Ciphers'] None With the rise of the Internet of Things, the security of such lightweight computing environments has become a hot topic. Lightweight block ciphers that can provide efficient performance and security by having a relatively simpler structure and smaller key and block sizes are drawing attention. Due to these characteristics, they can become a target for new attack techniques. One of the new cryptanalytic attacks that have been attracting interest is Neural cryptanalysis, which is a cryptanalytic technique based on neural networks. It showed interesting results with better results than the conventional cryptanalysis method without a great amount of time and cryptographic knowledge. The first work that showed good results was carried out by Aron Gohr in CRYPTO'19, the attack was conducted on the lightweight block cipher SPECK-/32/64 and showed better results than conventional differential cryptanalysis. In this paper, we first apply the Differential Neural Distinguisher proposed by Aron Gohr to the block ciphers HIGHT and GOST to test the applicability of the attack to ciphers with different structures. The performance of the Differential Neural Distinguisher is then analyzed by replacing the neural network attack model with five different models (Multi-Layer Perceptron, AlexNet, ResNext, SE-ResNet, SE-ResNext). We then propose a Related-key Neural Distinguisher and apply it to the SPECK-/32/64, HIGHT, and GOST block ciphers. The proposed Related-key Neural Distinguisher was constructed using the relationship between keys, and this made it possible to distinguish more rounds than the differential distinguisher.\n",
      "2023년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                                                                                                                                   title  \\\n",
      "0                                                                                                                                                                  영상신호를 입력으로 하는 3D ResNet기반 유아 행동 인식 기법   \n",
      "1                                                                                                                                                          흉부 X선 영상을 이용한 작은 층수 ResNet 기반 폐렴 진단 모델의 성능 평가   \n",
      "2                                                                                                                                        SwinResNet: Swin Transformer와 ResNet 융합을 통한 Volumetric 의료 영상 분할   \n",
      "3                                                                                                                                                             밀 종자 품종 및 물성 분류를 위한 ResNet50 모델 기반의 이미지 분석   \n",
      "4                                                                                                                                                                            ResNet/SVM 기반 GNSS 재밍 식별 기법   \n",
      "5                                                                                                                                       3D Object Generation and Renderer System based on VAE ResNet-GAN   \n",
      "6                                                                       Improving the Cyber Security over Banking Sector by Detecting the Malicious Attacks Using the Wrapper Stepwise Resnet Classifier   \n",
      "7                                                                                                                     Reversible Multipurpose Watermarking Algorithm Using ResNet and Perceptual Hashing   \n",
      "8                                                                                                                                                                        다양한 CNN 모델을 이용한 얼굴 영상의 나이 인식 연구   \n",
      "9                                                                                                                                위내시경 영상에서의 위 병변 자동 검출 모델 개발을 위한 RetinaNet 기반 backbone 네트워크에 따른 학습 성능 비교   \n",
      "10                                                                                                                                                                         심층 네트워크 모델에 기반한 어선 횡동요 시계열 예측   \n",
      "11                                                                                                                                                                             인공지능 기반 화자 식별 기술의 불공정성 분석   \n",
      "12                                                                                                                                                                 단시간 수중음향 신호를 활용한 합성곱 신경망 기반의 선박 소음 탐지   \n",
      "13                                                                                                                                                                           딥러닝을 이용한 의류 이미지의 텍스타일 소재 분류   \n",
      "14                                                                                                                                                                      템플릿 매칭 및 딥러닝 모델을 이용한 공정 결함 탐지 방법   \n",
      "15                                                                                                                                                      전이학습을 이용한 UNet 기반 건물 추출 딥러닝 모델의 학습률에 따른 성능 향상 분석   \n",
      "16                                                                                                                                                                     백본 네트워크에 따른 사람 속성 검출 모델의 성능 변화 분석   \n",
      "17                                                                                                                                                                            명함 이미지 회전 판단을 위한 딥러닝 모델 비교   \n",
      "18                                                                                                                                                                MEMS 라이다 센서를 활용한 심층학습 기반 조적벽체 결함 인식 기술   \n",
      "19                                                                                                                                                AI-Based Vehicle Damage Repair Price Estimation System   \n",
      "20                                                                                                                                                Transfer Learning for Effective Urolithiasis Detection   \n",
      "21                                                                                                                                                                                 딥러닝 기반 토마토 성숙도 판별 시스템   \n",
      "22                                                                                                        Localization of lung abnormalities on chest X-rays using self-supervised equivariant attention   \n",
      "23                                                                                                                                                             딥러닝 기법을 이용한 농업용저수지 CCTV 영상 기반의 수위계측 방법 개발   \n",
      "24                                                                                                                                                                CT 정도관리에서 ACR 팬텀을 이용한 딥러닝 모델 적용에 관한 연구   \n",
      "25                                                                                                                                                               시계열 이미지 데이터 기반 상품추천을 위한 CNN 모델 성능 비교 연구   \n",
      "26                                                            Classification of Short Circuit Marks in Electric Fire Case with Transfer Learning and Fine-Tuning the Convolutional Neural Network Models   \n",
      "27                                                                                                                                                                  예술 작품 아티스트 분류의 정확도 향상을 위한 CNN 구조 최적화   \n",
      "28                                                                                                                                                      단락흔 및 열흔 판별을 위한 CNN 기반 알고리즘의 모델별 성능 비교 분석에 관한 연구   \n",
      "29                                                                                                                                                                                아크페이스에 지식 증류를 적용한 얼굴인식   \n",
      "30                                                                                                                                                              딥 러닝 분류 모델을 이용한 직하방과 경사각 영상 기반의 벼 출수기 판별   \n",
      "31                                                                                                                                                                       상추잎 너비와 길이 예측을 위한 합성곱 신경망 모델 비교   \n",
      "32                                                                                                                                                        컴퓨터 단층촬영 영상에서 3번 요추부 슬라이스 검출을 위한 최적화 기반 딥러닝 모델   \n",
      "33                                                                                                                                                         범용 AI 컴파일러의 비공개 NPU 코드생성을 위한 공통 인터페이스 설계 및 검증   \n",
      "34                                                                                                                                              X-ray 및 초음파 영상을 활용한 고관절 이형성증 진단을  위한 특징점 검출 딥러닝 모델 비교 연구   \n",
      "35                                                                                                                                                                        도로 침수 탐지를 위한 딥러닝 모델 구현 및 성능 비교   \n",
      "36                                                                                                                                                                  농업용 저수지 CCTV 영상자료 기반 수위 인식 모델 적용성 검토   \n",
      "37                                                                                                                                                                      기계학습을 이용한 스마트 공장 자료의 불량 분류 모형 개발   \n",
      "38                                                                                                                                                                   GPS 및 딥러닝을 이용한 스마트 시티 투어 모바일 애플리케이션   \n",
      "39                                                                                                   Evaluation of Deep Learning Model for Scoliosis Pre-Screening Using Preprocessed Chest X-ray Images   \n",
      "40                                                                                                                                                                     준지도 학습과 전이 학습을 이용한 선로 체결 장치 결함 검출   \n",
      "41                                                                                                                                                               그래프 트랜스포머 기반 농가 사과 품질 이미지의 그래프 표현 학습 연구   \n",
      "42                                                                                                                                                                      초소형 IoT 장치에 구현 가능한 딥러닝 양자화 기술 분석   \n",
      "43                                                                                                                            Dog-Species Classification through CycleGAN and Standard Data Augmentation   \n",
      "44                                                                                          Biometric identification of Black Bengal goat: unique iris pattern matching system vs deep learning approach   \n",
      "45                                                                                  PartitionTuner: An operator scheduler for deep-learning compilers supporting multiple heterogeneous processing units   \n",
      "46                                                                                                                                                                   CT 정도관리를  위한  인공지능  모델  적용에  관한  연구   \n",
      "47                                                                                                                                                        포장층 이상구간에서 획득한 열화상 이미지 해석을 위한 CNN 알고리즘의 적용성 평가   \n",
      "48                                                                                                                                                                          과수화상병 판별을 위한 AI 모델 신뢰성 평가 연구   \n",
      "49                                                                                                                                                                              CNN 기술을 적용한 침수탐지 학습모델 개발   \n",
      "50                                                                                                                                            Identification of seed coat sculptures using deep learning   \n",
      "51                                                                                               A study on the effectiveness of intermediate features in deep learning on facial expression recognition   \n",
      "52                                                                                                                                                                    딥러닝과 XGBoost를 이용한 뇌졸중전조증상진단 애플리케이션   \n",
      "53                                                                                                                                                                       파편 탐지 성능 향상을 위한 딥러닝 초해상도화 효과 분석   \n",
      "54                                                                                                                                                                         전이 학습과 데이터 증강을 이용한 너구리와 라쿤 분류   \n",
      "55                                                                                                                  Self-Gated Rectified Linear Unit for Performance Improvement of Deep Neural Networks   \n",
      "56                                                                                                                                                                           딥러닝 기반의 실시간 상품 진열 상황 추정 시스템   \n",
      "57                                                                                                                                                                     CNN과 학습 가능한 상관필터를 결합한  객체 추적 알고리즘   \n",
      "58                                                                                                                                                                 데이터 재사용을 지원하는 HLS 기반 효율적인 합성곱 가속기의 설계   \n",
      "59                                                              A dual-experience pool deep reinforcement learning method and its application in fault diagnosis of rolling bearing with unbalanced data   \n",
      "60                                                                                                                                                                심층신경망을 이용한 스마트 양식장용 사료 공급 시점 감지 시스템 구현   \n",
      "61                                                                                                                                                          SAM Optimizer를 통한 위내시경 이미지 분류 CADx의 성능 향상 연구   \n",
      "62                                                                                                                                                                       내시경 영상에서의 딥러닝 기반 상부 위장관 랜드마크 식별   \n",
      "63                                                                                                                                                               뇌졸중 동물 모델의 분할 및 분류를 위한 방사광 영상 딥러닝 모델 분석   \n",
      "64                                                                                                                                                               엑스선 이미지 기반 결핵 예측 시스템을 위한 데이터 증강 전처리 조합법   \n",
      "65                                                                                                                                                      딥러닝 기반 농경지 속성분류를 위한 TIF 이미지와 ECW 이미지 간 정확도 비교 연구   \n",
      "66                                                  Deep Learning-Based Feature Extraction from Whole-Body PET/CT Employing Maximum Intensity Projection Images: Preliminary Results of Lung Cancer Data   \n",
      "67                                                                                                                                                                  인공지능 의료 영상인식 기술을 활용한 유방암 영상 진단 기법 연구   \n",
      "68                                                                                                                    A Taekwondo Poomsae Movement Classification Model Learned Under Various Conditions   \n",
      "69                                                                                                                                                          뇌성마비 환자의 자세 불균형 탐지를 위한 스마트폰 동영상 기반 보행 분석 시스템   \n",
      "70                                                                     Automatically Diagnosing Skull Fractures Using an Object Detection Method and Deep Learning Algorithm in Plain Radiography Images   \n",
      "71                                                                                                       A Defect Detection Algorithm of Denim Fabric Based on Cascading Feature Extraction Architecture   \n",
      "72                                                                                                                                                               데이터 증강을 위한 클래스 활성화 맵  기반 Random Erasing   \n",
      "73   Determination of the stage and grade  of periodontitis according to the  current classification of periodontal  and peri-implant diseases and  conditions (2018) using machine  learning algorithms   \n",
      "74                                                                                                                                                                              딥러닝을 이용한 농경지 팜맵 판독 적용 방안   \n",
      "75                                                                                                                                                                 CNN 기반 딥러닝 모델을 통한 폐암 컴퓨터 보조 진단 시스템 개발   \n",
      "76                                                                                                                                                          저비용 비전 시스템과 의미론적 이미지분할을 이용한 포장 패치 자동  탐지 시스템   \n",
      "77                                                                                                              CenterNet Based on Diagonal Half-length and Center Angle Regression for Object Detection   \n",
      "78                                                                                                                                                                              딥러닝을 이용한 음식 이미지 분류 기술 개발   \n",
      "79                                                                                                      Medical Diagonosis System Using AI Evolution Algorithms - CNN Based Chest X-ray Classification -   \n",
      "80                                                                                                     Comparative Analysis of Swin Transformer and Residual Neural Network for Pneumonia Classification   \n",
      "81                                                                        A Study on the Development of Deep Learning Algorithm for Determining External Quality of Welded Parts Using Transfer Learning   \n",
      "82                                                                                                            Research on Damage Identification of Buried Pipeline Based on Fiber Optic Vibration Signal   \n",
      "83                                                                                                                                                                       다중모드 특징융합 기반 배관계 누출판별 앙상블 모델 연구   \n",
      "84                                                                                                                                                                             이미지 인식 기반 국내 자생종 소나무 종 분류   \n",
      "85                                                                                                                                                                          모델 내부/외부 특징량 상관 학습을 통한 지식 증류   \n",
      "86                                                                                                                                                      열화상 영상을 활용한 CNN-Transformer 네트워크의 공장 설비 이상 진단 방법   \n",
      "87                                                                                                                                                           수경재배 환경에서 머신러닝 기반 불량 모종 진단을 위한 증강 데이터 활용 연구   \n",
      "88                                                                                                                                                                            학습 데이터가 없는 모델 탈취 방법에 대한 분석   \n",
      "89                                                                                                                                   Indoor Environment Drone Detection through DBSCAN and Deep Learning   \n",
      "90                                                                                                                                                                         합성곱 신경 회로망 모델을 활용한 흑색종 피부암 진단   \n",
      "91                                                                                                                                                             합리적 가격결정을 위한 전이학습모델기반 아보카도 분류 및 출하 예측 시스템   \n",
      "92                                                                                                                                                              기술 분석과 환경요소를 이용한 주가 예측률 향상을 위한 딥러닝 병렬 모델   \n",
      "93                                                                                                                                                              기상 요소 데이터를 이용한 단기 강수량예측 향상을 위한 딥러닝 병렬 모델   \n",
      "94                                                                                                                                                                             차량 단말기 기반 돌발상황 검지 알고리즘 개발   \n",
      "95                                          An Intercomparison of Deep‑Learning Methods for Super‑Resolution Bias‑Correction (SRBC) of Indian Summer Monsoon Rainfall (ISMR) Using CORDEX‑SA Simulations   \n",
      "96                                                                                                                                                          콘볼루션 신경망의 학습 성능 강화를 위한 그레디언트의 복합적 분석 및 활용 방법   \n",
      "97                                                                                                                                                                      압축센싱 수신기를 이용한 무선 주파수 지문식별 시스템 구현   \n",
      "98                                                                                                                                                                   딥러닝을 활용한 콘텐츠 내 텍스트 폰트  저작권 검출 모델 연구   \n",
      "99                                                                                                                    Cross‑domain health status assessment of three‑phase inverters using improved DANN   \n",
      "100                                                                                                                                   딥러닝 기반 의미론적 분할 기법을 통한 건물 자동추출 연구: 모델의 가중치 경중과 전이학습에 따른 정확도 변화 중심으로   \n",
      "101                                                                                                                                                                 섬유 제조 장비 최적화를 위한 딥러닝 기반 결함 분류 시스템 구축   \n",
      "102                                                                                                                            An effective deep learning model for ship detection from satellite images   \n",
      "103                                                                                                                        Related-key Neural Distinguisher on Block Ciphers SPECK-32/64, HIGHT and GOST   \n",
      "\n",
      "     date  \\\n",
      "0    2023   \n",
      "1    2023   \n",
      "2    2023   \n",
      "3    2023   \n",
      "4    2023   \n",
      "5    2023   \n",
      "6    2023   \n",
      "7    2023   \n",
      "8    2023   \n",
      "9    2023   \n",
      "10   2023   \n",
      "11   2023   \n",
      "12   2023   \n",
      "13   2023   \n",
      "14   2023   \n",
      "15   2023   \n",
      "16   2023   \n",
      "17   2023   \n",
      "18   2023   \n",
      "19   2023   \n",
      "20   2023   \n",
      "21   2023   \n",
      "22   2023   \n",
      "23   2023   \n",
      "24   2023   \n",
      "25   2023   \n",
      "26   2023   \n",
      "27   2023   \n",
      "28   2023   \n",
      "29   2023   \n",
      "30   2023   \n",
      "31   2023   \n",
      "32   2023   \n",
      "33   2023   \n",
      "34   2023   \n",
      "35   2023   \n",
      "36   2023   \n",
      "37   2023   \n",
      "38   2023   \n",
      "39   2023   \n",
      "40   2023   \n",
      "41   2023   \n",
      "42   2023   \n",
      "43   2023   \n",
      "44   2023   \n",
      "45   2023   \n",
      "46   2023   \n",
      "47   2023   \n",
      "48   2023   \n",
      "49   2023   \n",
      "50   2023   \n",
      "51   2023   \n",
      "52   2023   \n",
      "53   2023   \n",
      "54   2023   \n",
      "55   2023   \n",
      "56   2023   \n",
      "57   2023   \n",
      "58   2023   \n",
      "59   2023   \n",
      "60   2023   \n",
      "61   2023   \n",
      "62   2023   \n",
      "63   2023   \n",
      "64   2023   \n",
      "65   2023   \n",
      "66   2023   \n",
      "67   2023   \n",
      "68   2023   \n",
      "69   2023   \n",
      "70   2023   \n",
      "71   2023   \n",
      "72   2023   \n",
      "73   2023   \n",
      "74   2023   \n",
      "75   2023   \n",
      "76   2023   \n",
      "77   2023   \n",
      "78   2023   \n",
      "79   2023   \n",
      "80   2023   \n",
      "81   2023   \n",
      "82   2023   \n",
      "83   2023   \n",
      "84   2023   \n",
      "85   2023   \n",
      "86   2023   \n",
      "87   2023   \n",
      "88   2023   \n",
      "89   2023   \n",
      "90   2023   \n",
      "91   2023   \n",
      "92   2023   \n",
      "93   2023   \n",
      "94   2023   \n",
      "95   2023   \n",
      "96   2023   \n",
      "97   2023   \n",
      "98   2023   \n",
      "99   2023   \n",
      "100  2023   \n",
      "101  2023   \n",
      "102  2023   \n",
      "103  2023   \n",
      "\n",
      "                                                                                                                                                       keywords  \\\n",
      "0                                                              [유아 행동 인식, 딥러닝, ResNet, 영상신호, Children behavior recognition, deep learning, ResNet, video input]   \n",
      "1                   [딥러닝, 흉부 X선 영상, 폐렴 진단, 학습 가능 파라미터, Residual 블록, Deep learning, Chest X-ray image, Pneumonia detection, Trainable parameter, Residual block]   \n",
      "2                                                                        [Convolutional neural network, Medical image segmentation, ResNet, Swin Trans- former]   \n",
      "3                                                                                                  [Classification, Image Processing, ResNet50, Variety, Wheat]   \n",
      "4                                                                                               [GNSS, Jamming, Classification, Transfer Learning, ResNet, SVM]   \n",
      "5                                              [variational autoencoder, generative adversarial network, residual learning, generation, reconstruction, voxel.]   \n",
      "6                 [Cyber security, Malicious attacks, Cyber-physical systems, banking sector, Hierarchical network feature extraction, Wrapper stepwise ResNet]   \n",
      "7                                                       [Deep Residual Network, Multipurpose Watermarking, Perceptual Hashing, Reversible Visible Watermarking]   \n",
      "8                                                                                                            [Facial age estimation, CNN, AlexNet, VGG, ResNet]   \n",
      "9                                                        [Gastroscopy image, gastric lesion, detection, deep learning, RetinaNet, 위내시경 영상, 위 병변, 검출, 딥러닝, 레티나넷]   \n",
      "10                               [fishing boat, capsizing accident, deep learning model, Xception, ResNet50, CRNN, 어선, 전복 사고, 딥러닝 모델, Xception, ResNet50, CRNN]   \n",
      "11                                                                                        [Speaker Identification, Biased dataset, AI Fairness, CNN, VoxCeleb1]   \n",
      "12                                                                    [Underwater acoustics, Shipping noise, Deep learning, Convolutional neural network (CNN)]   \n",
      "13            [Clothing Textile, Material Classification, Image Deep Learning, ResNet, Vision Transformer, 의류 텍스타일, 소재 분류, 이미지 딥러닝, ResNet, Vision Transformer]   \n",
      "14                                     [결함 탐지, 템플릿 매칭, 딥러닝, 공장 자동화, ResNet34, Defect detection, Template Matching, Deep learning, Factory automation, ResNet34]   \n",
      "15                                                                                          [Semantic building segmentation, UNet, VGG19, ResNet50, 의미론적 영상 분할]   \n",
      "16                                                                    [Pedestrian attribute recognition, Deep neural networks, Backbone networks, Resnet, Swin]   \n",
      "17                                         [명함, 이미지 회전, 인공 신경망, 스마트 프린팅 시스템, business cards, image rotation, artificial neural networks, smart printing system]   \n",
      "18                                  [Deep Learning, MEMS LiDAR, 3D Laser Scanner, Masonry Wall, Defect Classification, 심층학습, MEMS 라이다, 3D 레이저 스캐너, 조적벽체, 결함 인식]   \n",
      "19               [차량 손상, 객체 탐지, 차량 제작 및 모델 분류, 이미지 분류, Vehicle Damage, Object Detection, Vehicle Make and Model Classification, ResNet50, Image Classification]   \n",
      "20                                                                    [Urolithiasis, Urinary Calculi, Deep learning, Machine learning, Artificial intelligence]   \n",
      "21                                                                                 [HSV, RGB, ResNet-50, Deep Learning, Ripe degree, Color, Noise cancellation]   \n",
      "22                          [Self-supervised equivariant attention, ResNet50, Siamese network, Weak supervision, Pixel correlation module, Self-attention, CAM]   \n",
      "23                                                                                                              [CCTV, deep  learning, ResNet-50, water  level]   \n",
      "24                                                     [딥러닝, 컴퓨터단층촬영, ACR 팬텀, 정도관리, Deep learning, ResNet18, Computed tomography, ACR phantom, Quality control]   \n",
      "25                                                        [CNN-based product recommendation model, Time series image data, AlexNet, VGG16, ResNet50, MobileNet]   \n",
      "26                                                  [Electric fire, Short-circuit, Convolutional neural network, VGG16, VGG19, Resnet50, InceptionV3, Xception]   \n",
      "27                                       [컴퓨터 비전, 합성곱 신경망, 예술 작품 아티스트 분류, 미세 조정, ResNet50, Computer Vision, CNN, Artwork Classification, Fine-tuning, ResNet50]   \n",
      "28   [1차 단락흔, 2차 단락흔, 열흔, Inception v3, Googlenet, Vgg16, Resnet50, Primary Arc-bead, Secondary Arc-bead, Molten mark Inception v3, Googlenet, Vgg16, Resnet50]   \n",
      "29                                                                                       [AI, Face Recognition, ArcFace, Knowledge Distillation, Deep Learning]   \n",
      "30                                                      [Rice heading date, CNN, Deep learning, Classification model, Image processing, Paddy field monitoring]   \n",
      "31                                    [data augmentation, lettuce imaging, plant growth, transfer learning, vertical farming, 데이터 증강, 상추이미지, 수직농장, 식물 생장, 전이학습]   \n",
      "32                                                                         [Medical image, Optimization, Computerized Tomography Data, Artificial intelligence]   \n",
      "33                                                                                                            [Nueral processing unit, Compiler, Deep learning]   \n",
      "34                           [영유아 고관절 이형성증, 초음파, X-ray, 딥러닝, 특징점 검, Developmental Dysplasia of Hip (DDH), Ultrasound, X-ray, Deep-learning, Keypoint detection]   \n",
      "35                                                 [road flooding prediction, road flooding detection, deep learning, pre-trained models, CNN network layer, .]   \n",
      "36                                               [Machine learning, Water level recognition, Reservoir, CCTV, Image processing, 기계학습, 수위 인식, 저수지, CCTV, 이미지 처리]   \n",
      "37                                              [스마트 공장, 제조 데이터, 빅데이터, 기계학습, 딥러닝, Smart factory, Manufacturing data, Big data, Machine learning, Deep learning]   \n",
      "38               [스마트 투어, 위치 기반 서비스, 딥러닝, 모바일 애플리케이션, 이미지 분류, Smart tourism, Location-based services, Deep learning, Mobile applications, Image classification]   \n",
      "39                                                                         [Scoliosis, Chest X-ray, Deep learning model, Preprocessed image, Data augmentation]   \n",
      "40                                                                                   [Fastener, Semi-supervised, Pretrained, Cost, 선로 체결 장치, 준지도 학습, 전이 학습, 비용]   \n",
      "41         [딥 러닝, 그래프 표현 학습, 사과 품질 분류, 랜덤워크 위치 인코딩, Deep learning, Graph representation learning, Apple Quality Classification, Randomwalk positional encoding]   \n",
      "42                                 [Internet of Things, Deep Learning, Quantization, Model Training, Experimental Configuration, 사물인터넷, 딥러닝, 양자화, 모델 훈련, 실험 구성]   \n",
      "43                                                                                                [CycleGAN, Data Augmentation, DNN, GAN, Image Classification]   \n",
      "44                                         [Biometric Identification, Black Bengal Goat, Deep Learning, Goat Identification, Iris Image, Iris Pattern Matching]   \n",
      "45                                                                             [deep neural network, deep-learning compiler, parallel processing, partitioning]   \n",
      "46                                                                     [CT, Artificial intelligence, Quality control, AAPM CT phantom, Quantitative evaluation]   \n",
      "47                                                                                                             [CNN, Infrared camera, Res Net 101, Squeeze Net]   \n",
      "48                                                                              [Trustworthiness, Grad-CAM, XAI, ResNet50V2, InceptionV3, Xception, Fireblight]   \n",
      "49                                                                                                     [Flooded Road, Deep Learning, Image Classification, CNN]   \n",
      "50                                                                                       [Allium seed coat deep learning image recognition seed coat sculpture]   \n",
      "51                                                                               [Intermediate Feature, Artificial Intelligence, Facial Expression Recognition]   \n",
      "52                                  [뇌졸중 진단, 입술 특징추출, 딥러닝, 의미론적 영상 분할, XGBoost, Stroke, Lips Feature Extraction, Deep Learning, Semantic Segmentation, XGBoost]   \n",
      "53                                                                                                                                                           []   \n",
      "54                              [딥러닝, 머신러닝, 전이 학습, 데이터 증강, 동물 분류, Deep learning, Machine Learning, Transfer Learning, Data Augmentation, Animal Classification]   \n",
      "55                                                                          [Image classificationActivation functionDeep neural networkAccuracyTime complexity]   \n",
      "56                                                                                              [딥러닝, 상품 전시, Deep Learning, Plan-o-gram, Product Display, YOLO]   \n",
      "57                                                           [Computer Vision, Object Tracking, Correlation Filter, Convolution Neural Networks, Deep Learning]   \n",
      "58                                                     [CNN acceleration, high-level synthesis, FPGA, data reuse, CNN 가속기, high-level synthesis, FPGA, 데이터 재사용]   \n",
      "59                                                 [· Deep reinforcement learning · Dual-experience pool · Unbalanced data · Rolling bearing · Fault diagnosis]   \n",
      "60                                                                         [Smart fish farm, LabVIEW, Automatic detection, Deep neural network, Classification]   \n",
      "61                                                     [CADx, Gastric Diagnosis, Classification, Convolution Neural Network, Deep learning, Vision Transformer]   \n",
      "62                                                                                                   [deep learning, artificial intelligence, endoscopic image]   \n",
      "63                                                                                      [Stroke, Synchrotron Radiation Imaging, Biomedical, CNN, Deep Learning]   \n",
      "64                                             [딥러닝, 헬스케어, 데이터 증강, X-선 이미지, 웹 프레임워크, Deep Learning, Health Care, Data Augmentation, X-ray Image, Web Framework]   \n",
      "65                                                                                                            [AI, deep  learning, agricultural  land, FarmMap]   \n",
      "66                                                                 [Deep learning · PET/CT · Maximum intensity projection · Convolutional neural network · FDG]   \n",
      "67                                                                [유방암, 이미지 분류, 이미지 분할, 데이터 증강, Breast cancer, Image recognition, segmentation, Classification]   \n",
      "68   [Pose Estimation, Machine Learning, Deep Learning, Convolutional Neural Network, Deep Convolutional Neural Network, 자세 분류, 머신러닝, 딥러닝, 합성곱 신경망, 심층 합성곱 신경망]   \n",
      "69                                                    [Gait analysis, Postural imbalance detection, Human pose estimation, Tilt correction, Temporal smoothing]   \n",
      "70                                                           [Deep learning · Artificial intelligence · Radiography · Skull fractures · Traumatic brain injury]   \n",
      "71                                                 [Cascading Feature Extraction Architecture, Denim Defect Detection, ImageNet, Robustness, Transfer Learning]   \n",
      "72                                                                               [Data Augmentation, Class Activation Map, Random Erasing, CNN, Generalization]   \n",
      "73                                                                                             [Classification, Deep learning, Machine learning, Periodontitis]   \n",
      "74                                                                              [Deep Learning, Agricultural Land, FarmMap, Aviation Image, 딥러닝, 농경지, 팜맵, 항공영상]   \n",
      "75                                            [CADx, Cancer Diagnosis, Classification, Convolution Neural Network, Deep learning, Lung cancer, Malignant Tumor]   \n",
      "76                                                         [Pavement patch detection, U-net, Automated pavement monitoring system, Inverse perspective mapping]   \n",
      "77                                                                 [Object detection, CenterNet, Prediction stability, Accuracy consistency, Convergence speed]   \n",
      "78                                                                                   [Deep Learning, ResNet, Food Image Classification, 딥러닝, ResNet, 음식 이미지 분류]   \n",
      "79                                                                                  [Medical diagnosis, CNN, ResNet, Pneumonia, Transfer learning, Chest x-ray]   \n",
      "80                         [Swin Transformer, ResNet, 딥러닝 모델, 폐렴 감지, 흉부 엑스레이, Swin Transformer, ResNet, Deep learning models, Pneumonia detection, Chest X-ray]   \n",
      "81                                                                     [Weld quality, Weld appearance, Deep learning, CNN (Convolution Neural Network), ResNet]   \n",
      "82                                                                           [Distributed fiber optic vibration sensing, Pattern recognition, Residual network]   \n",
      "83                                                          [deep learning, multi-mode feature fusion, ensemble, pipe leak detection, weight redistribution, .]   \n",
      "84                                                                                    [Pine Tree, Image Classification, CNN, Data Labeling, Feature Importance]   \n",
      "85                   [Knowledge distillation, model compression, transformer, correlation learning, Image classification, 지식 증류, 모델 압축, 트랜스포머, 상관관계 학습, 이미지 분류]   \n",
      "86                                                                                 [CNN, Transformer, Thermal imaging, Abnormality diagnosis, Factory facility]   \n",
      "87                                                      [hydroponic conditions, anti-caner leaf lettuce, machine learning, deep learning, data augmentation, .]   \n",
      "88                                                                                                                                                           []   \n",
      "89                                                                                                   [DBSCAN, Deep learning, FMCW radar, Object detection, UAV]   \n",
      "90                                                               [피부암, 흑색종, Skin Cancer, Melanoma, VGG-16, ResNet 50, DenseNet 121, Inception V3, WideResnet50]   \n",
      "91                                                                            [딥러닝, 전이 학습, Deep Learning, Transfer Learning, ImageNet, VGG16, ResNet, DenseNet]   \n",
      "92                                                                                     [stock forecast, deep neural network, parallel model, 1D-CNN, ResNet, .]   \n",
      "93                                                                             [precipitation forecast, deep neural network, parallel model, 1D-CNN, ResNet, .]   \n",
      "94               [Highway, Emergency situations, Detection, Scene classification, Artificial intelligence, Vehicle terminal, 고속도로, 돌발상황, 검지, 상황분류, 인공지능, 차량단말기]   \n",
      "95                                                                                         [Regional climate · Indian monsoon · Climate change · Deep learning]   \n",
      "96                                                                     [convolutional neural network, optimization, first-order optimization, gradient descent]   \n",
      "97                                                            [compressed sensing, radio frequency fingerprint identification, convolutional neural network, .]   \n",
      "98                                                                                   [Deep-Learning, Hangul Font, Classification, Segmentation, Text Detection]   \n",
      "99                                                      [Health status assessment, Transfer learning, Deep residual network, Domain adversarial neural network]   \n",
      "100                                           [Semantic Segmentation, Building Detection, Weight of the Model, Transfer Learning, 의미론적 분할, 건물 탐지, 모델 가중치, 전이학습]   \n",
      "101                                [딥러닝, 합성곱 신경망, 다중 클래스, 이미지 분류, 결함 검사, 섬유, Deep learning, CNN, Multi Class, Image classification, Defect inspection, Textile]   \n",
      "102                                                                   [Maritime monitoring · Optical satellite imagery · CNN · Remote sensing · Marine traffic]   \n",
      "103                                                         [Related-key Attack, Neural Cryptanalysis, Distinguisher, Deep Learning, Lightweight Block Ciphers]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    abstract  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                             본 연구에서는 다수의 유아가 등장하는 영상 내의 행동을 인식하기 위하여 딥러닝 기반의 유아 행동 인식 기술을 개발하였다. 유아들의 경우 동일한 행동이라도 표현과 방법이 다양하여 다양한 종류의 입력에 강건하게 분석될 수 있는 딥러닝 모델에 대한 개발이 필요하다. 본 연구에서는 입력 신호를 딥러닝의 입력에 맞도록 처리하고 3D ResNet을 사용하여 행동 인식 알고리즘을 제안하였다. 50명의 유아를 대상으로 13개 행동을 수행하는 영상 자료를 수집하였으며, 실험결과 13개의 행동 인식에 평균 72.21% 정확도를 보였다. 행동 중 서 있기 90.74%, 밀고 당기기 88.89%, 앉기 90.74%의 행동 인식률을 보였다. 향후 본 연구 결과물을 통해 일상생활에서 유아들의 행동 패턴을 자동으로 분석하고 서비스하는 연구에 활용될 수 있다.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None   \n",
      "3                                                                                                                             밀은 대표적인 식량 작물 중 하나이지만 최근 국내 밀 자급률은 1%에 불과하다. 밀의 자급량을 높이기 위해서는 밀 종자의 품종순도를 높여 고품질의 가공물을 얻을 수 있도록 품질 관리를 해야 한다. 본 연구에서는 밀의 품질을 자동으로 판정하는 기술을 마련하기 위하여 딥러닝 알고리즘을 활용하여 밀의 품종과 경질, 연질 여부를 분류하는 모델을 개발하고자 하였다. 우리나라 주요 보급 품종인 금강, 백강, 새금강, 조경, 황금알에 대하여 개발한 이미지 획득 시스템을 이용하여 총 21,256개의 밀 종자 낱알 이미지를 획득하였다. 획득한 이미지에서 낱알의 장축, 단축 길이와 RGB 각각의 평균 색상 값을 계산해 품종, 경도별로 비교하였다. 또한, ResNet50 모델을 이용하여 밀 종자 5품종과 경질, 연질을 분류하는 모델을 개발하였다. 그 결과, 학습, 검증, 테스트 그룹의 분류 정확도는 각각 98.17%, 96.68%, 96.40%를 나타냈다. 테스트 그룹의 혼동행렬을 확인한 결과, 대부분 성공적으로 분류가 이루어졌고 동일 품종에 대해서는 경질, 연질이 100% 정확도로 분류되는 것을 확인하였다. 이를 통해 딥러닝 알고리즘을 이용하여 밀 종자 품종 및 경질, 연질 여부를 판별할 수 있을 것으로 판단된다.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       None   \n",
      "8                                                                                                                                                                                                                                                               얼굴 영상으로부터 나이를 인식하는 기술의 응용분야가 증가함에 따라 이에 대한 연구가활발히 진행되고 있다. 얼굴 영상으로부터 나이를 인식하기 위해서는 나이를 표현하는 특징을추출하고, 추출된 특징으로 나이를 정확하게 분류하는 기술이 필요하다. 최근 영상 인식분야에서 다양한 CNN 기반 딥러닝 모델이 적용되어 성능이 크게 개선되고 있으며, 얼굴 나이인식 분야에서도 성능 개선을 위해 다양한 CNN 기반 딥러닝 모델이 적용되고 있다. 본논문에서는 다양한 CNN 기반 딥러닝 모델의 얼굴 나이 인식 성능을 비교하는 연구를 수행하였다.영상 인식 분야에서 많이 활용되고 있는 AlexNet, VGG-16, VGG-19, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152를 활용하여 얼굴 나이 인식을 위한 모델을 구성하고 성능을비교하였다. 실험 결과에서 ResNet-34를 이용한 얼굴 나이 인식 모델의 성능이 가장 우수하다는것을 확인하였다.   \n",
      "9                                                                              본 연구에서는 위내시경 검사 시에 보조 시스템으로 활용할 수 있도록 RetinaNet 네트워크를 사용하여 위내시경 영상에서의 위 병변의 위치를 자동으로 검출하는 모델을 개발하였다. 위암은 한국이나 일본 등의 아시아권에서 대부분 발생한다. 그러나 위내시경 검사는 동시에 진단이나 치료할 수 있으며, 조기 발견 시 치료 성공확률이 매우 높다. 그러나 실시간으로 진행되는 검사 특성상 숙련도나 경험이 결과에 영향을 주며, 업무의 피로도 상승과 집중력 하락으로 인해 검사의 정확도가 낮아지게 된다. RetinaNet 기반의 backbone 네트워크로 ResNet50, ResNet152, EfficientNetB0, EfficientNetB4 네트워크를 사용하여 학습한 모델의 검출 성능을 확인하고, 각 모델 간의 성능을 비교하였다. RetinaNet 기반 backbone 네트워크별 모델들의 평균 민감도(FP/images)는 ResNet50 73.72%(0.0489), ResNet152 78.26%(0.0458), EfficinetNetB0 79.67%(0.3268), EfficientNetB4 62.66%(0.0448)를 보였다. EfficientNetB0 네트워크는 가장 높은 민감도를 나타냈으나 FP/images가 매우 높게 나타나 두 성능치를 모두 만족하는 네트워크는 ResNet152였다.   \n",
      "10                                                                                                                                                                                                       통계에 따르면 어선의 전복 사고는 전체 전복 사고의 절반 이상을 차지한다. 이는 미숙한 조업, 기상 악화, 정비 미흡 등 다양한 원인으로 발생할 수 있다. 업계 규모와 영향도, 기술 복잡성, 지역적 다양성 등으로 인해 어선은 상선에 비해 상대적으로 연구가 부족한 실정이다. 본 연구에서는 이미지 기반 딥러닝 모델을 활용하여 어선의 횡동요 시계열을 예측하고자 한다. 이미지 기반 딥러닝은 시계열의 다양한 패턴을 학습하여 높은 성능을 낼 수 있다. 이를 위해 Xception, ResNet50, CRNN의 3가지의 이미지 기반 딥러닝 모델을 활용하였다. Xception과 ResNet50은 각각 177, 184개의 층으로 구성되어 있으며 이에 반해 CRNN은 22개의 비교적 얇은 층으로 구성되어 있다. 실험 결과 Xception 딥러닝 모델이 가장 낮은 0.04291의 sMAPE와 0.0198의 RMSE를 기록하였다. ResNet50과 CRNN은 각각 0.0217, 0.022의 RMSE를 기록하였다. 이를 통해 상대적으로 층이 더 깊은 모델의 정확도가 높음을 확인할 수 있다.   \n",
      "11                                                                                                                                                                                                                                                       Covid-19으로 인한 디지털화는 인공지능 기반의 음성인식 기술을 급속하게 발전시켰다. 그러나 이 기술은 데이터셋이 일부 집단에 편향될 경우 인종 및 성차별과 같은 불공정한 사회적 문제를 초래하고 인공지능 서비스의 신뢰성과 보안성을 열화시키는 요인이 된다. 본 연구에서는 대표적인 인공지능의 CNN(Convolutional Neural Network) 모델인 VGGNet(Visual Geometry Group Network), ResNet(Residual neural Network), MobileNet을 활용한 편향된 데이터 환경에서 정확도에 기반한 불공정성을 비교 및 분석한다. 실험 결과에 따르면 Top1-accuracy에서 ResNet34가 여성과 남성이 91%, 89.9%로 가장 높은 정확도를 보였고, 성별 간 정확도 차는 ResNet18이 1.8%로 가장 작았다. 모델별 성별 간의 정확도 차이는 서비스 이용 시 남녀 간의 서비스 품질에 대한 차이와 불공정한 결과를 야기한다.   \n",
      "12                                                                                                         해양에서 인간에 의해 발생하는 대부분의 소음은 어업 및 상업 운송과 관련된 선박 방사소음이 주요한 원인이다. 최근 선박소음을 자동으로 탐지하기 위한 방법으로 딥러닝 기술이 활용되고 있다. 본 연구에서는 1분 단위로 분할한 선박소음 신호 기반의 스펙트로그램 이미지를 합성곱 신경망 기반 학습을 수행하여 근거리 선박소음 및 배경소음을 자동으로 탐지하는 연구를 수행하였다. 현재까지 많이 사용되고 있는 합성곱 신경망 모델인 Inception-V3, ResNet-50, VGG-16와 본 연구에서 제안한 모델을 이용하여 1분 단위의 선박소음을 학습 및 평가를 수행하였다. 분석 결과 F1 점수는 모델별로 각각 Inception-V3 97.42%, ResNet-50 98.42%, VGG-16 98.16%, 제안된 모델은 97.88%로 나타나 선박소음을 탐지함에 있어 준수한 성능이 나타났다. 이 때, 제안된 모델은 F1 점수가 가장 높게 나타난 ResNet-50 모델에 비해 약 1/8의 적은 파라미터로 동등한 탐지 성능을 보이는 것을 확인할 수 있었다. 추후에는 다양한 선박소음 및 선박자동식별장치(AIS, Automatic Identification System) 자료를 동시에 활용하여 원거리 선박소음 또한 자동으로 탐지가 가능할 것으로 판단된다.   \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "17                                                                                                                                                                                                                                                                                                                                             고객이 온라인으로 요청한 명함을 자동으로 명함을 인쇄하는 스마트 명함 인쇄 시스템이 활성화되고 있다. 이때, 문제는 고객이 시스템에 제출한 명함이 비정상일 수 있다는 것이다. 본 논문에서는 인공 지능 기술을 도입하여 명함의 이미지가 비정상적으로 회전됐는지 여부를 판정하는 문제를 다룬다. 명함은 0도, 90도, 180도, 270도 회전한다고 가정하였다. 특별한 인공신경망을 설계하지 않고 기존의 VGG, ResNet, DenseNet 인공신경망을 적용하여 실험하였는데 모든 신경망이 97% 정도의 정확도로 이미지 회전을 분별할 수 있었다. DenseNet161은 97.9%의 정확도를 보였고 ResNet34도 97.2%의 정밀도를 보였다. 이는 문제가 단순할 경우, 복잡한 인공신경망이 아니어도 충분히 좋은 결과를 낼 수 있음을 시사한다.   \n",
      "18                                                                                                        건축물의 유지관리 및 안전점검은 대부분 점검자의 육안으로 진행하여 많은 시간과 인력이 소모된다는 문제점이 있다. 이를 보완하기 위해 영상처리기술 및 인공지능을 활용한 결함 인식 기술 개발이 활발하게 진행되고 있다. 하지만 기존의 영상처리 기법은 카메라를 통해 얻은 이미지를 분석하는 방식으로 주변 환경에 따라 성능이 변하는 한계가 있다. 최근 이를 해결하기 위해 3D 레이저 스캐닝 센서를 이용한 결함인식 방법을 개발하였으나 장치의 가격이 비싸 활발한 활용이 어렵다는 단점이 있다. 이에 본 연구는 기존 스캐닝 장치보다 가격이 저렴하고 신뢰할만한 성능을 보이고 있는 MEMS 라이다 센서를 이용해 조적벽체의 결함을 인식할 수 있는 기술을 개발하였다. 해당 연구는 조적벽체를 대상으로 하였으며, 실험실 환경에서 여러 종류의 결함을 가진 시험체를 제작하여 데이터를 획득하였다. 조적벽체 결한 인식 방법으로 인공지능을 활용한 연구에서 많이 사용하고 있는 ResNet-50과 VGG16 모델을 사용하여 결함을 인식하였으며, 성능평가 결과 ResNet-50은 98.75%, VGG16은 96.88%의 정확도를 보여주었다. 해당 연구 결과는 모바일 3D 레이저 스캐닝 장치와 결합하여 조적벽체의 실시간 결함 인식 기술 개발에 활용될 수 있을 것으로 판단된다.   \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "30                                                         벼의 출수기를 추정하는 것은 농업생산성과 관련된 중요한 과정 중 하나이지만 세계적인 이상기후의 증가로 벼의 출수기를 추정하는 것이 어려워지고 있다. 본 연구에서는 CNN 분류모델을 사용하여 다양한 영상 데이터에서 벼의 출수기를 추정하려고 시도하였다. 드론과 타워형 영상관측장치 그리고 일반 RGB 카메라로 촬영된 직하방과 경사각 영상을 수집하였다. 수집 한 영상은 CNN 모델의 입력데이터로 사용하기 위해서 전처리를 진행하였고, 사용된 CNN 아키텍처는 이미지 분류 모델에서 일반적으로 사용되는 ResNet50, InceptionV3 그리고 VGG19 를 사용하였다. 각각의 아키텍처는 모델의 종류, 영상의 유형과 관계없이 0.98 이상의 정확도를 나타내었다. 또한 CNN 분류 모델이 영상의 어떤 특징을 보고 분류하였는지 시각적으로 확인하기 위해서 Grad-CAM 을 사용하였다. Grad-CAM 결과 CNN 분류 모델은 벼의 출수를 이삭의 형태에 높은 가중치를 두어 분류 하는 것을 확인하였다. 다음으로 작성된 모델이 실제 논 포장 모니터링 이미지에서 벼의 출수기를 정확하게 추정하는지 확인하였다. 각각 다른 지역 4 개의 벼 포장에서 벼의 출수 기를 약 하루정도의 차이로 추정하는 것을 확인하였다. 이 방법을 통해서 다양한 논 포장의 모니터링 이미지를 활용하여 자동적이고 정량적으로 벼의 출수기를 추정 할 수 있다고 판단된다.   \n",
      "31   식물의 잎의 크기나 면적을 아는 것은 생장을 예측하고 실내농장의 생산성의 향상에 중요한 요소이다. 본 연구에서는 상추 잎 사진을 이용해 엽장과 엽폭을 예측할 수 있는 CNN기반모델을 연구하였다. 데이터의 한계와 과적합 문제를 극복하기 위해 콜백 함수를 적용하고, 모델의 일반화 능력을 향상시키기 위해 K겹 교차 검증을 사용했다. 또한 데이터 증강을 통한 학습데이터의 다양성을 높이기 위해 image generator를 사용하였다. 모델 성능을 비교하기 위해 VGG16, Resnet152, NASNetMobile 등 사전학습된 모델을 이용하였다. 그 결과너비 예측에서 R2 값 0.9436, RMSE 0.5659를 기록한 NASNetMobile이가장 높은 성능을 보였으며 길이 예측에서는 R2 값이 0.9537, RMSE가 0.8713로 나타났다. 최종 모델에는NASNetMobile 아키텍처, RMSprop 옵티마이저, MSE 손실 함수, ELU 활성화함수가 사용되었다. 모델의 학습 시간은Epoch당 평균 73분이 소요되었으며, 상추 잎 사진 한 장을 처리하는 데 평균 0.29초가 걸렸다. 본 연구는 실내 농장에서 식물의 엽장과 엽폭을 예측하는 CNN 기반 모델을 개발하였고이를 통해 단순한 이미지 촬영만으로도 식물의 생장 상태를신속하고 정확하게 평가할 수 있을 것으로 기대된다. 또한 그결과는 실시간 양액 조절 등의 적절한 농작업 조치를 하는데 활용됨으로써 농장의 생산성 향상과 자원 효율성을 향상시키는데 기여할 것이다.   \n",
      "32                                                                                                                                                                                              본 논문에서는 근감소증의 발병 여부와 정도를 확인하기 위해 3번 요추부 (L3) CT 영상을 검출하는 딥러닝 모델을 제안하는 것이다. 또한, CT 데이터 내에 L3 레벨과 L3 레벨이 아닌 부분의 데이터 불균형으로 인한 성능 저하의 문제점을 오버샘플링 비율과 클래스 가중치를 설계변수로 하는 최적화 기법을 제시하고자 한다. 모델 학습 및 검증을 위하여 강릉아산병원에 내원한 전립선암 환자 104명, 방광암 환자 46명의 총 150명의 전신 CT 영상이 활용되었다. 딥러닝 모델은 ResNet50을 활용하였으며, 최적화기법의 설계변수로는 모델 하이퍼파라미터 5종과 데이터 증강비율 및 클래스 가중치로 선정하였다. 제안하는 최적화 기반의 L3 레벨 추출 모델은 대조군 (하이퍼파라미터 5종만을 최적화한 모델)과 비교하여 중간 L3 오차가 약  1.0 슬라이스 감소한 것을 확인할 수 있었다. 본 연구결과를 통하여 정확한 L3 슬라이스 검출이 가능하며, 추가적으로 데이터 증강을 통한 오버 샘플링과 클래스 가중치 조절을 통해 데이터 불균형 문제를 효과적으로 해결할 수 있는 가능성을 제시할 수 있다.   \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                          본 논문에서는 NPU(Neural Processing Units)의 제조사별 비공개 백엔드 컴파일러와 범용 AI 컴파일러를 연결할 수 있는 공통 인터페이스를 제안한다. 이를 통해 다양한 AI 모델에 대한 지원과 기업 자산 보호가 가능하다. 제안된 인터페이스는 ONNX 표준을 따르며, ETRI의 NEST-C 컴파일러와 오픈엣지의 ENLIGHT 컴파일러를 해당 인터페이스로 통합했다. 실험 결과, 통합된 컴파일러는 ENLIGHT만을 사용한 것과 Resnet50과 MobileNetV2 두 종류의 모델에 대해서 100% 결과가 일치했다. 따라서 제안된 인터페이스는 NPU 백엔드 컴파일러의 범용성을 향상하면서도 비공개성을 유지할 수 있는 유용한 방안을 제공한다.   \n",
      "34                                                                      고관절 이형성증(Developmental Dysplasia of Hip, DDH)은 영유아 성장기에 흔히 발생하는 병리학적 상태로, 영유아의 성장을 방해하고 잠재적인 합병증을 유발하는 원인 중 하나이며 이를 조기에 발견하고 치료하는 것은 매우 중요하다. 기존의 DDH 진단 방법으로는 촉진법과 X-ray 또는 초음파 영상 기반 고관절에서의 특징점 검출을 이용한 진단 방법이 있지만 특징점 검출 시 객관성과 생산성에 제한점이 존재한다. 본 연구에서는 X-ray 및 초음파 영상을이용한 딥러닝 모델 기반 특징점 검출 방법을 제시하고, 다양한 딥러닝 모델을 이용하여 특징점 검출의 성능을 비교분석하였다. 또한, 부족한 의료 데이터를 보완하는 방법인 다양한 데이터 증강 기법을 제시하고 비교 평가하였다. 본연구에서는 Residual Network 152(ResNet152) 및 Simple & Complex augmentation 기법을 적용하였을 때 가장 높은 특징점 검출 성능을 보여주었으며, X-ray 영상에서 평균 Object Keypoint Similarity(OKS)가 약 95.33 %, 초음파영상에서는 약 81.21 %로 각각 측정되었다. 이러한 결과는 고관절 초음파 및 X-ray 영상에서 딥러닝 모델을 적용함으로써 DDH 진단 시 특징점 검출에 관한 객관성과 생산성을 향상시킬 수 있음을 보여준다.   \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "36                                                                                                                                 농업용 저수지는 농업용수 공급에 있어서 매우 중요한 생산기반시설로, 우리나라 농업용수의 60% 정도를 공급하고 있다. 다만, 여러 문제로 인해 농업용수의 효율적인 공급에 어려움이 발생하고 있으며, 효과적인 공급 및 관리 체계 구현을 위한 정확한 실시간 저수위 혹은 저수량 추정이 필요하다. 본 연구에서는 영상정보를 활용한 딥러닝 기반 농업용 저수지 수위 인식 모델을 제안하였다. 개발한 모델은 (1) CCTV 영상정보 자료 수집 및 분석, (2) U-Net 이미지 분할 방법을 통한 입력 자료 생성, 그리고 (3) CNN과 ResNet 모델을 통한 수위 인식 세 단계로 구성된다. 모델은 두 농업용 저수지(G저수지와 M저수지)의 영상자료와 저수위 시계열자료를 활용하여 구현하였다. 적용 결과 이미지 분할 모델의 성능은 매우 우수한 것으로 나타났으며, 수위 인식 모델의 경우 수위 분류 계급구간에 따라 성능이 상이한 것으로 나타났다. 특히 영상자료의 픽셀 변동이 클수록 정확도 80% 이상이 확보 가능한 것으로 확인되었으나, 그렇지 않은 경우, 정확도가 50% 수준인 것으로 나타났다. 본 연구에서 개발한 모델은 향후 이미지 자료가 추가로 확보될 경우, 그 활용도 및 정확도가 더 높아질 것으로 기대한다.   \n",
      "37                  정보기술의 발전으로 인해 현대사회의 다양한 분야에서 ICT 기술과의 융합이 가속화되면서 제조업 분야에서도 인공지능과 자동화 기술을 활용한 스마트 공장이 등장하였다. 스마트 공장은 실시간으로 자료를 수집하고 이를 분석하여 최적의 의사결정을 진행함으로써 생산 과정의 문제점을 개선하고 생산성과 효율성을 향상시키는 것을 목표로 한다. 본 연구에서는 스마트 공장에서 수집된 자료에 기계학습과 딥러닝 모형을 적용하여 제조공정의 생산성과 효율성을 향상시킬 수 있는 프레임워크를 구축하고자 한다. 먼저 생산 과정에서 발생하는 온도와 압력에 관련된 공정 환경 자료를 기계학습 방법인 로지스틱 회귀, 랜덤 포레스트, 그래디언트 부스티드 트리, 지지벡터기계를 사용하여 불량을 1차적으로 탐지한다. 다음으로 용접을 마치고 난 후 촬영된 제품의 용접 이미지 자료에 딥러닝 기법을 적용하여 불량을 탐지한다. 이를 위해 AlexNet, VGG-16, ResNet과 같은 합성곱 신경망 기반 모형을 사용하였다. 이후 각 자료에 대해 정확도, 정밀도, 재현율 등의 성능평가지표를 사용하여 구현된 모형들의 성능을 비교하고, 각 자료에 대해 가장 우수한 성능을 보이는 모형을 최종 모형으로 선택하였다. 공정 환경 및 이미지 자료에서 선택된 최적의 모형은 높은 정확도로 불량을 탐지해 낼 수 있었으며 이를 실제 제조공정에 적용하여 자동화된 불량 탐지 시스템을 구축한다면 공정의 생산성과 효율성을 크게 향상시킬 수 있을 것이라 기대된다.   \n",
      "38                                                                                                                                                                                                                                     본 논문에서는 GPS 및 딥러닝 기반의 스마트 시티 투어 모바일 애플리케이션을 제안한다. 제안된 애플리케이션은 딥러닝을 이용하여 랜드마크 이미지 인식 기능을 제공하고 위치기반 서비스를 통해 현재 사용자의 위치에 기반하여 주변 관광지 정보를 제공한다. 이미지 인식 서비스는 사용자가 애플리케이션에 랜드마크 이미지를 업로드하면 딥러닝 모델을 통해 이미지를 식별하고, 해당 랜드마크의 세부 정보를 제공한다. 랜드마크 이미지는 ResNet-D 모델을 사용하여 서울시에 있는 13개의 랜드마크에 대해 학습을 진행하여 최종 모델의 평균 분류 정확도는 약 0.957, 평균 F1-Score는 약 0.938로 좋은 성능을 얻었다. 또한 애플리케이션 내에서 위치기반 서비스를 통해 사용자의 현재 위치로부터 가까운 랜드마크의 정보를 알 수 있으며, 각 랜드마크의 정보를 다른 사용자에게 공유할 수 있다. 본 애플리케이션을 통해 여행 중이거나 여행을 계획 중인 외국인들에게 수도 서울에 대한 정보를 신속하게 제공하여 여행에 도움을 줄 수 있다.   \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "40           오늘날 인공지능 산업이 발전함에 따라 여러 분야에 걸쳐 인공지능을 통한 자동화 및 최적화가 이루어지고 있다. 국내의 철도 분야 또한 지도 학습을 이용한 레일의 결함을 검출하는 연구들을 확인할 수 있다. 그러나 철도에는 레일만이 아닌 다른 구조물들이 존재하며 그중 선로 체결 장치는 레일을 다른 구조물에 결합시켜주는 역할을 하는 장치로 안전사고의 예방을 위해서 주기적인 점검이 필요하다. 본 논문에는 선로 체결 장치의 데이터를 이용하여 준지도 학습(semi-supervised learning)과 전이 학습(transfer learning)을 이용한 분류기를 학습시켜 선로 안전 점검에 사용되는 비용을 줄이는 방안을 제안한다. 사용된 네트워크는 Resnet50이며 imagenet으로 선행 학습된 모델이다. 레이블이 없는 데이터에서 무작위로 데이터를 선정 후 레이블을 부여한 뒤 이를 통해 모델을 학습한다. 학습된 모델의 이용하여 남은 데이터를 예측 후 예측한 데이터 중 클래스 별 확률이 가장 높은 데이터를 정해진 크기만큼 훈련용 데이터에 추가하는 방식을 채택하였다. 추가적으로 초기의 레이블된 데이터의 크기가 끼치는 영향력을 확인해보기 위한 실험을 진행하였다. 실험 결과 최대 92%의 정확도를 얻을 수 있었으며 이는 지도 학습 대비 5% 내외의 성능 차이를 가진다. 이는 제안한 방안을 통해 추가적인 레이블링 과정 없이 비교적 적은 레이블을 이용하여 분류기의 성능을 기존보다 향상시킬 수 있을 것으로 예상된다.   \n",
      "41                                                                                                                        최근 농가의 사과 품질 선별 작업에서 인적자원의 한계를 극복하기 위해 합성곱 신경망(CNN) 기반 시스템이 개발되고 있다. 그러나 합성곱 신경망은 동일한 크기의 이미지만을 입력받기 때문에 샘플링 등의 전처리 과정이 요구될 수 있으며, 과도 샘플링의 경우 화질 저하, 블러링 등 원본 이미지의 정보손실 문제가 발생한다. 본 논문에서는 위 문제를 최소화하기 위하여, 원본 이미지의 패치 기반 그래프를 생성하고 그래프 트랜스포머 모델의 랜덤워크 기반 위치 인코딩 방법을 제안한다. 위 방법은 랜덤워크 알고리즘 기반 위치정보가 없는 패치들의 위치 임베딩 정보를 지속적으로 학습하고, 기존 그래프 트랜스포머의 자가 주의집중 기법을 통해 유익한 노드정보들을 집계함으로써 최적의 그래프 구조를 찾는다. 따라서 무작위 노드 순서의 새로운 그래프 구조와 이미지의 객체 위치에 따른 임의의 그래프 구조에서도 강건한 성질을 가지며, 좋은 성능을 보여준다. 5가지 사과 품질 데이터셋으로 실험하였을 때, 다른 GNN 모델보다 최소 1.3%에서 최대 4.7%의 학습 정확도가 높았으며, ResNet18 모델의 23.52M보다 약 15% 적은 3.59M의 파라미터 수를 보유하여 연산량 절감에 따른 빠른 추론 속도를 보이며 그 효과를 증명한다.   \n",
      "42                                                                                                                                                                                                                                                                                               많은 연산량을 가진 딥러닝은 초소형 IoT 장치나 모바일 장치에 구현하기가 어렵다. 최근에는 이러한 장치에서도 딥러닝을 구현할 수 있도록 모델의 연산량을 줄이는 딥러닝 경량화 기술이 소개되었다. 양자화는 연속적인 분포를가지는 파라미터 값들을 고정된 비트의 이산 값으로 표현하여 모델의 메모리 및 크기 등을 줄여 효율적으로 사용할수 있는 경량화 기법이다. 그러나 양자화로 인한 이산 값 표현으로 인해 모델의 정확도가 낮아지게 된다. 본 논문에서는정확도를 개선할 수 있는 다양한 양자화 기술을 소개한다. 먼저 기존 양자화 기술 중 APoT와 EWGS를 선택하여 동일한 환경에서 실험을 통해 결과를 비교 분석하였다. 선택된 기술은 ResNet모델에서 CIFAR-10 또는 CIFAR-100 데이터 세트로 훈련되고 테스트 되었다. 실험 결과 분석을 통해 기존 양자화 기술의 문제점을 파악하고 향후 연구에 대한방향성을 제시하였다.   \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "54                                                                                                                                                                                     최근 인간의 활동 범위가 증가함에 따라 외래종의 유입이 잦아지고 있고 환경에 적응하지 못해 유기된 외래종 중 2020년부터 유해 지정 동물로 지정된 라쿤이 문제가 되고 있다. 라쿤은 국내 토종 너구리와 크기나 생김새가 유사하여 일반적으로 포획하는데 있어서 구분이 필요하다. 이를 해결하기 위해서 이미지 분류에 특화된 CNN 딥러닝 모델인 VGG19, ResNet152V2, InceptionV3, InceptionResNet, NASNet을 사용한다. 학습에 사용할 파라미터는 많은 양의 데이터인 ImageNet으로 미리 학습된 파라미터를 전이 학습하여 이용한다. 너구리와 라쿤 데이터셋에서 동물의 외형적인 특징으로 분류하기 위해서 이미지를 회색조로 변환한 후 밝기를 정규화하였으며, 조정된 데이터셋에 충분한 학습을 위한 데이터를 만들기 위해 좌우 반전, 회전, 확대/축소, 이동을 이용하여 증강 기법을 적용하였다. 증강하지 않은 데이터셋은 FCL을 1층으로, 증강된 데이터셋은 4층 으로 구성하여 진행하였다. 여러 가지 증강된 데이터셋의 정확도를 비교한 결과, 증강을 많이 할수록 성능이 증가함을 확인하였다.   \n",
      "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "58                                                                                                                                                                                                                                                          본 논문에서는 HLS(High Level Synthesis)를 사용하여 개발, 검증이 쉬우면서 확장 가능한 CNN(Convolution Neural Network) 가속기를 설계하였다. DRAM 접근량을 줄이기 위한 타일 버퍼와 2차원 PE 배열을 가지는 weight stationary 가속기 구조를 설계하고, HLS의 디렉티브을 통해 PE 및 태스크 병렬성을 효율적으로 활용하는 가속기를 구현하였다. 한편, DRAM 접근을 생략할 수 있는 경우, 동적으로 타일 버퍼에서 데이터 재사용할 수 있도록 HLS 라이브러리의 Stream(FIFO)을 이용하여 구현하였다. 이를 통해 32×32 PE에서 13.7% 가속하여, Xilinx Alveo U200 FPGA에서 ResNet50 추론을 49 ms까지 가속하였다. Vitis HLS를 통해 PE의 개수를 손쉽게 확장하여, 서버 수준인 64×64 PE까지 835 GOPS의 성능 및 35.3 GOPS/W의 전력효율을 보이는 것을 확인하였다.   \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "64                                                                                                                                                        최근 헬스케어 및 의료 분야에서 컨볼루션 신경망(CNN) 모델을 활용한 X-선 이미지 분석 연구가 활발히 이루어지고 있다. X 선은 폐렴, 결핵, 유방암 등 다양한 흉부 질환을 비침습적으로 검사하는 주요 도구로서, 비용 효율적이며 많은 검사가 가능하다는장점을 갖는다. 하지만 기존의 X-선 이미지 기반 질환 진단 인공지능 모델은 복잡한 아키텍처를 가지며 많은 파라미터와 대량의데이터가 필요한 한계를 가진다. 본 연구에서는 이러한 문제를 해결하기 위해 적은 양의 X-선 이미지 데이터로 높은 결핵 예측 정확도를 달성할 수 있는 효과적인 전처리 조합법을 탐색하고 제안한다. 본 연구에서는 대표적인 데이터 증강법인 Random Erase,Random Flip, Random Augmentation을 함께 사용하는 전처리 조합법을 제안하였으며, 이를 ResNet50 모델과 EfficientNet-b0 모델에 적용하여 각 평균 11%, 9%의 성능 향상을 확인할 수 있었다. 또한, 전처리 조합이 적용된 모델을 쉽게 사용할 수 있도록 웹 프레임워크를 개발하였다. 사용자는 웹 프레임워크를 통해 입력 이미지를 수정할 수 있으며, 인공지능 결핵 판별 결과를 얻을 수 있다.   \n",
      "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "67                                                                                                                                                                                                                본 논문은 인공지능 의료영상인식 기술을 활용하여 유방암 진단을 하는 기법에 대해 연구하였다. 이를 위해 맘모그래피 이미지, 초음파 이미지, 조직병리 이미지를 사용하여, 이미지 분류 기법과 이미지 분할 기법을 통해 유방암 분류와 해당 환부 위치를 추론하는 과정의 정확도 향상을 위한 전략들에 대해 연구하였다. 즉, 성능 최적화를 위해 여러 이미지 분류 기술 및 이미지 분할 기법들과 관련 손실함수들 중에 각 의료영상 데이터 별 최적의기술을 선별하였고, 해당 성능 최적화를 위한 데이터 증강 기법을 제시하였다. 제시된 방법들을 통해 분석한 결과, 필터 기반의 데이터 증강 기술을 활용하면 이미지 분류 기술에서는 ResNet50이 가장 좋은 성능을, 이미지 분할기술로는 맘모그래피 영상과 초음파 영상 모두 UNet 기술이 가장 좋은 성능을 나타내었다. 해당 기술을 적용한결과, 맘모그래피 영상에서의 이미지 분할 작업에서는 33.3%, 초음파 영상에서의 이미지 분할 작업에서는 29.9%, 조직병리 이미지에서의 이미지분류 작업에서는 22.8%의 성능 향상을 나타내었다.   \n",
      "68                                                                                                                                                                                                                                                                              태권도 겨루기의 전자호구, 축구의 VAR 등 스포츠에서 기술 발전이 고도화되고 있다. 하지만태권도 품새는 사람이 직접 자세를 눈으로 보고 판단하며 지도하기 때문에 때로는 대회의 현장에서 판정시비가 일어난다. 본 연구는 인공지능을 이용하여 태권도 동작을 더 정확하게 판단하고평가할 수 있는 인공지능 모델을 제안한다. 본 연구에서는 촬영 및 수집한 데이터를 전처리한 후학습, 테스트, 검증 세트로 분리한다. 분리한 데이터를 각 모델과 조건을 적용하여 학습한 후 비교하여 가장 좋은 성능의 모델을 제시한다. 각 조건의 모델은 정확도, Precision, Recall, F1-Score, 학습 소요 시간, Top-n error의 값을 비교하였고 그 결과 ResNet50과 Adam을 사용한 조건에서 학습한 모델의 성능이 가장 우수한 것으로 나타났다. 본 연구에서 제시한 모델을 활용하여 교육 현장이나 대회 등 다양한 방면에서 활용할 수 있을 것으로 기대한다.   \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             본 연구는 20대와 한국인을 대상으로 한 건강관리 애플리케이션의 음식 이미지 분류 모델을 개선하는 것을 목표로 진행되었다. AI Hub에서 546,194개의 이미지를 수집하여 175개의 음식 클래스를 구성하였으며, ResNet 인공지능 모델을 학습하고 검증하였다. 추가적으로, 실제 촬영한 음식 이미지에 대한 인식 정확도가 상대적으로 낮게 나타나는 원인에 대해 고찰하고, 이를 해결하기 위한 방안으로 모델 성능을 최적화를 위한 다양한 방법을 분석하였다.   \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "83                                                                                                                                                                                                                                                                                                                                              본 논문은 플랜트 배관계의 노후화 문제로 인해 발생하는 미세 누출을 탐지하기 위한 다중모드 특징융합을 이용한 가중치 재분배 앙상블 모델을 제안한다. 수집된 데이터는 2D 주파수 패턴 특징과 2D RMS 패턴 특징으로 변환되었으며, 여러 센서에서 추출된 다양한 도메인 특징들이 서로 결합되어 볼륨 특징으로 구성하였다. 실험을 위해 ResNet 기반의 단일 모델을 설계하여 다양한 볼륨 특징들을 이용한 앙상블 구조를 조합하였다. 또한, 다수의 예측 모델을 결합하는 과정에서 발생할 수 있는 성능 불균형 문제를 해결하기 위해, 소프트맥스 함수를 기반으로 한 가중치 재분배를 적용하였다. 실험 결과, 주파수 및 RMS 볼륨 특징을 활용한 센서별 앙상블 모델이 98.91%라는 가장 높은 분류 정확도를 제공하는 것을 실험적으로 관찰할 수 있었다.   \n",
      "84                                                                                                                                                                   국내 소나무과 나무는 국내 자생 침엽수의 절반을 차지할 만큼 종 다양성이 가장 높다. 소나무가 국내 산림의 60% 이상을 차지한 적도 있었지만, 현재는 재선충, 산불 등으로 그 비율이 25%로 감소하였다. 국내 자생종 소나무의 종 분류를 위해 국립생물자원관에서 보유 중인 표준 이미지 데이터셋을 사용하여 3종의 소나무를 분류하였다. 또한, 성능이 검증된 ResNet 50과 같은 사전 학습모델을 변형하는 대신 이미지 인식 모델을 직접 구현하여 종 분류를 진행 85%의 정확도를 나타내었고, 이후, 인식 과정에서 도출되는 특징들을 판별하였다. 이미지 크기 및 증식이 분류에 미치는 영향을 파악하고자 이미지 크기 및 이미지 증식을 통한 성능 평가를 수행 이미지 크기는 약 3% 그리고 이미지 증식의 경우는 약 6.4% 성능 향상을 가져왔다. 또한, 데이터 레이블링 방식의 기계학습 모델과도 분류 성능을 비교 정리하였다. 이미지 인식 과정에서 종 분류에 가중치가 높게 반영된 특징을 추출하였으며, 이를 레이블링 기법에 사용된 특징 중요도와 비교, 레이블링 기법과 이미지 인식 기반에서 종 분류에 가장 유효하게 나타난 특징은 유사한 것으로 나타났다.   \n",
      "85                                                                                                                                                                                                                                                                   본 논문에서는 이종 모델의 특징맵 간 상관관계인 외부적 상관관계와 동종 모델 내부 특징맵 간 상관관계인 내부적 상관관계를 활용하여 교사 모델로부터 학생 모델로 지식을 전이하는 Internal/External Knowledge Distillation (IEKD)를 제안한다. 두 상관관계를 모두 활용하기 위하여 특징맵을 시퀀스 형태로 변환하고, 트랜스포머를 통해 내부적/외부적 상관관계를 고려하여 지식 증류에 적합한 새로운 특징맵을 추출한다. 추출된 특징맵을 증류함으로써 내부적 상관관계와 외부적 상관관계를 함께 학습할 수 있다. 또한 추출된 특징맵을 활용하여 feature matching을 수행함으로써 학생 모델의 정확도 향상을 도모한다. 제안한 지식 증류 방법의 효과를 증명하기 위해, CIFAR-100 데이터 셋에서 “ResNet-32×4/VGG-8” 교사/학생 모델 조합으로 최신 지식 증류 방법보다 향상된 76.23% Top-1 이미지 분류 정확도를 달성하였다.   \n",
      "86                                                                                                                      본 논문에서는 열화상 영상을 이용한 공장 설비 이상 진단에 최적화된 딥러닝 알고리즘을 제안한다. 이를 위하여 대조도 향상 알고리즘으로 열화상 영상의 대조도를 명확하게 변환하여 가장자리 정보를 강화하여 준다. 그 후에 Convolution Neural Network(CNN)와 Transformer Network 각각의 장점만을 이용하여 개발된 CvT(Convolutional vision Transformer)를 열화상 영상 기반의 고장 설비 이상 진단에 적합하게 수정한 modified CvT 개발을 통하여 공장 설비의 이상을 진단한다. AI Hub에서 제공되는 열화상 영상 중에서 공장 설비의 정상 및 이상 영상들을 추출하여 실험을 진행하였으며, 이를 통하여 기존 컴퓨터 비전 분야에서 보편적으로 사용되고 있는 CNN 기반의 ResNet, EfficientNet 그리고 Transformer 기반의 ViT(Vision Transformer), SwinT(Swin Transformer)보다 높은 정확도인 98.79%의 우수한 성능을 확인하였다. 결론적으로 CNN과 Transformer 융합 네트워크를 활용하였을 때 다른 열화상 영상을 이용한 공장 설비 이상 진단 알고리즘보다 우수한 성능을 보여준다는 것을 확인하였다.   \n",
      "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "88                                                                                                                                                                                 딥뉴럴네트워크 모델의 취약점으로 모델 탈취 방법이 있다. 이 방법은 대상 모델에 대하여 여러번의 반복된 쿼리를 통해서 유사 모델을 생성하여 대상 모델의 예측값과 동일하게 내는 유사 모델을 생성하는 것이다. 본 연구에서, 학습 데이터가 없이 대상 모델을 탈취하는 방법에 대해서 분석을 하였다. 생성 모델을 이용하여 입력 데이터를 생성하고 대상 모델과 유사 모델의 예측값이 서로 가까워지도록 손실함수를 정의하여 유사 모델을 생성한다. 이 방법에서 대상 모델의 입력 데이터에 대한 각 클래스의 logit(로직) 값을 이용하여 경사하강법으로 유사 모델이 그것과 유사하도록 학습하는 과정을 갖는다. 실험 환경으로 pytorch 머신러닝 라이브러리를 이용하였으며, 데이터셋으로 CIFAR10과 SVHN을 사용하였다. 대상 모델로 ResNet 모델을 이용하였다. 실험 결과로써, 모델 탈취방법은 CIFAR10에 대해서 86.18%이고 SVHN에 대해서 96.02% 정확도로 대상 모델과 유사한 예측값을 내는 유사 모델을 생성하는 것을 볼 수가 있었다. 추가적으로 모델 탈취 방법에 대한 고려사항와 한계점에 대한 고찰도 분석하였다.   \n",
      "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "91                                                                                                                                                                                                                                                                                                                                      타임지가 선정한 슈퍼푸드이며, 후숙 과일 중 하나인 아보카도는 현지가격과 국내 유통 가격이 크게 차이가 나는 식품 중 하나이다. 이러한 아보카도의 분류과정을 자동화한다면 다양한 분야에서 인건비를 줄여 가격을 낮출 수 있을 것이다. 본 논문에서는 아보카도의 데이터셋을 크롤링을 통하여 제작하고, 딥러닝 기반 전이학습모델을 다수 사용하여, 최적의 분류모델을 만드는 것을 목표로 한다. 실험은 제작한 데이터셋에서 분리한 데이터셋에서 딥러닝 기반 전이학습모델에 직접 대입하고, 해당 모델의 하이퍼 파라미터를 Fine-tuning하며 진행하였다. 제작된 모델은 아보카도의 이미지를 입력하였을 때, 해당 아보카도의 익은 정도를 99% 이상의 정확도로 분류하였으며, 아보카도 생산 및 유통가정의 인력감소 및 정확성을 높일 수 있는 데이터셋 및 알고리즘을 제안한다.   \n",
      "92                                                                                                                                                                                                                                                  본 연구는 주가 데이터, 기술 분석 데이터, 환경요소 데이터를 이용하여, 주가예측을 위한 딥러닝 병렬 모델을 제안하였다. 예측을 위한 데이터 셋은 3개로 나누었으며, 데이터 셋 1은 시가, 고가, 저가, 종가, 거래량이며, 데이터 셋 2는 기술 분석 데이터를 추가하였으며, 데이터 셋 3은 주가에 영향을 줄 수 있는 환율, 전산업생산지수를 추가하였다. 딥러닝 모델은 기본 모델로서 DNN, LSTM, 1D-CNN 모델과 병렬 모델로서 DNN 모델을 기본으로 1D-CNN을 병합한 DCNN 모델과 LSTM을 병합한 DLSTM 모델을 제안하였다. 실험 결과, DNN과 CNN 보다는 LSTM과 BiLSTM 모델의 성능이 높았으며, 특히 병렬모델인 DLSTM 모델이 가장 성능이 좋았다. 병렬 모델인 DLSTM 모델에 대한 데이터 셋 1의 RMSE는 0.0091, 데이터 셋 2의 RMSE는 0.0080, 데이터 셋 3의 RMSE는 0.0071로서 모든 데이터가 합쳐진 데이터 셋 3의 성능이 가장 좋았다.   \n",
      "93                                                                                                                                                                                                                                                                   강수량예측은 누적 강수량보다는 짧은 시간에 얼마나 많은 비가 집중적으로 내렸는지 알려주는 시간당 강수량에 따라서 피해 정도가 달라진다. 본 연구는 딥러닝 기본모델인 DNN, LSTM, BiLSTM, 1D-CNN 모델과 성능 향상을 위해 기본모델을 병합한 병렬 구조를 이용하여 단기 강수예측을 하였다. 병렬모델은 DNN 모델과 1D-CNN을 병합한 DCNN과 DNN 모델과 LSTM을 병합한 DLSTM 모델이다. 데이터셋은 강수일 만을 구축한 데이터셋, 6월부터 9월까지의 데이터셋, 5월부터 10월끼지의 데이터셋 등 3개의 데이터셋을 이용하였으며, 각 데이터셋에 대해서 세부적으로 7개의 데이터셋으로 구분하여, 총 21개의 데이터셋에 대하여 강수량을 예측하고 비교 평가하였다. 실험 결과, 세 번째 데이터셋이 가장 예측 결과가 좋았다. 특히, 5 번째 세부 데이터셋인 DLSTM 병렬 모델의 RMSE가 0.25로서, 다른 모델보다 10배 정도 월등히 예측 결과가 좋았다.   \n",
      "94                                                                                                                                                                                                                                                                 전방 낙하물과 같은 돌발상황이 발생했을 때 신속하고 적절한 정보 제공은 도로 위 이용자들의 편의를 가져다주고 2차 교통사고 또한 효과적으로 줄일 수 있다. 도로 상의 돌발상황은현재 국내에서 루프 검지기나 CCTV 등 ITS 기반 검지 체계를 사용하여 주로 검지하고 있다. 이러한 방식은 검지기의 검지 구간에서의 도로 위 데이터만을 얻을 수 있다. 때문에, 기존 ITS 기반 검지체계의 공간적 음영구간에서 돌발상황을 찾아내기 위하여 새로운 검지 수단이 필요하다. 이에 본 연구에서는 차량 내 설치된 단말기에서 촬영된 영상으로부터 돌발상황을 검지및 분류하는 ResNet 기반 알고리즘을 제안한다. 국내 고속도로 전방 주행영상을 수집하였고, 돌발상황 유형을 클래스로 정의하여 각 데이터를 라벨링한 후, 제안한 알고리즘으로 데이터를학습시켰다. 학습 결과, 개발한 알고리즘은 데이터 수가 상대적으로 적었던 일부 클래스를 제외하고 정의한 돌발상황 클래스에 대하여 높은 검지율을 보였다.   \n",
      "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Volumetric medical image segmentation is critical in diagnosing diseases and planning subse- quent treatment. The convolutional neural network (CNN)-based U-Net was proposed for con- ducting accurate and robust medical image segmentation since the skip connection of U-Net and deep feature representation significantly improved its performance. However, since CNN-based models mainly focus on local and low-level features, they cannot extract global and high-level features effectively. Meanwhile, the Vision Transformer developed in natural language process- ing is proposed to improve image classification performance by splitting an input image into patches and conducting linear embeddings of the patches, which can extract global features. However, the Vision Transformer has difficulty in handling detailed and low-level features. This study proposes SwinResNet which can effectively conduct volumetric medical image segmenta- tion by fusing the Swin Transformer and CNN models. The combination can take advantage of both models and complement each other. Swin Transformer and ResNet are used as encoders, and the receptive field blocks and aggregation modules are applied to the multi-level features extracted from both encoders. Comprehensive evaluation shows that the proposed approach out- performs well-known previous studies.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Wheat is one of the major food crops, but the domestic self-sufficiency rate is only 1%. To increase self-sufficiency, it is essential to manage the quality of wheat seeds by classifying the seed quality grades. This study used deep learning algorithms to classify wheat varieties and distinguish hard and soft wheat. Five wheat varieties used in the South were employed, and 21,256 images of individual wheat seeds were acquired using an image acquisition system. The length of the axes and the average values in the RGB channels were then calculated. Furthermore, the ResNet50 architecture was used to classify the five wheat varieties and hardness. The results revealed high classification performances for the training, validation, and test groups, with rates of 98.17%, 96.68%, and 96.40%, respectively. The confusion matrix of the test group indicated successful classification, and hardness was classified with 100% accuracy for the same varieties. Deep learning algorithms enable the determination of wheat seed varieties and hardness.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  None  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We present a method for generating 3D structures and rendering objects by combining VAE (Variational Autoencoder) and GAN (Generative Adversarial Network). This approach focuses on generating and rendering 3D models with improved quality using residual learning as the learning method for the encoder. We deep stack the encoder layers to accurately reflect the features of the image and apply residual blocks to solve the problems of deep layers to improve the encoder performance. This solves the problems of gradient vanishing and exploding, which are problems when constructing a deep neural network, and creates a 3D model of improved quality. To accurately extract image features, we construct deep layers of the encoder model and apply the residual function to learning to model with more detailed information. The generated model has more detailed voxels for more accurate representation, is rendered by adding materials and lighting, and is finally converted into a mesh model. 3D models have excellent visual quality and accuracy, making them useful in various fields such as virtual reality, game development, and metaverse.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           With the advancement of information technology, criminals employ multiple cyberspaces to promote cybercrime. To combat cybercrime and cyber dangers, banks and financial institutions use artificial intelligence (AI). AI technologies assist the banking sector to develop and grow in many ways. Transparency and explanation of AI's ability are required to preserve trust. Deep learning protects client behavior and interest data. Deep learning techniques may anticipate cyber-attack behavior, allowing for secure banking transactions. This proposed approach is based on a user-centric design that safeguards people's private data over banking. Here, initially, the attack data can be generated over banking transactions. Routing is done for the configuration of the nodes. Then, the obtained data can be preprocessed for removing the errors. Followed by hierarchical network feature extraction can be used to identify the abnormal features related to the attack. Finally, the user data can be protected and the malicious attack in the transmission route can be identified by using the Wrapper stepwise ResNet classifier. The proposed work outperforms other techniques in terms of attack detection and accuracy, and the findings are depicted in the graphical format by employing the Python tool.  \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         To effectively track the illegal use of digital images and maintain the security of digital image communicationon the Internet, this paper proposes a reversible multipurpose image watermarking algorithm based on a deepresidual network (ResNet) and perceptual hashing (also called MWR). The algorithm first combines perceptualimage hashing to generate a digital fingerprint that depends on the user’s identity information and imagecharacteristics. Then it embeds the removable visible watermark and digital fingerprint in two different regionsof the orthogonal separation of the image. The embedding strength of the digital fingerprint is computed usingResNet. Because of the embedding of the removable visible watermark, the conflict between the copyrightnotice and the user’s browsing is balanced. Moreover, image authentication and traitor tracking are realizedthrough digital fingerprint insertion. The experiments show that the scheme has good visual transparency andwatermark visibility. The use of chaotic mapping in the visible watermark insertion process enhances thesecurity of the multipurpose watermark scheme, and unauthorized users without correct keys cannot effectivelyremove the visible watermark.  \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        There is a growing interest in facial age estimation because many applications require age estimation techniques from facial images. In order to estimate the exact age of a face, a technique for extracting aging features from a face image and classifying the age according to the extracted features is required.Recently, the performance of various CNN-based deep learning models has been greatly improved in the image recognition field, and various CNN-based deep learning models are being used to improve performance in the field of facial age estimation. In this paper, age estimation performance was compared by learning facial features based on various CNN-based models such as AlexNet, VGG-16, VGG-19, ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152. As a result of experiment, it was confirmed that the performance of the facial age estimation models using ResNet-34 was the best.  \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Gastric cancer occurs mostly in Asian countries such as Korea and Japan. Gastroscopy allows diagnosis and treatment of gastric cancer at the same time, and the probability of successful treatment is very high at early detection. However, due to the nature of the inspection which progresses in real time, proficiency and experience of the clinician affect the results, and the accuracy of the inspectioncan decrease due to increased work fatigue and decreased concentration. In this study, we developed a model that automatically detects the regions of gastric lesion in gastroscopic images using the RetinaNet network so that it can be used as an auxiliary system during gastroscopy. We confirmed the detection performance of models trained using ResNet50, ResNet152, EfficientNetB0, and EfficientNetB4 networks in a RetinaNet-based backbone network, and compared the performance between each model. The average sensitivities (FP/images) of RetinaNet-based backbone network-models were 73.72% (0.0489) for ResNet50, 78.26% (0.0458) for ResNet152, 79.67% (0.3268) for EfficinetNetB0, and 79.67% (0.3268) for EfficientNetB4. The EfficientNetB0 network showed the highest sensitivity, but the FP/images were very high, so the network satisfying both performance values was ResNet152.  \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Fishing boat capsizing accidents account for more than half of all capsize accidents. These can occur for a variety of reasons, including inexperienced operation, bad weather, and poor maintenance. Due to the size and influence of the industry, technological complexity, and regional diversity, fishing ships are relatively under-researched compared to commercial ships. This study aimed to predict the rolling motion time series of fishing boats using an image-based deep learning model. Image-based deep learning can achieve high performance by learning various patterns in a time series. Three image-based deep learning models were used for this purpose: Xception, ResNet50, and CRNN. Xception and ResNet50 are composed of 177 and 184 layers, respectively, while CRNN is composed of 22 relatively thin layers. The experimental results showed that the Xception deep learning model recorded the lowest Symmetric mean absolute percentage error(sMAPE) of 0.04291 and Root Mean Squared Error(RMSE) of 0.0198. ResNet50 and CRNN recorded an RMSE of 0.0217 and 0.022, respectively. This confirms that the models with relatively deeper layers had higher accuracy.  \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Digitalization due to COVID-19 has rapidly developed artificial intelligence-based voice recognition technology. However, this technology causes unfair social problems, such as race and gender discrimination if datasets are biased against some groups, and degrades the reliability and security of artificial intelligence services. In this work, we compare and analyze accuracy-based unfairness in biased data environments using VGGNet (Visual Geometry Group Network), ResNet (Residual Neural Network), and MobileNet, which are representative CNN (Convolutional Neural Network) models of artificial intelligence. Experimental results show that ResNet34 showed the highest accuracy for women and men at 91% and 89.9% in Top1-accuracy, while ResNet18 showed the slightest accuracy difference between genders at 1.8%. The difference in accuracy between genders by model causes differences in service quality and unfair results between men and women when using the service.  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Most of the noise generated by humans in the ocean is ship radiated noise caused to fishing and commercial shipping. Recently, deep learning technology has been used to detect shipping noise. In this study, the convolutional neural network is trained by a shipping noise spectrogram divided into 1-minute units to detect a near distance ship. Inception-V3, ResNet-50, VGG-16 and the proposed model were used to learn and evaluate 1-minute shipping noise. As a result, the F1 scores were 97.42%, 98.42%, 98.16% and 97.88% for Inception-V3, ResNet-50, VGG-16 and the proposed model, respectively. These models showed satisfactory performance in detecting shipping noise. It was confirmed that the proposed model showed equivalent detection performance with about 1/8 parameters compared to ResNet-50. For future works, it is expected that it will be possible to detect long-distance shipping noise by using additional noise data and AIS(Automatic Identification System).  \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 As online transactions increase, the image of clothing has a great influence on consumer purchasing decisions. The importance of image information for clothing materials has been emphasized, and it is important for the fashion industry to analyze clothing images and grasp the materials used. Textile materials used for clothing are difficult to identify with the naked eye, and much time and cost are consumed in sorting. This study aims to classify the materials of textiles from clothing images based on deep learning algorithms. Classifying materials can help reduce clothing production costs, increase the efficiency of the manufacturing process, and contribute to the service of recommending products of specific materials to consumers. We used machine vision-based deep learning algorithms ResNet and Vision Transformer to classify clothing images. A total of 760,949 images were collected and preprocessed to detect abnormal images. Finally, a total of 167,299 clothing images, 19 textile labels and 20 fabric labels were used. We used ResNet and Vision Transformer to classify clothing materials and compared the performance of the algorithms with the Top-k Accuracy Score metric. As a result of comparing the performance, the Vision Transformer algorithm outperforms ResNet.  \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "15                                                                                                                                                                                                                                                                                                                                                  In recent times, semantic image segmentation methods using deep learning models have been widely used for monitoring changes in surface attributes using remote sensing imagery. To enhance the performance of various UNet-based deep learning models, including the prominent UNet model, it is imperative to have a sufficiently large training dataset. However, enlarging the training dataset not only escalates the hardware requirements for processing but also significantly increases the time required for training. To address these issues, transfer learning is used as an effective approach, enabling performance improvement of models even in the absence of massive training datasets. In this paper we present three transfer learning models, UNet-ResNet50, UNet-VGG19, and CBAM-DRUNet-VGG19, which are combined with the representative pretrained models of VGG19 model and ResNet50 model. We applied these models to building extraction tasks and analyzed the accuracy improvements resulting from the application of transfer learning. Considering the substantial impact of learning rate on the performance of deep learning models, we also analyzed performance variations of each model based on different learning rate settings. We employed three datasets, namely Kompsat-3A dataset, WHU dataset, and INRIA dataset for evaluating the performance of building extraction results. The average accuracy improvements for the three dataset types, in comparison to the UNet model, were 5.1% for the UNet-ResNet50 model, while both UNet-VGG19 and CBAM-DRUNet-VGG19 models achieved a 7.2% improvement.  \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Recently, with the development of deep learning technology, research on pedestrian attribute recognition technology using deep neural networks has been actively conducted. Existing pedestrian attribute recognition techniques can be obtained in such a way as global-based, regional-area-based, visual attention-based, sequential prediction-based, and newly designed loss function-based, depending on how pedestrian attributes are detected. It is known that the performance of these pedestrian attribute recognition technologies varies greatly depending on the type of backbone network that constitutes the deep neural networks model. Therefore, in this paper, several backbone networks are applied to the baseline pedestrian attribute recognition model and the performance changes of the model are analyzed. In this paper, the analysis is conducted using Resnet34, Resnet50, Resnet101, Swin-tiny, and Swinv2-tiny, which are representative backbone networks used in the fields of image classification, object detection, etc. Furthermore, this paper analyzes the change in time complexity when inferencing each backbone network using a CPU and a GPU.  \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                A smart business card printing system that automatically prints business cards requested by customers online is being activated. What matters is that the business card submitted by the customer to the system may be abnormal. This paper deals with the problem of determining whether the image of a business card has been abnormally rotated by adopting artificial intelligence technology. It is assumed that the business card rotates 0 degrees, 90 degrees, 180 degrees, and 270 degrees. Experiments were conducted by applying existing VGG, ResNet, and DenseNet artificial neural networks without designing special artificial neural networks, and they were able to distinguish image rotation with an accuracy of about 97%. DenseNet161 showed 97.9% accuracy and ResNet34 also showed 97.2% precision. This illustrates that if the problem is simple, it can produce sufficiently good results even if the neural network is not a complex one.  \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Most of the maintenance and safety inspections of buildings are performed with visual assessment of the inspector, which consumes a lot oftime and cost. With the development of computer vision and digital technologies such as 3D Laser scanners, automatic defect recognitionusing image processing and artificial intelligence has been widely studied. Current approach is largely relying on the image obtained from thecamera and the recognition performance could be varied depending on the surrounding environment. Recently, studies using 3D Laser scannerare being conducted to solve these problems. However, terrestrial laser scanners are expensive, so it is difficult to apply at the constructionsite. Therefore, this study proposed a method that can recognize masonry wall defects using a Microelectromechanical systems based LightDetection and Ranging sensor that having much lower price and reliable performance. This study was performed using masonry wallstructures and data were collected from samples having various types of defects in a laboratory environment. Masonry wall defects wererecognized using ResNet-50 and VGG16 models, which are widely used in previous studies. As a result of the classification, ResNet-50 andVGG16 achieved 98.75% and 96.88% accuracy, respectively. The results of this study can be utilized in the development of real-time defectrecognition method for a masonry wall at construction sites.  \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Artificial intelligence-based estimation of repair costs for damaged vehicles is an emerging field that relies on artificial intelligence and computer vision systems to automatically generate accurate cost estimates. This area of research is becoming increasingly important owing to its potential to streamline the automotive repair industry, enhance overall transparency, improve the accuracy of cost estimation, and expedite insurance claims processing. This paper proposes the identification of the make and model of a vehicle, classification of the damaged vehicle type, and estimation of repair costs based on prices from various vehicle manufacturers. The proposed method for achieving state-of-the-art performance and time-saving in this system is through the use of ResNet50 and transfer learning. We propose a vehicle make and model classification module as well as a damaged vehicle classification module based on ResNet50 and transfer learning to improve the accuracy of the results. The accuracy of vehicle make and model classification module is 88%, which is approximately 11% higher than that of other studies. The accuracy of damaged vehicle classification module in this study is 86%, which is 67% higher than that of other studies.  \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                Purpose: Urolithiasis is a common disease that can cause acute pain and complications. The objective of this study was to develop a deep learning model utilizing transfer learning for the rapid and accurate detection of urinary tract stones. By employing this method, we aim to improve the efficiency of medical staff and contribute to the progress of deep learning-based medical image diagnostic technology.Methods: The ResNet50 model was employed to develop feature extractors for detecting urinary tract stones. Transfer learning was applied by utilizing the weights of pretrained models as initial values, and the models were fine-tuned with the provided data. The model’s performance was evaluated using accuracy, precision-recall, and receiver operating characteristic curve metrics.Results: The ResNet-50-based deep learning model demonstrated high accuracy and sensitivity, outperforming traditional methods. Specifically, it enabled a rapid diagnosis of the presence or absence of urinary tract stones, thereby assisting doctors in their decision-making process.Conclusions: This research makes a meaningful contribution by accelerating the clinical implementation of urinary tract stone detection technology utilizing ResNet-50. The deep learning model can swiftly identify the presence or absence of urinary tract stones, thereby enhancing the efficiency of medical staff. We expect that this study will contribute to the advancement of medical imaging diagnostic technology based on deep learning.  \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Currently, farmers are showing problems such as aging and rising labor costs, and the development of smart agriculture that combines cutting-edge technologies such as AI, video analysis, and big data is essential to solve these problems. In this study, a deep learning-based tomato maturity determination system was studied for the development of smart agriculture. In order to preprocess tomato images, colors were extracted for each image with RGB and HSV color models, and the extracted images were preprocessed into images that were easier to learn through noise removal using Gaussian filters and data normalization through Standard Scaler. The pretreated tomato image learned a tomato maturity discrimination image according to color using a deep learning model called ResNet-50 and a tomato maturity discrimination model was obtained. If a tomato maturity determination model is stored separately and then combined with a camera to take an image, the rating of the captured tomato can be immediately checked, and the captured image is also stored in the database for future model updates. It is expected to contribute to the great development of smart agriculture by using this tomato maturity determination system to prevent damage that may occur due to missed harvest time and to produce a system that can determine maturity by using the system.  \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Chest X-Ray (CXR) images provide most anatomical details and the abnormalities on a 2D plane. Therefore, a 2D view of the 3D anatomy is sometimes sufficient for the initial diagnosis. However, close to fourteen commonly occurring diseases are sometimes difficult to identify by visually inspecting the images. Therefore, there is a drift toward developing computer-aided assistive systems to help radiologists. This paper proposes a deep learning model for the classification and localization of chest diseases by using image-level annotations. The model consists of a modified Resnet50 backbone for extracting feature corpus from the images, a classifier, and a pixel correlation module (PCM). During PCM training, the network is a weight-shared siamese architecture where the first branch applies the affine transform to the image before feeding to the network, while the second applies the same transform to the network output. The method was evaluated on CXR from the clinical center in the ratio of 70:20 for training and testing. The model was developed and tested using the cloud computing platform Google Colaboratory (NVidia Tesla P100 GPU, 16 GB of RAM). A radiologist subjectively validated the results. Our model trained with the configurations mentioned in this paper outperformed benchmark results.  \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This study aimed to evaluate the performance of water level classification from CCTV images in agricultural facilities such as reservoirs. Recently, theCCTV system, widely used for facility monitor or disaster detection, can automatically detect and identify people and objects from the images bydeveloping new technologies such as a deep learning system. Accordingly, we applied the ResNet-50 deep learning system based on ConvolutionalNeural Network and analyzed the water level of the agricultural reservoir from CCTV images obtained from TOMS (Total Operation ManagementSystem) of the Korea Rural Community Corporation. As a result, the accuracy of water level detection was improved by excluding night and rainfallCCTV images and applying measures. For example, the error rate significantly decreased from 24.39 % to 1.43 % in the Bakseok reservoir. We believethat the utilization of CCTVs should be further improved when calculating the amount of water supply and establishing a supply plan according tothe integrated water management policy.  \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     This study aimed to implement a deep learning model that can perform quantitative quality control through ACTS software used for quantitative evaluation of ACR phantom in CT quality control and evaluate its usefulness. By changing the scanning conditions, images of three modules of the ACR phantom's slice thickness (ST), low contrast resolution (LC), and high contrast resolution (HC) were obtained and classified as ACTS software. The deep learning model used ResNet18, implementing three models in which ST, HC, and LC were learned with epoch 50 and an integrated model in which three modules were learned with Epoch 10, 30, and 50 at once. The performance of each model was evaluated through Accuracy and Loss. When comparing and evaluating the accuracy and loss function values of the deep learning models by ST, LC, and HC modules, the Accuracy and Loss of the HC model were the best with 100% and 0.0081, and in the integrated model according to the Epoch value, Accuracy and Loss with epoch 50 were the best with 96.29% and 0.1856. This paper showed that quantitative quality control is possible through a deep learning model, and it can be used as a basis and evidence for applying deep learning to the CT quality control.  \n",
      "25                                                         In the modern world, advances in information technology have led to the expansion of e-commerce, making it important for automated recommendation systems to efficiently gather the flood of information and data to present consumers with their favorite products and services. Various techniques are used to improve the accuracy of product recommendation in existing e-commerce. Among them, there are chronic problems that use RNN, a multi classification-based product recommendation model. RNN is a deep learning model suitable for time series classification tasks, but it suffers from issues such as gradient vanishing and gradient exploding. To Compensate for these issues, CNN models are often used to effectively detect local patterns through kernels. In this study, we compare the performance of recommendation models based on an architecture that generates product recommendation models by training CNN models with time series data through three different imaging encodings: GAF, MTF and RP. In our experiments, we split the 540,000 published transaction dataset into train and test. The splitted data is constructed as time series data and zero-padded to equalize the size of the model’s input image. We train AlexNet, VGG16, ResNet50, and MobileNet models on images generated by the three imaging algorithms and compare their product recommendation accuracy with the performance of existing RNN recommendation models. We can see that the CNN models perform better than the LSTM. When imaged with the GAF algorithm and trained on the MobileNet model, the highest recommendation accuracy was achieved, and the learning time was also shortened, improving efficiency. Future research will include the advancement of imaging algorithms to improve the performance of product recommendation models and the development of CNN models optimized for time series image data.  \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           One of the most essential substances for detecting electric fire is electric fire short-circuit marks. The traces of which can be found before and after the electric fire as the short circuit occurs. There are different kinds of electric fire short circuit marks, for instance, grounded, primary, and secondary molten marks these are categorized into the different types of short-circuit marks primary short circuit marks appear before the electric fire occurrence, and secondary short circuit marks appear after an electric fire to identify and classify them is crucial and time-consuming steps and procedures are needed for that purpose in this study we have used five convolutional neural network models such as VGG16, VGG19, Xception, InceptionV3, and Resnet50 to classify the short-circuit marks image data. Furthermore, according to our experiment on dataset among these five models, the best result was of VGG16 because the model performed well without any overfitting problems when we trained the sets of electric fire short circuit image data by applying the data augmentation, transfer learning, and fine-tuning techniques. The validation accuracy result of the VGG16 model at 50 epochs was 92.7% with a validation loss rate of 0.2.  \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Metaverse is a modern new technology that is advancing quickly. The goal of this study is to investigate this technique from the perspective of computer vision as well as general perspective. A thorough analysis of computer vision related Metaverse topics has been done in this study. Its history, method, architecture, benefits, and drawbacks are all covered. The Metaverse's future and the steps that must be taken to adapt to this technology are described. The concepts of Mixed Reality (MR), Augmented Reality (AR), Extended Reality (XR) and Virtual Reality (VR) are briefly discussed. The role of computer vision and its application, advantages and disadvantages and the future research areas are discussed.  \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                This paper studied the model lightening and speed improvement of face recognition. To this end, we propose a method of training with MobileNetV2 through knowledge distillation as a student model and based on ArcFace and ResNet50 as a teacher model. ArcFace is face recognition model using Additive Angular Margin Loss. The main objective was to compare the performance of the student model against that of the teacher model in terms of similarity results. The findings indicated that the student model outperformed the teacher model in terms of similarity results. These results suggest that the MobileNetV2-based student model can achieve comparable face recognition performance to the more complex ResNet50-based teacher model, while being computationally more efficient.  \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Estimating the rice heading date is one of the most crucial agricultural tasks related to productivity. However, due to abnormal climates around the world, it is becoming increasingly challenging to estimate the rice heading date. Therefore, a more objective classification method for estimating the rice heading date is needed than the existing methods. This study, we aimed to classify the rice heading stage from various images using a CNN classification model. We collected top-view images taken from a drone and a phenotyping tower, as well as slanted-view images captured with a RGB camera. The collected images underwent preprocessing to prepare them as input data for the CNN model. The CNN architectures employed were ResNet50, InceptionV3, and VGG19, which are commonly used in image classification models. The accuracy of the models all showed an accuracy of 0.98 or higher regardless of each architecture and type of image. We also used Grad-CAM to visually check which features of the image the model looked at and classified. Then verified our model accurately measure the rice heading date in paddy fields. The rice heading date was estimated to be approximately one day apart on average in the four paddy fields. This method suggests that the water head can be estimated automatically and quantitatively when estimating the rice heading date from various paddy field monitoring images.  \n",
      "31                                                                                                                                                                                                                                                                                   Determining the size or area of a plant's leaves is an important factor in predicting plant growth and improving the productivity of indoor farms. In this study, we developed a convolutional neural network (CNN)-based model to accurately predict the length and width of lettuce leaves using photographs of the leaves. A callback function was applied to overcome data limitations and overfitting problems, and K-fold cross-validation was used to improve the generalization ability of the model. In addition, ImageDataGenerator function was used to increase the diversity of training data through data augmentation. To compare model performance, we evaluated pre-trained models such as VGG16, Resnet152, and NASNetMobile. As a result, NASNetMobile showed the highest performance, especially in width prediction, with an R_squared value of 0.9436, and RMSE of 0.5659. In length prediction, the R_squared value was 0.9537, and RMSE of 0.8713. The optimized model adopted the NASNetMobile architecture, the RMSprop optimization tool, the MSE loss functions, and the ELU activation functions. The training time of the model averaged 73 minutes per Epoch, and it took the model an average of 0.29 seconds to process a single lettuce leaf photo. In this study, we developed a CNN-based model to predict the leaf length and leaf width of plants in indoor farms, which is expected to enable rapid and accurate assessment of plant growth status by simply taking images. It is also expected to contribute to increasing the productivity and resource efficiency of farms by taking appropriate agricultural measures such as adjusting nutrient solution in real time.  \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this paper, we propose a deep learning model to detect lumbar 3 (L3) CT images to determine the occurrence and degree of sarcopenia. In addition, we would like to propose an optimization technique that uses oversampling ratio and class weight as design parameters to address the problem of performance degradation due to data imbalance between L3 level and non-L3 level portions of CT data. In order to train and test the model, a total of 150 whole-body CT images of 104 prostate cancer patients and 46 bladder cancer patients who visited Gangneung Asan Medical Center were used. The deep learning model used ResNet50, and the design parameters of the optimization technique were selected as six types of model hyperparameters, data augmentation ratio, and class weight. It was confirmed that the proposed optimization-based L3 level extraction model reduced the median L3 error by about 1.0 slices compared to the control model (a model that optimized only 5 types of hyperparameters). Through the results of this study, accurate L3 slice detection was possible, and additionally, we were able to present the possibility of effectively solving the data imbalance problem through oversampling through data augmentation and class weight adjustment.  \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This paper proposes a common interface for connecting proprietary back-end compilers of NPU (Neural Processing Units) manufacturers with general-purpose AI compilers, enabling support for various AI models while protecting corporate assets. The proposed interface adheres to the ONNX standard and integrates ETRI's NEST-C compiler with OPENEDGES's ENLIGHT compiler through this interface. Experimental results showed that the integrated compiler achieved 100% result consistency for two types of models, ResNet50 and MobileNetV2, compared to using ENLIGHT alone. Therefore, the proposed interface offers a valuable solution for enhancing the versatility of NPU back-end compilers while maintaining their proprietary nature.  \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Developmental Dysplasia of the Hip (DDH) is a pathological condition commonly occurring during the growth phase of infants. It acts as one of the factors that can disrupt an infant's growth and trigger potential complications. Therefore, it is critically important to detect and treat this condition early. The traditional diagnostic methods for DDH involve palpation techniques and diagnosis methods based on the detection of keypoints in the hip joint using X-ray or ultrasound imaging. However, there exist limitations in objectivity and productivity during keypoint detection in the hip joint. This study proposes a deep learning model-based keypoint detection method using X-ray and ultrasound imaging and analyzes the performance of keypoint detection using various deep learning models. Additionally, the study introduces and evaluates various data augmentation techniques to compensate the lack of medical data. This research demonstrated the highest keypoint detection performance when applying the residual network 152 (ResNet152) model with simple & complex augmentation techniques, with average Object Keypoint Similarity (OKS) of approximately 95.33 % and 81.21 % in X-ray and ultrasound images, respectively. These results demonstrate that the application of deep learning models to ultrasound and X-ray images to detect the keypoints in the hip joint could enhance the objectivity and productivity in DDH diagnosis.  \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Existing road flooding systems using a single sensor give an alarm when the water level reaches a certain value, making it difficult to determine road flooding and take first action. Therefore, in this paper, 8 models based on CNN were implemented to develop a real-time road flooding system using CCTV, and their performance was compared through learning and verification. Each learning model was trained with a batch size of 16 and 120 epochs, and as a result of the experiment, the deep learning models showed an average accuracy of 90%. In particular, in terms of accuracy, the ShuffleNet V1, SqueezeNet, and ResNet-50 models performed best in order. However, for real-time road flood detection and prediction, an appropriate number of parameters and short inference time are required for each model. Assuming that each CCTV is analyzed once every 10 seconds, it was analyzed that the ResNet-50 model could accommodate up to 800 CCTVs.  \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The agricultural reservoir is a critical water supply system in South Korea, providing approximately 60% of the agricultural water demand. However, the reservoir faces several issues that jeopardize its efficient operation and management. To address this issues, we propose a novel deep-learning-based water level recognition model that uses CCTV image data to accurately estimate water levels in agricultural reservoirs. The model consists of three main parts: (1) dataset construction, (2) image segmentation using the U-Net algorithm, and (3) CCTV-based water level recognition using either CNN or ResNet. The model has been applied to two reservoirs G-reservoir and M-reservoir with observed CCTV image and water level time series data. The results show that the performance of the image segmentation model is superior, while the performance of the water level recognition model varies from 50 to 80% depending on water level classification criteria (i.e., classification guideline) and complexity of image data (i.e., variability of the image pixels). The performance of the model can be improved if more numbers of data can be collected.  \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The rapid convergence of ICT (Information and Communication Technology) with various fields in modern society has led to the emergence of smart factories in the manufacturing industry. These factories leverage artificial intelligence and automation technology to enhance productivity and efficiency by collecting real-time data and making optimal decisions through analysis. In this study, we aimed to develop machine learning and deep learning models to improve manufacturing processes in smart factories. Firstly, we implemented a model using logistic regression, random forest, gradient boosted trees, and support vector machine to classify defects based on process environment data, including temperature and pressure. Next, we applied convolutional neural network models such as AlexNet, VGG-16, and ResNet to classify defective welding images captured after the welding process. We evaluated the performance of these models using metrics like accuracy, precision, and recall for each dataset and selected the top-performing model as the final choice.  \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this study, we propose a smart city tour mobile application based on GPS and deep learning. The proposed application provides the landmark image recognition service using deep learning and provides information on nearby tourist attractions based on the current user’s location through location-based services. The image recognition service identifies the image through a deep learning model when a user uploads a landmark image to the application and provides a detailed description of the landmark. We trained landmark images using the ResNet-D model on 13 landmarks in Seoul and achieved good performance of the final model with average classification accuracy and F1-score of around 0.957 and 0.938, respectively. In addition, location-based services within the application allow users to know information about landmarks close to their current location and share the information with other users. Foreigners who are traveling or planning to travel can easily obtain information about the capital Seoul with this application to aid their travel.  \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Scoliosis is a three-dimensional deformation of the spine that is a deformity induced by physical or disease-related causes as the spine is rotated abnormally. Early detection has a significant influence on the possibility of nonsurgical treatment. To train a deep learning model with preprocessed images and to evaluate the results with and without data augmentation to enable the diagnosis of scoliosis based only on a chest X-ray image. The preprocessed images in which only the spine, rib contours, and some hard tissues were left from the original chest image, were used for learning along with the original images, and three CNN(Convolutional Neural Networks) models (VGG16, ResNet152, and EfficientNet) were selected to proceed with training. The results obtained by training with the preprocessed images showed a superior accuracy to those obtained by training with the original image. When the scoliosis image was added through data augmentation, the accuracy was further improved, ultimately achieving a classification accuracy of 93.56% with the ResNet152 model using test data. Through supplementation with future research, the method proposed herein is expected to allow the early diagnosis of scoliosis as well as cost reduction by reducing the burden of additional radiographic imaging for disease detection.  \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Recently, according to development of artificial intelligence, a wide range of industry being automatic and optimized. Also we can find out some research of using supervised learning for deteceting defect of railway in domestic rail industry. However, there are structures other than rails on the track, and the fastener is a device that binds the rail to other structures, and periodic inspections are required to prevent safety accidents. In this paper, we present a method of reducing cost for labeling using semi-supervised and transfer model trained on rail fastener data. We use Resnet50 as the backbone network pretrained on ImageNet. At first we randomly take training data from unlabeled data and then labeled that data to train model.  After predict unlabeled data by trained model, we adopted a method of adding the data with the highest probability for each class to the training data by a predetermined size.  Futhermore, we also conducted some experiments to investigate the influence of the number of initially labeled data. As a result of the experiment, model reaches 92% accuracy which has a performance difference of around 5% compared to supervised learning. This is expected to improve the performance of the classifier by using relatively few labels without additional labeling processes through the proposed method.  \n",
      "41                                                                                                                                                                                                                                                                                                                                                                                                                       Recently, a convolutional neural network (CNN) based system is being developed to overcome the limitations of human resources in the apple quality classification of farmhouse. However, since convolutional neural networks receive only images of the same size, preprocessing such as sampling may be required, and in the case of oversampling, information loss of the original image such as image quality degradation and blurring occurs. In this paper, in order to minimize the above problem, to generate a image patch based graph of an original image and propose a random walk-based positional encoding method to apply the graph transformer model. The above method continuously learns the position embedding information of patches which don`t have a positional information based on the random walk algorithm, and finds the optimal graph structure by aggregating useful node information through the self-attention technique of graph transformer model. Therefore, it is robust and shows good performance even in a new graph structure of random node order and an arbitrary graph structure according to the location of an object in an image. As a result, when experimented with 5 apple quality datasets, the learning accuracy was higher than other GNN models by a minimum of 1.3% to a maximum of 4.7%, and the number of parameters was 3.59M, which was about 15% less than the 23.52M of the ResNet18 model. Therefore, it shows fast reasoning speed according to the reduction of the amount of computation and proves the effect.  \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Deep learning with large amount of computations is difficult to implement on micro-sized IoT devices or moblie devices. Recently, lightweight deep learning technologies have been introduced to make sure that deep learning can be implemented even on small devices by reducing the amount of computation of the model. Quantization is one of lightweight techniques that can be efficiently used to reduce the memory and size of the model by expressing parameter  values  with  continuous distribution as discrete values of fixed bits. However, the accuracy of the model is reduced due to discrete  value  representation  in  quantization.  In  this  paper,  we  introduce  various  quantization techniques to correct the accuracy. We selected APoT and EWGS from existing quantization techniques, and comparatively analyzed the results through experimentations The selected techniques were trained and tested with CIFAR-10 or CIFAR-100 datasets in the ResNet model. We found out problems with them through experimental results analysis and presented directions for future research.  \n",
      "43                                                                                                                                                                                                                                                 In the image field, data augmentation refers to increasing the amount of data through an editing method suchas rotating or cropping a photo. In this study, a generative adversarial network (GAN) image was created usingCycleGAN, and various colors of dogs were reflected through data augmentation. In particular, dog data fromthe Stanford Dogs Dataset and Oxford-IIIT Pet Dataset were used, and 10 breeds of dog, corresponding to 300images each, were selected. Subsequently, a GAN image was generated using CycleGAN, and four learninggroups were established: 2,000 original photos (group I); 2,000 original photos + 1,000 GAN images (groupII); 3,000 original photos (group III); and 3,000 original photos + 1,000 GAN images (group IV). The amountof data in each learning group was augmented using existing data augmentation methods such as rotating,cropping, erasing, and distorting. The augmented photo data were used to train the MobileNet_v3_Large,ResNet-152, InceptionResNet_v2, and NASNet_Large frameworks to evaluate the classification accuracy andloss. The top-3 accuracy for each deep neural network model was as follows: MobileNet_v3_Large of 86.4%(group I), 85.4% (group II), 90.4% (group III), and 89.2% (group IV); ResNet-152 of 82.4% (group I), 83.7%(group II), 84.7% (group III), and 84.9% (group IV); InceptionResNet_v2 of 90.7% (group I), 88.4% (groupII), 93.3% (group III), and 93.1% (group IV); and NASNet_Large of 85% (group I), 88.1% (group II), 91.8%(group III), and 92% (group IV). The InceptionResNet_v2 model exhibited the highest image classificationaccuracy, and the NASNet_Large model exhibited the highest increase in the accuracy owing to dataaugmentation.  \n",
      "44                                                                                           Objective: Iris pattern recognition system is well developed and practiced in human, however, there is a scarcity of information on application of iris recognition system in animals at the field conditions where the major challenge is to capture a high-quality iris image from a constantly moving non-cooperative animal even when restrained properly. The aim of the study was to validate and identify Black Bengal goat biometrically to improve animal management in its traceability system.Methods: Forty-nine healthy, disease free, 3 months±6 days old female Black Bengal goats were randomly selected at the farmer’s field. Eye images were captured from the left eye of an individual goat at 3, 6, 9, and 12 months of age using a specialized camera made for human iris scanning. iGoat software was used for matching the same individual goats at 3, 6, 9, and 12 months of ages. Resnet152V2 deep learning algorithm was further applied on same image sets to predict matching percentages using only captured eye images without extracting their iris features.Results: The matching threshold computed within and between goats was 55%. The accuracies of template matching of goats at 3, 6, 9, and 12 months of ages were recorded as 81.63%, 90.24%, 44.44%, and 16.66%, respectively. As the accuracies of matching the goats at 9 and 12 months of ages were low and below the minimum threshold matching percentage, this process of iris pattern matching was not acceptable. The validation accuracies of resnet152V2 deep learning model were found 82.49%, 92.68%, 77.17%, and 87.76% for identification of goat at 3, 6, 9, and 12 months of ages, respectively after training the model.Conclusion: This study strongly supported that deep learning method using eye images could be used as a signature for biometric identification of an individual goat.  \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Recently, embedded systems, such as mobile platforms, have multiple processing units that can operate in parallel, such as centralized processing units (CPUs) and neural processing units (NPUs). We can use deep-learning compilers to generate machine code optimized for these embedded systems from a deep neural network (DNN). However, the deep-learning compilers proposed so far generate codes that sequentially execute DNN operators on a single processing unit or parallel codes for graphic processing units (GPUs). In this study, we propose PartitionTuner, an operator scheduler for deep-learning compilers that supports multiple heterogeneous PUs including CPUs and NPUs. PartitionTuner can generate an operator-scheduling plan that uses all available PUs simultaneously to minimize overall DNN inference time. Operator scheduling is based on the analysis of DNN architecture and the performance profiles of individual and group operators measured on heterogeneous processing units. By the experiments for seven DNNs, PartitionTuner generates scheduling plans that perform 5.03% better than a static type-based operator-scheduling technique for SqueezeNet. In addition, PartitionTuner outperforms recent profiling-based operator-scheduling techniques for ResNet50, ResNet18, and SqueezeNet by 7.18%, 5.36%, and 2.73%, respectively.  \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                CT is a medical device that acquires medical images based on Attenuation coefficient of human organs related to X-rays. In addition, using this theory, it can acquire sagittal and coronal planes and 3D images of the human body. Then, CT is essential device for universal diagnostic test. But Exposure of CT scan is so high that it is regulated and managed with special medical equipment. As the special medical equipment, CT must implement quality control.In detail of quality control, Spatial resolution of existing phantom imaging tests, Contrast resolution and clinical image evaluation are qualitative tests. These tests are not objective, so the reliability of the CT undermine trust. Therefore, by applying an artificial intelligence classification model, we wanted to confirm the possibility of quantitative eval- uation of the qualitative evaluation part of the phantom test. We used intelligence classification models (VGG19, DenseNet201, EfficientNet B2, inception_resnet_v2, ResNet50V2, and Xception). And the fine-tuning process used for learning was additionally performed. As a result, in all classification models, the accuracy of spatial resolution was 0.9562 or higher, the precision was 0.9535, the recall was 1, the loss value was 0.1774, and the learning time was from a maximum of 14 minutes to a minimum of 8 minutes and 10 seconds. Through the experimental results, it was concluded that the artificial intelligence model can be applied to CT implements quality control in spatial res- olution and contrast resolution.  \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The presence of abnormalities in the subgrade of roads poses safety risks to users and results in significant maintenance costs. In this study, we aimed to experimentally evaluate the temperature distributions in abnormal areas of subgrade materials using infrared cameras and analyze the data with machine learning techniques. The experimental site was configured as a cubic shape measuring 50 cm in width, length, and depth, with abnormal areas designated for water and air. Concrete blocks covered the upper part of the site to simulate the pavement layer. Temperature distribution was monitored over 23 h, from 4 PM to 3 PM the following day, resulting in image data and numerical temperature values extracted from the middle of the abnormal area. The temperature difference between the maximum and minimum values measured 34.8°C for water, 34.2°C for air, and 28.6°C for the original subgrade. To classify conditions in the measured images, we employed the image analysis method of a convolutional neural network (CNN), utilizing ResNet-101 and SqueezeNet networks. The classification accuracies of ResNet-101 for water, air, and the original subgrade were 70%, 50%, and 80%, respectively. SqueezeNet achieved classification accuracies of 60% for water, 30% for air, and 70% for the original subgrade. This study highlights the effectiveness of CNN algorithms in analyzing subgrade properties and predicting subsurface conditions.  \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              This paper developed a training model to classify normal roads and flooded roads using artificial intelligence technology. We expanded the diversity of learning data using various data augmentation techniques and implemented a model that shows good performance in various environments. Transfer learning was performed using the CNN-based Resnet152v2 model as a pre-learning model. During the model learning process, the performance of the final model was improved through various parameter tuning and optimization processes. Learning was implemented in Python using Google Colab NVIDIA Tesla T4 GPU, and the test results showed that flooding situations were detected with very high accuracy in the test dataset.  \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Seed coat sculptures, including anticlinal and periclinal walls, are of great taxonomic importance. In this study, we identified seed coat patterns of Allium seeds using five deep learning methods namely, CNN, AlexNet, GoogleNet, ResNet50, and VGG16 for the first time. Selected images of seed coat patterns from over 100 Allium species reported in previously published literature and data from our samples were classified into seven types of anticlinal (irregular curved, irregular curved to nearly straight, straight, S, U, UO, and omega) and five types of periclinal walls (granule, small verruca, large verruca, marginal verruca, and verrucate verruca). The results revealed that GoogleNet and VGG16 achieved the highest classifi cation accuracy of 90.4% for the anticlinal wall, and VGG16 achieved the highest classification accuracy of 98.1% for the periclinal wall. Moreover, more than three, four, and five methods were combined, and their performance was investigated. Combining more than three methods was the most advantageous. The models achieved a suitable anticlinal wall classification using GoogleNet and periclinal wall classification using VGG16. In conclusion, using the machine-based method, we studied the seed coat of species of Allium on our own samples to see if the results of the machine-based method match with the human based classification.  \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The purpose of this study is to evaluate the impact of intermediate features on FER performance. To achieve this objective, intermediate features were extracted from the input images at specific layers (FM1~FM4) of the pre-trained network (Resnet-18). These extracted intermediate features and original images were used as inputs to the vision transformer (ViT), and the FER performance was compared. As a result, when using a single image as input, using intermediate features extracted from FM2 yielded the best performance (training accuracy: 94.35%, testing accuracy: 75.51%). When using the original image as input, the training accuracy was 91.32% and the testing accuracy was 74.68%. However, when combining the original image with intermediate features as input, the best FER performance was achieved by combining the original image with FM2, FM3, and FM4 (training accuracy: 97.88%, testing accuracy: 79.21%). These results imply that incorporating intermediate features alongside the original image can lead to superior performance. The findings can be referenced and utilized when designing the preprocessing stages of a deep learning model in FER. By considering the effectiveness of using intermediate features, practitioners can make informed decisions to enhance the performance of FER systems.  \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this paper, after segmenting the lip area in the facial image, using the featurepoints of the lip, it is determined whether there is a precursor symptom of stroke.UNet and FCN, which are semantic image segmentation models, were used forsegmentation of the lip region, and at this moment, VGGNet16, ResNet101, andDenseNet121 were used as backbone networks. As a result of the experiment, themIoU of UNet using DenseNet121 was the highest at 92.5%. And, as a result oflearning with XGBoost using the feature map of the lip area and diagnosing astroke, it showed 98.8% accuracy. As a result of comparison with the existingstroke diagnosis method, the accuracy improved by 7.74~10.8%.  \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The Arena Fragmentation Test(AFT) is designed to analyze warhead performance by measuring fragmentation data. In order to evaluate the results of the AFT, a set of AFT images are captured by high-speed cameras. To detect objects in the AFT image set, ResNet-50 based Faster R-CNN is used as a detection model. However, because of the low resolution of the AFT image set, a detection model has shown low performance. To enhance the performance of the detection model, Super-resolution(SR) methods are used to increase the AFT image set resolution. To this end, The Bicubic method and three SR models: ZSSR, EDSR, and SwinIR are used. The use of SR images results in an increase in the performance of the detection model. While the increase in the number of pixels representing a fragment flame in the AFT images improves the Recall performance of the detection model, the number of pixels representing noise also increases, leading to a slight decreases in Precision performance. Consequently, the F1 score is increased by up to 9 %, demonstrating the effectiveness of SR in enhancing the performance of the detection model.  \n",
      "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This technical paper proposes an activation function, self-gated rectified linear unit (SGReLU), to achieve high classification accuracy, low loss, and low computational time. Vanishing gradient problem, dying ReLU, noise vulnerability are also resolved in our proposed SGReLU function. SGReLU’s performance is evaluated on MNIST, Fashion-MNIST, and Imagenet datasets and compared with seven highly effective activation functions. We obtained that the proposed SGReLU outperformed other activation functions in most cases in VGG16, Inception v3, and ResNet50. In VGG16 and Inception v3, it achieved an accuracy of 90.87% and 95.01%, respectively, exceeding other functions with the second-fastest computing time in these networks.  \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In this paper, we developed a store management system that detects the display status of products in real time and manages them to comply with the plan-o-gram using real-time product recognition and display status estimation technology based on deep learning. To achieve this, we input store shelf images, localize each product using the YOLOv8 deep learning model trained with the SKU110K data set, and generate an image feature vector using an em-bedder that fine-tunes the ResNet-50 model, combining the pre-registered reference image feature vector and Compare and identify and classify products. The plan-o-gram compliance control algorithm has been supplemented so that the plan-o-gram compliance status generated through sequence alignment of the modified Needleman-Wunsch(NW) algorithm can be utilized in actual store situations. Previously, there were many errors by judging the four states: exact match(MT), missing item(MI), added item in the correct position(ME), and added or misplaced item or empty space(NM). Accordingly, in this paper, we subdivide NM into 5 states (addition, deletion, location change, remote placement, and position shift), expand it to a total of 8 states, and present an algorithm that can handle various characteristic cases.  \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Object tracking is considered a challenging problem due to various environmental changes contained in the video sequence. Object tracking is the use of information given in the first frame to estimate the area and trajectory of a target object in a video sequence. In this paper, we propose an algorithm for object tracking using multi-scale feature maps of Resnet-50 and correlation filters. To accommodate changes in object scale, we create an appearance model using different sized feature maps extracted from each block in the network. In order to maintain the robustness of the appearance model, it is adaptively updated according to the peak value of the response map when occlusion occurs due to obstacles. Experiments result using the OTB2015 dataset showed that the proposed algorithm achieved competitive results on several image attribution such as low resolution, scale variation, occlusion and out of view.  \n",
      "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A dual-experience pool deep reinforcement learning (DEPDRL) model is proposed for rolling bearing fault diagnosis with unbalanced data. In this method, a dualexperience pool structure is designed to store the sample data of majority and minority classes.A parallel double residual network model is established to extract deep features of the majority and minority input samples, respectively. In the process of training, the proposed balanced cross-sampling technique is used to randomly select samples from dual-experience pool in a certain proportion to realize the training of a double residual network model. We show the effectiveness of our method on three standard data sets, and compared with Resnet18, DCNN, DQN and DQNimb methods, the results show that DEPDRL has the best performance. Finally, with wavelet time-frequency graph as input, DEPDRL is applied to rolling bearing fault diagnosis with unbalanced test data. The results show that on a variety of unbalanced data sets, both the diagnostic accuracy and the G-means value of the DEPDRL are more than 5 % higher than other algorithms, which fully indicates that the DEPDRL has a very high fault diagnosis ability of rolling bearing with unbalanced data.  \n",
      "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In traditional fish farming way, the workers have to observe all of the pools every time and every day to feed at the right timing. This method causes tremendous stress on workers and wastes time. To solve this problem, we implemented an automatic detection system for feeding time using deep neural network. The detection system consists of two steps: classification of the presence or absence of feed and checking DO (Dissolved Oxygen) of the pool. For the classification, the pretrained ResNet18 model and transfer learning with custom dataset are used. DO is obtained from the DO sensor in the pool through HTTP in real time. For better accuracy, the next step, checking DO proceeds when the result of the classification is absence of feed several times in a row. DO is checked if it is higher than a DO reference value that is set by the workers. These actions are performed automatically in the UI programs developed with LabVIEW.  \n",
      "61                                                                                                                                                                                                            Gastric cancer has a high incidence in East Asians, and the risk increases over time. Often, gastric cancer presents no early symptoms, leading to missed treatments. Consequently, in Korea, support is provided to individuals over 40 years of age who undergo gastroscopy. However, as the number of gastroscopy patients increases, doctors' fatigue rises, becoming a factor that can lead to misdiagnosis. Therefore, this paper proposes a CADx (Computer-Aided Diagnosis) system for gastric lesion classification based on ConvNeXt and ViT (Vision Transformer), applying the SAM (Sharpness Aware Minimization) optimizer. ConvNeXt is a network that achieves high performance by incorporating techniques from Swin Transformer and the latest advancements, with ResNet-50 as the base model. ViT divides the image into smaller patches and uses these patches as input to the Transformer. This allows for learning relationships between patches and ultimately leads to image classification. To address the issue of limited data in medical images, the gastric abnormal dataset was augmented using the AutoAugment policy. The SAM Optimizer is an optimization technique that detects and minimizes the \"sharpness\" of the loss function that may occur during the deep learning model's learning process. Using this method, the sensitivity of classifying abnormal and normal gastroscopy images in ConvNeXt increased from 0.7167 to 0.9583 for the original dataset and from 0.7583 to 0.9833 for the augmented dataset. ViT exhibited a significant decrease from 0.9500 to 0.7750 in the original dataset but increased from 0.9500 to 0.9583 in the augmented dataset. This demonstrates that the SAM Optimizer can effectively enhance CADx performance.  \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Accurate identification of landmarks is critical for effective diagnosis and treatment in endoscopy, particularly in the upper gastrointestinal tract. However, there are many similar structures inside the stomach, and it might be difficult to accurately locate landmarks in camera images because of other factors such as air bubbles and the narrow field of view of wired endoscopic images. This study presents a comparative analysis experiment conducted with a model that can identify anatomical landmarks of the upper gastrointestinal tract with high accuracy through small-scale data-based local augmentation. We used five classes captured by esophagogastroduodenoscopy criterion, preprocessed medical image data to address the class imbalance, and compared the accuracies of ResNet50, MobileNetV2, and DensNet265 models. We used a dataset comprising 2,546 images of patients who underwent upper gastrointestinal endoscopy at Yonsei Severance Hospital. We augmented 4,632 images and evenly distributed them across five classes. Our results indicate that this is the most accurate model for improving diagnosis and treatment in upper gastrointestinal endoscopy. The ReseNet50 model achieved the highest accuracy at 74.88%, followed by the MobileNetV2 model at 78.91% and DensNet265 at 84.70%.  \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Stroke causes muscle dysfunction in lower limb depending on the brain damage. Therefore, it is important to identify the degree of muscle damaged and perform rehabilitation training for appropriate time of treatment. However, there is a limitation in that analysis using existing imaging techniques and artificial intelligence cannot analyze disease mechanisms.This paper aims to develop an AI GUI system using SRI to acquire damaged muscle regions, segment them into fiber and space areas, and classify them. For the segmentation, Attention U-Net performed best accuracy 95.32%. For the classification, ResNet50 with Attention U-Net performed best accuracy 99.07%. As a result of this, we designated the best performing network as suitable for stroke animal models. As an auxiliary tool for diagnosing the degree of stroke muscle damage in clinical practice, we constructed a system to analyze the degree of stroke fiber distribution on SRI images using pixel intensity values to show the results. Through this study, it is system that uses deep learning in the stroke animal model can be applied as a basic study for objective muscle tissue evaluation.  \n",
      "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this study, We conduct a comparative study of deep learning-based classification of agricultural field attributes using Tagged Image File (TIF) andEnhanced Compression Wavelet (ECW) images. The goal is to interpret and classify the attributes of agricultural fields by analyzing the differencesbetween these two image formats. “FarmMap,” initiated by the Ministry of Agriculture, Food and Rural Affairs in 2014, serves as the first digital mapof agricultural land in South Korea. It comprises attributes such as paddy, field, orchard, agricultural facility and ginseng cultivation areas. For thepurpose of comparing deep learning-based agricultural attribute classification, we consider the location and class information of objects, as well as theattribute information of FarmMap. We utilize the ResNet-50 instance segmentation model, which is suitable for this task, to conduct simulatedexperiments. The comparison of agricultural attribute classification between the two images is measured in terms of accuracy. The experimental resultsindicate that the accuracy of TIF images is 90.44%, while that of ECW images is 91.72%. The ECW image model demonstrates approximately 1.28%higher accuracy. However, statistical validation, specifically Wilcoxon rank-sum tests, did not reveal a significant difference in accuracy between thetwo images.  \n",
      "66                                                                                                                                                                              Purpose Deep learning (DL) has been widely used in various medical imaging analyses. Because of the difficulty in processingvolume data, it is difficult to train a DL model as an end-to-end approach using PET volume as an input for variouspurposes including diagnostic classification. We suggest an approach employing two maximum intensity projection (MIP)images generated by whole-body FDG PET volume to employ pre-trained models based on 2-D images.Methods As a retrospective, proof-of-concept study, 562 [18F]FDG PET/CT images and clinicopathological factors oflung cancer patients were collected. MIP images of anterior and lateral views were used as inputs, and image features wereextracted by a pre-trained convolutional neural network (CNN) model, ResNet-50. The relationship between the imageswas depicted on a parametric 2-D axes map using t-distributed stochastic neighborhood embedding (t-SNE), with clinicopathologicalfactors.Results A DL-based feature map extracted by two MIP images was embedded by t-SNE. According to the visualizationof the t-SNE map, PET images were clustered by clinicopathological features. The representative difference between theclusters of PET patterns according to the posture of a patient was visually identified. This map showed a pattern of clusteringaccording to various clinicopathological factors including sex as well as tumor staging.Conclusion A 2-D image-based pre-trained model could extract image patterns of whole-body FDG PET volume by usinganterior and lateral views of MIP images bypassing the direct use of 3-D PET volume that requires large datasets andresources. We suggest that this approach could be implemented as a backbone model for various applications for wholebodyPET image analyses.  \n",
      "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Gait analysis is an important tool in the clinical management of cerebral palsy, allowing for the assessment of condition severity, identification of potential gait abnormalities, planning and evaluation of interventions, and providing a baseline for future comparisons. However, traditional methods of gait analysis are costly and time-consuming, leading to a need for a more convenient and continuous method. This paper proposes a method for analyzing the posture of cerebral palsy patients using only smartphone videos and deep learning models, including a ResNet-based image tilt correction, AlphaPose for human pose estimation, and SmoothNet for temporal smoothing. The indicators employed in medical practice, such as the imbalance angles of shoulder and pelvis and the joint angles of spine-thighs, knees and ankles, were precisely examined. The proposed system surpassed pose estimation alone, reducing the mean absolute error for imbalance angles in frontal videos from 4.196° to 2.971° and for joint angles in sagittal videos from 5.889° to 5.442°.  \n",
      "70                                                                                                                                                                                                                                                                      Objective : Deep learning is a machine learning approach based on artificial neural network training, and object detection algorithm using deep learning is used as the most powerful tool in image analysis. We analyzed and evaluated the diagnostic performance of a deep learning algorithm to identify skull fractures in plain radiographic images and investigated its clinical applicability.Methods : A total of 2026 plain radiographic images of the skull (fracture, 991; normal, 1035) were obtained from 741 patients. The RetinaNet architecture was used as a deep learning model. Precision, recall, and average precision were measured to evaluate the deep learning algorithm’s diagnostic performance.Results : In ResNet-152, the average precision for intersection over union (IOU) 0.1, 0.3, and 0.5, were 0.7240, 0.6698, and 0.3687, respectively. When the intersection over union (IOU) and confidence threshold were 0.1, the precision was 0.7292, and the recall was 0.7650. When the IOU threshold was 0.1, and the confidence threshold was 0.6, the true and false rates were 82.9% and 17.1%, respectively. There were significant differences in the true/false and false-positive/false-negative ratios between the anteriorposterior, towne, and both lateral views (p=0.032 and p=0.003). Objects detected in false positives had vascular grooves and suture lines. In false negatives, the detection performance of the diastatic fractures, fractures crossing the suture line, and fractures around the vascular grooves and orbit was poor.Conclusion : The object detection algorithm applied with deep learning is expected to be a valuable tool in diagnosing skull fractures.  \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Defect detection is one of the key factors in fabric quality control. To improve the speed and accuracy of denimfabric defect detection, this paper proposes a defect detection algorithm based on cascading feature extractionarchitecture. Firstly, this paper extracts these weight parameters of the pre-trained VGG16 model on the largedataset ImageNet and uses its portability to train the defect detection classifier and the defect recognitionclassifier respectively. Secondly, retraining and adjusting partial weight parameters of the convolution layerwere retrained and adjusted from of these two training models on the high-definition fabric defect dataset. Thelast step is merging these two models to get the defect detection algorithm based on cascading architecture.Then there are two comparative experiments between this improved defect detection algorithm and otherfeature extraction methods, such as VGG16, ResNet-50, and Xception. The results of experiments show thatthe defect detection accuracy of this defect detection algorithm can reach 94.3% and the speed is also increasedby 1–3 percentage points.  \n",
      "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Random erasing offers various levels of occlusion for data augmentation. However, due to its uniform distribution of random selection, it sometimes occludes regions that are unrelated to the object of interest. In this paper, we propose a novel method that utilizes Gradient Weighted Class Activation Mapping (Grad-CAM) for estimating the location of the object of interest and selectively erasing the surrounding areas. By utilizing Grad-CAM, we improve random erasing for CNN models without requiring additional modules or architectural changes. We generate Grad-CAM after the intermediate epochs where CNN models have sufficient representational power for the training data. The hyperparameter that restrict the erasing to the vicinity of the object is set based on Grad-CAM, and experiments were conducted accordingly. As a result of our experiments, we observed a 0.33% decrease in error-rate for image classification tasks using ResNet-20 on the CIFAR-10 dataset.  \n",
      "73                                                                                                                                                                        Purpose: The current Classification of Periodontal and Peri-Implant Diseases and Conditions, published and disseminated in 2018, involves some difficulties and causes diagnostic conflicts due to its criteria, especially for inexperienced clinicians. The aim of this study was to design a decision system based on machine learning algorithms by using clinical measurements and radiographic images in order to determine and facilitate the staging and grading of periodontitis.Methods: In the first part of this study, machine learning models were created using the Python programming language based on clinical data from 144 individuals who presented to the Department of Periodontology, Faculty of Dentistry, Süleyman Demirel University. In the second part, panoramic radiographic images were processed and classification was carried out with deep learning algorithms.Results: Using clinical data, the accuracy of staging with the tree algorithm reached 97.2%, while the random forest and k-nearest neighbor algorithms reached 98.6% accuracy. The best staging accuracy for processing panoramic radiographic images was provided by a hybrid network model algorithm combining the proposed ResNet50 architecture and the support vector machine algorithm. For this, the images were preprocessed, and high success was obtained, with a classification accuracy of 88.2% for staging. However, in general, it was observed that the radiographic images provided a low level of success, in terms of accuracy, for modeling the grading of periodontitis.Conclusions: The machine learning-based decision system presented herein can facilitate periodontal diagnoses despite its current limitations. Further studies are planned to optimize the algorithm and improve the results.  \n",
      "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The Ministry of Agriculture, Food and Rural Affairs established the FarmMap, an digital map of agricultural land. In this study, using deep learning, we suggest the application of farm map reading to farmland such as paddy fields, fields, ginseng, fruit trees, facilities, and uncultivated land. The farm map is used as spatial information for planting status and drone operation by digitizing agricultural land in the real world using aerial and satellite images. A reading manual has been prepared and updated every year by demarcating the boundaries of agricultural land and reading the attributes. Human reading of agricultural land differs depending on reading ability and experience, and reading errors are difficult to verify in reality because of budget limitations. The farmmap has location information and class information of the corresponding object in the image of 5 types of farmland properties, so the suitable AI technique was tested with ResNet50, an instance segmentation model. The results of attribute reading of agricultural land using deep learning and attribute reading by humans were compared. If technology is developed by focusing on attribute reading that shows different results in the future, it is expected that it will play a big role in reducing attribute errors and improving the accuracy of digital map of agricultural land.  \n",
      "75   Lung cancer ranked second in Korea domestic cancer incidence in 2020 and second in death rate. Lung cancer often has no early symptoms, so patients often miss the time of treatment. Accordingly, in Korea, lung cancer has been included in the national cancer screening since 2019. However, among misdiagnosis cases, lung cancer had the highest misdiagnosis rate, and the accuracy of screening may vary depending on the medical specialist's skill level and fatigue. Accordingly, this paper proposed a lung cancer CADx(Computer-Aided Diagnosis) system based on EfficientNetV2-L and ConvNeXt-B. EfficientNetV2 is a model that can have high classification performance with a small number of parameters using the Training-Aware NAS (Neural Architecture Search) method. ConvNeXt is a network that achieves higher performance than ViT(Vision Transformer) by combining the latest techniques with ResNet-50 as a base model. Medical imaging generally suffers from a data shortage problem. Therefore, we augmented the lung cancer dataset using AutoAugment using the ImageNet augmentation policy. Through this method, the sensitivity in classifying malignant(lung cancer) and normal improved from 0.8354 to 0.9638 in EfficientNetV2 and from 0.9796 to 0.9963 in ConvNeXt.AUC (Area Under the ROC Curve) also improved from 0.9967 to 0.9974 for EfficientNetV2 and from 0.9973 to 1.0000 for ConvNeXt. Additionally, noise that may generally occur in CT images was added and compared through Gaussian noise.EfficientNetV2's Sensitivity was 0.7417 in the original model and 0.8954 in the model to which AutoAugment was applied, representing a decrease of 9.37% and 6.84%, respectively. In contrast, ConvNeXt exhibited a Sensitivity of 0.9796 in the original model and 0.9963 in the model to which AutoAugment was applied, showing no decrease in performance. This led to the development of a CADx system that demonstrates excellent performance.  \n",
      "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Local road management agencies, which require frequent road surface inspections to maintain a safe driving environment, face difficulties in manually assessing road conditions. Moreover, stopping on the shoulder of a high-speed highway to inspect the road surface is extremely dangerous for inspectors. To overcome these challenges, there have been many efforts to automate road surface image acquisition, assessment, and analysis. However, existing methods mainly focus on detecting damage such as potholes and cracks, and have limitations in monitoring the condition of road surfaces, such as the need for repairs or the status of maintenance. This study proposed an automatic pavement patch detection method incorporating image processing and deep neural network image segmentation to provide a global view of the road surface. The panoramic image of the road is obtained by converting the video images captured by the omnidirectional camera of vehicles into top-view images using inverse perspective and affixing the images together. Next, the U-Net-architecture-based deep neural network image segmentation technique is applied to detect patches. Testing using actual highway driving videos revealed performances of 0.827 and 0.821 in the U-Net model based on ResNet152 and EfficientNetb7 backbones.  \n",
      "77                                                                                                                                                                                                                                                                                                                  CenterNet, a novel object detection algorithm without anchor based on key points, regards the object as a single center point for prediction and directly regresses the object’s height and width. However, because the objects have different sizes, directly regressing their height and width will make the model difficult to converge and lose the intrinsic relationship between object’s width and height, thereby reducing the stability of the model and the consistency of prediction accuracy. For this problem, we proposed an algorithm based on the regression of the diagonal half-length and the center angle, which significantly compresses the solution space of the regression components and enhances the intrinsic relationship between the decoded components. First, encode the object’s width and height into the diagonal half-length and the center angle, where the center angle is the angle between the diagonal and the vertical centreline. Secondly, the predicted diagonal half-length and center angle are decoded into two length components. Finally, the position of the object bounding box can be accurately obtained by combining the corresponding center point coordinates. Experiments show that, when using CenterNet as the improved baseline and resnet50 as the Backbone, the improved model achieved 81.6% and 79.7% mAP on the VOC 2007 and 2012 test sets, respectively. When using Hourglass-104 as the Backbone, the improved model achieved 43.3% mAP on the COCO 2017 test sets. Compared with CenterNet, the improved model has a faster convergence rate and significantly improved the stability and prediction accuracy.  \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              This study was conducted with the aim of improving the food image classification model of a health care application targeting Koreans in their twenties. 546,194 images were collected from the Public Data Portal and AI Hub, and 175 food classes were constructed. The ResNet artificial intelligence model was trained and validated. Additionally, we deeply investigated the reasons for the relatively lower recognition accuracy of the actual food images, and we attempted various methods to optimize the model’s performance as a solution.  \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The application of artificial intelligence techniques, specifically evolution algorithms in medical diagnosis has shown promising potential to enhance the accuracy and efficiency of disease identification. These days the trend is evolutional algorithms be on the turn CNN((Conversion Neural Network) algorithms. As a result of that this paper research applied to medical devices using CNN technology is being actively carried out around the world. In this study, chest x-ray image classification is performed using CNN-based ResNet as a medical image reading assistance technology. In this study, we will classify pneumonia. A total of 5,863 x-ray images are used, and the data is divided into pneumonia and normal, and consists of 3,875 pneumonia image data and 1,341 normal image data. Shortcut is connected to a cnn composed of convolutional layers, pooling layers, and fully connected layers to improve performance using fine tuning on CNN-ResNet, which has been learned to minimize residual. In this paper, only the use of the pre-learning model was considered as fine tuning, but the batch size and learning rate also affect the learning of the model. It is expected that further research to find the appropriate proportions will allow the performance of the model to be maintained more stable.  \n",
      "80                                                                                                                                                                                                                                                               PPneumonia is a respiratory infectious disease that causes fluids to fill the lungs. It is considered one of the leading causes of infection-related deaths in children and seniors worldwide. Clinicians usually use chest X-ray images to diagnose pneumonia. However, pneumonia is prone to be misdiagnosed because it overlaps with cold and flu, causing severe and critical medical complications. Consequently, alternative supportive diagnostic methods are needed to minimize human errors and assist clinicians. Several attempts have used artificial intelligence systems, mainly in deep learning methods, to assist clinicians in early pneumonia diagnoses. However, further studies are required to consolidate the use of deep learning as an assistant tool to diagnose pneumonia accurately. In this study, we examine the Swin Transformer and the Residual Neural Network’s performance in classifying pneumonia and healthy chest X-ray images using the Guangzhou Women and Children’s Medical Center dataset and the COVID19, Pneumonia and Normal Chest X-ray Posteroanterior dataset. The experiment results demonstrate that the Swin Transformer achieves an accuracy of 98.9% in the Chest X-ray images dataset and 92.35% in the COVID19, Pneumonia and Normal Chest X-ray Posteroanterior dataset, while the Residual Neural Network achieves an accuracy of 97.9% and 88.8% respectively in classifying pneumonia. These results indicate that the Swin Transformer outperforms the Residual Neural Network as a tool for assisting clinicians in diagnosing pneumonia. Thus, the Swin Transformer may help in early decision-making, leading to treatment initiation and improving patient's health.  \n",
      "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Recently, deep learning has been applied to various welding techniques, such as laser welding, gas metal arc welding (GMAW), and resistance spot welding, and research on automation and quality prediction is being conducted.Even for GMAW, many researchers have attempted to predict quality through X-ray, current, and voltage measurements. If judgment in real time is not necessary, it is most effective to judge the quality of a welded part using an exterior image. Therefore, in this study, a welded appearance quality judgment model was analyzed using image deep learning. Welding defects were classified into pores, overlaps, craters, melting of the base material, cracks, and undercuts, and were divided into 7 categories including normal ones. In constructing the deep-learning model, transfer learning was performed using existing networks, such as ResNet and AlexNet. To improve the accuracy of deep learning, tests were performed while the optimization technique, maximum number of epochs, and minibatch size were changed. It was confirmed that the accuracy of weld defect prediction improved as the minibatch size increased, and the stochastic gradient descent model had the highest accuracy. Increasing the number of data for learning should make the technique of using images to judge the quality of GMAW welds using the proposed model more widely applicable.  \n",
      "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Pipelines play an important role in urban water supply and drainage, oil and gas transmission, etc.This paper presents a technique for pattern recognition of fiber optic vibration signals collected by a distributed vibration sensing (DVS) system using a deep learning residual network (ResNet). The optical fiber is laid on the pipeline, and the signal is collected by the DVS system and converted into a 64 × 64 single-channel grayscale image. The grayscale image is input into the ResNet to extract features, and finally the K-nearest-neighbors (KNN) algorithm is used to achieve the classification and recognition of pipeline damage.  \n",
      "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This paper proposes a weight redistribution ensemble model using multi-mode feature fusion for detecting micro leaks arising from aging problems in the power plant piping systems. The collected data was transformed into 2D frequency and RMS pattern features, and various domain features extracted from multiple sensors were combined to form volume features. For the experiment, a single model based on ResNet was designed, and an ensemble structure using various volume features was composed. Additionally, in order to resolve the performance imbalance problem that could arise in the process of combining multiple predictive models, we applied a softmax function-based weight redistribution. Experimental results showed that the ensemble model for each sensor, using frequency and RMS volume features, provided the highest classification accuracy of 98.91%.  \n",
      "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Pine trees in Korea are the tree species with the highest diversity, accounting for half of the native coniferous trees. Pine trees once accounted for more than 60% of the nations forests, but the proportion has decreased to 25% due to infestations and wildfires. To classify Koreas native pine trees, we use the standard image dataset held by the National Institute of Biological Resources to classify three species. Instead of modifying a pre-trained model like ResNet 50, which has been proven to perform well, we instead directly implement an image recognition model to perform species classification, which showed an accuracy of 85%, and then determined the features derived from the recognition process. To understand the impact of image size and growth on classification, we evaluated performance of the model through image size and image growth. The performance improvement was about 3% for image size and 6.4% for image growth. We also compare its performance with a machine learning model based on data labeling. We extract the most weighted features for species classification from the image recognition process and compare them with feature importance used in the labeling method, finding that the most effective features for species classification in the labeling method and image recognition are similar.  \n",
      "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In this paper, we propose an Internal/External Knowledge Distillation (IEKD), which utilizes both external correlations between feature maps of heterogeneous models and internal correlations between feature maps of the same model for transferring knowledge from a teacher model to a student model. To achieve this, we transform feature maps into a sequence format and extract new feature maps suitable for knowledge distillation by considering internal and external correlations through a transformer. We can learn both internal and external correlations by distilling the extracted feature maps and improve the accuracy of the student model by utilizing the extracted feature maps with feature matching. To demonstrate the effectiveness of our proposed knowledge distillation method, we achieved 76.23% Top-1 image classification accuracy on the CIFAR-100 dataset with the “ResNet-32×4/VGG-8” teacher and student combination and outperformed the state-of-the-art KD methods.  \n",
      "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this paper, we propose a deep learning algorithm optimized for diagnosing factory facility abnormalities using thermal imaging. For this purpose, the contrast of the thermal image is clearly converted with the contrast enhancement algorithm to enhance the edge information. After that, the Convolution Vision Transformer (CvT) developed using only the advantages of Convolution Neural Network (CNN) and Transformer Network is modified to suit the diagnosis of thermal image-based failure facility abnormalities. Experiments were conducted by extracting normal and abnormal images of factory facilities from the thermal image provided by AI Hub. Through this, we confirmed the excellent performance of 98.79% which is higher accuracy than CNN-based ResNet, EfficientNet, and Transformer-based Vision Transformer (ViT), SwinT (Swin Transformer), which are commonly used in the existing computer vision field. In conclusion, it was confirmed that when using the CNN and Transformer fusion network, it shows better performance than the factory facility failure diagnosis algorithm using other thermal imaging images.  \n",
      "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The need for big data analysis and artificial intelligence for smart agriculture is continuously requested. It is essential to collect and label large amounts of quality data for artificial intelligence research, but there is a relative lack of data in the hydroponic cultivation area. In this paper the performance of growth diagnostic ML model with original anti-cancer leaf lettuce data set was checked. Then ML model development and model performance improvement experiment was proceeded with augmented data. First, ML model learning and testing was conducted using DCGAN data only. As a result, the accuracy of ResNet was 61.2, DenseNet was 62.4. And model performance improvement experiment was proceeded by adding augmented data to the original data. As a result, the accuracy of ResNet increase from 86.5 to 88.2 and DenseNet from 92.9 to 94.7. In these experiments the possibility of using augmented data and its influence are studied for developing and improving the performance of diagnostic ML model.  \n",
      "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In this study, we analyzed how to steal the target model without training data. Input data is generated using the generative model, and a similar model is created by defining a loss function so that the predicted values of the target model and the similar model are close to each other. At this time, the target model has a process of learning so that the similar model is similar to it by gradient descent using the logit (logic) value of each class for the input data. The tensorflow machine learning library was used as an experimental environment, and CIFAR10 and SVHN were used as datasets. A similar model was created using the ResNet model as a target model. As a result of the experiment, it was found that the model stealing method generated a similar model with an accuracy of 86.18% for CIFAR10 and 96.02% for SVHN, producing similar predicted values to the target model. In addition, considerations on the model stealing method, military use, and limitations were also analyzed.  \n",
      "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           In an era marked by the increasing use of drones and the growing demand for indoor surveillance, the development of a robust application for detecting and tracking both drones and humans within indoor spaces becomes imperative. This study presents an innovative application that uses FMCW radar to detect human and drone motions from the cloud point. At the outset, the DBSCAN (Density-based Spatial Clustering of Applications with Noise) algorithm is utilized to categorize cloud points into distinct groups, each representing the objects present in the tracking area. Notably, this algorithm demonstrates remarkable efficiency, particularly in clustering drone point clouds, achieving an impressive accuracy of up to 92.8%. Subsequently, the clusters are discerned and classified into either humans or drones by employing a deep learning model. A trio of models, including Deep Neural Network (DNN), Residual Network (ResNet), and Long Short-Term Memory (LSTM), are applied, and the outcomes reveal that the ResNet model achieves the highest accuracy. It attains an impressive 98.62% accuracy for identifying drone clusters and a noteworthy 96.75% accuracy for human clusters.  \n",
      "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Avocado, a superfood selected by Time magazine and one of the late ripening fruits, is one of the foods with a big difference between local prices and domestic distribution prices. If this sorting process of avocados is automated, it will be possible to lower prices by reducing labor costs in various fields. In this paper, we aim to create an optimal classification model by creating an avocado dataset through crawling and using a number of deep learning-based transfer learning models. Experiments were conducted by directly substituting a deep learning-based transfer learning model from a dataset separated from the produced dataset and fine-tuning the hyperparameters of the model. When an avocado image is input, the model classifies the ripeness of the avocado with an accuracy of over 99%, and proposes a dataset and algorithm that can reduce manpower and increase accuracy in avocado production and distribution households.  \n",
      "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This study proposed a deep learning parallel model for stock price prediction using stock price data, technical analysis data, and environmental factor data. The data set for prediction was divided into three, data set 1 is the opening price, high price, low price, closing price, and trading volume, data set 2 added technical analysis data, and data set 3 is the exchange rate that can affect the stock price. the overall industrial production index was added. The deep learning model proposed DNN, LSTM, and 1D-CNN models as basic models, and a DCNN model that merged 1D-CNN based on the DNN model as a parallel model, and a DLSTM model that merged LSTM as a parallel model. As a result of the experiment, the performance of LSTM and BiLSTM models was higher than that of DNN and CNN, and in particular, the DLSTM model, a parallel model, performed the best. For the DLSTM model, which is a parallel model, the RMSE of data set 1 was 0.0091, the RMSE of data set 2 was 0.0080, and the RMSE of data set 3 was 0.0071. Data set 3, which combined all data, had the best performance.  \n",
      "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In precipitation forecasting, the degree of damage varies according to the amount of precipitation per hour, which tells how much rain has fallen intensively in a short period of time, rather than the cumulative amount of precipitation. In this study, short-term precipitation prediction was performed using deep learning basic models such as DNN, LSTM, BiLSTM, and 1D-CNN models and a parallel structure merging the basic models to improve performance. Parallel models include DCNN, which combines DNN and 1D-CNN, and DLSTM, which combines DNN and LSTM. As for the data set, three data sets were used: a data set built with only rainy days, a data set from June to September, and a data set from May to October. Each data set was divided into seven data sets in detail. The precipitation was predicted and compared for a total of 21 data sets. As a result of the experiment, the third data set had the best prediction results. In particular, the RMSE of the 5th detailed data set, the DLSTM   parallel model, was 0.25, which was about 10 times better than other models.  \n",
      "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 None  \n",
      "95                                                                                                                                                                       The Indian Summer Monsoon Rainfall (ISMR) plays a significant role in India’s agriculture and economy. Our understandingof the climate dynamics of the Indian summer monsoon has been enriched with general circulation models (GCMs)and regional climate models (RCMs). Systematic bias associated with these numerical simulations, however, needs to becorrected before we can obtain accurate or reliable projections of the future. Therefore, this study applies two state-of-theartdeep-learning (DL)-based super-resolution bias correction (SRBC) methods, viz. Autoencoder-Decoder (ACDC) and adeeper network Residual Neural Network (ResNet) to perform spatial downscaling and bias-correction on high-resolutionCORDEX-SA climatic simulations of precipitation. To do so, we obtained eight meteorological variables from CORDEXSARCM simulations along with a digital elevation model at a spatial resolution of 0.25°×0.25° as input. Indian MonsoonData Assimilation and Analysis, precipitation reanalysis re-grided to 0.05°×0.05° spatial resolution is chosen as output forthe training period 1979–2005. To evaluate the DL algorithms, the RCP 2.6 scenario of CORDEX-SA future simulationsfor the period 2006–2020 is chosen. Moreover, we also conducted a performance assessment of the representation of mean,variability, extreme, and frequency of rainfall associated with ISMR. The results of the experiments show that the DL methodResNet a highly efficient in (i) improving the spatial resolution of the climatic simulations from 0.25°×0.25° to 0.05°×0.05°,(ii) reducing the systematic biases of the extreme rainfall of ISMR from 21.18 mm to -7.86 mm, and (iii) providing a robustbias-corrected climate simulation of ISMR for future climate mitigation and adaptation studies.  \n",
      "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   A convolutional neural network(CNN) is a deep neural network which is composed of many layers with convolution filters and sub-sampling operations. Because such complex structure of CNNs makes its effective train difficult, it is necessary to study more complicated and stable methods to train the CNNs than the existing ones. Therefore, in this paper, we introduce various methods utilizing gradients complicatedly and propose a new optimizer for CNNs, called CGAU-CNN, based on the methods. In the practical experiments, we trained two CNN models, i.e, ResNet and DenseNet, by utilizing CGAU-CNN and evaluated its image classification accuracy. As a result, we found that CGAU-CNN could train them with faster convergence and better accuracy than the existing optimizers such as Adam.  \n",
      "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The spectrum dominance strategy, which has recently been emphasized in the field of electronic warfare, requires pervasive spectrum awareness. However, the ability to continuously monitor all critical portions of the spectrum and identify specific threats, which is a key requirement for pervasive spectrum awareness, is still weak. In this paper, we implemented a radio frequency fingerprint identification system and presented the performance of two key functions for pervasive spectrum awareness. The implemented system applies a compressed sensing receiving technique that can simultaneously detect radio frequency signals existing in different frequency bands for continuous monitoring of the spectrum. To implement a specific emitter identification function, the 2-D ResNet model based on the convolutional neural network was converted into a 1-D model and applied to radio frequency signal identification.  \n",
      "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Despite the diverse forms and designs in which fonts are widely utilized in our daily lives, including videos, print materials, products, websites, and mobile sites, numerous copyright issues continue to arise. Despite efforts to address these concerns and take measures for improvement, a variety of issues persist. This research serves as a preliminary study to enhance this situation. The goal was to implement a detection model for fonts used in texts within challenging media such as images and videos, aiming to mitigate font copyright issues that arise across various mediums. As a preliminary step, a model for recognizing fonts used within videos, image, pdf was developed. The model consists of two primary components: a text detection and background removal model within images, and a font recognition model used within the text. The text detection model was refined through various approaches such as image processing and deep learning to identify the optimal model. For the font recognition model, comparisons were made between CNN and ResNet models to select the most suitable one. As a result, an integrated 2-stage model was constructed. Validation was performed using arbitrary video data, revealing a top-1 rate is 76% and a top-5 rate is 94%.  \n",
      "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Information and large number of fault labels are required to achieve intelligent health status assessment of three-phase inverters. However, the current signals of inverters cannot be sufficiently collected since open-circuit faults (OCFs) occur briefly, which makes it difficult to determine the OCF mode of the various power switches. A transfer learning model that effectively uses a small amount of sample data to achieve domain adaptation is proposed to address this problem. First, collected fault-sensitive signals are subjected to a continuous wavelet transform (CWT) to obtain two-dimensional image data with more abundant fault feature information. Second, the source domain and target domain features are projected into the same feature space through a domain adversarial neural network (DANN) to achieve multi-domain feature extraction and adaptation. Then, in the feature extraction module of the DANN, the deep residual network (Resnet) structure is used to replace the typical convolutional neural network (CNN) structure. Finally, an intelligent diagnosis network is used to identify the health status of the inverter samples under variable conditions. Experimental results show that the proposed model can accurately and effectively realize the cross-domain health assessment of three-phase inverters in the case of small samples. The accuracy of the proposed model is better than that of other classical transfer learning models.  \n",
      "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Building objects are an essential spatial information source that can be used in fields such as 3D modeling, urban expansion, and environmental analysis. They are one of the geographical features for which continuous information construction is essential but are not easy to construct automatically. As a solution to this problem, methods have been proposed to develop new heavy neural networks or utilize transfer learning, but there are still limitations. This study conducted an experiment to determine the models classification performance according to the weight and the possibility of using the transfer learning technique using ImageNet weights in remote sensing. For this purpose, AiHubs land cover map learning dataset was used, and U-Net and Deeplab V3+ classification models using MobileNet and ResNet as backbone neural networks were utilized. As a result of the experiment, the classification accuracy was found to be highest when transfer learning was not performed with the MobileNet-based U-Net model (f1-score: 0.8483). Additionally, visually, it was confirmed that the model learned from scratch rather than transfer learning depicted the building closer to the ground truth. This means that a variety of methods can be used to perform transfer learning without the need to limit the neural network, and it suggests that if there is an amount of data at the level provided by AiHub, a model with a certain level of classification accuracy can be created.  \n",
      "101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In this paper, we propose a process of increasing productivity by applying a deep learning-based defect detection and classification system to the prepreg fiber manufacturing process, which is in high demand in the field of producing composite materials. In order to apply it to toe prepreg manufacturing equipment that requires a solution due to the occurrence of a large amount of defects in various conditions, the optimal environment was first established by selecting cameras and lights necessary for defect detection and classification model production. In addition, data necessary for the production of multiple classification models were collected and labeled according to normal and defective conditions. The multi-classification model is made based on CNN and applies pre-learning models such as VGGNet, MobileNet, ResNet, etc. to compare performance and identify improvement directions with accuracy and loss graphs. Data augmentation and dropout techniques were applied to identify and improve overfitting problems as major problems. In order to evaluate the performance of the model, a performance evaluation was conducted using the confusion matrix as a performance indicator, and the performance of more than 99% was confirmed. In addition, it checks the classification results for images acquired in real time by applying them to the actual process to check whether the discrimination values are accurately derived.  \n",
      "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Detecting ships from satellite images is a challenging task in the domain of remote sensing. It is very important for security, traffic management and to avoid smuggling etc. SAR (Synthetic Aperture Radar) is mostly used technology for Maritime monitoring but now researchers are increasingly studying Optical Satellite Images based technologies. Image processing and Computer Vision techniques were previously used to detect ships. In this work, Convolutional Neural Network based approach is used to detect ships from the satellite imagery. Several Deep Learning models have been used and tested for this kind of task. We used state of art model Inception-Resnet that is pre trained on Image-Net dataset. We used the dataset \"Ships in Satellite Imagery\" to detect the presence of ships in an image. The dataset is publicly available on Kaggle.The results indicate adoption of transfer learning and data augmentation yields a successful detection of ships with an accuracy of more than 99%. Similarly, exploring different deep learning models for this task provide results with high accuracy for less training time.  \n",
      "103                                                                                                                                                                                                                                                                                                  With the rise of the Internet of Things, the security of such lightweight computing environments has become a hot topic. Lightweight block ciphers that can provide efficient performance and security by having a relatively simpler structure and smaller key and block sizes are drawing attention. Due to these characteristics, they can become a target for new attack techniques. One of the new cryptanalytic attacks that have been attracting interest is Neural cryptanalysis, which is a cryptanalytic technique based on neural networks. It showed interesting results with better results than the conventional cryptanalysis method without a great amount of time and cryptographic knowledge. The first work that showed good results was carried out by Aron Gohr in CRYPTO'19, the attack was conducted on the lightweight block cipher SPECK-/32/64 and showed better results than conventional differential cryptanalysis. In this paper, we first apply the Differential Neural Distinguisher proposed by Aron Gohr to the block ciphers HIGHT and GOST to test the applicability of the attack to ciphers with different structures. The performance of the Differential Neural Distinguisher is then analyzed by replacing the neural network attack model with five different models (Multi-Layer Perceptron, AlexNet, ResNext, SE-ResNet, SE-ResNext). We then propose a Related-key Neural Distinguisher and apply it to the SPECK-/32/64, HIGHT, and GOST block ciphers. The proposed Related-key Neural Distinguisher was constructed using the relationship between keys, and this made it possible to distinguish more rounds than the differential distinguisher.  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                                                                                                        keyword  \\\n",
      "1                                                                                                           딥러닝   \n",
      "11                                                                                                Deep learning   \n",
      "2                                                                                                        ResNet   \n",
      "102                                                                                               Deep Learning   \n",
      "45                                                                                                          CNN   \n",
      "21                                                                                                     ResNet50   \n",
      "19                                                                                               Classification   \n",
      "205                                                                                                           .   \n",
      "5                                                                                                 deep learning   \n",
      "26                                                                                            Transfer Learning   \n",
      "115                                                                                                      이미지 분류   \n",
      "122                                                                                            Machine learning   \n",
      "59                                                                                                     Xception   \n",
      "183                                                                                                      데이터 증강   \n",
      "119                                                                                        Image Classification   \n",
      "254                                                                                           Data Augmentation   \n",
      "123                                                                                     Artificial intelligence   \n",
      "236                                                                                                       전이 학습   \n",
      "148                                                                                                       VGG16   \n",
      "74                                                                                           Vision Transformer   \n",
      "224                                                                                        Image classification   \n",
      "152                                                                                                    Resnet50   \n",
      "136                                                                                                        CCTV   \n",
      "262                                                                                         deep neural network   \n",
      "155                                                                                                     합성곱 신경망   \n",
      "166                                                                                                       Vgg16   \n",
      "311                                                                                                        CADx   \n",
      "145                                                                                             Quality control   \n",
      "60                                                                                                         CRNN   \n",
      "208                                                                                                        기계학습   \n",
      "63                                                                                                       딥러닝 모델   \n",
      "158                                                                                             Computer Vision   \n",
      "301                                                                                        high-level synthesis   \n",
      "137                                                                                              deep  learning   \n",
      "302                                                                                                        FPGA   \n",
      "126                                                                                                   ResNet-50   \n",
      "195                                                                                                       X-ray   \n",
      "81                                                                                                     ResNet34   \n",
      "313                                                                                  Convolution Neural Network   \n",
      "372                                                                                           Transfer learning   \n",
      "187                                                                                                        전이학습   \n",
      "87                                                                                                        VGG19   \n",
      "88                                                                                                   의미론적 영상 분할   \n",
      "363                                                                                            Object detection   \n",
      "178                                                                                           data augmentation   \n",
      "176                                                                                            Image processing   \n",
      "153                                                                                                 InceptionV3   \n",
      "325                                                                                                     FarmMap   \n",
      "170                                                                                                          AI   \n",
      "165                                                                                                   Googlenet   \n",
      "421                                                                                                      1D-CNN   \n",
      "374                                                                                            Swin Transformer   \n",
      "420                                                                                              parallel model   \n",
      "345                                                                                                    ImageNet   \n",
      "16                                                                                 Convolutional neural network   \n",
      "285                                                                                                      Stroke   \n",
      "226                                                                                                 Chest X-ray   \n",
      "290                                                                                            Machine Learning   \n",
      "13                                                                                          Pneumonia detection   \n",
      "435                                                                                convolutional neural network   \n",
      "288                                                                                                        머신러닝   \n",
      "287                                                                                       Semantic Segmentation   \n",
      "284                                                                                                     XGBoost   \n",
      "46                                                                                                      AlexNet   \n",
      "335                                                                                                       자세 분류   \n",
      "267                                                                                             AAPM CT phantom   \n",
      "334                                                                           Deep Convolutional Neural Network   \n",
      "333                                                                                Convolutional Neural Network   \n",
      "332                                                                                             Pose Estimation   \n",
      "268                                                                                     Quantitative evaluation   \n",
      "291                                                                                       Animal Classification   \n",
      "266                                                                                                          CT   \n",
      "269                                                                                             Infrared camera   \n",
      "270                                                                                                 Res Net 101   \n",
      "330                                                                                           Image recognition   \n",
      "271                                                                                                 Squeeze Net   \n",
      "329                                                                                               Breast cancer   \n",
      "272                                                                                             Trustworthiness   \n",
      "328                                                                                                      이미지 분할   \n",
      "273                                                                                                    Grad-CAM   \n",
      "327                                                                                                         유방암   \n",
      "331                                                                                                segmentation   \n",
      "264                                                                                         parallel processing   \n",
      "265                                                                                                partitioning   \n",
      "341                                                                                          Temporal smoothing   \n",
      "256                                                                                                         GAN   \n",
      "344                                                                                      Denim Defect Detection   \n",
      "343                                                                   Cascading Feature Extraction Architecture   \n",
      "257                                                                                    Biometric Identification   \n",
      "258                                                                                           Black Bengal Goat   \n",
      "259                                                                                         Goat Identification   \n",
      "342            Deep learning · Artificial intelligence · Radiography · Skull fractures · Traumatic brain injury   \n",
      "340                                                                                             Tilt correction   \n",
      "336                                                                                                  심층 합성곱 신경망   \n",
      "260                                                                                                  Iris Image   \n",
      "261                                                                                       Iris Pattern Matching   \n",
      "339                                                                                       Human pose estimation   \n",
      "263                                                                                      deep-learning compiler   \n",
      "338                                                                                Postural imbalance detection   \n",
      "337                                                                                               Gait analysis   \n",
      "274                                                                                                         XAI   \n",
      "326                  Deep learning · PET/CT · Maximum intensity projection · Convolutional neural network · FDG   \n",
      "322                                                                                                 X-ray Image   \n",
      "275                                                                                                  ResNet50V2   \n",
      "324                                                                                          agricultural  land   \n",
      "308                                                                                                     LabVIEW   \n",
      "283                                                                                                     입술 특징추출   \n",
      "307                                                                                             Smart fish farm   \n",
      "306  · Deep reinforcement learning · Dual-experience pool · Unbalanced data · Rolling bearing · Fault diagnosis   \n",
      "305                                                                                                     데이터 재사용   \n",
      "304                                                                                                     CNN 가속기   \n",
      "346                                                                                                  Robustness   \n",
      "300                                                                                            CNN acceleration   \n",
      "299                                                                                 Convolution Neural Networks   \n",
      "286                                                                                     Lips Feature Extraction   \n",
      "298                                                                                          Correlation Filter   \n",
      "297                                                                                             Object Tracking   \n",
      "296                                                                                                        YOLO   \n",
      "289                                                                                                       동물 분류   \n",
      "295                                                                                             Product Display   \n",
      "294                                                                                                 Plan-o-gram   \n",
      "293                                                                                                       상품 전시   \n",
      "309                                                                                         Automatic detection   \n",
      "310                                                                                         Deep neural network   \n",
      "282                                                                                                      뇌졸중 진단   \n",
      "318                                                                                                        헬스케어   \n",
      "323                                                                                               Web Framework   \n",
      "276                                                                                                  Fireblight   \n",
      "292                           Image classificationActivation functionDeep neural networkAccuracyTime complexity   \n",
      "277                                                                                                Flooded Road   \n",
      "321                                                                                                 Health Care   \n",
      "320                                                                                                     웹 프레임워크   \n",
      "319                                                                                                     X-선 이미지   \n",
      "278                                        Allium seed coat deep learning image recognition seed coat sculpture   \n",
      "281                                                                               Facial Expression Recognition   \n",
      "317                                                                                                  Biomedical   \n",
      "316                                                                               Synchrotron Radiation Imaging   \n",
      "315                                                                                            endoscopic image   \n",
      "279                                                                                        Intermediate Feature   \n",
      "280                                                                                     Artificial Intelligence   \n",
      "314                                                                                     artificial intelligence   \n",
      "312                                                                                           Gastric Diagnosis   \n",
      "303                                                                                                  data reuse   \n",
      "0                                                                                                      유아 행동 인식   \n",
      "347                                                                                        Class Activation Map   \n",
      "422                                                                                      precipitation forecast   \n",
      "433                                                                                                       차량단말기   \n",
      "432                                                                                                        인공지능   \n",
      "431                                                                                                        상황분류   \n",
      "430                                                                                                          검지   \n",
      "429                                                                                                        돌발상황   \n",
      "428                                                                                                        고속도로   \n",
      "427                                                                                            Vehicle terminal   \n",
      "426                                                                                        Scene classification   \n",
      "425                                                                                                   Detection   \n",
      "424                                                                                        Emergency situations   \n",
      "423                                                                                                     Highway   \n",
      "419                                                                                              stock forecast   \n",
      "436                                                                                                optimization   \n",
      "418                                                                                                    DenseNet   \n",
      "417                                                                                                WideResnet50   \n",
      "416                                                                                                Inception V3   \n",
      "415                                                                                                DenseNet 121   \n",
      "414                                                                                                   ResNet 50   \n",
      "413                                                                                                      VGG-16   \n",
      "412                                                                                                    Melanoma   \n",
      "411                                                                                                 Skin Cancer   \n",
      "410                                                                                                         흑색종   \n",
      "409                                                                                                         피부암   \n",
      "408                                                                                                         UAV   \n",
      "434                                          Regional climate · Indian monsoon · Climate change · Deep learning   \n",
      "437                                                                                    first-order optimization   \n",
      "406                                                                                                      DBSCAN   \n",
      "451                                                                                                       건물 탐지   \n",
      "462                                                                                               Distinguisher   \n",
      "461                                                                                        Neural Cryptanalysis   \n",
      "460                                                                                          Related-key Attack   \n",
      "459                     Maritime monitoring · Optical satellite imagery · CNN · Remote sensing · Marine traffic   \n",
      "458                                                                                                     Textile   \n",
      "457                                                                                           Defect inspection   \n",
      "456                                                                                                 Multi Class   \n",
      "455                                                                                                          섬유   \n",
      "454                                                                                                       결함 검사   \n",
      "453                                                                                                      다중 클래스   \n",
      "452                                                                                                      모델 가중치   \n",
      "450                                                                                                     의미론적 분할   \n",
      "438                                                                                            gradient descent   \n",
      "449                                                                                         Weight of the Model   \n",
      "448                                                                                          Building Detection   \n",
      "447                                                                           Domain adversarial neural network   \n",
      "446                                                                                       Deep residual network   \n",
      "445                                                                                    Health status assessment   \n",
      "444                                                                                              Text Detection   \n",
      "443                                                                                                Segmentation   \n",
      "442                                                                                                 Hangul Font   \n",
      "441                                                                                               Deep-Learning   \n",
      "440                                                                  radio frequency fingerprint identification   \n",
      "439                                                                                          compressed sensing   \n",
      "407                                                                                                  FMCW radar   \n",
      "405                                                                                            machine learning   \n",
      "348                                                                                              Random Erasing   \n",
      "361                                                                        Automated pavement monitoring system   \n",
      "375                                                                                                       폐렴 감지   \n",
      "373                                                                                                 Chest x-ray   \n",
      "371                                                                                                   Pneumonia   \n",
      "370                                                                                           Medical diagnosis   \n",
      "369                                                                                                   음식 이미지 분류   \n",
      "368                                                                                   Food Image Classification   \n",
      "367                                                                                           Convergence speed   \n",
      "366                                                                                        Accuracy consistency   \n",
      "365                                                                                        Prediction stability   \n",
      "364                                                                                                   CenterNet   \n",
      "362                                                                                 Inverse perspective mapping   \n",
      "255                                                                                                         DNN   \n",
      "377                                                                                        Deep learning models   \n",
      "359                                                                                    Pavement patch detection   \n",
      "358                                                                                             Malignant Tumor   \n",
      "357                                                                                                 Lung cancer   \n",
      "356                                                                                            Cancer Diagnosis   \n",
      "355                                                                                                        항공영상   \n",
      "354                                                                                                          팜맵   \n",
      "353                                                                                                         농경지   \n",
      "352                                                                                              Aviation Image   \n",
      "351                                                                                           Agricultural Land   \n",
      "350                                                                                               Periodontitis   \n",
      "349                                                                                              Generalization   \n",
      "376                                                                                                     흉부 엑스레이   \n",
      "378                                                                                                Weld quality   \n",
      "404                                                                                     anti-caner leaf lettuce   \n",
      "392                                                                                           model compression   \n",
      "403                                                                                       hydroponic conditions   \n",
      "402                                                                                            Factory facility   \n",
      "401                                                                                       Abnormality diagnosis   \n",
      "400                                                                                             Thermal imaging   \n",
      "399                                                                                                 Transformer   \n",
      "398                                                                                                     상관관계 학습   \n",
      "397                                                                                                       트랜스포머   \n",
      "396                                                                                                       모델 압축   \n",
      "395                                                                                                       지식 증류   \n",
      "394                                                                                        correlation learning   \n",
      "393                                                                                                 transformer   \n",
      "391                                                                                      Knowledge distillation   \n",
      "379                                                                                             Weld appearance   \n",
      "390                                                                                          Feature Importance   \n",
      "389                                                                                               Data Labeling   \n",
      "388                                                                                                   Pine Tree   \n",
      "387                                                                                       weight redistribution   \n",
      "386                                                                                         pipe leak detection   \n",
      "385                                                                                                    ensemble   \n",
      "384                                                                                   multi-mode feature fusion   \n",
      "383                                                                                            Residual network   \n",
      "382                                                                                         Pattern recognition   \n",
      "381                                                                   Distributed fiber optic vibration sensing   \n",
      "380                                                                            CNN (Convolution Neural Network)   \n",
      "360                                                                                                       U-net   \n",
      "232                                                                                                  Pretrained   \n",
      "253                                                                                                    CycleGAN   \n",
      "252                                                                                                       실험 구성   \n",
      "93                                                                                                         Swin   \n",
      "92                                                                                                       Resnet   \n",
      "91                                                                                            Backbone networks   \n",
      "90                                                                                         Deep neural networks   \n",
      "89                                                                             Pedestrian attribute recognition   \n",
      "86                                                                                                         UNet   \n",
      "85                                                                               Semantic building segmentation   \n",
      "84                                                                                           Factory automation   \n",
      "83                                                                                            Template Matching   \n",
      "82                                                                                             Defect detection   \n",
      "80                                                                                                       공장 자동화   \n",
      "79                                                                                                       템플릿 매칭   \n",
      "78                                                                                                        결함 탐지   \n",
      "77                                                                                                      이미지 딥러닝   \n",
      "76                                                                                                        소재 분류   \n",
      "75                                                                                                      의류 텍스타일   \n",
      "73                                                                                          Image Deep Learning   \n",
      "72                                                                                      Material Classification   \n",
      "71                                                                                             Clothing Textile   \n",
      "70                                                                           Convolutional neural network (CNN)   \n",
      "69                                                                                               Shipping noise   \n",
      "68                                                                                         Underwater acoustics   \n",
      "67                                                                                                    VoxCeleb1   \n",
      "94                                                                                                           명함   \n",
      "95                                                                                                       이미지 회전   \n",
      "96                                                                                                       인공 신경망   \n",
      "110                                                                                                        조적벽체   \n",
      "124                                                                                                         HSV   \n",
      "121                                                                                             Urinary Calculi   \n",
      "120                                                                                                Urolithiasis   \n",
      "118                                                                       Vehicle Make and Model Classification   \n",
      "117                                                                                            Object Detection   \n",
      "116                                                                                              Vehicle Damage   \n",
      "114                                                                                               차량 제작 및 모델 분류   \n",
      "113                                                                                                       객체 탐지   \n",
      "112                                                                                                       차량 손상   \n",
      "111                                                                                                       결함 인식   \n",
      "109                                                                                                  3D 레이저 스캐너   \n",
      "97                                                                                                  스마트 프린팅 시스템   \n",
      "108                                                                                                    MEMS 라이다   \n",
      "107                                                                                                        심층학습   \n",
      "106                                                                                       Defect Classification   \n",
      "105                                                                                                Masonry Wall   \n",
      "104                                                                                            3D Laser Scanner   \n",
      "103                                                                                                  MEMS LiDAR   \n",
      "101                                                                                       smart printing system   \n",
      "100                                                                                  artificial neural networks   \n",
      "99                                                                                               image rotation   \n",
      "98                                                                                               business cards   \n",
      "66                                                                                                  AI Fairness   \n",
      "65                                                                                               Biased dataset   \n",
      "64                                                                                       Speaker Identification   \n",
      "18                                                                                           Swin Trans- former   \n",
      "31                                                                                                   generation   \n",
      "30                                                                                            residual learning   \n",
      "29                                                                               generative adversarial network   \n",
      "28                                                                                      variational autoencoder   \n",
      "27                                                                                                          SVM   \n",
      "25                                                                                                      Jamming   \n",
      "24                                                                                                         GNSS   \n",
      "23                                                                                                        Wheat   \n",
      "22                                                                                                      Variety   \n",
      "20                                                                                             Image Processing   \n",
      "17                                                                                   Medical image segmentation   \n",
      "33                                                                                                       voxel.   \n",
      "15                                                                                               Residual block   \n",
      "14                                                                                          Trainable parameter   \n",
      "12                                                                                            Chest X-ray image   \n",
      "10                                                                                                  Residual 블록   \n",
      "9                                                                                                    학습 가능 파라미터   \n",
      "8                                                                                                         폐렴 진단   \n",
      "7                                                                                                      흉부 X선 영상   \n",
      "6                                                                                                   video input   \n",
      "4                                                                                 Children behavior recognition   \n",
      "3                                                                                                          영상신호   \n",
      "32                                                                                               reconstruction   \n",
      "34                                                                                               Cyber security   \n",
      "62                                                                                                        전복 사고   \n",
      "49                                                                                               gastric lesion   \n",
      "61                                                                                                           어선   \n",
      "58                                                                                          deep learning model   \n",
      "57                                                                                           capsizing accident   \n",
      "56                                                                                                 fishing boat   \n",
      "55                                                                                                         레티나넷   \n",
      "54                                                                                                           검출   \n",
      "53                                                                                                         위 병변   \n",
      "52                                                                                                      위내시경 영상   \n",
      "51                                                                                                    RetinaNet   \n",
      "50                                                                                                    detection   \n",
      "48                                                                                            Gastroscopy image   \n",
      "35                                                                                            Malicious attacks   \n",
      "47                                                                                                          VGG   \n",
      "44                                                                                        Facial age estimation   \n",
      "43                                                                              Reversible Visible Watermarking   \n",
      "42                                                                                           Perceptual Hashing   \n",
      "41                                                                                    Multipurpose Watermarking   \n",
      "40                                                                                        Deep Residual Network   \n",
      "39                                                                                      Wrapper stepwise ResNet   \n",
      "38                                                                      Hierarchical network feature extraction   \n",
      "37                                                                                               banking sector   \n",
      "36                                                                                       Cyber-physical systems   \n",
      "125                                                                                                         RGB   \n",
      "127                                                                                                 Ripe degree   \n",
      "128                                                                                                       Color   \n",
      "211                                                                                                      이미지 처리   \n",
      "221                                                                                               Smart tourism   \n",
      "220                                                                                                  모바일 애플리케이션   \n",
      "219                                                                                                   위치 기반 서비스   \n",
      "218                                                                                                      스마트 투어   \n",
      "217                                                                                                    Big data   \n",
      "216                                                                                          Manufacturing data   \n",
      "215                                                                                               Smart factory   \n",
      "214                                                                                                        빅데이터   \n",
      "213                                                                                                      제조 데이터   \n",
      "212                                                                                                      스마트 공장   \n",
      "210                                                                                                         저수지   \n",
      "223                                                                                         Mobile applications   \n",
      "209                                                                                                       수위 인식   \n",
      "207                                                                                                   Reservoir   \n",
      "206                                                                                     Water level recognition   \n",
      "204                                                                                           CNN network layer   \n",
      "203                                                                                          pre-trained models   \n",
      "202                                                                                     road flooding detection   \n",
      "201                                                                                    road flooding prediction   \n",
      "200                                                                                          Keypoint detection   \n",
      "199                                                                                               Deep-learning   \n",
      "198                                                                                                  Ultrasound   \n",
      "222                                                                                     Location-based services   \n",
      "225                                                                                                   Scoliosis   \n",
      "196                                                                                                       특징점 검   \n",
      "241                                                                                                 랜덤워크 위치 인코딩   \n",
      "251                                                                                                       모델 훈련   \n",
      "250                                                                                                         양자화   \n",
      "249                                                                                                       사물인터넷   \n",
      "248                                                                                  Experimental Configuration   \n",
      "247                                                                                              Model Training   \n",
      "246                                                                                                Quantization   \n",
      "245                                                                                          Internet of Things   \n",
      "244                                                                              Randomwalk positional encoding   \n",
      "243                                                                                Apple Quality Classification   \n",
      "242                                                                               Graph representation learning   \n",
      "240                                                                                                    사과 품질 분류   \n",
      "227                                                                                         Deep learning model   \n",
      "239                                                                                                   그래프 표현 학습   \n",
      "238                                                                                                        딥 러닝   \n",
      "237                                                                                                          비용   \n",
      "235                                                                                                      준지도 학습   \n",
      "234                                                                                                    선로 체결 장치   \n",
      "233                                                                                                        Cost   \n",
      "231                                                                                             Semi-supervised   \n",
      "230                                                                                                    Fastener   \n",
      "229                                                                                           Data augmentation   \n",
      "228                                                                                          Preprocessed image   \n",
      "197                                                                        Developmental Dysplasia of Hip (DDH)   \n",
      "194                                                                                                         초음파   \n",
      "129                                                                                          Noise cancellation   \n",
      "143                                                                                         Computed tomography   \n",
      "159                                                                                      Artwork Classification   \n",
      "157                                                                                                       미세 조정   \n",
      "156                                                                                               예술 작품 아티스트 분류   \n",
      "154                                                                                                      컴퓨터 비전   \n",
      "151                                                                                               Short-circuit   \n",
      "150                                                                                               Electric fire   \n",
      "149                                                                                                   MobileNet   \n",
      "147                                                                                      Time series image data   \n",
      "146                                                                      CNN-based product recommendation model   \n",
      "144                                                                                                 ACR phantom   \n",
      "142                                                                                                    ResNet18   \n",
      "161                                                                                                      1차 단락흔   \n",
      "141                                                                                                        정도관리   \n",
      "140                                                                                                      ACR 팬텀   \n",
      "139                                                                                                     컴퓨터단층촬영   \n",
      "138                                                                                                water  level   \n",
      "135                                                                                                         CAM   \n",
      "134                                                                                              Self-attention   \n",
      "133                                                                                    Pixel correlation module   \n",
      "132                                                                                            Weak supervision   \n",
      "131                                                                                             Siamese network   \n",
      "130                                                                       Self-supervised equivariant attention   \n",
      "160                                                                                                 Fine-tuning   \n",
      "162                                                                                                      2차 단락흔   \n",
      "193                                                                                                영유아 고관절 이형성증   \n",
      "180                                                                                                plant growth   \n",
      "192                                                                                                    Compiler   \n",
      "191                                                                                      Nueral processing unit   \n",
      "190                                                                                Computerized Tomography Data   \n",
      "189                                                                                                Optimization   \n",
      "188                                                                                               Medical image   \n",
      "186                                                                                                       식물 생장   \n",
      "185                                                                                                        수직농장   \n",
      "184                                                                                                       상추이미지   \n",
      "182                                                                                            vertical farming   \n",
      "181                                                                                           transfer learning   \n",
      "179                                                                                             lettuce imaging   \n",
      "163                                                                                                          열흔   \n",
      "177                                                                                      Paddy field monitoring   \n",
      "175                                                                                        Classification model   \n",
      "174                                                                                           Rice heading date   \n",
      "173                                                                                      Knowledge Distillation   \n",
      "172                                                                                                     ArcFace   \n",
      "171                                                                                            Face Recognition   \n",
      "169                                                                                    Molten mark Inception v3   \n",
      "168                                                                                          Secondary Arc-bead   \n",
      "167                                                                                            Primary Arc-bead   \n",
      "164                                                                                                Inception v3   \n",
      "463                                                                                   Lightweight Block Ciphers   \n",
      "\n",
      "     count  \n",
      "1       18  \n",
      "11      17  \n",
      "2       16  \n",
      "102     16  \n",
      "45      12  \n",
      "21       9  \n",
      "19       8  \n",
      "205      6  \n",
      "5        6  \n",
      "26       5  \n",
      "115      5  \n",
      "122      4  \n",
      "59       4  \n",
      "183      4  \n",
      "119      4  \n",
      "254      4  \n",
      "123      4  \n",
      "236      3  \n",
      "148      3  \n",
      "74       3  \n",
      "224      3  \n",
      "152      3  \n",
      "136      3  \n",
      "262      3  \n",
      "155      3  \n",
      "166      2  \n",
      "311      2  \n",
      "145      2  \n",
      "60       2  \n",
      "208      2  \n",
      "63       2  \n",
      "158      2  \n",
      "301      2  \n",
      "137      2  \n",
      "302      2  \n",
      "126      2  \n",
      "195      2  \n",
      "81       2  \n",
      "313      2  \n",
      "372      2  \n",
      "187      2  \n",
      "87       2  \n",
      "88       2  \n",
      "363      2  \n",
      "178      2  \n",
      "176      2  \n",
      "153      2  \n",
      "325      2  \n",
      "170      2  \n",
      "165      2  \n",
      "421      2  \n",
      "374      2  \n",
      "420      2  \n",
      "345      2  \n",
      "16       2  \n",
      "285      2  \n",
      "226      2  \n",
      "290      2  \n",
      "13       2  \n",
      "435      2  \n",
      "288      2  \n",
      "287      2  \n",
      "284      2  \n",
      "46       2  \n",
      "335      1  \n",
      "267      1  \n",
      "334      1  \n",
      "333      1  \n",
      "332      1  \n",
      "268      1  \n",
      "291      1  \n",
      "266      1  \n",
      "269      1  \n",
      "270      1  \n",
      "330      1  \n",
      "271      1  \n",
      "329      1  \n",
      "272      1  \n",
      "328      1  \n",
      "273      1  \n",
      "327      1  \n",
      "331      1  \n",
      "264      1  \n",
      "265      1  \n",
      "341      1  \n",
      "256      1  \n",
      "344      1  \n",
      "343      1  \n",
      "257      1  \n",
      "258      1  \n",
      "259      1  \n",
      "342      1  \n",
      "340      1  \n",
      "336      1  \n",
      "260      1  \n",
      "261      1  \n",
      "339      1  \n",
      "263      1  \n",
      "338      1  \n",
      "337      1  \n",
      "274      1  \n",
      "326      1  \n",
      "322      1  \n",
      "275      1  \n",
      "324      1  \n",
      "308      1  \n",
      "283      1  \n",
      "307      1  \n",
      "306      1  \n",
      "305      1  \n",
      "304      1  \n",
      "346      1  \n",
      "300      1  \n",
      "299      1  \n",
      "286      1  \n",
      "298      1  \n",
      "297      1  \n",
      "296      1  \n",
      "289      1  \n",
      "295      1  \n",
      "294      1  \n",
      "293      1  \n",
      "309      1  \n",
      "310      1  \n",
      "282      1  \n",
      "318      1  \n",
      "323      1  \n",
      "276      1  \n",
      "292      1  \n",
      "277      1  \n",
      "321      1  \n",
      "320      1  \n",
      "319      1  \n",
      "278      1  \n",
      "281      1  \n",
      "317      1  \n",
      "316      1  \n",
      "315      1  \n",
      "279      1  \n",
      "280      1  \n",
      "314      1  \n",
      "312      1  \n",
      "303      1  \n",
      "0        1  \n",
      "347      1  \n",
      "422      1  \n",
      "433      1  \n",
      "432      1  \n",
      "431      1  \n",
      "430      1  \n",
      "429      1  \n",
      "428      1  \n",
      "427      1  \n",
      "426      1  \n",
      "425      1  \n",
      "424      1  \n",
      "423      1  \n",
      "419      1  \n",
      "436      1  \n",
      "418      1  \n",
      "417      1  \n",
      "416      1  \n",
      "415      1  \n",
      "414      1  \n",
      "413      1  \n",
      "412      1  \n",
      "411      1  \n",
      "410      1  \n",
      "409      1  \n",
      "408      1  \n",
      "434      1  \n",
      "437      1  \n",
      "406      1  \n",
      "451      1  \n",
      "462      1  \n",
      "461      1  \n",
      "460      1  \n",
      "459      1  \n",
      "458      1  \n",
      "457      1  \n",
      "456      1  \n",
      "455      1  \n",
      "454      1  \n",
      "453      1  \n",
      "452      1  \n",
      "450      1  \n",
      "438      1  \n",
      "449      1  \n",
      "448      1  \n",
      "447      1  \n",
      "446      1  \n",
      "445      1  \n",
      "444      1  \n",
      "443      1  \n",
      "442      1  \n",
      "441      1  \n",
      "440      1  \n",
      "439      1  \n",
      "407      1  \n",
      "405      1  \n",
      "348      1  \n",
      "361      1  \n",
      "375      1  \n",
      "373      1  \n",
      "371      1  \n",
      "370      1  \n",
      "369      1  \n",
      "368      1  \n",
      "367      1  \n",
      "366      1  \n",
      "365      1  \n",
      "364      1  \n",
      "362      1  \n",
      "255      1  \n",
      "377      1  \n",
      "359      1  \n",
      "358      1  \n",
      "357      1  \n",
      "356      1  \n",
      "355      1  \n",
      "354      1  \n",
      "353      1  \n",
      "352      1  \n",
      "351      1  \n",
      "350      1  \n",
      "349      1  \n",
      "376      1  \n",
      "378      1  \n",
      "404      1  \n",
      "392      1  \n",
      "403      1  \n",
      "402      1  \n",
      "401      1  \n",
      "400      1  \n",
      "399      1  \n",
      "398      1  \n",
      "397      1  \n",
      "396      1  \n",
      "395      1  \n",
      "394      1  \n",
      "393      1  \n",
      "391      1  \n",
      "379      1  \n",
      "390      1  \n",
      "389      1  \n",
      "388      1  \n",
      "387      1  \n",
      "386      1  \n",
      "385      1  \n",
      "384      1  \n",
      "383      1  \n",
      "382      1  \n",
      "381      1  \n",
      "380      1  \n",
      "360      1  \n",
      "232      1  \n",
      "253      1  \n",
      "252      1  \n",
      "93       1  \n",
      "92       1  \n",
      "91       1  \n",
      "90       1  \n",
      "89       1  \n",
      "86       1  \n",
      "85       1  \n",
      "84       1  \n",
      "83       1  \n",
      "82       1  \n",
      "80       1  \n",
      "79       1  \n",
      "78       1  \n",
      "77       1  \n",
      "76       1  \n",
      "75       1  \n",
      "73       1  \n",
      "72       1  \n",
      "71       1  \n",
      "70       1  \n",
      "69       1  \n",
      "68       1  \n",
      "67       1  \n",
      "94       1  \n",
      "95       1  \n",
      "96       1  \n",
      "110      1  \n",
      "124      1  \n",
      "121      1  \n",
      "120      1  \n",
      "118      1  \n",
      "117      1  \n",
      "116      1  \n",
      "114      1  \n",
      "113      1  \n",
      "112      1  \n",
      "111      1  \n",
      "109      1  \n",
      "97       1  \n",
      "108      1  \n",
      "107      1  \n",
      "106      1  \n",
      "105      1  \n",
      "104      1  \n",
      "103      1  \n",
      "101      1  \n",
      "100      1  \n",
      "99       1  \n",
      "98       1  \n",
      "66       1  \n",
      "65       1  \n",
      "64       1  \n",
      "18       1  \n",
      "31       1  \n",
      "30       1  \n",
      "29       1  \n",
      "28       1  \n",
      "27       1  \n",
      "25       1  \n",
      "24       1  \n",
      "23       1  \n",
      "22       1  \n",
      "20       1  \n",
      "17       1  \n",
      "33       1  \n",
      "15       1  \n",
      "14       1  \n",
      "12       1  \n",
      "10       1  \n",
      "9        1  \n",
      "8        1  \n",
      "7        1  \n",
      "6        1  \n",
      "4        1  \n",
      "3        1  \n",
      "32       1  \n",
      "34       1  \n",
      "62       1  \n",
      "49       1  \n",
      "61       1  \n",
      "58       1  \n",
      "57       1  \n",
      "56       1  \n",
      "55       1  \n",
      "54       1  \n",
      "53       1  \n",
      "52       1  \n",
      "51       1  \n",
      "50       1  \n",
      "48       1  \n",
      "35       1  \n",
      "47       1  \n",
      "44       1  \n",
      "43       1  \n",
      "42       1  \n",
      "41       1  \n",
      "40       1  \n",
      "39       1  \n",
      "38       1  \n",
      "37       1  \n",
      "36       1  \n",
      "125      1  \n",
      "127      1  \n",
      "128      1  \n",
      "211      1  \n",
      "221      1  \n",
      "220      1  \n",
      "219      1  \n",
      "218      1  \n",
      "217      1  \n",
      "216      1  \n",
      "215      1  \n",
      "214      1  \n",
      "213      1  \n",
      "212      1  \n",
      "210      1  \n",
      "223      1  \n",
      "209      1  \n",
      "207      1  \n",
      "206      1  \n",
      "204      1  \n",
      "203      1  \n",
      "202      1  \n",
      "201      1  \n",
      "200      1  \n",
      "199      1  \n",
      "198      1  \n",
      "222      1  \n",
      "225      1  \n",
      "196      1  \n",
      "241      1  \n",
      "251      1  \n",
      "250      1  \n",
      "249      1  \n",
      "248      1  \n",
      "247      1  \n",
      "246      1  \n",
      "245      1  \n",
      "244      1  \n",
      "243      1  \n",
      "242      1  \n",
      "240      1  \n",
      "227      1  \n",
      "239      1  \n",
      "238      1  \n",
      "237      1  \n",
      "235      1  \n",
      "234      1  \n",
      "233      1  \n",
      "231      1  \n",
      "230      1  \n",
      "229      1  \n",
      "228      1  \n",
      "197      1  \n",
      "194      1  \n",
      "129      1  \n",
      "143      1  \n",
      "159      1  \n",
      "157      1  \n",
      "156      1  \n",
      "154      1  \n",
      "151      1  \n",
      "150      1  \n",
      "149      1  \n",
      "147      1  \n",
      "146      1  \n",
      "144      1  \n",
      "142      1  \n",
      "161      1  \n",
      "141      1  \n",
      "140      1  \n",
      "139      1  \n",
      "138      1  \n",
      "135      1  \n",
      "134      1  \n",
      "133      1  \n",
      "132      1  \n",
      "131      1  \n",
      "130      1  \n",
      "160      1  \n",
      "162      1  \n",
      "193      1  \n",
      "180      1  \n",
      "192      1  \n",
      "191      1  \n",
      "190      1  \n",
      "189      1  \n",
      "188      1  \n",
      "186      1  \n",
      "185      1  \n",
      "184      1  \n",
      "182      1  \n",
      "181      1  \n",
      "179      1  \n",
      "163      1  \n",
      "177      1  \n",
      "175      1  \n",
      "174      1  \n",
      "173      1  \n",
      "172      1  \n",
      "171      1  \n",
      "169      1  \n",
      "168      1  \n",
      "167      1  \n",
      "164      1  \n",
      "463      1  \n",
      "데이터가 저장되었습니다: resnet_2023_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2023_academic_riss.csv\n",
      "ResNet과 SIFT를 이용한 특허 도면의 유사도 평가 및  검색 연구 2024 ['특허 도면', '이진 이미지', '분류 및 검색', 'ResNet-50', 'SIFT', '유사도 비교', 'Patent drawings', 'binary images', 'classification and retrieval', 'ResNet-50', 'SIFT', 'similarity Comparison'] 특허 문헌의 유사성 평가 및 검색 연구는 특허 문헌의 효율적인 관리뿐 아니라 산업 및 기술 분야에서효율적이고 빠른 정보 수집을 위해 중요한 주제로 다뤄지고 있다. 특히 특허 도면은 산업 기술의 발전과 혁신의 결과물을 시각적으로 표현해왔으나 지금까지 텍스트에 비해 중요하게 다뤄지지 못한 측면이 있다.본 연구는 효과적인 특허 도면의 검색을 위해 딥러닝의 대표 모델 ResNet-50과 전통적인 컴퓨터비전알고리즘 SIFT를 이용하여 유사성을 평가하는 연구이다. 먼저 시각적 유형의 유사성을 평가하기 위해총 10,827개의 특허도면을 이용한 유형 분류 실험을 진행했으며 분류성능은 95%가 넘는 Accuracy를나타냈다. 두 번째로 기술도면 5,000개를 사용하여 ResNet과 SIFT를 이용한 검색 실험을 진행하여 각모델의 유사성을 평가하는 특징을 살펴보았다. 마지막으로 원본 데이터 50개와 원본 데이터를 다양한형태로 증강한 데이터 4,800개를 이용하여 편집 유형별로 검색 및 매칭한 결과, ResNet은 72.54%, SIFT는 86.71%의 평균적인 매칭 결과를 나타냈다.연구 수행 결과, 이미지 전체의 정보를 이용하여 유사도를 비교하는 ResNet-50과 달리 SIFT는 이미지내 특징점 등 속성 정보를 이용하여 유사도를 판단하므로 시각적으로 유사한 이미지를 찾는 일에는ResNet이, 같은 이미지를 찾는 일에는 SIFT가 더 강점이 있는 것으로 평가할 수 있다. The study of the similarity evaluation and retrieval of patent documents is critical not only for the efficient management of patent literature, but also for the rapid and effective collection of information in industrial and technological fields. Patent drawings visually represent the outcomes of technological advancements and innovations, but have not been given as much importance as texts in the past.This study evaluated the similarity of patent drawings for effective retrieval using the representative deep-learning model, ResNet-50, and the traditional computer vision algorithm, scale-invariant feature transform (SIFT). First, a classification experiment using 10,827 patent drawings was conducted to evaluate the similarity of the visual types, achieving a classification performance with an accuracy exceeding 95%. Second, a retrieval experiment using 5,000 technical drawings was conducted to compare the features of ResNet and SIFT based on their similarity. Finally, the retrieval and matching performances of ResNet and SIFT were evaluated using 50 original data samples and 4,800 augmented data samples created by various forms of editing. ResNet demonstrated an average matching performance of 72.54%, whereas SIFT achieved an average matching performance of 86.71%.The findings reveal that, unlike ResNet-50, which compares similarity using the entire image information, SIFT evaluates similarity based on attribute information, such as key points within the image. Consequently, ResNet is advantageous for identifying visually similar images, whereas SIFT excels in identifying identical images.\n",
      "ResNet50 알고리즘을 활용한 백혈구 이미지 분석 연구 2024 ['백혈구', '인공지능', '딥러닝', 'ResNet-50', '이미지 분석', 'White blood cells', 'Artificial Intelligence', 'Deep Learning', 'ResNet-50', 'Image analysis'] 백혈구 연구는 면역학, 세포생물학, 유전학 분야에서 수 세기에 걸쳐 이뤄졌으며, 백혈구의 기능, 발달, 면역 반응, 건강 영향을 이해하는데 기여하였다. 이를 토대로 감염병, 암, 면역 질환의 치료와 예방 방법이 발전하였다. 특히 림프구와 T세포 연구는 백신 개발과 암 치료에 혁명적인 영향을 미쳤다. 백혈구 연구는 혈액 관련 질병 예방과 치료, 호중구, 단핵구, 호산구 등의 세포 연구로도 자가면역 질환, 알레르기, 염증성 질환의 치료를 개선하였다. 그러나 백혈구의 정확한 기능, 상호작용, 면역 조절 연구는 아직 미흡하다. 최근 디지털 기술 발전으로 대규모 이미지 데이터 생성이 가능해졌으며, AI 알고리즘을 훈련하는 데 활용되고 있다. 특히 딥러닝 기술은 이미지 분석 분야에서 중요한 역할을 하며, 백혈구 연구에도 응용되고 있다. 이 연구는 ResNet-50 모델의 실제 응용 가능성을 확인하고자 하였으며, 모델의 성능을 확인한 결과 ResNet-50모델에 Epoch를 30으로 설정했을 때 성능이 균형적이고 안정적으로 나타나는 것을 확인하였다. White blood cell research spans centuries in the fields of immunology, cell biology, and genetics and has contributed to our understanding of white blood cell function, development, immune responses, and health effects. Based on this, treatment and prevention methods for infectious diseases, cancer, and immune diseases have been developed. In particular, lymphocyte and T cell research has had a revolutionary impact on vaccine development and cancer treatment. White blood cell research has been used to prevent and treat blood-related diseases, and research on cells such as neutrophils, monocytes, and eosinophils has also improved the treatment of autoimmune diseases, allergies, and inflammatory diseases. However, research on the exact function, interaction, and immune regulation of white blood cells is still insufficient. Recent advances in digital technology have made it possible to generate large-scale image data, which is being used to train AI algorithms. In particular, deep learning technology plays an important role in the field of image analysis and is also applied to white blood cell research. This study sought to confirm the actual applicability of the ResNet-50 model, and as a result of checking the model's performance, it was confirmed that when Epoch was set to 30 in the ResNet-50 model, the performance was balanced and stable.\n",
      "웹사이트 게시글 및 상품 리뷰 검색 기능 향상: ResNet-Transformer 모델을 이용한 BM25 랭킹 알고리즘 성능 개선 2024 ['레즈넷', '트랜스포머', '레즈넷-트랜스포머', '웹사이트 검색', '랭킹 알고리즘', 'BM25', 'ResNet', 'Transformer', 'ResNet-Transformer', 'Website search', 'BM25', 'Rangking algorithm'] None This paper proposes a method to improve the search functionality for website posts and product reviews by using a ResNet-Transformer model in conjunction with the BM25 ranking algorithm. BM25 is a widely used algorithm in text-based search that ranks documents by evaluating their relevance to user queries. However, it has limitations in capturing local features of words and understanding the context of a sentences. To address these issues, this study applies a classification approach that combines the ResNet model, which excels at extracting local features, with the Transformer model, known for its strong contextual understanding, as weights for BM25. Experimental results demonstrate that the proposed method improves the nDCG metric by 9.38% and the aP@5 metric by 11.82% compared to BM25 alone. This suggests that implementing this method in search engines across various websites can provide more accurate results for post and review searches.\n",
      "가상공간에서 ResNet을 활용한 공간감 증대를 통한 시각 및 음향 환경 향상에 관한 연구 2024 ['Virtual Space', 'YOLOv5', 'ResNet', 'Indoor and Outdoor', 'Accuracy', 'Immersive', '가상 공간', 'YOLOv5', 'ResNet', '실내와 실외', '정확도', '몰입감'] 가상공간 환경에서 사용자의 몰입과 경험을 극대화하기 위해서는 시각적, 음향적 효과가 핵심적인 역할을 해야 한다. 하지만 온라인상에서 현장감이 부족하거나, 음질이 떨어져 시각 및 음향의 질적 수준이 오프라인에 비해 떨어진다는 의견이 존재한다. 가상 공간 내에서 실내외를 정확히 식별하여 사용자의 움직임에 따라 상호작용을 통해 현실감을 높이기 위해 가상환경 시스템에서 현실적인 효과를 제공할 필요가 있다. 이를 위해서는 가상공간에서의 실내외 환경 식별 정확도를 높이기 위한 연구가 진행되어야 한다. COCO DataSet 내 객체 수에 따른 실내외 식별 방법과 기존 알고리즘을 수정한 실내외 식별 방법에 따른 실내외 식별 방법을 통해 결과값에 대한 정확도를 향상할 수 있다. 이에 본 연구에서는 객체 수 및 기존 알고리즘 수정을 통한 가상공간 특성의 정확도 문제를 해결한다. 이는 향후 객체의 특징을 학습하여 데이터화 시키고 사용 DataSet 이외의 객체들을 식별할 수 있게 하여 새로운 공간 구축을 위한 변화의 방향을 제시하고자 한다. To maximize the user immersion and experience in a virtual space environment, visual and acoustic effects must play a pivotal role. However, there are opinions that the quality level of visuals and sound is lower than offline due to a lack of realism online or poor sound quality. It is necessary to provide realistic effects in a virtual environment system to accurately identify the indoor and outdoor space within the virtual space and increase the sense of reality through interaction according to the user movement. To this end, research should be conducted to increase the accuracy of identifying the indoor and outdoor environment in the virtual space. The accuracy of the obtained results can be enhanced by utilizing indoor and outdoor identification methods based on the object count within the Common Objects in Context (COCO) DataSet and the existing algorithm. Therefore, this study solves the problem of the accuracy of virtual space characteristics through the number of objects and the revision of the existing algorithm. This aims to suggest the direction of change for the construction of a new space by learning the characteristics of objects in the future, making them data, and allowing the identification of objects other than the DataSet used.\n",
      "음성 감정인식의 정확도 향상을 위한 DA-S기법을 활용한 ResNet 모델 2024 ['ResNet', 'Speech Emotion Recognition', 'Deep Learning', 'Data Augmentation'] None In recent years, there has been active research on emotion recognition based on speech data that can be utilized in various platforms. Despite the significant progress in emotion recognition research based on the Korean language within the country, the main issue remains the lack of Korean language databases. Due to the absence of such data, there are cases where overfitting issues arise in models proposed in previous studies. Therefore, this study proposes a ResNet model using the data augmentation with saturation (DA-S) method to improve the performance of speech emotion recognition using the existing model. In this study, the number of data was increased from 5,596 to 11,192 by applying DA-S with the AI-HUB database. Consequently, the proposed model successfully addressed the overfitting issue, resulting in a 31.76% improvement in the accuracy of speech emotion recognition.Furthermore, experiments were conducted using a total of 11,192 data samples, including both the original data and the data with DA-S applied to demonstrate the effects of data augmentation techniques in transforming and expanding data, as well as performance improvements due to the increase in data volume. The result showed that there was a 23.4% improvement when DA-S was applied.\n",
      "수면 소리 데이터를 활용한 1D ResNet 딥러닝 모델 기반 수면무호흡증 탐지 연구 2024 ['Sleep apnea', 'Polysomnography', 'Sleep sound', 'Deep learning', 'MFCC'] 수면무호흡증(Sleep Apnea)은 전 세계적으로 심각한 건강 문제를 일으키는 질환으로, 주로 수면 중 상부 기도의 폐쇄로 인해 발생한다. 현재 수면무호흡증의 표준 진단 방법인 수면다원검사(Polysomnography, PSG)는 높은 비용과 복잡성 등의 한계가 있다. 본 연구에서는 수면 중 발생하는 소리 데이터를 활용하여 수면무호흡증을 탐지하는 비침습적 방법을 제안하였다. 소리 데이터는 MFCC(Mel-Frequency Cepstral Coefficients)로 특징을 추출하였으며, 1D CNN 기반의 ResNet(Residual Network) 모델을 통해 분류를 수행하였다. 실험 결과, 제안된 모델은 5겹 교차 검증을 통해 평균 정확도(Accuracy) 97.8%, 재현율(Recall) 97.7%, 민간도(Precision) 97.9%, AUC 0.978의 성능을 달성하였다. 향후 연구에서는 데이터셋을 확장하고 다양한 딥러닝 모델을 실험하여 모델의 성능을 더욱 향상시킬 계획이다. 본 연구는 수면무호흡증 탐지의 정확성을 높이고, 효율적인 건강관리 시스템 구축에 기여할 수 있을 것으로 기대한다. Sleep Apnea is a serious global health issue caused primarily by the obstruction of the upper airway during sleep. The current standard diagnostic method for sleep apnea, Polysomnography (PSG), has limitations such as high cost and complexity. In this study, we propose a non-invasive method to detect sleep apnea by utilizing sound data recorded during sleep. The sound data features were extracted using Mel-Frequency Cepstral Coefficients (MFCC), and classification was performed using a 1D CNN-based ResNet (Residual Network) model. The experimental results show that the proposed model achieved an average accuracy of 97.8%, a recall of 97.7%, a precision of 97.9%, and an AUC of 0.978 through 5-fold cross-validation. Future research will focus on expanding the dataset and experimenting with various deep learning models to further improve the model's performance. This study is expected to contribute to improving the accuracy of sleep apnea detection and the development of an efficient healthcare management system.\n",
      "ONNX 기반 런타임 성능 분석: YOLO와 ResNet 2024 ['ONNX', '딥 모델', 'Inference Runtime', '객체 감지', '이미지 분류', 'Deep Model', 'Object Detection', 'Image Classification'] None In the field of computer vision, models such as You Look Only Once (YOLO) and ResNet are widely used due to their real-time performance and high accuracy. However, to apply these models in real-world environments, factors such as runtime compatibility, memory usage, computing resources, and real-time conditions must be considered. This study compares the characteristics of three deep model runtimes: ONNX Runtime, TensorRT, and OpenCV DNN, and analyzes their performance on two models. The aim of this paper is to provide criteria for runtime selection for practical applications. The experiments compare runtimes based on the evaluation metrics of time, memory usage, and accuracy for vehicle license plate recognition and classification tasks. The experimental results show that ONNX Runtime excels in complex object detection performance, OpenCV DNN is suitable for environments with limited memory, and TensorRT offers superior execution speed for complex models.\n",
      "잔차블록을 적용한 향상된 ResNet VAE-GAN  3D 객체 생성 시스템 2024 ['Voxel', 'Mesh', '3D object generation', 'VAE', 'GAN', 'Residual block'] 3D 모델링은 게임, AR, VR, 메타버스 등 다양한 분야에서 활용되고 있다. 최근 컴퓨터 하드웨어의 성능 향상으로 3D 공간에서의 시각화와 연산이 가속화되고 있으며, GAN 기술의 진보로 3D 객체를 생성하는 방법이 연구되고 있다. GAN 기반 네트워크는 이미지를 입력으로 받아 복셀(Voxel)을 생성하고, Wasserstein 손실 함수 도입 및 그래디언트 패널티 적용을 통해 학습하는 3D-VAE-IWGAN 방식을 제안하였다. GAN은 훈련에 포함되지 않은 여러 모델을 생성할 수 있지만, 아티팩트가 생기는 문제가 있다. 또 다른 방식으로는 2D에서 지도 학습하고 3D에서는 비지도 학습을 통해 3D 레이블 생성 비용을 줄인 DIB-R과 같은 네트워크가 제안되었다. DIB-R은 아티팩트를 줄일 수 있지만, 오토인코더 기반 네트워크로는 다양한 모델을 생성하기 어렵다. 본 논문은 3D-VAE-IWGAN에서 성능을 높인 Variational Autoencoder(VAE)와 Generative Adversarial Network(GAN)을 결합한 VAE-GAN에 잔차블록(Residual block)을 적용하는 방법을 제안하며 이미지 생성자와 판별자에 더 많은 특징을 추출하여 고품질 이미지 생성 및 잠재 공간 보간 성능이 향상된 시스템을 제안한다. 기존 네트워크와 비교한 결과는 의자 클래스에서 137.15로 116.33% 더 나은 결과를 보였고 침대에서도 137.24로 130.4%로 향상된 결과를 보였다. 3D modeling is used in various fields such as games, AR, VR, and metaverse. Recently, visualization and computation in 3D space are accelerating due to improvements in the performance of computer hardware, and methods for generating 3D objects are being researched due to advances in GAN technology. The GAN-based network proposed a 3D-VAE-IWGAN method that receives images as input, generates voxels, and learns by introducing a Wasserstein loss function and applying a gradient penalty. GAN can generate multiple models that are not included in training, but there is a problem with artifacts occurring. As another method, a network such as DIB-R, which reduces the cost of 3D label generation through supervised learning in 2D and unsupervised learning in 3D, has been proposed. DIB-R can reduce artifacts, but it is difficult to generate diverse models with an autoencoder-based network. This paper proposes a method of applying residual blocks to VAE-GAN, which combines Variational Autoencoder (VAE) and Generative Adversarial Network (GAN), which improves performance in 3D-VAE-IWGAN, and further improves the image generator and discriminator. We propose a system that extracts many features to generate high-quality images and improve latent space interpolation performance.  The results of index compared to the existing network showed a better result of 116.33% at 137.15 in the chair class, and an improvement of 130.4% at 137.24 in the bed class.\n",
      "고추 작물의 정밀 질병 진단을 위한 딥러닝 모델 통합 연구: YOLOv8, ResNet50, Faster R-CNN의 성능 분석 2024 ['스마트 농업', '작물 질병 진단', '욜로v8', '레스넷50', '딥러닝 모델 비교', 'Smart Agriculture', 'Crop Disease Diagnosis', 'YOLOv8', 'ResNet50', 'Deep Learning Model Comparison'] 본 연구의 목적은 YOLOv8, ResNet50, Faster R-CNN 모델을 활용하여 고추 작물의 질병을 진단하고, 각 모델의 성능을 비교하는 것이다. 첫 번째 모델은 YOLOv8을 사용하여 질병을 진단하였고, 두 번째 모델은 ResNet50을 단독으로 사용하였다. 세 번째 모델은 YOLOv8과 ResNet50을 결합하여 질병을 진단하였으며, 네 번째 모델은 Faster R-CNN을 사용하여 질병을 진단하였다. 각 모델의 성능은 정확도, 정밀도, 재현율, F1-Score 지표로 평가된다. 연구 결과, YOLOv8과 ResNet50을 결합한 모델이 가장 높은 성능을 보였으며, YOLOv8 단독모델도 높은 성능을 나타냈다. The purpose of this study is to diagnose diseases in pepper crops using YOLOv8, ResNet50, and Faster R-CNN models and compare their performance. The first model utilizes YOLOv8 for disease diagnosis, the second model uses ResNet50 alone, the third model combines YOLOv8 and ResNet50, and the fourth model uses Faster R-CNN. The performance of each model was evaluated using metrics such as accuracy, precision, recall, and F1-Score. The results show that the combined YOLOv8 and ResNet50 model achieved the highest performance, while the YOLOv8 standalone model also demonstrated high performance.\n",
      "핵의학 갑상샘 팬텀 영상에서 보간법이 초고분해능 ResNet 모델성능에 미치는 영향에 관한 연구 2024 ['핵의학', '보간법', 'ResNet모델', '정량분석', 'Nuclear medicine', 'Interpolation techniques', 'ResNet model', 'Quantitative analysis'] 핵의학 영상은 다양한 보간법을 적용하여 획득 영상의 축소 및 확대를 통하여 영상의 질을 개선한다. 또한, 입력 데이터셋과 정답 데이터셋 간의 특징을 추출하는 딥러닝 알고리즘 기반의 영상처리 기술도 영상의 질 향상에 크게 작용하고 있다. 이에 본 연구의 목적은 갑상샘 팬텀 영상을 이용하여 다양한 보간 방법을 적용하여 획득한 입력 데이터에 따른 딥러닝 알고리즘의 성능을 정량적 분석 인자를 이용하여 평가하고자 한다. 256 × 256 크기의 매트릭스로 갑상샘 팬텀 영상을 99mTcO₄- 37M㏃로 총 200장을 각각 1분씩 획득하여 정답 영상을 생성하였다. 최근접 이웃, 선형, 이차, 삼차, 오차 보간 방법을 다운샘플링과 업샘플링에서 적용하여 입력 데이터셋을 구축하였다. 학습률 0.0001, 학습횟수 300회로 설정된 초고분해능 잔차학습네트워크 (super resolution residual network, SRResNet) 구조를 구현하였으며, 데이터셋은 8:1:1 비율로 훈련, 검증 및 테스트 세트로 분할하여 학습하였다. 생성된 출력 영상은 변동계수 (coefficient of variationi, COV) 및 신호대잡음비 (contrast to noise ratio, CNR)를 사용하여 분석하였다. 그 결과 SRResNet 네트워크는 저분해능 영상을 생성하는데 삼차 보간법을 적용했을 때 가장 우수한 COV 및 CNR 값을 보였다. 그러므로 본 연구는 보간법 기반 딥러닝 알고리즘을 적용한 핵의학 갑상샘 영상의 질화질 개선 측면에서 입력 데이터로써의 적절한 보간법 적용이 생성 영상에 미치는 영상이 크다는 것을 확인하였으며, 영상의 질 향상을 위한 검사에 따른 적절한 보간법이 설정되어야 함을 증명하였다. The nuclear medicine imaging can improve the image quality through the application of various interpolation techniques. Additionally, deep learning algorithm, which perform feature extraction between input and label datasets, is widely utilized to improve image quality in nuclear medicine. Thus, the purpose of this study was to confirm the performance of deep learning algorithm according to applied various interpolation methods as input data using the thyroid phantom images. A total of 200 thyroid phantom images, each sized 256 × 256, were acquired at an activity of 37 M㏃ for 1 minute to generate the label images. Interpolation methods including nearest neighbor, bilinear, biquadratic, bicubic, biquartic, and biquintic were applied during both downsampling and upsampling processes. The super-resolution residual network (SRResNet) architecture was implemented with a learning rate of 0.0001 and 300 epochs, using an 8:1:1 ratio for train, validation, and test sets, respectively. The generated output images analyzed using coefficient of variation (COV) and contrast to noise ratio (CNR). Consequently, the SRResNet algorithm, which used low-resolution images generated with the bicubic interpolation method, showed the highest performance. This study demonstrates the importance of selecting appropriate interpolation methods for generating input images to improve the accuracy of the SRResNet algorithm in nuclear medicine thyroid imaging and even in other medical fields for diagnosis.\n",
      "Similarity Analysis Model with 6CH ResNet Structure 2024 ['Convolutional Neural Network (CNN)', 'Image Similarity', 'Large Waste'] None Largescale waste similarity analysis is crucial for automating waste management on a large scale. It involvesconfirming the match between waste discharged from homes and that collected by agencies, which is essentialfor a stable automated system. This paper compares feature extraction methods for similarity measurement,including the scaleinvariant feature transform (SIFT) algorithm with added HSV color features, convolutionalneural networkbased encoders, and a modified 6channel (6CH) ResNet for endtoend learning. The resultsdemonstrate that the 6CH ResNet achieves up to 4.9% higher accuracy than both the basic SIFT method andencoders, as well as the SIFT algorithm with HSV color features. Implementing the 6CH ResNet in automatedwaste management systems can enhance object similarity measurement while using fewer computing resources.\n",
      "Online to Offline 상점을 위한 한글 메뉴판 인식 : 어텐션 메커니즘을 적용한 VGG-ResNet 융합 모델 2024 ['Online to Offline(O2O)', 'OCR(Optical Character Recognition)', 'Text Recognition', 'Attention Mechanism', 'Korean Menu'] None None\n",
      "GAN기반의 Semi Supervised Learning을 활용한 이미지 생성 및 분류 2024 ['인공지능', 'GAN', 'ResNet50', 'Image Classification', 'Neural Network', 'Artificial intelligence', 'Generative Adversarial Network', 'ResNet50', 'Image Classification', 'Neural Network'] None This study deals with a method of combining image generation using Semi Supervised Learning based on GAN (Generative Adversarial Network) and image classification using ResNet50. Through this, a new approach was proposed to obtain more accurate and diverse results by integrating image generation and classification.The generator and discriminator are trained to distinguish generated images from actual images, and image classification is performed using ResNet50. In the experimental results, it was confirmed that the quality of the generated images changes depending on the epoch, and through this, we aim to improve the accuracy of industrial accident prediction. In addition, we would like to present an efficient method to improve the quality of image generation and increase the accuracy of image classification through the combination of GAN and ResNet50.\n",
      "주거 및 공공장소 이상행동탐지를 위한 서비스 설계 2024 ['Anomaly Detection', 'AI Learning Data', 'ResNet50', '3D-CNN', 'GridCV', 'GRU', '이상행동 탐지', 'AI 학습 데이터', 'ResNet50', '3D-CNN', 'GridCV', 'GRU'] 본 연구에서는 보안과 범죄 예방 강화의 하나로 공공 CCTV와 보안 카메라의 영상 데이터를 사용하여주거 및 공용 공간에서 이상행동을 탐지하기 위한 AI 학습 데이터 세트의 구축과 이를 활용한 모델을 시범 개발하였다. AI 학습 데이터 세트와 모델은 민간 기업의 AI 기술 발전과 AI 프로젝트 개발을 촉진하기 위해 설계되었다.데이터 세트 구축 시 비디오 프레임에서 특징을 추출하기 위하여 ResNet50을, JSON 파일에서 스켈레톤 포인트를처리하기 위하여 3D-CNN을 사용하여 모듈화 하였다. 이 데이터를 사전에 정의된 이상행동에 따라 Labeling 하였다. 또한 GridCV를 사용하여 SVM 분류기와 비디오 시퀀스 처리를 위한 GRU를 활용하였다. 모델의 학습 성능평가에서는 주요 정확도(main accuracy)가 지속해서 향상되었으며, 상세 손실(detailed loss) 또한 감소하는 추세를 보였다. 이를 바탕으로 학습된 모델은 주어진 비디오 시퀀스에서 나타나는 행동의 범주를 예측할 수 있다. 본연구에서 구축된 AI 학습 데이터 세트와 모델 시범 개발로 즉각적인 이상행동 감지를 통한 범죄예방 및 범인 검거를 위해 인공지능 학습에 필요한 데이터 확보, 구축 및 배포하여 민간기업의 AI 기술 발전 및 인공지능 사업의발전을 도모하고자 이상행동 탐지 기능 개발의 실용성에 대한 귀중한 인사이트를 제공하여 공공 안전 분야에서AI 애플리케이션의 발전에 기여할 것으로 기대된다. This study presents the construction of an AI learning dataset and the prototypical development of a model for detecting anomalous behaviors in residential and public spaces, as part of an effort to enhance security and crime prevention. The AI learning dataset and model were designed to stimulate the advancement of AI technology and the development of AI projects in private companies. During the dataset construction, ResNet50 was modularized to extract features from video frames, and 3D-CNN was used to process skeleton points from JSON files. This data was then labeled according to predefined anomalous behaviors.Furthermore, GridCV was employed to utilize the SVM classifier and GRUs for processing video sequences. The learning performance evaluation of the model demonstrated a continuous improvement in main accuracy and a decreasing trend in detailed loss.. The trained model can predict the category of behavior appearing in a given video sequence. The AI learning dataset and model prototyped in this study provide valuable insights into the practicality of developing anomaly detection functions. It is expected to contribute to the advancement of AI applications in the field of public safety by securing, constructing, and distributing data necessary for AI learning for immediate anomaly detection, crime prevention, and offender apprehension.\n",
      "CNN 기반의 전이학습과 데이터 증강을 통한  화재 영상 분류 개선 2024 ['산불 감지', 'CNN', '전이학습', '데이터 증강', 'ResNet50', 'Wildfire Detection', 'Convolutional Neural Networks', 'Transfer Learning', 'Data Augmentation', 'ResNet50'] 전 세계는 기상이변의 영향으로 산불 등 자연 재해가 끊이지 않고 있으며 이로 인한 사회 안전에 심각한 위협이 되고 있다. 특히 대한민국 동해안 지역은 매년 산불 피해로 인한 막대한 재산 피해가 발생하고 있다. 초기 화재 감지 모델의 필수적인 개발은 훈련을 위한 제한된 이미지 데이터와 관련된 도전을 극복해야 하며, 이는 과적합의 위험을 증가시킨다. 이를 해결하기 위해, 랜덤하게 50% 범위까지 회전, 랜덤하게 20% 범위까지 축소 및 확대, 랜덤하게 50%까지 가로 및 세로 뒤집기를 적용하였다. 성능 평가에서 6층 신경망이 7층 신경망보다 더 우수한 성능을 보였으며, 이는 제한된 데이터셋을 가지고 계층 수를 늘리는 것이 바람직하지 않음을 의미한다. 또한, 화재 이미지 분류를 위한 딥러닝 기반 CNN 모델과 ResNet50 전이 학습 모델의 평가는 전이 학습의 우수한 효과를 확인하였다. 이러한 발견은 초기 화재 감지 모델 개발에 도움이 될 것으로 기대하며, 미래 시스템을 위한 귀중한 통찰력을 제공할 것이다. Natural disasters, such as wildfires, due to climate change are a constant and serious threat to the world and societal safety. Every year, the eastern coastal region of South Korea experiences significant property damage due to wildfires. The imperative development of early fire detection models necessitates overcoming challenges associated with limited image data for training, which elevates the risk of overfitting. To address this, data augmentation techniques, including random rotation (up to 50%), random scaling (up to 20%), and random horizontal and vertical flipping (up to 50%), were employed to augment the training dataset. Performance evaluation indicated that the 6-layer neural network outperformed its 7-layer counterpart, highlighting the impracticality of increasing layer count with a limited dataset. Furthermore, an assessment of deep learning-based CNN models and ResNet50 transfer learning models for fire image classification underscored the superior efficacy of transfer learning. These findings hold promise for advancing early fire detection model development, offering valuable insights for future systems.\n",
      "Implant Thread Shape Classification by Placement Site from Dental Panoramic Images Using Deep Neural Networks 2024 ['Artificial intelligence', 'Convolutional neural networks', 'Classification', 'Deep learning', 'Implant system'] None Purpose: In this study, we aimed to classify an implant system by comparing the types of implant thread shapes shown on radiographs using various Convolutional Neural Networks (CNNs), particularly Xception, InceptionV3, ResNet50V2, and ResNet101V2. The accuracy of the CNN based on the implant site was compared.Materials and Methods: A total of 1000 radiographic images, consisting of eight types of implants, were preprocessed by resizing and CLAHE filtering, and then augmented. CNNs were trained and validated for implant thread shape prediction. Grad-CAM was used to visualize class activation maps (CAM) on the implant threads shown within the radiographic image.Results: Averaged over 10 validation folds, each model achieved an AUC of over 0.96: AUC of 0.961 (95% CI 0.952–0.970) with Xception, 0.973 (95% CI 0.966-0.980) with InceptionV3, 0.980 (95% CI 0.974-0.988) with ResNet50V2, and 0.983 (95% CI 0.975-0.992) with ResNet101V2. Accuracy was higher in the posterior region than in the anterior area in all four models. Most CAMs highlighted the implant surface where the threads were present; however, some showed responses in other areas.Conclusion: The CNN models accurately classified implants in all areas of the oral cavity according to the thread shape, using radiographic images.\n",
      "콘크리트 구조물 균열 파악을 위한 분류 모델 분석 2024 ['Concrete Crack', 'Convolution Neural Network', 'Classification'] None Purpose: The current study was used to propose a model for identifying cracks in concrete structures by comparing and analyzing various models using Convolutional Neural Networks (CNNs).Methods: Two CNN-based classification models, VGG-16 and ResNet-50, were developed, compared, and analyzed. A confusion matrix was employed as a performance indicator to evaluate their performance in individual instances.Results: The comparative analysis indicated that ResNet-50 outperformed VGG-16 in performance metrics. Additionally, the inference speed based on test data revealed a significant difference, with ResNet-50 requiring 35 seconds compared to VGG-16's 77 seconds.Conclusion: The ResNet-50 showed excellent performance in confusion matrix-based performance indicators and inference speed. It shows strong potential for practical applications in identifying concrete crack structures in real-world scenarios.\n",
      "Violent crowd flow detection from surveillance cameras using deep transfer learning-gated recurrent unit 2024 ['deep learning', 'deep transfer learning', 'video processing', 'violence detection'] None Violence can be committed anywhere, even in crowded places. It is hence necessary to monitor human activities for public safety. Surveillance cameras can monitor surrounding activities but require human assistance to continuously monitor every incident. Automatic violence detection is needed for early warning and fast response. However, such automation is still challenging because of low video resolution and blind spots. This paper uses ResNet50v2 and the gated recurrent unit (GRU) algorithm to detect violence in the Movies, Hockey, and Crowd video datasets. Spatial features were extracted from each frame sequence of the video using a pretrained model from ResNet50V2, which was then classified using the optimal trained model on the GRU architecture. The experimental results were then compared with wavelet feature extraction methods and classification models, such as the convolutional neural network and long short-term memory. The results show that the proposed combination of ResNet50V2 and GRU is robust and delivers the best performance in terms of accuracy, recall, precision, and F1-score. The use of ResNet50V2 for feature extraction can improve model performance.\n",
      "자생 매미 음성 분류를 위한 딥러닝 접근 : 주파수 변화 분석과 모델 최적화 2024 ['Artificial Intelligence', 'Deep-learning', 'CNN', 'Classification', 'Spectogram'] 본 논문은 한국에서 서식하는 매미과 속인 말매미 등 12종의 국내 자생종 매미의 음성 데이터를 활용하여 딥러닝기법을 통해 매미 종을 분류하는 새로운 접근 방식을 제시하였다. 표준화된 데이터 전처리 및 시간에 따른 주파수 변화를 시각적으로 나타내는 그래픽 표현방식인 스펙트로그램(Spectrogram)의 적용을 통해 주파수 변화와 시간적 변화를동시에 시각화하여 데이터의 특성을 파악하고 활용하며 딥러닝 모델인 ResNet34, ResNet50, AlexNet 모델을 적용하였다. 드롭아웃 기법을 적용하여 과적합(Overfitting)을 방지하며, 다양한 학습률(Learning Rates)을 적용하여 모델의학습 및 검증 과정을 최적화하였다. 이러한 접근을 통해 98% 이상의 높은 정확도로 매미 종을 식별을 검증하였다. 본연구는 인공지능 기술인 CNN(Convolutional Neural Network)를 활용하여 생물 다양성 보존과 종 식별의 정확성을높이기 위해 수행하였으며, 음성 데이터 기반의 딥러닝 시스템이 생태학적 연구와 환경 모니터링에 크게 기여할 수 있음을 시사한다. 나아가 본 연구는 생태계 보존 및 관리에 중요한 도구로 활용될 수 있음을 보여주며, 인공지능 기술과 생물분류학을 결합하여 향후 생물 다양성 연구와 환경 보호를 위한 새로운 방법을 제시할 수 있다 This paper presents a novel approach to classifying cicada species by using deep learning techniques that utilize acoustic data of 12 cicada species found in Korea, including Meimuna opalifera.Standardized data preprocessing and the application of spectrograms, which visually represent frequency changes over time, were used to simultaneously visualize both frequency and temporal changes, allowing for species identification from data characteristics. Deep learning models such as ResNet34, ResNet50, and AlexNet were applied. Dropout techniques were employed to prevent overfitting, and various learning rates were applied to optimize the training and validation processes of the models. The approach successfully identified cicada species with an accuracy of over 98%. This study enhances the accuracy of species identification and conservation of biodiversity by using the artificial intelligence technology of a convolutional neural network. It suggests that deep learning systems based on acoustic data can significantly contribute to ecological research and environmental monitoring. Furthermore, this study has the potential for use as an essential tool in ecosystem conservation and management, combining AI and taxonomy to propose new methods for future biodiversity research and environmental protection.\n",
      "임베디드 시스템에서의 객체 탐지 네트워크의 가속을 위한  중요도 탐색 필터 가지치기 기법 연구 2024 ['Object detection', 'Network compression', 'Pruning', 'Inference time'] 최근, 컴퓨터 기술이 발달하면서 CNN 기반의 객체 탐지 네트워크와 관련된 연구가 활발히 진행되고 있다. 하지만 많은 수의 CNN은 제한된 메모리와 연산량을 가지는 임베디드 환경에서의 추론을 어렵게 하는 원인이 된다. 이 문제의 대표적인 해결 방법으로 네트워크 가지치기 기법이 있다. 네트워크 가지치기 기법은 중복된 역할을 하는 파라미터를 제거하여 추론 시 요구되는 메모리와 연산량을 감소시켜 임베디드 보드에서의 추론을 용이하게 할 수 있다. 하지만 대부분의 가지치기 기법은 두 단계의 학습을 요구하여 많은 시간과 자원이 소모되고 가지치기에 따른 채널의 관계 변화를 반영하지 못해 최적의 경량 네트워크를 보장할 수 없다. 따라서 본 논문에서는 최적의 경량 네트워크를 얻기 위한 중요도 탐색 기법을 제안하고 가지치기의 과정을 단순화하여 한 단계의 훈련만으로 가지치기가 가능한 중요도 탐색 필터 가지치기 기법을 제안한다. 본 논문에서는 VGG-16과 ResNet-50을 백본 네트워크로 가지는 SSD 네트워크에 가지치기를 적용하고 Jetson Xavier NX에서 추론 속도를 측정하였다. ResNet-50을 이용하는 네트워크에서 실험 결과 mAP(0.5)는 가지치기 비율에 따라 0.5 %, 0.7 %, 1.0 % 감소하였지만, 추론 시간은 12.75 %, 16.03 %, 21.66 % 향상되었다. 또한 학습 시간은 다른 기법보다 최대 43.85 % 빠르며 유사한 가지치기 비율의 네트워크와 비교할 때 높은 성능을 가진다. In recent years, with the development of computer technology, research on CNN-based object detection networks has been actively conducted. However, a large number of CNNs can make inference difficult in embedded environments with limited memory and computation. A typical solution to this problem is network pruning. Network pruning can facilitate inference on embedded boards by reducing the amount of memory and computation required by removing redundant parameters. However, most pruning methods require two stages of training, which consumes a lot of time and resources, and cannot guarantee an optimal lightweight network because they cannot reflect the changes in channel relationships due to pruning. Therefore, this paper proposes an importance search method to obtain an optimal lightweight network, and simplifies the pruning process to propose an importance search filter pruning method that can be pruned with only one stage of training. In this paper, we apply pruning to the SSD network with VGG-16 and ResNet-50 as the backbone network, and measure the inference speed on Jetson Xavier NX. In the network using ResNet-50, the experimental results showed that mAP(0.5) decreased by 0.5%, 0.7%, and 1.0% depending on the pruning ratio, but inference time improved by 12.75%, 16.03%, and 21.66%. In addition, the learning time is up to 43.85% faster than other methods and has high performance when compared to networks with similar pruning ratios.\n",
      "Prediction Model of Spinal Osteoporosis Using Lumbar Spine X-Ray from Transfer Learning Deep Convolutional Neural Networks 2024 ['Deep learning', 'Lumbar vertebrae', 'Osteoporosis', 'Spine', 'X-rays'] None Objective: Osteoporosis is highly prevalent among older adults and women. This condition leads to a deterioration in bone mineral density and microarchitecture, significantly increasing the risk of fractures. Additionally, osteoporosis commonly results in complications such as screw loosening and non-union during spinal surgery. Deep-learning algorithms have now achieved an accuracy comparable to the current human margin of error. Therefore, this study explored the potential of using transfer learning in deep learning algorithms to predict, diagnose, and screen for osteoporosis using commonly obtained sagittal spine X-rays from patients with spinal conditions.Methods: We retrospectively evaluated 2,300 consecutive patients who underwent dual energy X-ray absorptiometry (DXA) and lumbar sagittal plain X-ray exams between 2013 and 2021. The exclusion criteria included: (1) a gap of more than 1 year between the DXA and X-ray exams; (2) vertebrae that had undergone vertebroplasty; (3) lack of spine anterior-posterior DXA; and (4) images that were unassessable. Ultimately, 256 patients (images) were included in the study. Transfer learning was applied using convolutional neural network (CNN) techniques, specifically visual geometry group (VGG) 16, VGG 19, ResNet50, and Xception.Results: The most accurate CNN model in the training group was ResNet50, with an accuracy of 0.95. ResNet50 showed the best performance, with an accuracy of 0.82, precision of 0.80, recall of 0.86, and F1-score of 0.83. Additionally, its area under the curve (0.76) was higher than that other CNN models. The confusion matrix for ResNet50’s performance displayed the outcomes for images predicted as osteoporosis (n=12) among the test data osteoporosis images (n=14)Conclusion: Artificial intelligence (AI) technology employing deep learning techniques is significantly nearing human capabilities in the role of diagnostic assistance. The diagnosis of osteoporosis using bone mineral density is expected to evolve into a comprehensive diagnostic aid or decision-making tool with the integration of AI in the future.\n",
      "Research on a Lightweight Deep Learning Model Suitable for Face Recognition for Mobile Devices 2024 ['embedding environment', 'lightweight deep learning', 'face recognition', 'MobileFaceNet', 'ResNet'] None Recently, research on lightweight deep learning has been applied to various fields due to issues such as cost reduction, security, and power consumption due to decentralization. The lightweight deep learning model provides distributed processing of data and various services through a mobile environment. In this study, we compare two lightweight facial recognition deep learning models suitable for the mobile environment and propose a more suitable model. MobileFaceNet is a model optimized for deployment in an embedding environment, and we sought to find a more suitable model by comparing it with the ResNet model that has been recently studied. WebFace42M was used as the dataset, and landmarks were extracted using RetinaFace as a face alignment technique, and faces were aligned using opencv's affine transformation. As a result of applying the two models, ResNet-100 showed better performance in the same embedding environment.\n",
      "Cox Model Improvement Using Residual Blocks in Neural Networks: A Study on the Predictive Model of Cervical Cancer Mortality 2024 ['자궁경부암', '생존 예측 모델', '콕스 비례 위험', '기계 학습', '심층 신경망', 'ResNet', 'Cervical Cancer', 'Survival Prediction Model', 'Cox Proportional Hazards', 'Machine Learning', 'Deep Neural Networks', 'ResNet'] None Cervical cancer is the fourth most common cancer in women worldwide, and more than 604,000 new cases were reported in 2020alone, resulting in approximately 341,831 deaths. The Cox regression model is a major model widely adopted in cancer research, butconsidering the existence of nonlinear associations, it faces limitations due to linear assumptions. To address this problem, this paperproposes ResSurvNet, a new model that improves the accuracy of cervical cancer mortality prediction using ResNet's residual learningframework. This model showed accuracy that outperforms the DNN, CPH, CoxLasso, Cox Gradient Boost, and RSF models comparedin this study. As this model showed accuracy that outperformed the DNN, CPH, CoxLasso, Cox Gradient Boost, and RSF models comparedin this study, this excellent predictive performance demonstrates great value in early diagnosis and treatment strategy establishment inthe management of cervical cancer patients and represents significant progress in the field of survival analysis.\n",
      "딥러닝을 활용한 전시 정원 디자인 유사성 인지 모형 연구 2024 None None The purpose of this study is to propose a method for evaluating the similarity of Show gardens using Deep Learning models, specifically VGG-16 and ResNet50. A model for judging the similarity of show gardens based on VGG-16 and ResNet50 models was developed, and was referred to as DRG (Deep Recognition of similarity in show Garden design). An algorithm utilizing GAP and Pearson correlation coefficient was employed to construct the model, and the accuracy of similarity was analyzed by comparing the total number of similar images derived at 1st (Top1), 3rd (Top3), and 5th (Top5) ranks with the original images. The image data used for the DRG model consisted of a total of 278 works from the Le Festival International des Jardins de Chaumont-sur-Loire, 27 works from the Seoul International Garden Show, and 17 works from the Korea Garden Show. Image analysis was conducted using the DRG model for both the same group and different groups, resulting in the establishment of guidelines for assessing show garden similarity. First, overall image similarity analysis was best suited for applying data augmentation techniques based on the ResNet50 model. Second, for image analysis focusing on internal structure and outer form, it was effective to apply a certain size filter (16cm × 16cm) to generate images emphasizing form and then compare similarity using the VGG-16 model. It was suggested that an image size of 448 × 448 pixels and the original image in full color are the optimal settings. Based on these research findings, a quantitative method for assessing show gardens is proposed and it is expected to contribute to the continuous development of garden culture through interdisciplinary research moving forward.\n",
      "드론 식별을 위한 CNN 기반 이미지 분류 모델 성능 비교 2024 ['Drones', 'Transfer learning', 'Image classification', 'Convolutional Neural Networks', 'Aerial targets', '드론', '전이 학습', '이미지 분류', '합성곱 신경망', '공중표적'] 최근 전장에서의 드론 활용이 정찰뿐만 아니라 화력 지원까지 확장됨에 따라, 드론을 조기에 자동으로 식별하는 기술의 중요성이 더욱 증가하고 있다. 본 연구에서는 드론과 크기 및 외형이 유사한 다른 공중 표적들인 새와 풍선을 구분할 수 있는 효과적인 이미지 분류 모델을 확인하기 위해, 인터넷에서 수집한 3,600장의 이미지 데이터셋을 사용하고, 세 가지 사전 학습된 합성곱 신경망 모델(VGG16, ResNet50, InceptionV3)의 특징 추출기능과 추가 분류기를 결합한 전이 학습 접근 방식을 채택하였다. 즉, 가장 우수한 모델을 확인하기 위해 세 가지 사전 학습된 모델(VGG16, ResNet50, InceptionV3)의 성능을 비교 분석하였으며, 실험 결과 InceptionV3 모델이 99.66%의 최고 정확도를 나타냄을 확인하였다. 본 연구는 기존의 합성곱 신경망 모델과 전이 학습을 활용하여 드론을 식별하는 새로운 시도로써, 드론 식별 기술의 발전에 크게 기여 할 것으로 기대된다. Recent developments in the use of drones on battlefields, extending beyond reconnaissance to firepower support, have greatly increased the importance of technologies for early automatic drone identification. In this study, to identify an effective image classification model that can distinguish drones from other aerial targets of similar size and appearance, such as birds and balloons, we utilized a dataset of 3,600 images collected from the internet. We adopted a transfer learning approach that combines the feature extraction capabilities of three pre-trained convolutional neural network models (VGG16, ResNet50, InceptionV3) with an additional classifier. Specifically, we conducted a comparative analysis of the performance of these three pre-trained models to determine the most effective one. The results showed that the InceptionV3 model achieved the highest accuracy at 99.66%. This research represents a new endeavor in utilizing existing convolutional neural network models and transfer learning for drone identification, which is expected to make a significant contribution to the advancement of drone identification technologies.\n",
      "Identification of Atrial Fibrillation With Single-Lead Mobile ECG During Normal Sinus Rhythm Using Deep Learning 2024 ['Artificial Intelligence', 'Atrial Fibrillation', 'Electrocardiography', 'Mobile Applications', 'Probability Learning'] None Background: The acquisition of single-lead electrocardiogram (ECG) from mobile devices offers a more practical approach to arrhythmia detection. Using artificial intelligence for atrial fibrillation (AF) identification enhances screening efficiency. However, the potential of singlelead ECG for AF identification during normal sinus rhythm (NSR) remains under-explored.This study introduces a method to identify AF using single-lead mobile ECG during NSR.Methods: We employed three deep learning models: recurrent neural network (RNN), long short-term memory (LSTM), and residual neural networks (ResNet50). From a dataset comprising 13,509 ECGs from 6,719 patients, 10,287 NSR ECGs from 5,170 patients were selected. Single-lead mobile ECGs underwent noise filtering and segmentation into 10-second intervals. A random under-sampling was applied to reduce bias from data imbalance. The final analysis involved 31,767 ECG segments, including 15,157 labeled as masked AF and 16,610 as Healthy.Results: ResNet50 outperformed the other models, achieving a recall of 79.3%, precision of 65.8%, F1-score of 71.9%, accuracy of 70.5%, and an area under the receiver operating characteristic curve (AUC) of 0.79 in identifying AF from NSR ECGs. Comparative performance scores for RNN and LSTM were 0.75 and 0.74, respectively. In an external validation set, ResNet50 attained an F1-score of 64.1%, recall of 68.9%, precision of 60.0%, accuracy of 63.4%, and AUC of 0.68.Conclusion: The deep learning model using single-lead mobile ECG during NSR effectively identified AF at risk in future. However, further research is needed to enhance the performance of deep learning models for clinical application.\n",
      "Classification of mandibular molar furcation involvement in periapical radiographs by deep learning 2024 ['Mandible', 'Molar', 'Periodontitis', 'Radiography', 'Deep Learning'] None Purpose: The purpose of this study was to classify mandibular molar furcation involvement (FI) in periapical radiographs using a deep learning algorithm.Materials and Methods: Full mouth series taken at East Carolina University School of Dental Medicine from 2011-2023 were screened. Diagnostic-quality mandibular premolar and molar periapical radiographs with healthy or FI mandibular molars were included. The radiographs were cropped into individual molar images, annotated as “healthy” or “FI,” and divided into training, validation, and testing datasets. The images were preprocessed by PyTorch transformations. ResNet-18, a convolutional neural network model, was refined using the PyTorch deep learning framework for the specific imaging classification task. CrossEntropyLoss and the AdamW optimizer were employed for loss function training and optimizing the learning rate, respectively. The images were loaded by PyTorch DataLoader for efficiency. The performance of ResNet-18 algorithm was evaluated with multiple metrics, including training and validation losses, confusion matrix, accuracy, sensitivity, specificity, the receiver operating characteristic (ROC) curve, and the area under the ROC curve.Results: After adequate training, ResNet-18 classified healthy vs. FI molars in the testing set with an accuracy of 96.47%, indicating its suitability for image classification.Conclusion: The deep learning algorithm developed in this study was shown to be promising for classifying mandibular molar FI. It could serve as a valuable supplemental tool for detecting and managing periodontal diseases.\n",
      "Improving Chest X-ray Image Classification via Integration of Self-Supervised Learning and Machine Learning Algorithms 2024 ['Chest X-ray image', 'Contrastive learning', 'Image classification', 'Self-supervised learning'] None In this study, we present a novel approach for enhancing chest X-ray image classification (normal, Covid-19, edema, massnodules, and pneumothorax) by combining contrastive learning and machine learning algorithms. A vast amount of unlabeleddata was leveraged to learn representations so that data efficiency is improved as a means of addressing the limited availabilityof labeled data in X-ray images. Our approach involves training classification algorithms using the extracted features from alinear fine-tuned Momentum Contrast (MoCo) model. The MoCo architecture with a Resnet34, Resnet50, or Resnet101backbone is trained to learn features from unlabeled data. Instead of only fine-tuning the linear classifier layer on the MoCopretrainedmodel, we propose training nonlinear classifiers as substitutes for softmax in deep networks. The empirical resultsshow that while the linear fine-tuned ImageNet-pretrained models achieved the highest accuracy of only 82.9% and the linearfine-tuned MoCo-pretrained models an increased highest accuracy of 84.8%, our proposed method offered a significantimprovement and achieved the highest accuracy of 87.9%.\n",
      "불법 주정차 단속을 위한 딥러닝 기반 이미지 인식 모델 2024 ['Deep learning', 'Illegal parking', 'ResNet18', 'YOLOv8'] None Recently, research on the convergence of drones and artificial intelligence technologies have been conducted in various industrial fields. In this paper, we propose an illegal parking vehicle recognition model using deep learning-based object recognition and classification algorithms. The model of object recognition and classification consist of YOLOv8 and ResNet18, respectively. The proposed model was trained using image data collected in general road environment, and the trained model showed high accuracy in determining illegal parking. From simulation results, it was confirmed that the proposed model has generalization performance to identify illegal parking vehicles from various images.\n",
      "LLVM IR 대상 악성코드 탐지를 위한 이미지 기반 머신러닝 모델 2024 ['LLVM IR', 'Image based', 'Malware Detection', 'ResNet50V2'] 최근 정적분석 기반의 시그니처 및 패턴 탐지 기술은 고도화되는 IT 기술에 따라 한계점이 드러나고 있다. 이는 여러 아키텍처에 대한 호환 문제와 시그니처 및 패턴 탐지의 본질적인 문제이다. 악성코드는 자신의 정체를 숨기기 위하여 난독화, 패킹 기법 등을 사용하고 있으며 또한, 코드 재정렬, 레지스터 변경, 분기문 추가 등 기존 정적분석 기반의 시그니처 및 패턴 탐지 기법을 회피하고 있다. 이에 본 논문에서는 이러한 문제를 해결할 수 있는 머신러닝을 통한 LLVM IR 코드 이미지 기반 악성코드 정적분석 자동화 기술을 제안한다. 바이너리가 난독화되거나 패킹된 사실에 불구하고 정적 분석 및 최적화를 위한 중간언어인 LLVM IR로 디컴파일한다. 이후 LLVM IR 코드를 이미지로 변환하여 CNN을 이용한 알고리즘 중 전이 학습 및 Keras에서 지원하는 ResNet50v2으로 학습하여 악성코드를 탐지하는 모델을 제시한다. None\n",
      "Deep Learning-Based Defect Detection in Cu-Cu Bonding Processes 2024 ['Cu-Cu Bonding', 'Deep Learning', 'Defect Detection', 'Defect Map', 'ResNet', 'CNN'] None Cu-Cu bonding, one of the key technologies in advanced packaging, enhances semiconductor chip performance, miniaturization, and energy efficiency by facilitating rapid data transfer and low power consumption. However, the quality of the interface bonding can significantly impact overall bond quality, necessitating strategies to quickly detect and classify in-process defects. This study presents a methodology for detecting defects in wafer junction areas from Scanning Acoustic Microscopy images using a ResNet-50 based deep learning model. Additionally, the use of the defect map is proposed to rapidly inspect and categorize defects occurring during the Cu-Cu bonding process, thereby improving yield and productivity in semiconductor manufacturing.\n",
      "A Cost-Effective Blind Spot Detection System with High Recall Rate using Deep Learning and a Comparative Analysis of Implementation Hardware Platforms 2024 ['Blind Spot Detection', 'Advanced Driver Assistance Systems', 'Deep Learning Accelerator', 'FPGA', 'ResNet-18', 'Energy Efficiency', 'Automotive Safety', 'Vehicle Detection', 'Model Parameter Quantization'] None Blind Spot Detection (BSD) is critical for enhancing vehicle safety and is an integral component of many Advanced Driver Assistance Systems (ADAS). In this work, we propose a novel, cost-effective BSD solution utilizing a deep learning approach with the vehicle’s existing rearview camera. While previous works such as Histogram of Oriented Gradients (HOG) combined with Support Vector Machine (SVM) have been used for BSD, these approaches often struggle to maintain high accuracy, particularly in diverse real-world environments. To address these limitations, we employed ResNet-18, a deep learning network, to improve both recall and precision in detecting vehicles in the blind spot. Our approach was evaluated across multiple hardware platforms, including CPU, GPU, and Field Programmable Gate Arrays (FPGA). We conducted comparative analyses in terms of detection performance, processing speed, energy consumption, and cost efficiency. The results showed that our deep learning-based BSD system achieved a 100% recall rate on all hardware platforms, ensuring no critical events were missed, thereby greatly enhancing safety. Among the platforms, FPGA demonstrated superior energy efficiency. Over time, FPGA emerged as the most cost-effective platform due to its low operational costs. This work contributes to the development of more reliable and efficient BSD systems by leveraging deep learning and identifying the optimal hardware platforms for real-time vehicle detection in ADAS applications.\n",
      "Deep Learning-Based Defect Detection in Cu-Cu Bonding Processes 2024 ['Cu-Cu Bonding', 'Deep Learning', 'Defect Detection', 'Defect Map', 'ResNet', 'CNN'] None Cu-Cu bonding, one of the key technologies in advanced packaging, enhances semiconductor chip performance, miniaturization, and energy efficiency by facilitating rapid data transfer and low power consumption. However, the quality of the interface bonding can significantly impact overall bond quality, necessitating strategies to quickly detect and classify in-process defects. This study presents a methodology for detecting defects in wafer junction areas from Scanning Acoustic Microscopy images using a ResNet-50 based deep learning model. Additionally, the use of the defect map is proposed to rapidly inspect and categorize defects occurring during the Cu-Cu bonding process, thereby improving yield and productivity in semiconductor manufacturing.\n",
      "한정된 레이블 데이터를 이용한 효율적인 철도 표면 결함 감지 방법 2024 ['Rail surface', 'Semi-supervised', 'thresholding', 'sorting', 'selection.'] None In this research, we propose a Semi-Supervised learning based railroad surface defect detection method. The Resnet50 model, pretrained on ImageNet, was employed for the training. Data without labels are randomly selected, and then labeled to train the ResNet50 model. The trained model is used to predict the results of the remaining unlabeled training data. The predicted values exceeding a certain threshold are selected, sorted in descending order, and added to the training data. Pseudo-labeling is performed based on the class with the highest probability during this process. An experiment was conducted to assess the overall class classification performance based on the initial number of labeled data. The results showed an accuracy of 98% at best with less than 10% labeled training data compared to the overall training data.\n",
      "엣지 딥 러닝 가속기의 추론 성능 분석 2024 ['Deep learning', 'Deep learning accelerator', 'Inference', 'Edge device', 'Performance analysis'] 엣지 장치에서 딥 러닝 기반 추론을 위해 추론 가속기가 탑재되고 있다. 딥 러닝 추론 가속기를 통해 연산 성능과 에너지 효율을 증가시킬 수 있다. 하지만 가속기에 최적화되지 않은 모델 구조와 설정을 사용하면 메모리 접근 등의 오버헤드로 인해 최적 성능을 낼 수 없다. 본 논문에서는 사전 학습된 MobileNet v2, ResNet50 v1 모델을 사용해 NVIDIA Jetson에서 Graphic Processing Unit (GPU)와 Deep Learning Accelerator (DLA)의 추론 성능을 분석하였다. 실험을 통해 DLA에 최적화되지 않은 모델을 실행하면 GPU보다 최대 5.1배 추론 시간이 증가함을 보였다. 특히, 프로파일링을 통해 DLA에서 지원하지 않는 연산을 GPU로 폴백 (fallback)하는 과정의 오버헤드로 추론 시간이 증가함을 보였다. Inference accelerators are currently being utilized for deep learning inference on edge devices. Deep learning inference accelerators can enhance computational performance and energy efficiency. However, it is important to note that optimal performance cannot be achieved if the model structure and settings (e.g. hyperparameters) are not optimized for the accelerator, which can result in overheads, such as frequent memory access. This study analyses the inference performance of the Graphic Processing Unit (GPU) and the Deep Learning Accelerator (DLA) on NVIDIA Jetson with pre-trained MobileNet v2 and ResNet50 v1 models. The results of our experiments show that running non-optimized models on the DLA results in up to 5.1 times longer inference time compared to the GPU. This paper showed through profiling that the increase in inference time is due to the overhead of GPU fallback to perform operations not supported by DLA.\n",
      "어깨 초음파 영상에서 딥러닝 알고리즘을 이용한 컴퓨터 자동진단의 응용 2024 ['컴퓨터 자동 진단', '딥러닝 알고리즘', '초음파 영상', '영상 구분', '영상 병변 탐지', 'Computer Automated Diagnostics', 'Deep learning algorithm', 'Ultrasound  image', 'Image classification', 'Image Lesion Detection'] 본 연구는 딥러닝 알고리즘을 이용해 어깨 초음파 영상에서 이두근 건의 정상 및 병변을 구분하고, 영상 속 삼출액 병변을 탐지하는 컴퓨터 자동 진단 성능을 평가하고자 한다. D 병원에서 진료받은 어깨 통증 환자들의 초음 파 영상 증례 260건을 사용하였다. 딥러닝으로서 구분 알고리즘에는 ResNet-50, 탐지 알고리즘에는 DeepLabV3+ 를 적용하였으며 성능 평가 지표로 ROC 곡선, AUC, F1-Score 등을 사용하였다. 결과로 구분 알고리즘에서 정확 도 95%, 정밀도 100%, 재현율 91%, AUC 94%를 탐지 알고리즘에서 전역 정확도 97%, 평균 IOU 85%, F1-Score 66% 등을 나타냈다. 본 논문의 제시 모델을 바탕으로 추가 데이터 획득 및 여러 알고리즘을 적용한다면 임상에서 초음파 자동 진단 시스템으로의 응용이 가능하다고 판단된다. This study aims to evaluate the computer's automatic diagnosis performance to distinguish normal and lesions of biceps from shoulder ultrasound images using a deep learning algorithm and to detect exudate lesions in the images. 260 cases of ultrasound imaging of shoulder pain patients treated at D hospital were used. As deep learning, ResNet-50 was applied to the classification algorithm and DeepLabV3+ was applied to the detection algorithm, and ROC curves, AUC, and F1-Score were used as performance evaluation indicators. As a result, 95% accuracy, 100% precision, 91% reproduction rate, and 94% AUC in the classification algorithm showed 97% global accuracy, 85% average IOU, and 66% F1-Score in the detection algorithm. Based on the model presented in this paper, it is judged that the automatic ultrasound diagnosis system can be applied in clinical practice if additional data is acquired and several algorithms are applied.\n",
      "반려동물 안구 질환을 위한 딥러닝 모델 기반 진단 시스템 2024 ['artificial intelligence', 'deep learning', 'ocular diseases', 'system', 'convolutional neural network', '인공지능', '딥러닝', '안구 질환', '시스템', '합성곱 신경망'] 반려동물의 안구 질환은 늦은 진단과 치료로 인해 실명 등의 중대한 결과를 초래할 수 있다. 이는 반려동물 양육 가정의 증가에 따라 점점 더 중요한 문제로 부상하고 있다. 본 논문은 이 문제에 대응하기 위해 인공지능 기반의 조기 진단과 치료 방향을 제시하는 시스템을 설계하고 구현한다. 제안하는 시스템은 AIHUB의 라벨링된 데이터셋을 활용하여 ResNet과 EfficientNet 모델을 최적화한다. 또한 고정밀 질병 분류가 가능하도록 하여 전문가의 진단을 보조하고, 진료가 어려운 지역의 사용성을 높인다. 결과적으로 제안하는 시스템은 반려동물의 안구 건강을 효과적으로 관리하고 보호자의 부담을 경감할 수 있다. 성능평가를 통해, 제안하는 모델은 반려동물의 안구 질환 분류에서 90% 이상의 높은 정확도를 나타냄을 보인다. Pet eye diseases can have serious consequences, including blindness, if not diagnosed and treated promptly. This issue is becoming increasingly important as more households own pets. In this paper, we present a system that uses artificial intelligence to provide early diagnosis and treatment recommendations for pet eye diseases. We use labeled data sets from AIHUB to optimize ResNet and EfficientNet models for diagnosing these diseases. The proposed system helps experts classify diseases with high precision and makes it more accessible in areas with limited medical services. As a result, the system effectively manages and protects the ocular health of cats and dogs, reducing the burden on their caregivers. Performance evaluations demonstrate that the proposed model achieves over 90% accuracy in classifying eye diseases.\n",
      "EfficientNet-B0 outperforms other CNNs in image-based five-class embryo grading: a comparative analysis 2024 ['blastocyst', 'convolutional neural networks', 'deep learning', 'embryo', 'in vitro fertilization'] None Background: Evaluating embryo quality is crucial for the success of in vitro fertilization procedures. Traditional methods, such as the Gardner grading system, rely on subjective human assessment of morphological features, leading to potential inconsistencies and errors. Artificial intelligence-powered grading systems offer a more objective and consistent approach by reducing human biases and enhancing accuracy and reliability.Methods: We evaluated the performance of five convolutional neural network architectures—EfficientNet-B0, InceptionV3, ResNet18, ResNet50, and VGG16— in grading blastocysts into five quality classes using only embryo images, without incorporating clinical or patient data. Transfer learning was applied to adapt pretrained models to our dataset, and data augmentation techniques were employed to improve model generalizability and address class imbalance.Results: EfficientNet-B0 outperformed the other architectures, achieving the highest accuracy, area under the receiver operating characteristic curve, and F1-score across all evaluation metrics. Gradient-weighted Class Activation Mapping was used to interpret the models’ decision-making processes, revealing that the most successful models predominantly focused on the inner cell mass, a critical determinant of embryo quality.Conclusions: Convolutional neural networks, particularly EfficientNet-B0, can significantly enhance the reliability and consistency of embryo grading in in vitro fertilization procedures by providing objective assessments based solely on embryo images. This approach offers a promising alternative to traditional subjective morphological evaluations.\n",
      "합성곱 신경망을 이용한 상지 엑스선 영상 분류 모델 유용성 평가 2024 ['합성곱 신경망', '딥러닝', '의료 영상', '영상 분류', '검사 오류', 'Convolutional Neural Network', 'Deep Learning', 'Medical Image', 'Image Classification', 'Examination Error'] 본 연구는 엑스선 검사 과정에서 환자와 코드를 정확히 확인하지 않아 발생할 수 있는 실수나 오류를 예방하는 것을 목표로 하고 있다. 이를 통해 방사선사의 작업 효율성을 높이고 의료 사고를 방지하며, 합성곱 신경망 기반 이미지 분류 기술을 활용한 실질적인 임상 적용 방안을 제안하고자 한다. 연구는 19,381개의 상지 근골격계 엑스선 이미지를 7개의 영역, 19개의 class로 분류하였으며, 학습, 검증, 평가 세트 비율을 8:1:1로 분할하였다. 딥러닝 모델은 VGG-16, DenseNet-121, ResNet-152v2와 같은 심층 학습 모델을 사용하여 정확도, 정밀도, 재현율, F1 스코어 및 혼동 행렬을 기반으로 모델 성능을 평가하였다. 학습결과 DenseNet-121의 전체 정확도에서 87.77%, 평균 class 정확도에서 98.71%, 정밀도에서 91.78%, 재현율에서 86.93%, F1스코어에서 86.71%를 보였다. 모든지표에서 DenseNet-121이 가장 높은 성능을 보였다. 본 연구는 상지 X선 이미지를 활용한 다양한 심층 학습 모델의 성능을 평가하였으며, 충분한 성능을 보여주었다. 이를 통해 작업 효율성을 높이고 의료 사고 방지가능성을 확인하였다. This study aims to prevent errors that may occur during the radiography examination process, such as misinterpretation of images, by utilizing artificial intelligence, a core technology of the Fourth Industrial Revolution. Through this, we sought to enhance the work efficiency of radiologic technologists, prevent medical accidents. We labeled 19,381 upper ex- tremity musculo-skeletal X-ray images into 7 regions and 19 classes, and divided them into training, validation, and test sets at a ratio of 8:1:1. We used deep learning models such as VGG-16, DenseNet-121, and ResNet-152v2 to evaluate model performance based on accuracy, precision, recall, F1-score, and confusion matrix. The results showed that DenseNet-121 achieved an overall accuracy of 87.77%, an average class accuracy of 98.71%, a precision of 91.78%, a recall of 86.93%, and an F1 score of 86.71%. DenseNet-121 demonstrated the highest performance across all metrics. This study evaluated the performance of various deep learning models using upper extremity radiographic image and demonstrated sufficient performance. Through this, the potential to improve work efficiency and prevent medical accidents was confirmed.\n",
      "비휘발성 메모리 기반 IMC을 활용한단일 코어/다중 레이어 CNN 가속기 최적화 2024 ['Design space exploration', 'IMC', 'CNN', 'accelerator optimization', 'Non-volatile memory'] 이 논문에서는 비휘발성 메모리 기반 In-Memory Computing(IMC)를 활용한 단일 코어/다중 레이어 CNN 가속기의 설계 초기단계에서 가속기의 성능과 면적을 예측하여 최적 메모리 데이터 플로우 및 인터페이스를 탐색할 수 있는 새로운 설계 공간 탐색기를제안한다. 이를 위해 다양한 메모리 레이아웃, 인터페이스, 매핑 방법을 탐색 공간에 포함하였다. 설계 옵션들을 완전 탐색 방식으로탐색하고, 성능과 면적을 예측하여 최적 메모리 데이터 플로우 및 인터페이스를 탐색했다. ResNet-18를 목표 네트워크로 설정하고,제안하는 설계 공간 탐색기를 통해 찾아낸 최적 메모리 데이터 플로우 및 인터페이스는 baseline 대비 면적 효율 측면에서 약 132배 향상이 가능함을 확인했다. This paper presents a novel Design Space Explorer(DSE) that can predict the performance and area of asingle-core/multi-layer CNN accelerator using non-volatile memory-based In-Memory Computing(IMC) at an earlystage of design to explore the optimal memory data flow and interface. To achieve this, we include variousmemory layouts, interfaces, and mapping methods in the exploration space. Design options were explored in anexhaustive search manner, and the optimal memory data flow and interfaces were explored by predictingperformance and area. Using ResNet-18 as the target network, we found that the optimal memory data flow andinterface found by the proposed DSE can improve the area efficiency by about 132 times compared to thebaseline.\n",
      "CNN 기반 인코더와 Transformer 기반 인코더의 이미지 캡셔닝 성능 비교 분석 2024 ['image captioning', 'residual network 50', 'visual geometry group-16', 'vision transformers', 'shifted window transformer', '.'] 이미지 캡셔닝은 이미지의 특징을 추출하여 이미지를 인식하고 자연어 처리와 결합하여 이미지에 대한 설명을 생성하는 작업이다. 이미지 캡셔닝 결과는 때때로 부자연스러운 텍스트를 생성한다. 이러한 문제의 원인을 정확하게 파악하기 위해 인코더들의 성능을 비교 실험한다. 이미지 캡션 생성 과정은 인코더, 디코더 구조를 가진다. 인코더에서 얻어지는 이미지 특징 추출 결과에 따라 디코더에서 생성되는 텍스트에 많은 영향을 미친다. 그에 따라 CNN 계열의 Resnet50, VGG-16과 트랜스포머 계열의 비전 트랜스포머, 스윈 트랜스포머 인코더의 성능을 비교하여 캡션 생성에 있어서 결정적인 영향을 주는지를 분석한다. 정성 및 정량 평가한 결과를 수치화하고 그래프 및 표로 제시하여 CNN 계열과 트랜스포머 계열의 인코딩 결과를 비교 분석하였다. Image captioning involves extracting features from an image to recognize its content and combining them with natural language processing to generate a description of the image. However, the results of image captioning sometimes generate unnatural text. To accurately identify the cause of this issue, a comparative experiment of various encoders’ performance is conducted. The image caption generation process employs an encoder-decoder architecture. Since the text generated by the decoder is heavily influenced by the results of image feature extraction obtained from the encoder. This study compares the performance of CNN-based encoders, such as ResNet50 and VGG-16, with Transformer-based encoders, like Vision Transformer and Swin Transformer, to analyze whether they have a decisive impact on caption generation. This study quantified the results of the qualitative and quantitative evaluation and presented them in graphs and tables to compare and analyze the encoding performance between CNN-based and Transformer-based models.\n",
      "영상에서 효율적인 잡음제거를 위한 dRED-TL-GAN 모델 2024 ['딥러닝', '잡음제거', 'deformable 컨볼루션', 'GAN', 'Deep learning', 'deformable convolution', 'dRED-TL-GAN', 'image denoising'] 영상에서 잡음은 시각적인 왜곡이나 불편을 주는 것 외에도 영상 시스템에서 성능 저하를 가져옴으로써 영상에서 잡음제거는 영상처리에서 중요한 전처리 과정이다. 본 논문에서는 영상에서 잡음제거를 위해 GAN 모델에서 파생된 DCGAN 기반의 deformable RED and transfer learning based generative adversarial networks (dRED-TL-GAN)모델을 제안하고자 한다. 제안된 dRED-TL-GAN 모델에서 생성자는 인코더-디코더 구조로 이루어진 deformable RED 구조이고, 판별자는 전이학습 기반 구조이다. 여기서 deformable RED 구조는 인코더의 컨볼루션 층에서 표준 컨볼루션 대신 deformable 컨볼루션을 사용하여 영상의 특징을 고려하였고, 판별자는 ResNet-18 모델을 사용하여 학습 속도가 분류 정확도를 높혔다. 본 논문에서 제안된 dRED-TL-GAN 모델의 성능 평가를 위해 전통적인 Mean 필터, Median 필터와 BM3D 필터, 그리고 기존 딥러닝의 DnCNN 모델, RED-CNN 모델 그리고 DCGAN 모델을 고려하였으며, 다양한 잡음, 즉, 가우시안 잡음 (Caussian noise), 포아송 잡음 (Poisson noise) 그리고 스팩클 잡음 (Speckle noise)으로 훼손된 얼굴 영상을 대상으로 실험하였다. 성능 실험은 정성적인 평가와 정량적인 평가로 구성되며, 정성적인 평가 결과, Mean 필터, Median 필터, 그리고 BM3D 필터를 포함한 공간 필터들은 대체로 잡음이 남아있고, 또한 호릿한 영상을 얻었고, 제안된 dRED-TL-GAN 모델은 다른 딥러닝 모델보다 에지있는 선명한 영상을 얻었다. 또한, 정량적인 평가 척도인 Peak signal-to-noise ratio (PSNR), Mean squared error (MSE) 그리고 Structural similarity index measure (SSIM) 면에서 dRED-TL-GAN 모델은 모든 잡음과 모든 평가 척도에서 가장 좋은 성능 수치를 얻었다. Noise in images not only causes visual distortion or inconvenience, but also reduces performance in the imaging system, so image denoising is an important preprocessing process in image processing. In this paper, we propose a dRED-TL-GAN model based on DCGAN, derived from GAN, to remove noise from images. The generator of the dRED-TL-GAN model is a deformable RED structure consisting of an encoder-decoder structure, and the discriminator is a transfer learning-based structure. Here, the deformable RED structure used deformable convolutin in the encoder’s convolution layer to remove noise by considering the characteristics of the image, and used the ResNet-18 model in the discriminator to increase learning speed and classification accuracy. To evaluate the performance of the proposed dRED-TL-GAN model, traditional filters including Mean filter, Median filter, and BM3D filter, and existing deep learning models including DnCNN model, RED-CNN model, and DCGAN model were considered. An performance experiment was conducted on face images damaged by various noises, namely Gaussian noise, Poisson noise, and Speckle noise. The performance experiment consists of qualitative and quantitative evaluations. First, in the qualitative evaluation, spatial filters including the Mean filter, Median filter, and BM3D filter generally remained noisy and resulted in blurry results, and the propose dRED-TL-GAN model obtained clearer images with edges than other deep learning models. Additionally, in a quantitative evaluation using PSNR, MSE, and SSIM metrics, the dRED-TL-GAN model performs well under all noises considered and on all evaluation metrics.\n",
      "흉부 X-선 영상을 이용한 Vision transformer 기반 폐렴 진단 모델의 성능 평가 2024 ['딥러닝', '폐렴 진단', '흉부 X-선 영상', 'Vision transformer', 'Deep learning', 'Pneumonia detection', 'Chest X-ray image'] None The various structures of artificial neural networks, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have been extensively studied and served as the backbone of numerous models. Among these, a transformer architecture has demonstrated its potential for natural language processing and become a subject of in-depth research. Currently, the techniques can be adapted for image processing through the modifications of its internal structure, leading to the development of Vision transformer (ViT) models. The ViTs have shown high accuracy and performance with large data-sets. This study aims to develop a ViT-based model for detecting pneumonia using chest X-ray images and quantitatively evaluate its performance. The various architectures of the ViT-based model were constructed by varying the number of encoder blocks, and different patch sizes were applied for network training. Also, the performance of the ViT-based model was compared to the CNN-based models, such as VGGNet, GoogLeNet, and ResNet. The results showed that the traninig efficiency and accuracy of the ViT-based model depended on the number of encoder blocks and the patch size, and the F1 scores of the ViT-based model ranged from 0.875 to 0.919. The training effeciency of the ViT-based model with a large patch size was superior to the CNN-based models, and the pneumonia detection accuracy of the ViT-based model was higher than that of the VGGNet. In conclusion, the ViT-based model can be potentially used for pneumonia detection using chest X-ray images, and the clinical availability of the ViT-based model would be improved by this study.\n",
      "Deep Learning Model and its Application for the Diagnosis of Exudative Pharyngitis 2024 ['Artificial Intelligence', 'Deep Learning', 'Diagnosis', 'Pharyngitis', 'Telemedicine'] None Objectives: Telemedicine is firmly established in the healthcare landscape of many countries. Acute respiratory infections arethe most common reason for telemedicine consultations. A throat examination is important for diagnosing bacterial pharyngitis,but this is challenging for doctors during a telemedicine consultation. A solution could be for patients to upload imagesof their throat to a web application. This study aimed to develop a deep learning model for the automated diagnosis ofexudative pharyngitis. Thereafter, the model will be deployed online. Methods: We used 343 throat images (139 with exudativepharyngitis and 204 without pharyngitis) in the study. ImageDataGenerator was used to augment the training data. Theconvolutional neural network models of MobileNetV3, ResNet50, and EfficientNetB0 were implemented to train the dataset,with hyperparameter tuning. Results: All three models were trained successfully; with successive epochs, the loss and trainingloss decreased, and accuracy and training accuracy increased. The EfficientNetB0 model achieved the highest accuracy(95.5%), compared to MobileNetV3 (82.1%) and ResNet50 (88.1%). The EfficientNetB0 model also achieved high precision(1.00), recall (0.89) and F1-score (0.94). Conclusions: We trained a deep learning model based on EfficientNetB0 that candiagnose exudative pharyngitis. Our model was able to achieve the highest accuracy, at 95.5%, out of all previous studies thatused machine learning for the diagnosis of exudative pharyngitis. We have deployed the model on a web application that canbe used to augment the doctor’s diagnosis of exudative pharyngitis.\n",
      "교통사고 영상 분석을 통한 과실 판단을 위한 딥러닝 기반 방법 연구 2024 ['Action recognition', 'Computer vision', 'Deep learning', 'Image classification', 'Video analysis'] 자율주행 차량에 대한 연구가 활발하게 이뤄지고 있다. 자율주행 차량이 등장함에 따라 기존의 차량과 자율주행 차량이 공존하는 과도기가 올 것이며, 이러한 과도기에는 사고율이 더욱 높아질 것이라 예상된다. 현재 교통사고 발생 시 손해보험협회의 ‘자동차 사고 과실 비율 인정기준’에 따라서 과실 비율을 측정한다. 그러나, 발생한 사고가 어떠한 유형의 사고인지 조사하는 데 소모되는 비용이 매우 크다. 또한 이미 과실 비율 책정이 완료된 사례에 대해서도 재심의를 요구하는 과실 비율 분쟁도 늘어나는 추세이다. 이러한 시간적, 물적 비용을 줄이기 위해 자동으로 과실 비율을 판단하는 딥러닝 모델을 제안하고자 한다. 본 논문에서는  ResNet-18 이미지 분류 모델과 TSN을 통한 비디오 행동 인식을 통해 사고 영상을 바탕으로 과실 비율을 판단하고자 한다. 모델이 상용화된다면, 과실 비율을 측정하는데 소요되는 시간을 획기적으로 단축할 수 있다. 또한 피의자에게 제공할 수 있는 과실 비율에 대한 객관적인 지표가 생기므로 과실 비율 분쟁도 완화될 것으로 기대된다. Research on autonomous vehicles is being actively conducted. As autonomous vehicles emerge, there will be a transitional period in which traditional and autonomous vehicles coexist, potentially leading to a higher accident rate. Currently, when a traffic accident occurs, the fault ratio is determined according to the criteria set by the General Insurance Association of Korea. However, the time required to investigate the type of accident is substantial. Additionally, there is an increasing trend in fault ratio disputes, with requests for reconsideration even after the fault ratio has been determined. To reduce these temporal and material costs, we propose a deep learning model that automatically determines fault ratios. In this study, we aimed to determine fault ratios based on accident video through a image classification model based on ResNet-18 and video action recognition using TSN. If this model commercialized, could significantly reduce the time required to measure fault ratios. Moreover, it provides an objective metric for fault ratios that can be offered to the parties involved, potentially alleviating fault ratio disputes.\n",
      "생체정보 보호를 위한 CNN 기반의 홍채 지문 영역 분할 2024 ['Segmentation', 'Detection', 'Artificial Intelligence Learning', 'Biometric Information', 'CNN', '영역 분할', '감지', '인공지능 학습', '생체 정보', '합성곱 신경망 모델'] 스마트 기기의 발달과 고해상도 이미징 기술의 대중화로 인해, 지문이 노출되거나, 화상 회의, 화상 통화 등 고화질 얼굴 사진에서 홍채 정보가 노출되고 있다. 스마트기기의 대중화는 일상적인 디지털 활동에서 무분별한 사진 공유로 인해, 개인의 생체정보인 지문이나 홍채가 노출되어 위조 및 악용될 가능성이 높아지고 있다. 이러한 문제를 해결하기 위해서 본 논문에서는 원본 이미지로부터 생체정보를 보호하고 보안을 강화할 목적으로 CNN의 영역 분할기법을 활용하여 이미지 내 지문과 홍채를 식별 보호하는 방안을 제안한다. 제안된 모델은 입력 이미지에서 지문 및홍채를 식별한 후 해당 영역에 블러 처리를 적용하며, 이를 원본 이미지와 결합하여 보안성을 강화한다. U-Net 및ResNet-34를 백본으로 사용한 모델 구조를 통해 학습 시간 및 성능을 비교한다. With the development of smart devices and the popularization of high-resolution imaging technology, fingerprints arebeing exposed, and iris information is being exposed in high-definition facial photos such as video conferences and videocalls. The popularization of smart devices has increased the possibility of personal biometric information such asfingerprints and irises being exposed and forged and misused due to indiscriminate photo sharing in everyday digitalactivities. To solve this problem, this paper proposes a method to identify and protect fingerprints and irises in imagesby utilizing CNN's region segmentation technique for the purpose of protecting biometric information from the originalimage and enhancing security. The proposed model identifies fingerprints and irises in the input image, applies blurringto the corresponding regions, and combines them with the original image to enhance security. The learning time andperformance are compared through model structures using U-Net and ResNet-34 as backbones.\n",
      "가상현실 기반의 고정밀 얼굴 통증 인식을 위한 빠른 안구 움직임 검출 알고리즘 2024 ['Virtual Reality', 'Pain Recognition', 'Eye Movement Detection', 'Pediatric Medical Treatment', 'Deep Learning'] 본 논문에서는 고정밀 얼굴 통증 인식을 위한 새로운 가상현실(VR) 기반 급속 안구 운동 검출 알고리즘을 제안한다. 어린이는 종종 예방 접종 및 치과 시술에 대한 두려움과 거부감을 나타내어 치료행위를 어렵게 한다. 그러므로 제안 방법은 VR을 통해 만화를 제공하여 어린이의 주의를 딴 데로 돌리는 한편, 무의식적인 눈의 움직임을 통해 통증 수준을 즉시 평가할 수 있다. 본 시스템은 VR 기술을 진보된 안구 운동 탐지 및 얼굴 표정 인식 알고리즘과 통합하여, 객관적인 통증 평가를 제공한다. 정밀한 안구 추적은 CamShift와 AdaBoost 알고리즘을 통해 구현되며, 통증 분류의 정확도는 ResNet18 및 Swin Transformer 아키텍처를 통합한 얼굴 인식 시스템을 통해 향상된다. 공개적으로 사용이 가능한 데이터 세트을 이용한 실험 결과는 제안된 방법이 통증 인식에서 높은 정확도를 달성하는 데 효과적임을 보여준다. 향후, 다양한 분야에서 VR 기반의 통증 평가 시스템을 적용하여 그 정확도를 높일 것으로 기대된다. This paper proposes a novel VR-based rapid eye movement detection algorithm for high-precision facial pain recognition. Children often show fear and resistance to vaccinations and dental procedures, making treatment difficult. Therefore, the proposed method provides cartoons through VR to divert children's attention, while pain levels can be immediately assessed through eye movements. By integrating VR technology with advanced eye movement detection and facial expression recognition algorithms, the system provides objective pain assessment. Precise eye tracking is achieved using CamShift and AdaBoost algorithms, while pain classification accuracy is enhanced through a facial recognition system integrating ResNet18 and Swin Transformer architectures. Experimental results using publicly available datasets demonstrate the effectiveness of the proposed method in achieving high accuracy in pain recognition. In the future, it is expected that the accuracy of the VR-based pain assessment system will be improved by applying it in various fields.\n",
      "DNN을 위한 비트 단위 파라미터 조작 프레임워크 및 파라미터와 정확도 간의 상호 연관성 분석 2024 ['딥 뉴럴 네트워크', '파라미터', '비트 연산', '정확도', '강인성', 'Deep Neural Network', 'parameters', 'bit-wise operations', 'accuracy', 'robustness'] 최근 DNN이 다양한 산업에 확산되면서 IoT 기기 및 엣지 컴퓨팅에 적합한 경량 모델에 관한 연구가 급증하고 있 다. 본 논문에서는 기존에 없던 딥러닝 모델의 파라미터를 1 비트 단위로 조작할 수 있는 자동화 프레임워크를 개발 하며 파라미터 비트와 모델 정확도 사이의 관계를 실험 및 연구한다. 본 연구는 제안된 프레임워크를 사용하여 ImageNet 데이터셋으로 사전 학습된 DNN 모델 중 CNN 모델들의 파라미터를 하위 n-bit를 0, 1 또는 랜덤한 값으로 치환하는 3가지 방법을 통해 각각 정보 손실 발생시키면서 파라미터와 정확도 간의 강인성을 비트 단위로 실험하였다. 주요 모델로는 InceptionV3, InceptionResnetV2, ResNet50, Xception, DenseNet121, Mobile NetV1, MobileNetV2 을 사용하였다. 실험 결과, 성능이 낮은 모델일수록 하위 비트의 정보 손실에 민감하여 성 능이 좋은 모델보다 정확도를 유지하는 비트 수가 적다는 것을 실험적으로 확인했고, 파라미터와 정확도 간의 강인 성이 높다는 것을 확인하였다. 이러한 실험을 바탕으로 모델별 유효 파라미터 비트를 설정하여 파라미터를 줄이며 정확도를 유지할 수 있다. Recently, with the proliferation of DNNs in various industries, there has been a surge in research on lightweight models suitable for IoT devices and edge computing. In this paper, we propose an automated framework that enables manipulation of deep learning model parameters at a 1-bit level, a capability not previously available. We investigate the relationship between parameter bits and model accuracy. Using the developed framework, we systematically experimented with the parameters of CNN models pre-trained on the ImageNet dataset by setting the lower n-bit to 0, 1, or a random value while each method inducing distinct information loss. The primary models evaluated include InceptionV3, InceptionResnetV2, ResNet50, Xception, DenseNet121, MobileNetV1, and MobileNetV2. Experimental results show that models with lower performance are more sensitive to information loss in the lower bits, requiring fewer bits to maintain accuracy compared to high-performing models. This concludes a high robustness between parameters and accuracy.\n",
      "Oriented object detection in satellite images using convolutional neural network based on ResNeXt 2024 ['box-boundary-aware vector', 'convolutional neural network', 'oriented object detection', 'ResNeXt101', 'satellite imagery'] None Most object detection methods use a horizontal bounding box that causes problems between adjacent objects with arbitrary directions, resulting in misaligned detection. Hence, the horizontal anchor should be replaced by a rotating anchor to determine oriented bounding boxes. A two-stage process of delineating a horizontal bounding box and then converting it into an oriented bounding box is inefficient. To improve detection, a box-boundary-aware vector can be estimated based on a convolutional neural network. Specifically, we propose a ResNeXt101 encoder to overcome the weaknesses of the conven-tional ResNet, which is less effective as the network depth and complexity increase. Owing to the cardinality of using a homogeneous design and multi-branch architecture with few hyperparameters, ResNeXt captures better information than ResNet. Experimental results demonstrate more accurate and faster oriented object detection of our proposal compared with a baseline, achieving a mean average precision of 89.41% and inference rate of 23.67 fps.\n",
      "정보보안을 위한 생체 인식 모델에 관한 연구 2024 ['생체 인식', '멀티모달', '특징 융합', '딥러닝', 'Biometrics', 'Multimodal', 'Feature Fusion', 'Deep Learning'] 생체 인식은 사람의 생체적, 행동적 특징 정보를 특정 장치로 추출하여 본인 여부를 판별하는 기술이다. 생체 인식 분야에서 생체 특성 위조, 복제, 해킹 등 사이버 위협이 증가하고 있다. 이에 대응하여 보안 시스템이 강화되고 복잡해지며, 개인이 사용하기 어려워지고 있다. 이를 위해 다중 생체 인식 모델이 연구되고 있다. 기존 연구들은 특징 융합 방법을 제시하고 있으나, 특징 융합 방법 간의 비교는 부족하다. 이에 본 논문에서는 지문, 얼굴, 홍채 영상을 이용한 다중 생체 인식 모델의 융합 방법을 비교평가했다. 특징 추출을 위해 VGG-16, ResNet-50, EfficientNet-B1, EfficientNet-B4, EfficientNet-B7, Inception-v3를 사용했으며, 특성 융합을 위해 ‘Sensor-Level’, ‘Feature-Level’, ‘Score-Level’, ‘Rank-Level’ 융합 방법을 비교 평가했다. 비교 평가 결과 ‘Feature-Level’ 융합 방법에서 EfficientNet-B7 모델이 98.51%의 정확도를 보이며 높은 안정성을 보였다. 그러나 EfficietnNet-B7모델의 크기가 크기 때문에 생체 특성 융합을 위한 모델 경량화 연구가 필요하다. Biometric recognition is a technology that determines whether a person is identified by extracting information on a person's biometric and behavioral characteristics with a specific device. Cyber threats such as forgery, duplication, and hacking of biometric characteristics are increasing in the field of biometrics. In response, the security system is strengthened and complex, and it is becoming difficult for individuals to use. To this end, multiple biometric models are being studied. Existing studies have suggested feature fusion methods, but comparisons between feature fusion methods are insufficient. Therefore, in this paper, we compared and evaluated the fusion method of multiple biometric models using fingerprint, face, and iris images. VGG-16, ResNet-50, EfficientNet-B1, EfficientNet-B4, EfficientNet-B7, and Inception-v3 were used for feature extraction, and the fusion methods of 'Sensor-Level', 'Feature-Level', 'Score-Level', and 'Rank-Level' were compared and evaluated for feature fusion. As a result of the comparative evaluation, the EfficientNet-B7 model showed 98.51% accuracy and high stability in the 'Feature-Level' fusion method. However, because the EfficietnNet-B7 model is large in size, model lightweight studies are needed for biocharacteristic fusion.\n",
      "볼륨-플로우 그래프 기반 폐질환 분류를 위한앙상블 딥러닝 모델 2024 ['합성곱 신경망', '앙상블 딥러닝 모델', '폐질환', '볼륨-플로우 그래프', 'Convolutional Neural Network', 'Ensemble Deep Learning Model', 'Pulmonary Disease', 'Flow Volume Loops'] 만성 폐쇄성 폐질환은 만성적인 기도 폐쇄를 특징으로 하는 호흡기 질환이다. 만성 폐쇄성폐질환은 초기에 자각 증상이 거의 없어, 대부분 중증 상태로 악화된다. 또한, 인종, 성별, 키,몸무게 등 다양한 요인을 포함한 폐 질환 분류 회귀식은 복잡하고, 정확한 판별을 위해서는지속적인 갱신을 필요로 한다. 따라서 폐질환의 초기 진단이 용이하도록 간편한 휴대형 페기능 검사기를 통해 산출 가능한 볼륨-플로우 그래프 이미지 기반 분류 모델이 요구된다.본 논문에서는 폐질환 조기 진단을 위해 볼륨-플로우 그래프 이미지의 전처리 및 합성곱 신경망 기반 앙상블 딥러닝 모델을 구현하였고, 이를 검증했다. 합성곱 신경망 기반 앙상블 딥러닝 모델은 VGG16, VGG19, ResNet50, 그리고 MobileNet 구조 기반 4개의 모델로 구성되며, 전부 전이학습 및 미세조정하여 사용하였다. 세부적으로는 부족한 수의 학습 데이터를볼륨-플로우 그래프 이미지의 특성을 고려하여 적합한 데이터 증강기법을 적용하였고, 4개의 모델들은 가중치 기반의 간접투표 방식을 사용했다. 최종 앙상블 모델은 단순히 폐질환유무를 판별하는 것이 아닌 정상, 제한성 폐질환, 폐쇄성 폐질환, 그리고 혼합성 폐질환과 같이 총 4개의 클래스로 분류하는 모델임에도 불구하고, 테스트 데이터를 통한 성능은 정확도90.91%, 가중치 평균 정밀도 91.11%, 가중치 평균 재현율 90.91%로 높은 수치를 보였다. Chronic Obstructive Pulmonary Disease (COPD) is a respiratory disease characterized bychronic airway obstruction. COPD often progresses to a severe stage, since thereare few noticeable symptoms in the early stages. Also regression equations involingvarious factors such as race, gender, height, and weight to determine whether ornot there is pulmonary disease is complex and needs to be updated periodically.Therefore, there is a demand for a system that can easily analze the presence orabsence of the pulmonary disease, even for non-experts. In this paper, aCNN-based flow volume loops classification model using ensemble learning andappropriate data pre-processing algorithms was proposed and validated to diagnosepulmonary disease in the early stages. The ensemble model was organized by fourCNN models based on VGG16, VGG19, Resnet50, and MobileNet and used transferlearning and fine-tuning for each pre-trained model. Specifically, to overcome asmall amount of data, several data augmentation techniques that took into accountthe characteristics of flow volume loops were used, and soft voting was employedfor the ensemble model. The proposed ensemble model not only could diagnose thepresence or absence of pulmonary disease but could also classify into a total of fourcategories: normal, restrictive, obstructive, and combined pulmonary diseases. As aresult of the experiment, the performance of the proposed ensemble model showedan accuracy of 90.91%, precision of 91.11%, and recall of 90.91%.\n",
      "배 병해충 이미지 분류를 위한 딥러닝 최적 모델 선택에 관한 연구 2024 ['배 병해충', '딥러닝 모델', '이미지 분류', '검역', '데이터 증강', 'Pear Pests and Diseases', 'Deep Learning Models', 'Image Classification', 'Quarantine', 'Data Augmentation'] None With the increase in agricultural exports, pest and disease quarantine measures have been strengthened globally. Upon detection of pests or diseases in agricultural products, the entire shipment must be recalled or discarded. Therefore, detecting pests during the post-harvest sorting process is critical. This study aims to identify the optimal deep-learning model for classifying healthy and pest-infested pears during sorting. To achieve this, a dataset was created by collecting images of pest-infested pears under conditions similar to publicly available healthy pear images. The study compares CNN-based models (ResNet, MobileNet, EfficientNet, ConvNext) and a transformer-based model (ViT) using the dataset. Standard learning parameters and data augmentation techniques were also evaluated. Accuracy and Grad-CAM were used to analyze model performance. The results indicate that ResNet101 achieved the best performance based on accuracy and Grad-CAM.\n",
      "3D CNN-LSTM 알고리즘을 이용한 손동작 비디오 영상 분류 2024 ['Artificial Intelligence', 'Deep Learning', 'CNN', 'LSTM', 'Video Classification', 'Hand Gesture Classification'] 손동작 인식은 이미지나 비디오 데이터로부터 인간의 동작 및 제스처를 식별하는 행동인식기술의 한 형태이다. 디지털 기술의 발전으로 제품에 스마트 기능이 추가되는 사례가 많아지면서 동작인식의 편리성과 효율성도 강조 되고 있다. 본 연구는 손동작 인식을 시도하기 위한 과정으로, 손동작을 기반으로 클래스를 나누어 각각의 클래스를 분류해내는 비디오 분류 연구를 진행한다. 비디오 영상 자체로 딥러닝 분류를 하게 되면 정확도도 높으며, 이미지를 통한 비디오 분류보다 다양한 분야에서 활용이 가능하다는 장점이 있다. 제시된 알고리즘은 3D CNN(Convolutional neural network)과 LSTM(Long Short-Term Memory)이 결합된 형태로 이루어져 있다. 개발한 3D CNN은 이미지나 비디오의 특징 추출에 주로 사용하는 2D CNN 중 ResNet-18의 구조에서 고안하였다. LSTM은 순차 데이터를 학습, 처리, 분류하는 데 주로 사용되고 있는 RNN(Recurrent Neural Network)중의 한 종류이다. 3D CNN을 통해 비디오의 특징을 추출하고, LSTM을 통해 추출된 특징의 시퀀스를 학습 후 각 비디오 시퀀스를 손동작의 변화를 기준으로 하는 다섯 가지 클래스로 분류하였으며 비디오 분류 결과 정확도 평균 87%를 보여 주었다. Hand gesture recognition is a subset of motion recognition technology that identifies human actions from image or video data. With the advancement of digital technology and the increasing integration of smart functions into products, the convenience and efficiency of motion recognition have become more prominent. This research aims to explore the process of hand gesture recognition by classifying video sequences based on hand gestures. The research focuses on video classification using deep learning techniques, which offer higher accuracy and broader applicability compared to image-based video classification. The proposed algorithm combines a 3D Convolutional Neural Network (3D CNN) with a Long Short-Term Memory network (LSTM). The developed 3D CNN is based on the ResNet-18 architecture, which is commonly used for feature extraction in images and videos. The LSTM, an extension of the Recurrent Neural Network (RNN), is employed to learn, process, and classify sequential data. The 3D CNN extracts features from video sequences, and the LSTM learns these feature sequences to classify each video sequence into one of five classes based on variations in hand gestures. The combined network, utilizing 3D CNN for feature extraction and LSTM for sequence learning, provides a robust approach to classify hand gestures in video sequences, demonstrating potential for diverse applications in various fields. The video classification accuracy reached approximately 87%.\n",
      "A study of Strawberry Maturity Classification Using Improved Faster R-CNN 2024 ['Convolutional Neural Network (CNN)', 'Faster R-CNN', 'Image Classification', 'RoI Align', 'Strawberry Maturity'] None In strawberry cultivation, maturity classification plays an important role in ensuring the efficiency and quality of harvesting. In this study, we propose an Improved Faster R-CNN model to address these challenges, using MobileNetV3-Large as the backbone network to achieve a lightweight model, and introducing RoI Align to improve the spatial accuracy of the feature map. Experiments are conducted using the KGCV_Strawberry dataset, with precision, recall, F1 score, and mean average precision (mAP) measured for performance evaluation. The experimental results show that the proposed model achieves an average precision of 71.35%, recall of 71.07%, and F1 score of 71.21% across all classes. In particular, the proposed model achieves 63% performance on mAP0.5 and 58% performance on mAP0.5:0.95, which is comparable to existing ResNet-based models while achieving faster inference speed. The proposed model achieves a processing speed of 27.6543 ms, which is about 2 ms faster than existing ResNet-based models. This indicates that the goal of creating a lightweight model with improved image processing capability was achieved with minimal performance degradation. This research is expected to contribute to the development of automated strawberry cultivation systems in greenhouse environments and has the potential to be applied to various agricultural environments in the future.\n",
      "시간-주파수 도메인 변환 및 W2GAN-GP 모델 기반의 향상된 오디오 데이터 증강 2024 ['Audio augmentation', 'WGAN-GP', 'Time-frequency transformation', 'Speech classification', 'Imbalanced data'] 최근 딥러닝 기술은 다양한 분야의 분류 시스템에 활용됨에 따라 점차 딥러닝 모델의 성능을 극대화하기 위한 연구가 활발하게 진행되고 있다. 딥러닝 모델의 성능은 학습 데이터의 양과 품질에 따라 많은 영향을 받으며, 딥러닝 모델은 깊고 복잡하게 설계될수록 더 많은 학습 데이터가 요구된다. 또한 학습 데이터가 부족하거나 클래스 간의 데이터 불균형이 존재할 경우, 과 적합 현상이 발생하며 성능이 저하되는 문제가 발생한다. 음성 및 오디오를 활용하는 분야에서 분류 성능을 높이기 위해서 학습 데이터 확장 및 클래스 간 불균형 문제는 중요한 이슈이며, 이를 해결하기 위한 연구는 반드시 필요하다. 본 논문에서는 2차원 이미지 증강을 위해 고안된 WGAN 모델을 개선하여 1차원 오디오 신호를 효과적으로 증강하는 W2GAN-GP 증강 모델을 제안한다. 원 신호 데이터를 입력 받아 시간-주파수 변환 기법을 이용하여 1차 증강을 수행하고, 제안된 W2GAN-GP 모델을 이용하여 2차 증강을 통한 듀얼 오디오 신호 증강 기법을 제안한다. 또한 제안한 증강기법으로 생성된 오디오 데이터의 유효성을 검증하기 위해서 ResNet50 및 DenseNet 분류 모델을 이용하여 분류 정확도를 측정하였다. 분류 모델을 통한 검증 결과, 증강을 수행하지 않은 경우보다 약 27~30% 의 정확도가 높아진 것을 확인할 수 있었다. Recently, as deep learning technology is being utilized in classification systems in various fields, research is being actively conducted to maximize the performance of deep learning models. The performance of deep learning models is greatly affected by the amount and quality of training data, and the deeper and more complex the design of a deep learning model, the more training data is required. Additionally, if training data is insufficient or there is data imbalance between classes, over-fitting occurs and performance deteriorates. In order to improve classification performance in fields that utilize voice and audio, learning data expansion and imbalance between classes are important issues, and research to resolve these issues is essential. In this paper, we propose a W2GAN-GP augmentation model that effectively augments one-dimensional audio signals by improving the WGAN model designed for two-dimensional image augmentation. We propose a dual audio signal augmentation technique by performing the first augmentation on the original signal data using the time-frequency transform technique, and then performing the second augmentation using the proposed W2GAN-GP model. To verify the validity of the audio data generated by the proposed augmentation technique, classification accuracy was measured using ResNet50 and DenseNet classification models. As a result of verification through the classification model, it was confirmed that the accuracy increased by about 27 to 30% compared to the case where augmentation was not performed.\n",
      "Vision Transformer를 활용한 운전자 이상행동 분류 딥러닝 시스템 2024 ['도로 교통', '운전자 이상행동', '딥러닝', 'Vision Transformer', 'Road traffic', 'Driver abnormal behavior', 'Deep learning', 'Vision Transformer'] 도로 교통 사고와 교통 위반 행동은 현대 사회에서 급증하는 문제로, 이에 대한 효과적인 대응이 필요하다. 이러한 사고와 위반 행동은 세계적으로 증가하는 추세를 보이며, 그로 인한 사회 및 경제적 영향은 상당히 심각하다. 주로 운전자의 부주의로 발생하는 도로 교통 사고를 예방하기 위해, 딥러닝과 머신러닝을 활용한 시스템이 구축되고 있다. 이전의 연구 들은 주로 운전자의 이미지를 기반으로 한 모델을 사용하여 운전자의 이상행동을 감지하는 데 초점을 맞추었다. 그러나 이러한 기존 연구들은 대부분 컨볼루션 기반의 모델을 사용하여 운전자의 이상행동을 감지하고 분류하는 데 중점을 두고 있다. 컨볼루션 기반 모델은 초기 학습 단계에서 이미지에서 특정 패턴 및 특징을 학습하고, 이를 고정된 크기의 필터로 추출하는 특징이 있다. 이는 다양한 운전 상황에 대한 적응성이 제한된다는 한계가 있다. 따라서 본 논문은 컨볼루션 기반 모델의 한계를 극복하고자, Vision Transformer 모델을 활용한 운전자 이상행동 분류 모델을 구축하였다. 해당 모델의 우수성을 확인하기 위해 기존 연구에서 사용된 ResNet-101, VGG19, Xception, ConvNeXt 등의 모델과 분류 성능 평가 지표를 기반으로 비교 분석을 실시하였다. 비교 분석 결과, Vision Transformer 모델이 기존의 컨볼루션 기반 모델들보다 탁월한 성능을 보여주었다. 이러한 결과는 Vision Transformer의 학습 방식이 다양한 특징 및 패턴을 효과적으로 학습하고 이를 활용할 수 있음을 시사한다. 본 연구는 도로 교통 안전성 향상을 위한 혁신적인 모델의 가능성을 제시하며, 더 나아가 안전 운전 문화의 정착과 사회적 이익을 증진시킬 수 있다. The surge in road traffic accidents and traffic violations is a pressing issue in modern society, demanding effective responses. These incidents display a global upward trend, with significant societal and economic repercussions. To mitigate road accidents, primarily caused by driver negligence, systems leveraging deep learning and machine learning are being developed. Previous research has predominantly focused on models based on driver images for detecting abnormal driving behavior, with a predominant emphasis on convolutional models. Convolutional models learn specific patterns and features from images during the initial stages of training, extracting them using fixed-size filters, thereby limiting adaptability to diverse driving scenarios. This paper addresses the limitations of convolutional models by introducing a driver abnormal behavior classification model using the Vision Transformer. To validate the superiority of this model, a comparative analysis was conducted with well-established models such as ResNet-101, VGG19, Xception, and ConvNeXt, employing classification performance metrics from previous studies. The results of the comparative analysis demonstrate that the Vision Transformer model outperforms traditional convolutional models. This outcome indicates the effectiveness of Vision Transformer’s learning approach in efficiently capturing and utilizing various features and patterns. This research not only presents the potential for an innovative model to enhance road traffic safety but also pledges to contribute to the establishment of a safety-oriented driving culture and the enhancement of societal benefits.\n",
      "BCED-Net: Breast Cancer Ensemble Diagnosis Network using transfer learning and the XGBoost classifier with mammography images 2024 ['Breast cancer classification', 'Feature extraction and concatenation', 'Performance evaluation', 'XGBoost classifier'] None Objectives Breast cancer poses a significant global health challenge, characterized by complex origins and the potential for life-threatening metastasis. The critical need for early and accurate detection is underscored by the 685,000 lives claimed by the disease worldwide in 2020. Deep learning has made strides in advancing the prompt diagnosis of breast cancer. However, obstacles persist, such as dealing with high-dimensional data and the risk of overfitting, necessitating fresh approaches to improve accuracy and real-world applicability.Methods In response to these challenges, we propose BCED-Net, which stands for Breast Cancer Ensemble Diagnosis Network. This innovative framework leverages transfer learning and the extreme gradient boosting (XGBoost) classifier on the Breast Cancer RSNA dataset. Our methodology involved feature extraction using pre-trained models—namely, Resnet50, EfficientnetB3, VGG19, Densenet121, and ConvNeXtTiny—followed by the concatenation of the extracted features. Our most promising configuration combined features extracted from deep convolutional neural networks—namely Resnet50, EfficientnetB3, and ConvNeXtTiny—that were classified using the XGBoost classifier.Results The ensemble approach demonstrated strong overall performance with an accuracy of 0.89. The precision, recall, and F1-score values, which were all at 0.86, highlight a balanced trade-off between correctly identified positive instances and the ability to capture all actual positive samples.Conclusion BCED-Net represents a significant leap forward in addressing persistent issues such as the high dimensionality of features and the risk of overfitting.\n",
      "패션 카테고리 오버샘플링 자동화 시스템 2024 ['CNN', 'Deep learning', 'Oversampling', 'ResNet50', 'YOLOv8', 'Fashion category', '컨볼루션 신경망', '딥러닝', '오버샘플링', '레즈넷50', '욜로 버전 8', '패션 카테고리'] None None\n",
      "An Improved Classification Model Based on Feature Fusion for Orchid Species 2024 ['Orchid species · Classifcation model · Feature fusion · ResNet34'] None Orchid is a kind of terrestrial herb and it has elegant fower posture, quiet fower fragrance, rich colors and noble moral, therefore it has high ornamental value and is deeply loved by people. There are many kinds of orchids, and some of them are similar in shape, texture and color, which make people difcult to quickly and correctly distinguish them. As the existing classifcation model of orchid species have the problems of low accuracy rate and long classifcation time because of the inter species similarities and intra species diferences in orchid species, thus infuencing its wide application. In order to solve the problem above, in this paper, an improved classifcation model based on feature fusion is proposed for orchid species. The achievement of the paper lies in the fact that we successfully developed a classifcation model based on feature fusion to realize the high-efcient classifcation for orchid species. Specifcally, in our scheme, frstly we obtained 12 orchid image sets with number of 12,227 images by network and feld photography; Secondly we analyzed and studied the semantic relationship of diferent scale features from acquired orchid images above; Thirdly we designed an improved classifcation model based on feature fusion on the basis of the semantic relationship above; At last, we used the classifcation model above to realize the high-efcient classifcation for 12 orchid species. The experimental results showed that our proposed classifcation model based on feature fusion in this paper can realize 92.98% classifcation accuracy rate compared with classifcation models without using feature fusion technology, which can greatly improve the classifcation efciency for orchid species.\n",
      "머신러닝기반 오이 생육 최적 예측 모델에 관한 연구 2024 ['작물 질병 진단', '딥러닝 모델 비교', '레스넷50', '스마트 농업', '욜로v8', 'Crop Disease Diagnosis', 'Deep Learning Model Comparison', 'ResNet50', 'Smart Agriculture', 'YOLOv8'] None None\n",
      "딥러닝 기반의 빛간섭단층촬영 이미지에서 안구 방향 분류 모델 2024 ['Deep learning', 'Eye', 'Optical coherence tomography'] None Purpose: To develop a deep learning model classifying the laterality of optical coherence tomography (OCT) images.Methods: The study included two-dimensional OCT images (horizontal/vertical macular section) from Seoul National University Hospital.A deep learning model based on ResNet-18 was developed and trained to classify whether OCT images were horizontal or vertical sections and to predict the laterality of the images. Analysis of the results included calculating a mean area under the receiver operating characteristic curve (AUROC) and evaluating accuracy, specificity, and sensitivity. Gradient-weighted class activation for mapping visualization highlighted critical regions for classification.Results: A total of 5,000 eyes of 2,500 patients (10,000 images) was included in the development process. The test dataset consisted of 1,000 eyes of 500 patients (590 eyes without macular abnormalities, 208 epiretinal membranes, 111 age-related macular degenerations, 56 central macular edemas, 23 macular holes, and 12 other macular abnormalities). The deep learning model predicted the OCT section of the eyes in the test dataset with a mean AUROC of 0.9967. The accuracy, sensitivity, and specificity were 0.9835, 0.9870, and 0.9800, respectively.The model predicted the laterality of the eyes in horizontal OCT images with a mean AUROC of 1.0000. The accuracy, sensitivity, and specificity were 0.9970, 1.0000, and 0.9940, respectively. Using vertical OCT images, deep learning models failed to demonstrate any predictive performance in laterality classification.Conclusions: We developed a deep learning model to classify the horizontal/vertical sections of OCT images and predict the laterality of horizontal OCT images with high accuracy, sensitivity, and specificity.\n",
      "An Ensemble Deep Transfer Learning Model for Multi-dimensional Image Classification of Histological Prostate Biopsy Patterns 2024 ['Multi-dimensional', 'Histological', 'Prostate Cancer', 'Ensemble Deep Transfer Learning', 'Image Classification'] None Early prostate cancer diagnosis by pathologists remains challenging. Recent advances in computer-aided detection (CAD), artificial intelligence (AI), and machine learning (ML) allow prostate cancer grading. This study explored the accuracy of prostate cancer detection by deep learning techniques, particularly convolutional neural networks (CNNs). We performed three-way binary classification based on images cropped to 256 × 256 and 512 × 512 pixels using an ensemble deep CNN model. Six pre-trained CNN models (MobileNet, VGG-16, ResNet-50, DenseNet-121, Inception-V3, and EfficientNet-B0) were integrated to classify histopathological features. The overall accuracy for the combined 256 × 256 and 512×512 pixel images was 94.9%. Additionally, in separate classifications of 256×256 and 512×512 images, we achieved overall accuracies of 90.8% and 94.3%, respectively. Consequently, our method effectively distinguishes benign from malignant samples, approaching near-perfect accuracy.\n",
      "Automated Detection of COVID-19 in Chest Radiographs: Leveraging Machine Learning Approaches 2024 ['COVID-19 pandemic', 'Machine learning models', 'Chest X-ray classification', 'Automated identification', 'Medical diagnosis'] None The World Health Organization (WHO) has designated the COVID-19 pandemic a global health emergency, prompting responses all over the world. The fatality rate is between 2% and 5%, and millions of people around the world have been infected. While the WHO recommends tests, resource-intensive testing has motivated the development of CNN technology for automated identification. Research employing machine learning models shows great accuracy in classifying X-ray and CT images for COVID-19 detection. These models include denseNet201, resnet50V2, inceptionv3, mobile net, and custom CNNs. The interpretation of chest X-rays has come a long way, yet there are still obstacles to overcome. In this paper, we present a way for using a machine learning model to categorize chest X-ray pictures into normal, COVID-19, viral pneumonia, and lung opacity, demonstrating the model's efficacy in assisting medical diagnosis, especially in time-sensitive situations like COVID-19.\n",
      "주행 시스템을 위한 그리드 레벨 다중 클래스 이뮬 분류 모델 2024 ['soiling detection', 'woodscape dataset', 'image classification', 'autonomous driving', '.'] None This study proposes a soiling detection algorithm to identify and locate contamination in vehicle camera lenses. Research on AI applications that utilize cameras and distance sensors in driving systems is directly related to the advancement of autonomous driving systems., Detecting soiling, such as mud and water droplets, is a particularly critical issue. Traditional methods using piezoelectric, ultrasonic, and thermal sensors can introduce additional design and maintenance complexity. Therefore, this study aims to reduce this complexity by utilizing existing surround-view cameras installed on vehicles without additional sensors. The proposed algorithm employs image-processing techniques and a lightweight neural network architecture based on ResNet18 to detect lens contamination in real-time across various driving environments. Experiments were conducted using 5,000 images from the WoodScape Soiling Dataset. The input images were divided into a 16 × 16 grid and classified into four labels: opaque, semi-transparent, transparent, and clean. The proposed grid-level multiclass soiling classification model demonstrated effective performance in detecting contamination, including mud, water droplets, and foggy dust. This study is expected to enhance the safety and convenience of driving systems.\n",
      "Convolutional Neural Network Technique for Distinguishing Nine Varieties of Vegetable Crops 2024 ['Confusion matrix', 'Convolutional neural network', 'DenseNet201', 'Image classification', 'Vegetables'] None In the field of agriculture, conducting research on neural network models for image classification is necessary to accurately categorize crops based on their types and health conditions and distinguish them from other species, to minimize crop losses. This study aimed to compare multiple neural network models to select the optimal model that can classify the images of nine vegetable seedlings, such as carrot, Kimchi cabbage, kohlrabi, lettuce, mallow, mustard, pak-choi, spinach, and sweet pepper. The best model was selected based on its accuracy (precision, recall, and F1 score) from eight trained models, namely DenseNet201, InceptionResNetV2, InceptionV3, MobileNetV2, ResNet152V2, VGG16, VGG19, and Xception. To train the models, a 9-class dataset, 20 epochs, 32 batch sizes, Adam optimizer, and a learning rate of 0.001 were used. The DenseNet201 model exhibited the highest accuracy and was, therefore, selected as the optimal model. With a batch size of 128, Adam optimizer, and a learning rate of 0.001, this model exhibited high precision, recall, and F1 score, and its superiority was confirmed using a confusion matrix. As a result, the DenseNet201 model is expected to improve the recognition performance of the model by using images of various plant species, exploring more networks, and optimizing the hyperparameters to achieve higher recognition accuracy.\n",
      "Deep learning to assess bone quality  from panoramic radiographs: the  feasibility of clinical application  through comparison with an implant  surgeon and cone-beam computed  tomography 2024 ['Artificial intelligence', 'Bone density', 'Deep learning', 'Dental implant', 'Dental radiography'] None Purpose: Bone quality is one of the most important clinical factors for the primary stability and successful osseointegration of dental implants. This preliminary pilot study aimed to evaluate the clinical applicability of deep learning (DL) for assessing bone quality using panoramic (PA) radiographs compared with an implant surgeon’s subjective tactile sense and cone-beam computed tomography (CBCT) values.Methods: In total, PA images of 2,270 edentulous sites for implant placement were selected, and the corresponding CBCT relative gray value measurements and bone quality classification were performed using 3-dimensional dental image analysis software. Based on the pre-trained and fine-tuned ResNet-50 architecture, the bone quality classification of PA images was classified into 4 levels, from D1 to D4, and Spearman correlation analyses were performed with the implant surgeon’s tactile sense and CBCT values.Results: The classification accuracy of DL was evaluated using a test dataset comprising 454 cropped PA images, and it achieved an area under the receiving characteristic curve of 0.762 (95% confidence interval [CI], 0.714–0.810). Spearman correlation analysis of bone quality showed significant positive correlations with the CBCT classification (r=0.702; 95% CI, 0.651–0.747; P<0.001) and the surgeon’s tactile sense (r=0.658; 95% CI, 0.600–0.708, P<0.001) versus the DL classification.Conclusions: DL classification using PA images showed a significant and consistent correlation with CBCT classification and the surgeon’s tactile sense in classifying the bone quality at the implant placement site. Further research based on high-quality quantitative datasets is essential to increase the reliability and validity of this method for actual clinical applications.\n",
      "Deep learning to assess bone quality from panoramic radiographs: the feasibility of clinical application through comparison with an implant surgeon and cone-beam computed tomography 2024 ['Artificial intelligence', 'Bone density', 'Deep learning', 'Dental implant', 'Dental radiography'] None Purpose: Bone quality is one of the most important clinical factors for the primary stability and successful osseointegration of dental implants. This preliminary pilot study aimed to evaluate the clinical applicability of deep learning (DL) for assessing bone quality using panoramic (PA) radiographs compared with an implant surgeon's subjective tactile sense and cone-beam computed tomography (CBCT) values. Methods: In total, PA images of 2,270 edentulous sites for implant placement were selected, and the corresponding CBCT relative gray value measurements and bone quality classification were performed using 3-dimensional dental image analysis software. Based on the pre-trained and fine-tuned ResNet-50 architecture, the bone quality classification of PA images was classified into 4 levels, from D1 to D4, and Spearman correlation analyses were performed with the implant surgeon's tactile sense and CBCT values. Results: The classification accuracy of DL was evaluated using a test dataset comprising 454 cropped PA images, and it achieved an area under the receiving characteristic curve of 0.762 (95% confidence interval [CI], 0.714-0.810). Spearman correlation analysis of bone quality showed significant positive correlations with the CBCT classification (r=0.702; 95% CI, 0.651-0.747; P<0.001) and the surgeon's tactile sense (r=0.658; 95% CI, 0.600-0.708, P<0.001) versus the DL classification. Conclusions: DL classification using PA images showed a significant and consistent correlation with CBCT classification and the surgeon's tactile sense in classifying the bone quality at the implant placement site. Further research based on high-quality quantitative datasets is essential to increase the reliability and validity of this method for actual clinical applications.\n",
      "Sharpness-Aware Minimization을 적용한 Separable Vision Transformer 기반 악성코드 유형 분류 기법 2024 ['Malware', 'Classification Method', 'Separable Vision Transformer', 'SharpnessAware Minimization', '악성코드', '유형 분류 기법', 'Separable Vision Transformer', 'Sharpness-Aware Minimization'] None \"The methods of classifying malware family through malware visualization generate malware images and then classify malware family using artificial intelligence models such as Convolutional Neural Networks(CNNs). However, such methods are vulnerable to malware obfuscation techniques. In this paper, we propose a malware classification method based on the Separable Vision Transformer(SepViT) that is robust against obfuscation techniques. The proposed method performs malware family classification using a SepViT model enhanced with Sharpness-Aware Minimization(SAM) after visualizing malware as grayscale images. From the experimental results using the Microsoft Malware Classification Challenge dataset, we show that SAM Optimizer-based SepViT used in the proposed method can classify malware family more accurately than four methods(ResNet18, ViT, CrossViT, SepViT). We also analyze the basis for classification of the proposed method using Grad-Cam. In addition, from the experiments using AndroDex dataset, we show that the proposed method shows good detection performance even in the presence of obfuscation in malware.\"\n",
      "연속 웨이블릿 변환을 이용한 On-board Charger의 전이 학습 기반 고장 진단 알고리즘 2024 ['on-board charger', 'fault diagnosis', 'wavelet transform', 'transfer learning', 'convolutional neural network', '.'] None In this study, a method is proposed for diagnosing the normal operation and faults of an on-board charger (OBC), the battery charging device of a vehicle, using the convolutional neural network algorithm. In the conducted experiments, faults were defined as short and open states of the switching component in the power factor correction (PFC) stage of the OBC topology. To achieve this, the current data of the PFC boost inductor collected through PSIM simulations were reconstructed into images using continuous wavelet transform. In the MATLAB environment, transfer learning was implemented using pre-trained models such as GoogLeNet, ResNet50, ShuffleNet, and MobileNetV2 for diagnosing faults in the OBC. Experimental results revealed that MobileNetV2 achieved a validation accuracy of 95.91%. Subsequently, the performance of the model was analyzed using a confusion matrix on the test set, yielding an accuracy of 96.53%. These findings underscore the effectiveness of the proposed approach in classifying normal and faulty data.\n",
      "다층 다요소 시스템의 최적화를 위한 진화연산의 탐색공간 축소 - 딥러닝 가지치기 사례 연구 2024 ['Multilayers Multi-elements System', 'Evolutionary Computation', 'Gene Expression by Rules', 'CNN', 'Filter Pruning'] None Optimization of multi-layer, multi-element systems such as deep learning is an NP-hard problem that requires determining the number of layers, the number of elements in a layer, and the types of elements. In such a system, deleting redundant elements to reduce the size while maintaining performance is crucial to conserve resources and improve the efficiency of the system. This is a very complex and challenging problem because it consists of a large number of multi-layered and multi-element systems with a huge search space. Evolutionary computation is widely used for large-scale optimization problems due to its high efficiency, but it is difficult to apply due to the characteristics of evolutionary computation when the calculation of the fitness function is complex. To solve this problem, we propose a technique that dramatically reduces the search space by improving the representation of the gene.We verify its feasibility by applying it to a case study, CNN pruning. We use the ResNet56 model for the CIFAR10 dataset and compare it with existing pruning approaches.\n",
      "Dental Age Estimation in Children Using Convolution Neural Network Algorithm: A Pilot Study 2024 ['Convolutional neural networks', 'Deep learning', 'Dental age estimation'] None Purpose: Recently, deep learning techniques have been introduced for age estimation, with automated methods based on radiographic analysis demonstrating high accuracy. In this study, we applied convolutional neural network (CNN) techniques to the lower dentition area on orthopantomograms (OPGs) of children to develop an automated age estimation model and evaluate its accuracy for use in forensic dentistry.Methods: In this study, OPGs of 2,856 subjects aged 3-14 years were analyzed. The You Only Look Once (YOLO) V8 object detection technique was applied to extract the mandibular dentition area on OPGs, designating it as the region of interest (ROI). First, 200 radiographs were randomly selected, and were used to train a model for extracting the ROI. The trained model was then applied to the entire dataset. For the CNN image classification task, 80% of OPGs were allocated to the training set, while the remaining 20% were used as the test set. A transfer learning approach was employed using the ResNet50 and VGG19 backbone models, with an ensemble technique combining these models to improve performance. The mean absolute error (MAE) on the test set was used as the validation metric, and the model with the lowest MAE was selected.Results: In this study, the age estimation model developed using mandibular dentition region from OPGs achieved MAE and root mean squared error (RMSE) values of 0.501 and 0.742, respectively, on the test set, and MAE and RMSE values of 0.273 and 0.354, respectively, on the training set.Conclusions: The automated age estimation model developed in this study demonstrated accuracy comparable to that of previous research and shows potential for applications in forensic investigations. Increasing the sample size and incorporating diverse deep learning techniques are expected to further enhance the accuracy of future age estimation models.\n",
      "Improving the Recognition of Known and Unknown Plant Disease Classes Using Deep Learning 2024 ['deep learning', 'plant disease recognition', 'transfer learning', 'unknown disease recognition'] None Recently, there has been a growing emphasis on identifying both known and unknown diseases in plant disease recognition. In this task, a model trained only on images of known classes is required to classify an input image into either one of the known classes or into an unknown class. Consequently, the capability to recognize unknown diseases is critical for model deployment. To enhance this capability, we are considering three factors. Firstly, we propose a new logits-based scoring function for unknown scores. Secondly, initial experiments indicate that a compact feature space is crucial for the effectiveness of logits-based methods, leading us to employ the AM-Softmax loss instead of Cross-entropy loss during training. Thirdly, drawing inspiration from the efficacy of transfer learning, we utilize a large plant-relevant dataset, PlantCLEF2022, for pre-training a model. The experimental results suggest that our method outperforms current algorithms. Specifically, our method achieved a performance of 97.90 CSA, 91.77 AUROC, and 90.63 OSCR with the ResNet50 model and a performance of 98.28 CSA, 92.05 AUROC, and 91.12 OSCR with the ConvNext base model. We believe that our study will contribute to the community.\n",
      "Inceptionv3-LSTM-COV: A multi-label framework for identifying adverse reactions to COVID medicine from chemical conformers based on Inceptionv3 and long short-term memory 2024 ['adverse medicine reactions', 'COVID medicine development', 'Inceptionv3', 'LSTM', 'multi-label'] None Due to the global COVID-19 pandemic, distinct medicines have been devel-oped for treating the coronavirus disease (COVID). However, predicting and identifying potential adverse reactions to these medicines face significant chal-lenges in producing effective COVID medication. Accurate prediction of adverse reactions to COVID medications is crucial for ensuring patient safety and medicine success. Recent advancements in computational models used in pharmaceutical production have opened up new possibilities for detecting such adverse reactions. Due to the urgent need for effective COVID medication development, this research presents a multi-label Inceptionv3 and long short-term memory methodology for COVID (Inceptionv3-LSTM-COV) medicine development. The presented experimental evaluations were conducted using the chemical conformer image of COVID medicine. The features of the chemi-cal conformer are denoted utilizing the RGB color channel, which is extracted using Inceptionv3, GlobalAveragePooling2D, and long short-term memory (LSTM) layers. The results demonstrate that the efficiency of the Inceptionv3-LSTM-COV model outperformed the previous study’s perfor-mance and achieved better results compared to MLCNN-COV, Inceptionv3, ResNet50, MobileNetv2, VGG19, and DenseNet201 models. The proposed model reported the highest accuracy value of 99.19% in predicting adverse reactions to COVID medicine.\n",
      "Classification of Pulmonary Nodules in 2-[ 18 F]FDG PET/CT Images with a 3D Convolutional Neural Network 2024 ['Convolutional neural networks · Positron emission tomography · 2-[18F]FDG PET/CT · Pulmonary nodules · Artificial intelligence'] None Purpose 2-[18F]FDG PET/CT plays an important role in the management of pulmonary nodules. Convolutional neuralnetworks (CNNs) automatically learn features from images and have the potential to improve the discrimination betweenmalignant and benign pulmonary nodules. The purpose of this study was to develop and validate a CNN model for classificationof pulmonary nodules from 2-[18F]FDG PET images.Methods One hundred thirteen participants were retrospectively selected. One nodule per participant. The 2-[18F]FDG PETimages were preprocessed and annotated with the reference standard. The deep learning experiment entailed random datasplitting in five sets. A test set was held out for evaluation of the final model. Four-fold cross-validation was performed fromthe remaining sets for training and evaluating a set of candidate models and for selecting the final model. Models of threetypes of 3D CNNs architectures were trained from random weight initialization (Stacked 3D CNN, VGG-like and Inceptionv2-like models) both in original and augmented datasets. Transfer learning, from ImageNet with ResNet-50, was also used.Results The final model (Stacked 3D CNN model) obtained an area under the ROC curve of 0.8385 (95% CI: 0.6455–1.0000)in the test set. The model had a sensibility of 80.00%, a specificity of 69.23% and an accuracy of 73.91%, in the test set, foran optimised decision threshold that assigns a higher cost to false negatives.Conclusion A 3D CNN model was effective at distinguishing benign from malignant pulmonary nodules in 2-[18F]FDGPET images.\n",
      "MLCNN-COV: A multilabel convolutional neural network-based framework to identify negative COVID medicine responses from the chemical three-dimensional conformer 2024 ['chemical three-dimensional conformers', 'convolutional neural network', 'COVID medicine development', 'negative medicine reactions', 'transfer-learning'] None To treat the novel COronaVIrus Disease (COVID), comparatively fewer medicines have been approved. Due to the global pandemic status of COVID, several medicines are being developed to treat patients. The modern COVID medicines development process has various challenges, including predicting and detecting hazardous COVID medicine responses. Moreover, correctly pre-dicting harmful COVID medicine reactions is essential for health safety. Significant developments in computational models in medicine development can make it possible to identify adverse COVID medicine reactions. Since the beginning of the COVID pandemic, there has been significant demand for developing COVID medicines. Therefore, this paper presents the transfer-learning methodology and a multilabel convolutional neural network for COVID (MLCNN-COV) medicines development model to identify negative responses of COVID medicines. For analysis, a framework is proposed with five multilabel transfer-learning models, namely, MobileNetv2, ResNet50, VGG19, DenseNet201, and Inceptionv3, and an MLCNN-COV model is designed with an image augmentation (IA) technique and validated through experiments on the image of three-dimensional chemical conformer of 17 number of COVID medicines. The RGB color channel is utilized to represent the feature of the image, and image features are extracted by employing the Convolution2D and MaxPooling2D layer. The findings of the current MLCNN-COV are promising, and it can identify individual adverse reactions of medicines, with the accuracy ranging from 88.24% to 100%, which outper-formed the transfer-learning model’s performance. It shows that three-dimensional conformers adequately identify negative COVID medicine responses.\n",
      "재난약자 및 취약시설에 대한 APC실증에 관한 연구 2024 ['재난취약시설', '요구조자', '자동계수기', '인공지능', '영상인식', 'Disaster-vulnerable Facilities', 'Victims', 'Auto People Counter', 'AI', 'Image Recognition'] None Purpose: The purpose of this study is to improve the recognition rate of APC (Auto People Counting), which accurately identifies the remaining claimants and provides them to response agencies such as fire departments when a disaster occurs in a disaster-vulnerable facility such as a nursing hospital. Currently, when a disaster occurs, response agencies arrive at the disaster site and ask building officials directly to determine the status of those in need within the building. This may be inaccurate information about the rescuer, which may expand the scope of work of the response agency and pose a risk to the safety of the rescuer. APC automatically counts the number of people entering and leaving the building and provides real-time information on the number of people remaining, making it possible to accurately determine the status of those in need in the event of a disaster. The purpose of this study is to select the optimal artificial intelligence algorithm so that APC can more accurately count the number of people entering.Method: In this study, baseline modeling was performed using a CNN model to improve the algorithm that recognizes images of entering personnel through cameras targeting APCs installed and operated in actual disaster-vulnerable facilities. The study was conducted by analyzing the performance of various algorithms to select the top seven candidates and using a transfer learning model to select the optimal algorithm with the best performance.Research Results: As a result of the experiment, the precision and recall of the Densenet201 and Resnet152v2 models, which had the best time and performance, were confirmed to show 100% accuracy for all labels. Among these, the Densenet201 model showed higher performance.Conclusion: Among various artificial intelligence algorithms, the optimal algorithm that can be applied to APC was selected. This will improve the recognition rate of APC and enable quick and safe rescue operations by accurately identifying the information of rescuers in the event of a disaster. This is expected to contribute to ensuring the safety of rescuers performing rescue operations as well as the safe rescue of rescuers. In the future, additional research on algorithm analysis and learning is required to accurately identify the number of people entering disaster-vulnerable facilities in various disaster situations such as haze.\n",
      "Crop Leaf Disease Identification Using Deep Transfer Learning 2024 ['Agricultural Artificial Intelligence', 'Crop Leaf Disease Identification', 'Plant Protection', 'Transfer Learning'] None Traditional manual identification of crop leaf diseases is challenging. Owing to the limitations in manpower and resources, it is challenging to explore crop diseases on a large scale. The emergence of artificial intelligence technologies, particularly the extensive application of deep learning technologies, is expected to overcome these challenges and greatly improve the accuracy and efficiency of crop disease identification. Crop leaf disease identification models have been designed and trained using large-scale training data, enabling them to predict different categories of diseases from unlabeled crop leaves. However, these models, which possess strong feature representation capabilities, require substantial training data, and there is often a shortage of such datasets in practical farming scenarios. To address this issue and improve the feature learning abilities of models, this study proposes a deep transfer learning adaptation strategy. The novel proposed method aims to transfer the weights and parameters from pre-trained models in similar large-scale training datasets, such as ImageNet. ImageNet pre-trained weights are adopted and fine-tuned with the features of crop leaf diseases to improve prediction ability. In this study, we collected 16,060 crop leaf disease images, spanning 12 categories, for training. The experimental results demonstrate that an impressive accuracy of 98% is achieved using the proposed method on the transferred ResNet-50 model, thereby confirming the effectiveness of our transfer learning approach.\n",
      "아음속 수송체 알루미늄 프레임의 비선형 유도초음파 주파수 응답 – 합성곱 신경망 분석 기반 미세 감육 진단 가능성 연구 2024 ['구조 진단', '유도초음파', '심층학습', '합성곱 신경망', '객체검출', 'Structural Health Monitoring', 'Guided Wave', 'Deep Learning', 'Convolution Neural Network', 'Object Detection'] None None\n",
      "Deep Learning-based Arabic Sign Recognition System for Automated Communication with Hearing Impaired Individuals 2024 ['Arabic Sign Language', 'Deep Neural Networks', 'ResNet', 'CNN', 'Automatic Recognition', 'Hearing Impaired'] None Arabic Sign Language (ArSL) is used by individuals who are hard of hearing or deaf in Arab countries, as well as others around the world who use it for religious purposes. for the need for automated systems to facilitate the learning and communication of ArSL is therefore significant. Such systems would allow people to learn Arabic Sign Language and use it to communicate among themselves and with the surrounding community. This paper presents the development of an automatic recognition system capable of accurately identifying Arabic signs through hand gestures. In this paper, two Residual Network (ResNet) Configurations, Version 1 (V1) and Version 2 (V2), are proposed and detailed. The proposed ResNet V1 achieved an average accuracy of 98.83%, while ResNet V2 achieved an average accuracy of 98.84%. The results described in this paper far exceed those reported in the extant literature. The high accuracy of the proposed system shows the potential for integrating the system with education tools and assistive technologies for people with special needs.\n",
      "Deep Learning Driven Human Posture Location  in Physical Education Teaching 2024 ['Human Posture', 'Physical Education Teaching', 'Deep Learning', 'ResNet.'] None The study of human posture is widely applied in physical education teaching, human motion recognition, and other aspects. With the rise of online teaching, the lack of convenient physical education teaching methods has been able to improve. However, due to the complex structure of human body, the study of human posture is a hard problem of consciousness problem in the area of computer vision. This article mainly studies human posture research algorithms based on deep learning. It uses 101-layer network of ResNet to detect the key points of human body in the image and obtains the categories and coordinates of these key points. In this article, a 101-layer network of ResNet model is constructed to fully learn the visual features of key points in human posture. Secondly, the key point location loss function is improved, and the human posture research is realized by using huber loss function instead of mean square error (MSE) loss function. Finally, experimental analysis shows that compared to traditional integral pose regression (IPR) and location adaptive integral pose regression (LAIPR), the use of ResNet based human posture estimation method for human posture recognition improves precision. It has practical significance for physical education teaching applications.\n",
      "Deep Learning-Based Plant Health State Classification Using Image  Data 2024 ['Deep Learning', 'Convolutional Neural Networks', 'Channel-wise Attention', 'Depthwise Separable Convolution', 'Attention-Enhanced ResNet', 'Plant Health State Classification', '딥러닝', '합성곱 신경망', '채널 어텐션', '깊이 분리 합성곱', 'Attention-Enhanced ResNet', '작물 건강 상태 분류'] None Tomatoes are rich in nutrients like lycopene, β-carotene, and vitamin C. However, they often suffer from biological and environmental stressors, resulting in significant yield losses. Traditional manual plant health assessments are error-prone and inefficient for large-scale production. To address this need, we collected a comprehensive dataset covering the entire life span of tomato plants, annotated across 5 health states from 1 to 5. Our study introduces an Attention-Enhanced DS-ResNet architecture with Channel-wise attention and Grouped convolution, refined with new training techniques. Our model achieved an overall accuracy of 80.2% using 5-fold cross-validation, showcasing its robustness in precisely classifying the health states of tomato plants\n",
      "상품 카테고리 자동분류를 위한 BERT-분류기 아키텍처 연구 2024 ['문장 분류', '문장 유사도', 'Sentence BERT', 'CNN', 'ResNet', 'Transformer', 'Classification', 'Sentence classification', 'Sentence similarity', 'Sentence BERT', 'CNN', 'ResNet', 'Transformer', 'Classification'] 본 연구는 생활 속 존재하는 다양한 상품들의 명칭을 BERT를 통해 임베딩 벡터화한 다음 이를 기반으로 상품 카테고리 예측을 수행하는 아키텍처에 대한 연구이다. 아키텍처의 성능은 상품 명칭으로부터 임베딩 추출을 수행하는 BERT 모델과, 추출된 임베딩으로 카테고리 예측을 수행하는 분류기에 의해 결정된다. 따라서 본 연구는 우선 상품 명칭 분류에 적합한 BERT 모델을 선정하고, 선정된 BERT 모델에 다양한 분류기를 적용하여 가장 높은 성능을 달성하는 BERT-분류기 조합을 찾고자 하였다. 최초 적합한 BERT 모델 선정에는 단순한 CNN 분류기를 사용하였으며 이를 baseline으로 다른 분류기와 성능을 비교하였다. 아키텍처의 성능은 카테고리 정답에 대한 precision, recall, f1 score, accuracy로 정량화하여 평가하였다. 실험 결과 BERT 측면에서는, Sentence BERT 모델이 비교 대상인 일반 BERT 모델보다 적합함을 확인하였다. 그리고 분류기 측면에서는, Sentence BERT와 CNN으로 구성된 baseline 대비하여 Residual Block이 추가 적용된 분류기가 더 높은 성능을 보였다. 본 연구에 사용된 Sentence BERT 모델의 경우 한국어 데이터가 학습되지 않은 단순 모델로, 향후 추가적 연구를 통해 다양한 한국어 데이터를 학습시켜 Domain Adaptation을 수행할 경우 추가적 성능 향상이 기대된다. This research focuses on an architecture that vectorizes the names of various products found in daily life using BERT, followed by predicting product categories based on these embeddings. The architecture's performance is determined by the BERT model, which extracts embeddings from product names, and the classifier that predicts categories from these embeddings. Consequently, this research initially aimed to identify a BERT model suitable for classifying product names and then find the most efficient combination of BERT model and classifier by applying various classifiers to the chosen BERT model. A simple CNN classifier was employed for the initial selection of a suitable BERT model, serving as a baseline for performance comparison with other classifiers. The architecture's effectiveness was quantified using precision, recall, f1 score, and accuracy for category predictions. Experimental results showed that the Sentence BERT model was more suitable for this task than a conventional BERT model. Additionally, classifiers enhanced with Residual Blocks demonstrated superior performance compared to the baseline combination of Sentence BERT and CNN. The Sentence BERT model used in this study, not trained on Korean data, suggests that further improvements could be achieved through Domain Adaptation by training with diverse Korean datasets.\n",
      "모션블러 이미지에 대한 CNN 모델의 균열 검출 성능 2024 ['CNN (convolutional neural network)', 'Motion blur', 'Dataset', 'Crack detection', 'F1 score', 'CNN (convolutional neural network)', '모션블러', '데이터셋', '균열 검출', 'F1 score'] None In this study, we analyzed the effect of motion blur on images used for detecting cracksin concrete tunnel linings on the performance of CNN models. Motion-blurred imageswith intensities ranging from 10 to 50 were generated on the Kaggle and KICT datasets.A semantic segmentation model with ResNet 18, ResNet 34, VGG 11, and Alex-Net as backbones for feature extraction was employed, all pre-trained on the U-Netarchitecture. The performance of these models in crack detection was then assessed.It was observed that detection accuracy decreased across all models as the intensity ofmotion blur increased for each dataset. Within the same model, the F1-score on theKICT dataset showed over 20% higher performance than on the Kaggle dataset. Thisstudy demonstrates that CNN-based crack detection performance is affected by thequality of the image data and that the crack detection accuracy of CNN models canvary depending on the quality of the dataset used in training.\n",
      "SERN 기반 운전자의 다중 행동 특징을 이용한 졸음 검출 시스템 2024 ['졸음 인식', '운전자 행동 특징', 'SERN', '다중 특징', '딥러닝', 'drowsiness detection', 'driver behavior features', 'SERN', 'Multiple features', 'deap learning'] 최근 교통사고의 주요한 원인 중 하나인 운전자 졸음으로 인한 교통사고를 예방하기 위해 졸음 인식 연구가 활발히 진행되는 중이다. 기존 졸음 인식 시스템은 운전자의 신체적 특징을 이용하여 졸음 상태를 인식하지만 신체 부위 폐 색에 의한 가려짐으로 제한되는 한계가 있다. 본 논문에서는 운전자의 다중 행동 특징을 이용한 SERN(Squeeze and Excitation Resnet Network) 기반 졸음 인식 시스템을 제안한다. 제안한 시스템은 다중 행동적 특징 기반 특징 추출 과정, 데이터의 계층적 레이블링 세분화 과정, SERN 모델에 의한 졸음 인식 과정으로 구성된다. 공개 DB인 NTHU-DDD를 사용한 실험 결과, 제안하는 SERN 모델 기반 운전자 졸음 검출 성능이 기존 네트워크 모델 보다 정확도 1.03% 우수함을 확인했다. Recently, driver drowsiness has been one of the major causes of traffic accidents, and study on drowsiness detection has been actively conducted to prevent drowsiness-related accidents. Existing drowsiness detection systems recognize the drowsy state of the driver using the driver's physical features, but they have limitations due to occlusion caused by obstructed body parts. In this paper, we propose a drowsiness detection system based on the squeeze and excitation resnet network (SERN) using the driver's multi-behavioral features. The proposed system consists of a multibehavioral feature extraction process, a hierarchical data labeling refinement process, and a drowsiness detection process using the SERN model. As a result of an experiment using public DB’s NTHU-DDD, it was confirmed that the proposed SERN model based driver drowsiness detection performance was 1.03% better than the existing network model.\n",
      "합성곱 신경망 기반 화재 인식 모델 최적화 연구: Layer Importance Evaluation 기반 접근법 2024 ['레이어 중요도 평가', '전이 학습 모델', '합성곱 신경망 최적화', '실시간 화재 감지', '기여도', 'Layer Importance Evaluation', 'Transfer Learning Model', 'CNN Optimization', 'Real-Time Fire Detection', 'Contribution'] 본 연구는 Layer Importance Evaluation을 통해 도출된 화재 감지에 최적화된 딥러닝 아키텍처를 제안한다. 기존의 합성곱 신경망(Convolutional Neural Network, CNN) 기반 화재 감지 시스템의 불필요한 복잡성과 연산을 초래하는 문제점을 해결하기 위해, Layer Importance Evaluation 기법을 통해 가중치 및 활성화 값에 근거한 모델의 내부 레이어의 동작을 분석하고, 화재 감지에 기여도가 높은 레이어를 식별한 뒤, 식별한 레이어만으로 모델을 재구성하여, 기존 모델과의 성능 지표를 비교 분석하였다. Xception, VGG19, ResNet, EfficientNetB5 등 네 가지 전이 학습 모델을 사용하여 화재 데이터를 학습시킨 후, Layer Importance Evaluation기법을 적용하여 각 레이어의 가중치와 활성화 값을 분석한 뒤 기여도가 가장 높은 상위 랭크 레이어들을 선별하여 새로운 모델을 구축하였다. 연구 결과, 구현된 아키텍처는 기존 모델 대비 약 80% 가량 경량화 된 파라미터로도 동등한 성능을 유지하며, 약 3~5배가량 신속한 학습 속도를 가지면서도 기존의 복잡한 전이학습 모델에 비해 정확도, 손실, 혼동행렬 지표에서 동등한 성능을 출력함으로써, 화재 감시 장비의 효율성을 높이는 데 기여할 수 있음을 확인하였다. This study proposes a deep learning architecture optimized for fire detection derived through Layer Importance Evaluation. In order to solve the problem of unnecessary complexity and operation of the existing Convolutional Neural Network (CNN)-based fire detection system, the operation of the inner layer of the model based on the weight and activation values was analyzed through the Layer Importance Evaluation technique, the layer with a high contribution to fire detection was identified, and the model was reconstructed only with the identified layer, and the performance indicators were compared and analyzed with the existing model. After learning the fire data using four transfer learning models: Xception, VGG19, ResNet, and EfficientNetB5, the Layer Importance Evaluation technique was applied to analyze the weight and activation value of each layer, and then a new model was constructed by selecting the top rank layers with the highest contribution. As a result of the study, it was confirmed that the implemented architecture maintains the same performance with parameters that are about 80% lighter than the existing model, and can contribute to increasing the efficiency of fire monitoring equipment by outputting the same performance in accuracy, loss, and confusion matrix indicators compared to conventional complex transfer learning models while having a learning speed of about 3 to 5 times faster.\n",
      "전이학습을 이용한 신발 이미지 스타일 분류모델 연구 2024 ['전이학습', '신발 스타일 분류', '신발 아웃솔', '신발 갑피', 'Transfer Learning', 'Shoes Style Classification', 'Shoes Outsole', 'Shoes Upper', 'ConvNeXt'] 전자상거래의 성장과 4차산업혁명 기술 발전에 따라 패션 산업에서 인공지능을 접목한 서비스가 활발히 도입되고 있으나 신발 산업은 아직 관련 연구가 깊게 되어 있지 않아 활용 사례 및 데이터셋이 부족하다. 본 논문에서는 웹크롤링으로 운동화, 스니커즈 이미지를 수집하고 디자인 및 제조 관점을 반영하여 신발 스타일과 아웃솔, 갑피에 대해서 라벨링하였다. 구축한 약 2만건의 데이터셋을 대상으로 ResNet, ConvNeXt, ViT, Swin Transfomer를 전이학습하고 각 모델의 결과를 비교하였다. 그 결과 ConvNeXt 모델에서 가장 우수한 결과를 얻었고 과적합을 방지하기 위해 추가로 파인튜닝하여 테스트 데이터셋 대상으로 정확도 87%의 결과를 얻었다. 본 연구를 바탕으로 신발 산업에서 디자인 및 제조 기술에 딥러닝 모델을 활용하여 생산성을 향상시킬 수 있을 것이라 기대한다. With the growth of e-commerce and the development of the 4th Industrial Revolution technologies, services using AI are being actively introduced in the fashion industry. However, the shoes industry has not yet been studied in-depth on AI and there is a lack of datasets and use cases. In this paper, we collected images of sneakers and running shoes by web crawling and labeled shoe styles, outsoles, and uppers from design and manufacturing perspectives. We trained ResNet, ConvNeXt, ViT and Swin Transformer on our dataset and compared the results of each model. As a result, the ConvNeXt model obtained the best results and was further fine-tuned to prevent overfitting, resulting in 87% accuracy on the test dataset. Based on this study, We expect that deep learning will be used in design and manufacturing process to improve productivity in the shoe industry.\n",
      "합성곱 신경망 기반 화재 인식 모델 최적화 연구: Layer Importance Evaluation 기반 접근법 2024 None 본 연구는 Layer Importance Evaluation을 통해 도출된 화재 감지에 최적화된 딥러닝 아키텍처를 제안한다. 기존의 합성곱 신경망(Convolutional Neural Network, CNN) 기반 화재 감지 시스템의 불필요한 복잡성과 연산을 초래하는 문제점을 해결하기 위해, Layer Importance Evaluation 기법을 통해 가중치 및 활성화 값에 근거한 모델의 내부 레이어의 동작을 분석하고, 화재 감지에 기여도가 높은 레이어를 식별한 뒤, 식별한 레이어만으로 모델을 재구성하여, 기존 모델과의 성능 지표를 비교 분석하였다. Xception, VGG19, ResNet, EfficientNetB5 등 네 가지 전이 학습 모델을 사용하여 화재 데이터를 학습시킨 후, Layer Importance Evaluation기법을 적용하여 각 레이어의 가중치와 활성화 값을 분석한 뒤 기여도가 가장 높은 상위 랭크 레이어들을 선별하여 새로운 모델을 구축하였다. 연구 결과, 구현된 아키텍처는 기존 모델 대비 약 80% 가량 경량화 된 파라미터로도 동등한 성능을 유지하며, 약 3~5배가량 신속한 학습 속도를 가지면서도 기존의 복잡한 전이학습 모델에 비해 정확도, 손실, 혼동행렬 지표에서 동등한 성능을 출력함으로써, 화재 감시 장비의 효율성을 높이는 데 기여할 수 있음을 확인하였다. This study proposes a deep learning architecture optimized for fire detection derived through Layer Importance Evaluation. In order to solve the problem of unnecessary complexity and operation of the existing Convolutional Neural Network (CNN)-based fire detection system, the operation of the inner layer of the model based on the weight and activation values was analyzed through the Layer Importance Evaluation technique, the layer with a high contribution to fire detection was identified, and the model was reconstructed only with the identified layer, and the performance indicators were compared and analyzed with the existing model. After learning the fire data using four transfer learning models: Xception, VGG19, ResNet, and EfficientNetB5, the Layer Importance Evaluation technique was applied to analyze the weight and activation value of each layer, and then a new model was constructed by selecting the top rank layers with the highest contribution. As a result of the study, it was confirmed that the implemented architecture maintains the same performance with parameters that are about 80% lighter than the existing model, and can contribute to increasing the efficiency of fire monitoring equipment by outputting the same performance in accuracy, loss, and confusion matrix indicators compared to conventional complex transfer learning models while having a learning speed of about 3 to 5 times faster.\n",
      "UAV Imagery-based Automatic Classification of Ground Surface Types for Earthworks 2024 ['Automated construction equipment', 'Ground surface', 'Unmanned aerial vehicle', 'Multi-label classification', 'Computer vision'] None The construction industry is introducing autonomous heavy equipment to overcome labor shortages and improve productivity. For autonomous heavy equipment to work on earthmoving at sites, the equipment needs to recognize and understand ground surface types. However, the ground surface types are manually inspected in practice, and related studies are lacking. To address this issue, the authors developed and tested models that automatically classify ground surface types from images acquired by an unmanned aerial vehicle using a deep learning-based multi-label classification method that applies Binary Relevance (BR) and Label Powerset (LP) methods with Residual Neural Network (ResNet) and Vision Transformer classification network (VIT). The model performances were comparatively evaluated through experiments conducted on actual construction sites. The results showed that the BR model with ResNet is the best model in terms of automated ground surface type identification during earthmoving. The results are expected to broaden the understanding of complex and expansive construction sites for autonomous vehicles and thus facilitate deployment of autonomous heavy equipment by helping them to understand working areas and any obstacles on construction sites quickly and effectively, which will reduce the cost and time needed for on-site ground surface management.\n",
      "시퀀스 데이터 기반 단일 및 복합 변조 레이더 신호 변조 식별 2024 ['Radar signal modulation identification', 'Composite modulated radar signal', 'Deep learning model'] 상대방에 대한 사전 정보 없이 수집된 레이더 신호의 변조를 식별하는 기술은 전자기전에서 매우 중요한 역할을 수행하며, 이를 통해 획득한 변조 방식에 관한 정보는 전자기전에서 전략 수립과 우위 확보를 위해 활용될 수 있다. 본 논문에서는 딥러닝 모델을 이용하여 37종의 단일 및 복합 변조 레이더 신호를 식별하는 시퀀스 데이터 기반의 레이더 신호 변조 식별 기법을 제안하고 시퀀스 데이터와 딥러닝 모델에 따른 변조 식별 성능을 분석한다. 제안하는 기법은 수신 레이더 신호를 시간 또는 주파수 영역에서 분석하여 시퀀스 데이터를 생성한 후, 이를 딥러닝 모델에 입력하여 변조 방식을 식별한다. 이때, 모델에 입력되는 시퀀스 데이터로 레이더 신호의 실수부 및 허수부로 구성한 시간 영역 데이터와 이산 푸리에 변환의 실수부 및 허수부로 구성한 주파수 영역 데이터를 고려하며, 딥러닝 모델로는 ResNet, DenseNet, Inception-v3를 고려한다. 컴퓨터 모의실험을 통해 주파수 영역 데이터를 모델 입력으로 사용하는 것이 시간 영역 데이터를 사용하는 것보다 변조 식별 성능이 우수함을 보이며, 본 논문에서 고려한 딥러닝 모델이 기존에 제안된 시퀀스 데이터 기반 레이더 변조 식별 딥러닝 모델보다 우수한 변조 식별 성능을 가짐을 확인한다. 또한, 딥러닝 모델별 변조 식별 성능을 비교하여 DenseNet201을 사용하는 경우에 가장 높은 변조 식별 성능을 보임을 입증한다. Identifying the modulation scheme of collected radar signals without prior information plays an important role in electromagnetic warfare, and the information on the modulation scheme obtained through modulation identification of radar signal can be used to establish strategies and secure superiority in electromagnetic warfare. In this paper, using a deep learning model, we propose a sequence data-based modulation identification method for 37 types of single and composite modulated radar signals. In addition, we analyze the modulation identification performance according to the sequence data and the deep learning model. The proposed method generates sequence data by analyzing received radar signals in the time or frequency domain, and then using this data as input to the deep learning model, it identifies the modulation scheme. As sequence data, we consider the time domain data consisting of real and imaginary parts of the radar signal and the frequency domain data consisting of real and imaginary parts of the discrete Fourier transform. For deep learning models, we consider ResNet, DenseNet, and Inception-v3. Through computer simulations, we show that using the frequency domain data as the input of the model, the modulation identification performance is better compared to using the time domain data. It is also confirmed that the deep learning models considered in this paper show better performance than the existing deep learning model of the sequence data-based modulation identification method. Furthermore, by comparing the performance of each deep learning model, we demonstrate that the DenseNet201 exhibits the best modulation identification performance.\n",
      "다중 시멘틱 세그멘테이션 AI 기반의 가전용 크림프 하네스 검사 모델 개발 2024 ['Wire harness', 'Crimp harness inspection', 'Multi class semantic segmentation', 'U-Net'] 와이어 하네스는 가전제품, 전기자동차, 자율주행 자동차에서 혈류의 역할을 하는 임의의 전기적회로의 상호 연결을 제공하는 와이어의 그룹으로 정의되며 품질이 가장 중요한 척도이다. 와이어 하네스 품질 불량은 제품 고장, 화재, 인명사고와 직결되기 때문이다. 본 논문은 와이어 하네스 압착공정 결과물인 크림프 하네스의 불량을 판정하기 위해, 다중 시멘틱 세그멘테이션 기법을 활용하는 방법을 제안하였다. 크림프 하네스의 불량 판정 문제는 전처리된 하네스 데이터로부터 연속된 5개 세그먼트들의 높이 및 폭의 길이를 정확히 측정하면 가능한 것으로 판단되었다. 이에 착안하여 대표적인 시멘틱 세그멘테이션 모델인 U-Net을 기반으로 다중 세그먼트 식별에 적합한 AI 모델의 개발을 목표로 하였다. 다중 세그먼트 식별을 위해 U-Net의 인코더 부분을 ResNet 34, EfficientNet B1 및 Mix Transformer B0로 변형한 모델을 제안하였다. 인공지능 모델 개발을 위해, 데이터셋 구축에는 와이어 하네스 제조공장에서 수집된 크림프 하네스 이미지들을 사용하였다. 개발된 다중 시멘틱 세그멘테이션 AI 모델은 테스트 데이터셋에 대해 95.14%의 판별 정확도를 나타내었다. 제안된 방법은 종래의 방법(수작업, 압착센서 측정값 및 규칙 기반의 영상처리)의 단점을 개선한, 균일한 고품질 유지와 인건비 절감이 가능하다. Wire harness is defined as groups of wires providing interconnections for arbitrary electrical circuits that serve as the bloodstream in consumer electronics, electric vehicles, and autonomous cars. Poor wire harness quality is directly related to produce product failures, fires, and human casualties. This paper proposes a method that utilizes multi-class semantic segmentation techniques to determine the defects of the crimp harness, which is the result of a product of the wire harness crimp process. The problem of defect detection of crimp harness can be solved by accurately measuring the height and width of five consecutive segments from the preprocessed harness data. With this insight, we aimed to develop an AI model suitable for multi-segment identification based on U-Net, a representative semantic segmentation model. To identify multiple segments, we proposed a model that modifies the encoder part of U-Net using Resnet 34, EfficientNet B1, and Mix Transformer B0. For AI model development, images of crimp harnesses collected from a wire harness manufacturing plant were used to build the dataset. The developed multi-class semantic segmentation AI model showed a discernment accuracy of 95.14% on the test dataset. Through the method proposed in this paper, it is possible to maintain uniform high quality and reduce labor costs, which improves the shortcomings of the existing crimp harness quality inspection(manual, crimp sensor measurements, and rule-based image processing).\n",
      "Drone Detection Using Dynamic-DBSCAN and Deep Learning in an Indoor Environment 2024 ['DBSCAN', 'Deep Learning', 'FMCW Radar', 'Object Detection', 'UAV'] None Drones have found extensive utility in both public and personal places. Consequently, the accurate detection and tracking of drones have emerged as pivotal endeavors in terms of ensuring their optimal performance. This research paper introduces a novel application for discerning the movements of humans and drones from cloud points through the utilization of frequency-modulated continuous wave radar. The dynamic density-based spatial clustering of applications with noise (Dynamic-DBSCAN) algorithm was employed to classify cloud points into separate groups corresponding to the number of objects within the tracking area. Compared to the original DBSCAN algorithm, this method increased accuracy by about 16.8%, achieving an accuracy of up to 93.99%. Subsequently, a trio of deep learning algorithms-long short-term memory, deep neural network, and residual network (ResNet)—were harnessed to ascertain the categorization of each group as either human or drone. According to the results, ResNet achieved the best accuracy rate of 97.72%. Overall, this study underscores the efficacy of the proposed method in accurately and efficiently distinguishing between human and drone entities for effective monitoring and management.\n",
      "잔차 신경망을 활용한 펫 로봇용 화자인식 경량화 2024 ['Speaker recognition', 'CNN', 'Resnet', 'Lightweight', 'Classification'] None None\n",
      "잔차 신경망을 활용한 펫 로봇용 화자인식 경량화 2024 ['Speaker recognition', 'CNN', 'Resnet', 'Lightweight', 'Classification'] 화자인식은 개개인마다 다른 음성 주파수를 분석하여 미리 저장된 음성과 비교해 본인 여부를 판단하는 하나의 기술을 의미한다.딥러닝 기반의 화자인식은 여러 분야에 적용되고 있으며, 펫 로봇도 그 중 하나이다. 하지만 펫 로봇의 하드웨어 성능은 딥러닝 기술의 많은 메모리 공간과 연산에 있어 매우 제한적인 상황이다. 이는 펫 로봇이 사용자와 실시간 상호작용에 있어 해결해야 할 중요한 문제점이다. 딥러닝 모델의 경량화는 위와 같은 문제를 해결하기 위한 하나의 중요한 방법으로 자리하였으며, 최근 많은 연구가진행되고 있다. 이 논문에서는 특정한 명령어 형태인 펫 로봇용 음성 데이터 세트를 구축하고 잔차(Residual)를 활용한 모델들의 결과를 비교해 펫 로봇용 화자인식의 경량화 연구의 결과를 서술하며, 결론에서는 제안한 방법에 대한 결과와 향후 연구방안에 대해서술한다. Speaker recognition refers to a technology that analyzes voice frequencies that are different for each individualand compares them with pre-stored voices to determine the identity of the person. Deep learning-based speakerrecognition is being applied to many fields, and pet robots are one of them. However, the hardware performance ofpet robots is very limited in terms of the large memory space and calculations of deep learning technology. This isan important problem that pet robots must solve in real-time interaction with users. Lightening deep learning modelshas become an important way to solve the above problems, and a lot of research is being done recently. In thispaper, we describe the results of research on lightweight speaker recognition for pet robots by constructing a voicedata set for pet robots, which is a specific command type, and comparing the results of models using residuals. Inthe conclusion, we present the results of the proposed method and Future research plans are described.\n",
      "Few-Shot 기반 작물 기상피해 판별 시스템 2024 ['Smart Farm', 'Few-Shot', 'Classification', 'ResNet', 'DenseNet'] None This study introduces a model utilizing Few-shot Learning to effectively classify and discern weather-induced damages in crops, focusing on cold and heat damages affecting peaches, apples, and pears. It addresses the challenge of limited data availability in agriculture by leveraging Few-shot Learning, offering a promising solution for data scarcity issues. The model demonstrates robust classification capabilities under constrained data conditions, highlighting the potential of AI and machine learning technologies to tackle significant challenges in modern agriculture related to weather damage. The research suggests avenues for future work, including model performance enhancement, integration with real-time monitoring systems, and broader application across various crops and weather conditions, aiming to contribute to sustainable agriculture and food security.\n",
      "패션 이미지 데이터를 활용한 딥러닝 기반의 의류속성 분류 2024 ['패션 이미지', '의류 속성 분류', '딥러닝', 'Fashion Image', 'Clothing Attribute Classification', 'Deep Learning', 'ResNet', 'EfficientNet'] None None\n",
      "Design of Automatic Defect Classification System  for Wafer Edge Defect Inspection 2024 ['CNN', 'Deep Learning', 'Image Processing', 'Auto Defect Classification', 'Wafer Edge Defect'] None This paper proposes an automatic defect classification (ADC) system to detect and classify bare wafer edge defects that occur during the extreme wafer thinning process required for advanced chip stacking technologies such as TSV and HBM. The proposed system combines a convolutional neural network (ResNet) with traditional image processing techniques (OpenCV-based frequency domain filtering) to effectively classify and visualize wafer edge defects. Experimental results demonstrate that the system achieves high accuracy in defect classification and detection, providing an efficient solution to prevent wafer damage and yield reduction.\n",
      "Attention BiFPN 기반 화재 검출 딥러닝 알고리즘에 대한 연구 2024 ['fire detection', 'neural network', 'attention mechanism', 'bi-directional pyramid', 'feature pyramid network', '.'] 최근 빈번하게 발생하는 화재를 인공지능 및 머신러닝 기법을 활용하여 조기에 감지하고 효과적인 대응에 초점을 맞춘 연구들이 활발히 진행되고 있다. 화재 영상에서 복잡한 배경 및 환경 요인의 불확실성으로 인해 불꽃 또는 연기 영역을 더욱 정확하게 검출할 수 있는 기법이 요구된다. 본 논문에서는 Attention 기반 BiFPN 구조의 ResNet 알고리즘을 제안한다. 화재영역을 정확하게 검출하기 위하여 어텐션 메커니즘과 척도 불변 BiFPN 구조를 유기적으로 결합하여 객체와 배경정보를 분리함과 동시에 서로간의 연관성 유지를 통해 불필요한 정보를 제거하고, 필요한 정보만을 강조함으로써 불꽃 및 연기 영역검출 결과가 객체 크기와 상관없이 일관된 정확도를 유지하는 방법을 제안한다. 실험을 통해 불꽃은 98%의 정확도와 98.67%의 정밀도, 연기는 96.67%의 정확도와 96.67%의 정밀도의 화재 검출에 대한 평가 결과를 확인하였다. Recent research has been actively focused on utilizing artificial intelligence and machine learning techniques to detect fires early and respond effectively, given the frequent occurrences of fires. There is a demand for techniques that can accurately detect flame or smoke areas in fire images due to the uncertainty of complex backgrounds and environmental factors. In this paper, to accurately detect fire areas, we propose a method that seamlessly combines attent ion mechanisms and scale-invariant Bidirectional Feature Pyramid Network(BiFPN) structures to separate object and background information while maintaining their correlations. It removes unnecessary information and emphasizes only the necessary information to maintain consistent accuracy in flame and smoke areas regardless of their sizes. Evaluation results showed that fires could be detected with 98% accuracy and 98.67% precision for flames and 96.67% accuracy and 96.67% precision for smoke.\n",
      "자율주행 자동차 인지 성능 향상을 위한 복수의 인공신경망 결과 값 합성법 2024 ['딥러닝', '전이 학습', '인공신경망', '이미지 인식', '모델 합성', 'Deep learning', 'Transfer learning', 'Neural network', 'Image classification', 'Model fusion'] 전이 학습은 이미 학습된 딥러닝 모델을 초기 모델로 활용하여, 다른 데이터에서 높은 성능 을 발휘하는 기술이다. 특히, 학습에 사용할 데이터의 질과 양이 충분하지 않을 때 전이 학습은 매우 유용한 것으로 알려져 있기에 높은 성능 안정성과 많은 데이터를 필요로 하는 자율주행 차 인식 분야에 응용될 수 있다. 그러나 많은 선행 연구들은 전이 학습 알고리즘 자체의 성능 향상에 초점을 맞추었다. 본 연구는 기존의 알고리즘 중심 접근 방식에서 벗어나, 여러 데이터 셋의 특징 추출기 출력을 융합한 심층 모델 융합 기법을 전이 학습에 적용하고자 한다. 실험 결과, 이러한 심층 모델 융합 기법이 전이 학습의 성능을 향상시킬 수 있음을 확인했다. 이 결 과는 앞으로 데이터가 부족한 자율주행 자동차 분야에서 전이 학습에 활용되어 물체 인지 성 능의 향상을 달성할 수 있을 것으로 기대된다. Transfer learning is a technique that leverages a deep learning model trained on a specific dataset as an initial model that allows fast training of a high-performing model on another dataset. Because a pre-trained model already learns how to extract the features from previously trained data, it allows for faster and better performance on new datasets compared to models that are initialized randomly. Transfer learning is particularly useful when the quality and quantity of the data to be learned are insufficient. On the other hand, most studies focused on improving the performance of transfer learning algorithms themselves. This study departs from the existing algorithm-centric research approach and aims to incorporate deep model fusion techniques that combine the outputs of feature extractors from different datasets into transfer learning. These experiments show that the application of deep model fusion improves the performance of transfer learning. These findings will be applicable to transfer learning in various domains with limited data.\n",
      "Automatic cancer nuclei segmentation on histological images: comparison study of deep learning methods 2024 ['Cancer · Medical segmentation · Histology · Convolutional neural networks · Augmentation'] None Cancer is one of the most common health problems aff ecting individuals worldwide. In the fi eld of biomedical engineering, one of the main methods for cancer diagnosis is the analysis of histological images of tissue structures and cell nuclei using artifi cial intelligence. Here, we compared the performance of 15 deep learning methods viz: UNet, Deep-UNet, UNet-CBAM, RA-UNet, SA-Unet and Nuclei-SegNet, UNet-VGG2016, UNet-Resnet-101, TransResUNet, Inception-UNet, Att-UNet++ , FF-UNet, Att-UNet, Res-UNet and a new model, DanNucNet, in pathological nuclei segmentation on tissue slices from different organs on fi ve open datasets: MoNuSeg, CoNSeP, CryoNuSeg, Data Science Bowl, and NuInsSeg. Before training on the data, the pixel intensity and color distribution were analyzed, and diff erent augmentation techniques were applied. The results showed that the UNet-based model with 34.57 million Deep-UNet parameters performed the best, outperforming all models in terms of the Dice coeffi cient from 3.13 to 22.91%. The implementation of Deep-UNet in this context provides a valuable tool for accurate extraction of cancer cell nuclei from histological images, which in turn will contribute to further developments in cancer pathology and digital histology.\n",
      "Anchor-Free 기반 3D 객체 검출을 이용한 과수 꼭지 검출 시스템 구현 2024 ['딥러닝', '디지털 농업', '3D 객체 검출', '과수 꼭지 검출', 'Deep learning', 'Digital agriculture', '3D Object detection', 'Fruit stem detection'] None This paper proposes a fruit stem detection system using Anchor-Free-based 3D object detection model for fruit stem removal. The fruit stem is an important part that affects the hygiene, food safety, quality, and freshness of fruits. It can save manpower and time by automating existing manual-dependent tap removal operations, and increase efficiency in related agricultural fields. The FCAF3D model is a 3D object detection algorithm that predicts the skin and stem of the fruit, respectively, showing high detection performance even in the small size of the fruit stem. The network structure of the model consists of ResNet, GSDN, and FCOS networks, which handle various object scales through the FPN structure. In this paper, model training was performed using apples as an example, and the learned model showed high accuracy in the apple dataset, and the bounding box coordinate value of the test results can be used in the fruit stem removal system.Experimental results showed that the FCAF3D model showed high performance in detecting fruit stem.\n",
      "EfficientNet과 LSTM을 활용한 병해  진행도 예측 시스템 2024 ['Image Classification', 'Time Series Data Analysis', 'Time Series Forecasting', 'Disease', 'XAI', 'EfficientNet', 'LSTM'] None Modern agriculture leverages various smart farming technologies to manage crops efficiently, but ensuring optimal growth for each crop remains challenging. This paper proposes a novel approach that integrates an image classification model with a time series analysis model to address this issue. The proposed system uses EfficientNet to classify crop diseases and LSTM (Long Short-Term Memory) to analyze time series data to predict the progression of the disease based on its presence. The model performance was evaluated using Accuracy, Precision, Recall, and F1 Score metrics. The image classification model achieved 91% Precision, 92% Recall, 97% Accuracy, and 94% F1 Score, outperforming ResNet, DenseNet, and SENet. Additionally, the model's reliability was verified using Grad-CAM (Gradient-weighted Class Activation Map), an XAI technique. The time series analysis model demonstrated a Recall of 88% and Precision and F1 Score of 86%. The proposed system is accessible via a web page, allowing easy access for users, and includes features for writing posts and comments to facilitate a user community. This system aims to advance smart farming technology, increase crop yields, and minimize agricultural losses, ultimately contributing to improved crop management efficiency.\n",
      "A Dynamic Head Gesture Recognition Method for Real-time Intention Inference and Its Application to Visual Human-robot Interaction 2024 ['Computer vision', 'deep learning', 'head gesture', 'human-robot interaction.'] None Head gesture is a natural and non-verbal communication method for human-computer and human-robot interaction, conveying attitudes and intentions. However, the existing vision-based recognition methods cannot meet the precision and robustness of interaction requirements. Due to the limited computational resources, applying most high-accuracy methods to mobile and onboard devices is challenging. Moreover, the wearable device-based approach is inconvenient and expensive. To deal with these problems, an end-to-end two-stream fusion network named TSIR3D is proposed to identify head gestures from videos for analyzing human attitudes and intentions. Inspired by Inception and ResNet architecture, the width and depth of the network are increased to capture motion features sufficiently. Meanwhile, convolutional kernels are expanded from the spatial domain to the spatiotemporal domain for temporal feature extraction. The fusion position of the two-stream channel is explored under an accuracy/complexity trade-off to a certain extent. Furthermore, a dynamic head gesture dataset named DHG and a behavior tree are designed for human-robot interaction. Experimental results show that the proposed method has advantages in real-time performance on the remote server or the onboard computer. Furthermore, its accuracy on the DHG can surpass most state-of-the-art vision-based methods and is even better than most previous approaches based on head-mounted sensors. Finally, TSIR3D is applied on Pepper Robot equipped with Jetson TX2.\n",
      "IRAE-UNet: InceptionResNetV2 -Attention Encoder based UNet Semantic Segmentation of Aerial Imagery 2024 ['remote sensing', 'attention', 'image segmentation', 'inception', '원격 감지', '어텐션', '영상 분할', 'Inception'] None Remote sensing applications play a vital role in various areas such as urban planning, agriculture, and environmental monitoring. Remote sensing image segmentation, in particular, is a prominent domain that aims to address the challenges in these applications. Deep learning has significantly improved the efficiency and accuracy of remote sensing image segmentation by automating the identification of regions of interest. However, most existing methods struggle with capturing both global and local information in the images, which is crucial for accurate pixel classification. To overcome this limitation, this paper presents an enhanced version of the U-Net architecture that incorporates the InceptionResNetV2-Attention based encoder. This proposed method effectively combines the strengths of the Inception and ResNet architectures, along with the attention mechanism. The efficacy of the proposed network is verified using two publicly available datasets. The Semantic Drone Dataset consists of satellite images, while the NITRDrone dataset comprises images captured from Unmanned Aerial Vehicles (UAVs). The results demonstrate that the proposed architecture performs well on imagery obtained from different platforms, achieving  a dice-coefficient of 85.04% and 88.70% for each dataset respectively, outperforming other networks.\n",
      "Benchmark study on a novel online dataset for standard evaluation of deep learning-based pavement cracks classification models 2024 ['Convolutional neural networks', 'Pavement cracks', 'Deep learning', 'Benchmark study', 'Crack classification'] None Highway agencies and practitioners expect to have the most efficient method with adequate accuracy when choosing a deep learning-based model for pavement crack classification. However, many works are implemented on their own dataset, making them hard to compare with each other, and also less persuasive and robust. Therefore, a Road Cracks Classification Dataset is proposed to serve as a standard and open-source dataset. Based on this dataset, a benchmark study of fourteen deep learning classification methods is evaluated. Two parameters, the Ratio of F1 and Training Time (RFT) and Ratio of F1 and Prediction Time (RFP), are proposed to quantify the efficiency of networks. The results show that ConvNeXt_base reaches the highest accuracy among all models but requires the longest training time. AlexNet takes the least training time among all models, but gains the lowest accuracy. Of the four crack types, the block crack has the lowest accuracy, which means it is the most difficult to detect. SqueezeNet1_0 has the highest efficiency among all models in converting the computing power to accuracy. Wide ResNet 50_2 consumes the longest prediction time among CNN models, while the ConvNeXt_base has the highest feasibility on real-time tasks. To implement a suitable deep learning-based pavement crack inspection, we recommend a good balance between computational cost and accuracy. Based on this, we provide practical recommendations according to different user groups.\n",
      "A deep learning model for estimating sedation levels using heart rate variability and vital signs: a retrospective cross-sectional study at a center in South Korea 2024 ['conscious sedation', 'deep learning', 'heart rate', 'machine learning', 'patient monitoring', 'pediatric intensive care unit', 'vital signs'] None Background:Optimal sedation assessment in critically ill children remains challenging due to the subjective nature of behavioral scales and intermittent evaluation schedules. This study aimed to develop a deep learning model based on heart rate variability (HRV) parameters and vital signs to predict effective and safe sedation levels in pediatric patients.Methods: This retrospective cross-sectional study was conducted in a pediatric intensive care unit at a tertiary children’s hospital. We developed deep learning models incorporating HRV parameters extracted from electrocardiogram waveforms and vital signs to predict Richmond Agitation-Sedation Scale (RASS) scores. Model performance was evaluated using the area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). The data were split into training, validation, and test sets (6:2:2), and the models were developed using a 1D ResNet architecture.Results: Analysis of 4,193 feature sets from 324 patients achieved excellent discrimination ability, with AUROC values of 0.867, 0.868, 0.858, 0.851, and 0.811 for whole number RASS thresholds of −5 to −1, respectively. AUPRC values ranged from 0.928 to 0.623, showing superior performance in deeper sedation levels. The HRV metric SDANN2 showed the highest feature importance, followed by systolic blood pressure and heart rate.Conclusions: A combination of HRV parameters and vital signs can effectively predict sedation levels in pediatric patients, offering the potential for automated and continuous sedation monitoring in pediatric intensive care settings. Future multi-center validation studies are needed to establish broader applicability.\n",
      "An Implementation of Effective CNN Model for AD Detection 2024 [\"Alzheimer's disease (AD)\", 'Magnetic Resonance Imaging (MRI)', 'Convolution Neural Network (CNN)'] None This paper focuses on detecting Alzheimer’s Disease (AD). The most usual form of dementia is Alzheimer's disease, which causes permanent cause memory cell damage. Alzheimer's disease, a neurodegenerative disease, increases slowly over time. For this matter, early detection of Alzheimer's disease is important.  The purpose of this work is using Magnetic Resonance Imaging (MRI) to diagnose AD.  A Convolution Neural Network (CNN) model, Reset, and VGG the pre-trained learning models are used.  Performing analysis and validation of layers affects the effectiveness of the model. T1-weighted MRI images are taken for preprocessing from ADNI. The Dataset images are taken from the Alzheimer's Disease Neuroimaging Initiative (ADNI). 3D MRI scans into 2D image slices shows the optimization method in the training process while achieving 96% and 94% accuracy in VGG 16 and ResNet 18 respectively. This study aims to classify AD from brain 3D MRI images and obtain better results.\n",
      "국방 데이터를 활용한 인셉션 네트워크 파생 이미지 분류 AI의 설명 가능성 연구 2024 ['Deep Learning(딥러닝)', 'Image Classification(이미지 분류)', 'AI(인공지능)', 'XAI(설명 가능한 인공지능)', 'Inception Network(인셉션 네트워크)', 'LIME Algorithm(라임 알고리즘)'] None In the last 10 years, AI has made rapid progress, and image classification, in particular, are showing excellentperformance based on deep learning. Nevertheless, due to the nature of deep learning represented by a black box,it is difficult to actually use it in critical decision-making situations such as national defense, autonomous driving,medical care, and finance due to the lack of explainability of judgement results. In order to overcome theselimitations, in this study, a model description algorithm capable of local interpretation was applied to the inceptionnetwork-derived AI to analyze what grounds they made when classifying national defense data. Specifically, weconduct a comparative analysis of explainability based on confidence values by performing LIME analysis from theInception v2_resnet model and verify the similarity between human interpretations and LIME explanations.Furthermore, by comparing the LIME explanation results through the Top1 output results for Inception v3,Inception v2_resnet, and Xception models, we confirm the feasibility of comparing the efficiency and availabilityof deep learning networks using XAI.\n",
      "2024년 데이터 크롤링 완료.\n",
      "=== 데이터프레임 ===\n",
      "                                                                                                                                                                                          title  \\\n",
      "0                                                                                                                                                      ResNet과 SIFT를 이용한 특허 도면의 유사도 평가 및  검색 연구   \n",
      "1                                                                                                                                                              ResNet50 알고리즘을 활용한 백혈구 이미지 분석 연구   \n",
      "2                                                                                                                      웹사이트 게시글 및 상품 리뷰 검색 기능 향상: ResNet-Transformer 모델을 이용한 BM25 랭킹 알고리즘 성능 개선   \n",
      "3                                                                                                                                            가상공간에서 ResNet을 활용한 공간감 증대를 통한 시각 및 음향 환경 향상에 관한 연구   \n",
      "4                                                                                                                                                     음성 감정인식의 정확도 향상을 위한 DA-S기법을 활용한 ResNet 모델   \n",
      "5                                                                                                                                               수면 소리 데이터를 활용한 1D ResNet 딥러닝 모델 기반 수면무호흡증 탐지 연구   \n",
      "6                                                                                                                                                               ONNX 기반 런타임 성능 분석: YOLO와 ResNet   \n",
      "7                                                                                                                                                    잔차블록을 적용한 향상된 ResNet VAE-GAN  3D 객체 생성 시스템   \n",
      "8                                                                                                                       고추 작물의 정밀 질병 진단을 위한 딥러닝 모델 통합 연구: YOLOv8, ResNet50, Faster R-CNN의 성능 분석   \n",
      "9                                                                                                                                         핵의학 갑상샘 팬텀 영상에서 보간법이 초고분해능 ResNet 모델성능에 미치는 영향에 관한 연구   \n",
      "10                                                                                                                                          Similarity Analysis Model with 6CH ResNet Structure   \n",
      "11                                                                                                                          Online to Offline 상점을 위한 한글 메뉴판 인식 : 어텐션 메커니즘을 적용한 VGG-ResNet 융합 모델   \n",
      "12                                                                                                                                             GAN기반의 Semi Supervised Learning을 활용한 이미지 생성 및 분류   \n",
      "13                                                                                                                                                                  주거 및 공공장소 이상행동탐지를 위한 서비스 설계   \n",
      "14                                                                                                                                                        CNN 기반의 전이학습과 데이터 증강을 통한  화재 영상 분류 개선   \n",
      "15                                                                                Implant Thread Shape Classification by Placement Site from Dental Panoramic Images Using Deep Neural Networks   \n",
      "16                                                                                                                                                                  콘크리트 구조물 균열 파악을 위한 분류 모델 분석   \n",
      "17                                                                                     Violent crowd flow detection from surveillance cameras using deep transfer learning-gated recurrent unit   \n",
      "18                                                                                                                                                   자생 매미 음성 분류를 위한 딥러닝 접근 : 주파수 변화 분석과 모델 최적화   \n",
      "19                                                                                                                                         임베디드 시스템에서의 객체 탐지 네트워크의 가속을 위한  중요도 탐색 필터 가지치기 기법 연구   \n",
      "20                                                                   Prediction Model of Spinal Osteoporosis Using Lumbar Spine X-Ray from Transfer Learning Deep Convolutional Neural Networks   \n",
      "21                                                                                               Research on a Lightweight Deep Learning Model Suitable for Face Recognition for Mobile Devices   \n",
      "22                                                                 Cox Model Improvement Using Residual Blocks in Neural Networks: A Study on the Predictive Model of Cervical Cancer Mortality   \n",
      "23                                                                                                                                                              딥러닝을 활용한 전시 정원 디자인 유사성 인지 모형 연구   \n",
      "24                                                                                                                                                             드론 식별을 위한 CNN 기반 이미지 분류 모델 성능 비교   \n",
      "25                                                                             Identification of Atrial Fibrillation With Single-Lead Mobile ECG During Normal Sinus Rhythm Using Deep Learning   \n",
      "26                                                                                          Classification of mandibular molar furcation involvement in periapical radiographs by deep learning   \n",
      "27                                                                       Improving Chest X-ray Image Classification via Integration of Self-Supervised Learning and Machine Learning Algorithms   \n",
      "28                                                                                                                                                               불법 주정차 단속을 위한 딥러닝 기반 이미지 인식 모델   \n",
      "29                                                                                                                                                        LLVM IR 대상 악성코드 탐지를 위한 이미지 기반 머신러닝 모델   \n",
      "30                                                                                                                              Deep Learning-Based Defect Detection in Cu-Cu Bonding Processes   \n",
      "31                                       A Cost-Effective Blind Spot Detection System with High Recall Rate using Deep Learning and a Comparative Analysis of Implementation Hardware Platforms   \n",
      "32                                                                                                                              Deep Learning-Based Defect Detection in Cu-Cu Bonding Processes   \n",
      "33                                                                                                                                                         한정된 레이블 데이터를 이용한 효율적인 철도 표면 결함 감지 방법   \n",
      "34                                                                                                                                                                        엣지 딥 러닝 가속기의 추론 성능 분석   \n",
      "35                                                                                                                                                       어깨 초음파 영상에서 딥러닝 알고리즘을 이용한 컴퓨터 자동진단의 응용   \n",
      "36                                                                                                                                                              반려동물 안구 질환을 위한 딥러닝 모델 기반 진단 시스템   \n",
      "37                                                                                      EfficientNet-B0 outperforms other CNNs in image-based five-class embryo grading: a comparative analysis   \n",
      "38                                                                                                                                                          합성곱 신경망을 이용한 상지 엑스선 영상 분류 모델 유용성 평가   \n",
      "39                                                                                                                                                 비휘발성 메모리 기반 IMC을 활용한단일 코어/다중 레이어 CNN 가속기 최적화   \n",
      "40                                                                                                                                             CNN 기반 인코더와 Transformer 기반 인코더의 이미지 캡셔닝 성능 비교 분석   \n",
      "41                                                                                                                                                            영상에서 효율적인 잡음제거를 위한 dRED-TL-GAN 모델   \n",
      "42                                                                                                                                         흉부 X-선 영상을 이용한 Vision transformer 기반 폐렴 진단 모델의 성능 평가   \n",
      "43                                                                                                           Deep Learning Model and its Application for the Diagnosis of Exudative Pharyngitis   \n",
      "44                                                                                                                                                        교통사고 영상 분석을 통한 과실 판단을 위한 딥러닝 기반 방법 연구   \n",
      "45                                                                                                                                                              생체정보 보호를 위한 CNN 기반의 홍채 지문 영역 분할   \n",
      "46                                                                                                                                                  가상현실 기반의 고정밀 얼굴 통증 인식을 위한 빠른 안구 움직임 검출 알고리즘   \n",
      "47                                                                                                                                         DNN을 위한 비트 단위 파라미터 조작 프레임워크 및 파라미터와 정확도 간의 상호 연관성 분석   \n",
      "48                                                                                            Oriented object detection in satellite images using convolutional neural network based on ResNeXt   \n",
      "49                                                                                                                                                                     정보보안을 위한 생체 인식 모델에 관한 연구   \n",
      "50                                                                                                                                                           볼륨-플로우 그래프 기반 폐질환 분류를 위한앙상블 딥러닝 모델   \n",
      "51                                                                                                                                                         배 병해충 이미지 분류를 위한 딥러닝 최적 모델 선택에 관한 연구   \n",
      "52                                                                                                                                                          3D CNN-LSTM 알고리즘을 이용한 손동작 비디오 영상 분류   \n",
      "53                                                                                                                    A study of Strawberry Maturity Classification Using Improved Faster R-CNN   \n",
      "54                                                                                                                                               시간-주파수 도메인 변환 및 W2GAN-GP 모델 기반의 향상된 오디오 데이터 증강   \n",
      "55                                                                                                                                                  Vision Transformer를 활용한 운전자 이상행동 분류 딥러닝 시스템   \n",
      "56                                                                BCED-Net: Breast Cancer Ensemble Diagnosis Network using transfer learning and the XGBoost classifier with mammography images   \n",
      "57                                                                                                                                                                        패션 카테고리 오버샘플링 자동화 시스템   \n",
      "58                                                                                                                  An Improved Classification Model Based on Feature Fusion for Orchid Species   \n",
      "59                                                                                                                                                                 머신러닝기반 오이 생육 최적 예측 모델에 관한 연구   \n",
      "60                                                                                                                                                            딥러닝 기반의 빛간섭단층촬영 이미지에서 안구 방향 분류 모델   \n",
      "61                                                                 An Ensemble Deep Transfer Learning Model for Multi-dimensional Image Classification of Histological Prostate Biopsy Patterns   \n",
      "62                                                                                                 Automated Detection of COVID-19 in Chest Radiographs: Leveraging Machine Learning Approaches   \n",
      "63                                                                                                                                                            주행 시스템을 위한 그리드 레벨 다중 클래스 이뮬 분류 모델   \n",
      "64                                                                                                  Convolutional Neural Network Technique for Distinguishing Nine Varieties of Vegetable Crops   \n",
      "65   Deep learning to assess bone quality  from panoramic radiographs: the  feasibility of clinical application  through comparison with an implant  surgeon and cone-beam computed  tomography   \n",
      "66        Deep learning to assess bone quality from panoramic radiographs: the feasibility of clinical application through comparison with an implant surgeon and cone-beam computed tomography   \n",
      "67                                                                                                              Sharpness-Aware Minimization을 적용한 Separable Vision Transformer 기반 악성코드 유형 분류 기법   \n",
      "68                                                                                                                                        연속 웨이블릿 변환을 이용한 On-board Charger의 전이 학습 기반 고장 진단 알고리즘   \n",
      "69                                                                                                                                           다층 다요소 시스템의 최적화를 위한 진화연산의 탐색공간 축소 - 딥러닝 가지치기 사례 연구   \n",
      "70                                                                                                  Dental Age Estimation in Children Using Convolution Neural Network Algorithm: A Pilot Study   \n",
      "71                                                                                                     Improving the Recognition of Known and Unknown Plant Disease Classes Using Deep Learning   \n",
      "72                   Inceptionv3-LSTM-COV: A multi-label framework for identifying adverse reactions to COVID medicine from chemical conformers based on Inceptionv3 and long short-term memory   \n",
      "73                                                                                     Classification of Pulmonary Nodules in 2-[ 18 F]FDG PET/CT Images with a 3D Convolutional Neural Network   \n",
      "74                             MLCNN-COV: A multilabel convolutional neural network-based framework to identify negative COVID medicine responses from the chemical three-dimensional conformer   \n",
      "75                                                                                                                                                                 재난약자 및 취약시설에 대한 APC실증에 관한 연구   \n",
      "76                                                                                                                                Crop Leaf Disease Identification Using Deep Transfer Learning   \n",
      "77                                                                                                                           아음속 수송체 알루미늄 프레임의 비선형 유도초음파 주파수 응답 – 합성곱 신경망 분석 기반 미세 감육 진단 가능성 연구   \n",
      "78                                                                             Deep Learning-based Arabic Sign Recognition System for Automated Communication with Hearing Impaired Individuals   \n",
      "79                                                                                                                  Deep Learning Driven Human Posture Location  in Physical Education Teaching   \n",
      "80                                                                                                                      Deep Learning-Based Plant Health State Classification Using Image  Data   \n",
      "81                                                                                                                                                            상품 카테고리 자동분류를 위한 BERT-분류기 아키텍처 연구   \n",
      "82                                                                                                                                                                모션블러 이미지에 대한 CNN 모델의 균열 검출 성능   \n",
      "83                                                                                                                                                         SERN 기반 운전자의 다중 행동 특징을 이용한 졸음 검출 시스템   \n",
      "84                                                                                                                               합성곱 신경망 기반 화재 인식 모델 최적화 연구: Layer Importance Evaluation 기반 접근법   \n",
      "85                                                                                                                                                                 전이학습을 이용한 신발 이미지 스타일 분류모델 연구   \n",
      "86                                                                                                                               합성곱 신경망 기반 화재 인식 모델 최적화 연구: Layer Importance Evaluation 기반 접근법   \n",
      "87                                                                                                            UAV Imagery-based Automatic Classification of Ground Surface Types for Earthworks   \n",
      "88                                                                                                                                                           시퀀스 데이터 기반 단일 및 복합 변조 레이더 신호 변조 식별   \n",
      "89                                                                                                                                                    다중 시멘틱 세그멘테이션 AI 기반의 가전용 크림프 하네스 검사 모델 개발   \n",
      "90                                                                                                              Drone Detection Using Dynamic-DBSCAN and Deep Learning in an Indoor Environment   \n",
      "91                                                                                                                                                                   잔차 신경망을 활용한 펫 로봇용 화자인식 경량화   \n",
      "92                                                                                                                                                                   잔차 신경망을 활용한 펫 로봇용 화자인식 경량화   \n",
      "93                                                                                                                                                                   Few-Shot 기반 작물 기상피해 판별 시스템   \n",
      "94                                                                                                                                                              패션 이미지 데이터를 활용한 딥러닝 기반의 의류속성 분류   \n",
      "95                                                                                                           Design of Automatic Defect Classification System  for Wafer Edge Defect Inspection   \n",
      "96                                                                                                                                                     Attention BiFPN 기반 화재 검출 딥러닝 알고리즘에 대한 연구   \n",
      "97                                                                                                                                                     자율주행 자동차 인지 성능 향상을 위한 복수의 인공신경망 결과 값 합성법   \n",
      "98                                                                                       Automatic cancer nuclei segmentation on histological images: comparison study of deep learning methods   \n",
      "99                                                                                                                                                 Anchor-Free 기반 3D 객체 검출을 이용한 과수 꼭지 검출 시스템 구현   \n",
      "100                                                                                                                                                      EfficientNet과 LSTM을 활용한 병해  진행도 예측 시스템   \n",
      "101                                                           A Dynamic Head Gesture Recognition Method for Real-time Intention Inference and Its Application to Visual Human-robot Interaction   \n",
      "102                                                                                          IRAE-UNet: InceptionResNetV2 -Attention Encoder based UNet Semantic Segmentation of Aerial Imagery   \n",
      "103                                                              Benchmark study on a novel online dataset for standard evaluation of deep learning-based pavement cracks classification models   \n",
      "104                         A deep learning model for estimating sedation levels using heart rate variability and vital signs: a retrospective cross-sectional study at a center in South Korea   \n",
      "105                                                                                                                                   An Implementation of Effective CNN Model for AD Detection   \n",
      "106                                                                                                                                                국방 데이터를 활용한 인셉션 네트워크 파생 이미지 분류 AI의 설명 가능성 연구   \n",
      "\n",
      "     date  \\\n",
      "0    2024   \n",
      "1    2024   \n",
      "2    2024   \n",
      "3    2024   \n",
      "4    2024   \n",
      "5    2024   \n",
      "6    2024   \n",
      "7    2024   \n",
      "8    2024   \n",
      "9    2024   \n",
      "10   2024   \n",
      "11   2024   \n",
      "12   2024   \n",
      "13   2024   \n",
      "14   2024   \n",
      "15   2024   \n",
      "16   2024   \n",
      "17   2024   \n",
      "18   2024   \n",
      "19   2024   \n",
      "20   2024   \n",
      "21   2024   \n",
      "22   2024   \n",
      "23   2024   \n",
      "24   2024   \n",
      "25   2024   \n",
      "26   2024   \n",
      "27   2024   \n",
      "28   2024   \n",
      "29   2024   \n",
      "30   2024   \n",
      "31   2024   \n",
      "32   2024   \n",
      "33   2024   \n",
      "34   2024   \n",
      "35   2024   \n",
      "36   2024   \n",
      "37   2024   \n",
      "38   2024   \n",
      "39   2024   \n",
      "40   2024   \n",
      "41   2024   \n",
      "42   2024   \n",
      "43   2024   \n",
      "44   2024   \n",
      "45   2024   \n",
      "46   2024   \n",
      "47   2024   \n",
      "48   2024   \n",
      "49   2024   \n",
      "50   2024   \n",
      "51   2024   \n",
      "52   2024   \n",
      "53   2024   \n",
      "54   2024   \n",
      "55   2024   \n",
      "56   2024   \n",
      "57   2024   \n",
      "58   2024   \n",
      "59   2024   \n",
      "60   2024   \n",
      "61   2024   \n",
      "62   2024   \n",
      "63   2024   \n",
      "64   2024   \n",
      "65   2024   \n",
      "66   2024   \n",
      "67   2024   \n",
      "68   2024   \n",
      "69   2024   \n",
      "70   2024   \n",
      "71   2024   \n",
      "72   2024   \n",
      "73   2024   \n",
      "74   2024   \n",
      "75   2024   \n",
      "76   2024   \n",
      "77   2024   \n",
      "78   2024   \n",
      "79   2024   \n",
      "80   2024   \n",
      "81   2024   \n",
      "82   2024   \n",
      "83   2024   \n",
      "84   2024   \n",
      "85   2024   \n",
      "86   2024   \n",
      "87   2024   \n",
      "88   2024   \n",
      "89   2024   \n",
      "90   2024   \n",
      "91   2024   \n",
      "92   2024   \n",
      "93   2024   \n",
      "94   2024   \n",
      "95   2024   \n",
      "96   2024   \n",
      "97   2024   \n",
      "98   2024   \n",
      "99   2024   \n",
      "100  2024   \n",
      "101  2024   \n",
      "102  2024   \n",
      "103  2024   \n",
      "104  2024   \n",
      "105  2024   \n",
      "106  2024   \n",
      "\n",
      "                                                                                                                                                                                                                                           keywords  \\\n",
      "0                                                                                           [특허 도면, 이진 이미지, 분류 및 검색, ResNet-50, SIFT, 유사도 비교, Patent drawings, binary images, classification and retrieval, ResNet-50, SIFT, similarity Comparison]   \n",
      "1                                                                                                                         [백혈구, 인공지능, 딥러닝, ResNet-50, 이미지 분석, White blood cells, Artificial Intelligence, Deep Learning, ResNet-50, Image analysis]   \n",
      "2                                                                                                                [레즈넷, 트랜스포머, 레즈넷-트랜스포머, 웹사이트 검색, 랭킹 알고리즘, BM25, ResNet, Transformer, ResNet-Transformer, Website search, BM25, Rangking algorithm]   \n",
      "3                                                                                                                                 [Virtual Space, YOLOv5, ResNet, Indoor and Outdoor, Accuracy, Immersive, 가상 공간, YOLOv5, ResNet, 실내와 실외, 정확도, 몰입감]   \n",
      "4                                                                                                                                                                            [ResNet, Speech Emotion Recognition, Deep Learning, Data Augmentation]   \n",
      "5                                                                                                                                                                                  [Sleep apnea, Polysomnography, Sleep sound, Deep learning, MFCC]   \n",
      "6                                                                                                                                                [ONNX, 딥 모델, Inference Runtime, 객체 감지, 이미지 분류, Deep Model, Object Detection, Image Classification]   \n",
      "7                                                                                                                                                                                     [Voxel, Mesh, 3D object generation, VAE, GAN, Residual block]   \n",
      "8                                                                                                           [스마트 농업, 작물 질병 진단, 욜로v8, 레스넷50, 딥러닝 모델 비교, Smart Agriculture, Crop Disease Diagnosis, YOLOv8, ResNet50, Deep Learning Model Comparison]   \n",
      "9                                                                                                                                       [핵의학, 보간법, ResNet모델, 정량분석, Nuclear medicine, Interpolation techniques, ResNet model, Quantitative analysis]   \n",
      "10                                                                                                                                                                              [Convolutional Neural Network (CNN), Image Similarity, Large Waste]   \n",
      "11                                                                                                                                 [Online to Offline(O2O), OCR(Optical Character Recognition), Text Recognition, Attention Mechanism, Korean Menu]   \n",
      "12                                                                             [인공지능, GAN, ResNet50, Image Classification, Neural Network, Artificial intelligence, Generative Adversarial Network, ResNet50, Image Classification, Neural Network]   \n",
      "13                                                                                                                          [Anomaly Detection, AI Learning Data, ResNet50, 3D-CNN, GridCV, GRU, 이상행동 탐지, AI 학습 데이터, ResNet50, 3D-CNN, GridCV, GRU]   \n",
      "14                                                                                                          [산불 감지, CNN, 전이학습, 데이터 증강, ResNet50, Wildfire Detection, Convolutional Neural Networks, Transfer Learning, Data Augmentation, ResNet50]   \n",
      "15                                                                                                                                          [Artificial intelligence, Convolutional neural networks, Classification, Deep learning, Implant system]   \n",
      "16                                                                                                                                                                                     [Concrete Crack, Convolution Neural Network, Classification]   \n",
      "17                                                                                                                                                                    [deep learning, deep transfer learning, video processing, violence detection]   \n",
      "18                                                                                                                                                                        [Artificial Intelligence, Deep-learning, CNN, Classification, Spectogram]   \n",
      "19                                                                                                                                                                                 [Object detection, Network compression, Pruning, Inference time]   \n",
      "20                                                                                                                                                                                   [Deep learning, Lumbar vertebrae, Osteoporosis, Spine, X-rays]   \n",
      "21                                                                                                                                                      [embedding environment, lightweight deep learning, face recognition, MobileFaceNet, ResNet]   \n",
      "22                                                                         [자궁경부암, 생존 예측 모델, 콕스 비례 위험, 기계 학습, 심층 신경망, ResNet, Cervical Cancer, Survival Prediction Model, Cox Proportional Hazards, Machine Learning, Deep Neural Networks, ResNet]   \n",
      "23                                                                                                                                                                                                                                               []   \n",
      "24                                                                                                               [Drones, Transfer learning, Image classification, Convolutional Neural Networks, Aerial targets, 드론, 전이 학습, 이미지 분류, 합성곱 신경망, 공중표적]   \n",
      "25                                                                                                                                   [Artificial Intelligence, Atrial Fibrillation, Electrocardiography, Mobile Applications, Probability Learning]   \n",
      "26                                                                                                                                                                                     [Mandible, Molar, Periodontitis, Radiography, Deep Learning]   \n",
      "27                                                                                                                                                        [Chest X-ray image, Contrastive learning, Image classification, Self-supervised learning]   \n",
      "28                                                                                                                                                                                               [Deep learning, Illegal parking, ResNet18, YOLOv8]   \n",
      "29                                                                                                                                                                                            [LLVM IR, Image based, Malware Detection, ResNet50V2]   \n",
      "30                                                                                                                                                                        [Cu-Cu Bonding, Deep Learning, Defect Detection, Defect Map, ResNet, CNN]   \n",
      "31                                                    [Blind Spot Detection, Advanced Driver Assistance Systems, Deep Learning Accelerator, FPGA, ResNet-18, Energy Efficiency, Automotive Safety, Vehicle Detection, Model Parameter Quantization]   \n",
      "32                                                                                                                                                                        [Cu-Cu Bonding, Deep Learning, Defect Detection, Defect Map, ResNet, CNN]   \n",
      "33                                                                                                                                                                               [Rail surface, Semi-supervised, thresholding, sorting, selection.]   \n",
      "34                                                                                                                                                         [Deep learning, Deep learning accelerator, Inference, Edge device, Performance analysis]   \n",
      "35                                                                         [컴퓨터 자동 진단, 딥러닝 알고리즘, 초음파 영상, 영상 구분, 영상 병변 탐지, Computer Automated Diagnostics, Deep learning algorithm, Ultrasound  image, Image classification, Image Lesion Detection]   \n",
      "36                                                                                                                  [artificial intelligence, deep learning, ocular diseases, system, convolutional neural network, 인공지능, 딥러닝, 안구 질환, 시스템, 합성곱 신경망]   \n",
      "37                                                                                                                                                       [blastocyst, convolutional neural networks, deep learning, embryo, in vitro fertilization]   \n",
      "38                                                                                                         [합성곱 신경망, 딥러닝, 의료 영상, 영상 분류, 검사 오류, Convolutional Neural Network, Deep Learning, Medical Image, Image Classification, Examination Error]   \n",
      "39                                                                                                                                                              [Design space exploration, IMC, CNN, accelerator optimization, Non-volatile memory]   \n",
      "40                                                                                                                            [image captioning, residual network 50, visual geometry group-16, vision transformers, shifted window transformer, .]   \n",
      "41                                                                                                                                           [딥러닝, 잡음제거, deformable 컨볼루션, GAN, Deep learning, deformable convolution, dRED-TL-GAN, image denoising]   \n",
      "42                                                                                                                                               [딥러닝, 폐렴 진단, 흉부 X-선 영상, Vision transformer, Deep learning, Pneumonia detection, Chest X-ray image]   \n",
      "43                                                                                                                                                                   [Artificial Intelligence, Deep Learning, Diagnosis, Pharyngitis, Telemedicine]   \n",
      "44                                                                                                                                                       [Action recognition, Computer vision, Deep learning, Image classification, Video analysis]   \n",
      "45                                                                                                                   [Segmentation, Detection, Artificial Intelligence Learning, Biometric Information, CNN, 영역 분할, 감지, 인공지능 학습, 생체 정보, 합성곱 신경망 모델]   \n",
      "46                                                                                                                                          [Virtual Reality, Pain Recognition, Eye Movement Detection, Pediatric Medical Treatment, Deep Learning]   \n",
      "47                                                                                                                                   [딥 뉴럴 네트워크, 파라미터, 비트 연산, 정확도, 강인성, Deep Neural Network, parameters, bit-wise operations, accuracy, robustness]   \n",
      "48                                                                                                                              [box-boundary-aware vector, convolutional neural network, oriented object detection, ResNeXt101, satellite imagery]   \n",
      "49                                                                                                                                                                 [생체 인식, 멀티모달, 특징 융합, 딥러닝, Biometrics, Multimodal, Feature Fusion, Deep Learning]   \n",
      "50                                                                                                         [합성곱 신경망, 앙상블 딥러닝 모델, 폐질환, 볼륨-플로우 그래프, Convolutional Neural Network, Ensemble Deep Learning Model, Pulmonary Disease, Flow Volume Loops]   \n",
      "51                                                                                                          [배 병해충, 딥러닝 모델, 이미지 분류, 검역, 데이터 증강, Pear Pests and Diseases, Deep Learning Models, Image Classification, Quarantine, Data Augmentation]   \n",
      "52                                                                                                                                           [Artificial Intelligence, Deep Learning, CNN, LSTM, Video Classification, Hand Gesture Classification]   \n",
      "53                                                                                                                                         [Convolutional Neural Network (CNN), Faster R-CNN, Image Classification, RoI Align, Strawberry Maturity]   \n",
      "54                                                                                                                                             [Audio augmentation, WGAN-GP, Time-frequency transformation, Speech classification, Imbalanced data]   \n",
      "55                                                                                                                            [도로 교통, 운전자 이상행동, 딥러닝, Vision Transformer, Road traffic, Driver abnormal behavior, Deep learning, Vision Transformer]   \n",
      "56                                                                                                                                 [Breast cancer classification, Feature extraction and concatenation, Performance evaluation, XGBoost classifier]   \n",
      "57                                                                                                                            [CNN, Deep learning, Oversampling, ResNet50, YOLOv8, Fashion category, 컨볼루션 신경망, 딥러닝, 오버샘플링, 레즈넷50, 욜로 버전 8, 패션 카테고리]   \n",
      "58                                                                                                                                                                               [Orchid species · Classifcation model · Feature fusion · ResNet34]   \n",
      "59                                                                                                          [작물 질병 진단, 딥러닝 모델 비교, 레스넷50, 스마트 농업, 욜로v8, Crop Disease Diagnosis, Deep Learning Model Comparison, ResNet50, Smart Agriculture, YOLOv8]   \n",
      "60                                                                                                                                                                                               [Deep learning, Eye, Optical coherence tomography]   \n",
      "61                                                                                                                                        [Multi-dimensional, Histological, Prostate Cancer, Ensemble Deep Transfer Learning, Image Classification]   \n",
      "62                                                                                                                            [COVID-19 pandemic, Machine learning models, Chest X-ray classification, Automated identification, Medical diagnosis]   \n",
      "63                                                                                                                                                              [soiling detection, woodscape dataset, image classification, autonomous driving, .]   \n",
      "64                                                                                                                                                  [Confusion matrix, Convolutional neural network, DenseNet201, Image classification, Vegetables]   \n",
      "65                                                                                                                                                       [Artificial intelligence, Bone density, Deep learning, Dental implant, Dental radiography]   \n",
      "66                                                                                                                                                       [Artificial intelligence, Bone density, Deep learning, Dental implant, Dental radiography]   \n",
      "67                                                                          [Malware, Classification Method, Separable Vision Transformer, SharpnessAware Minimization, 악성코드, 유형 분류 기법, Separable Vision Transformer, Sharpness-Aware Minimization]   \n",
      "68                                                                                                                                       [on-board charger, fault diagnosis, wavelet transform, transfer learning, convolutional neural network, .]   \n",
      "69                                                                                                                                     [Multilayers Multi-elements System, Evolutionary Computation, Gene Expression by Rules, CNN, Filter Pruning]   \n",
      "70                                                                                                                                                                            [Convolutional neural networks, Deep learning, Dental age estimation]   \n",
      "71                                                                                                                                                       [deep learning, plant disease recognition, transfer learning, unknown disease recognition]   \n",
      "72                                                                                                                                                         [adverse medicine reactions, COVID medicine development, Inceptionv3, LSTM, multi-label]   \n",
      "73                                                                                                                 [Convolutional neural networks · Positron emission tomography · 2-[18F]FDG PET/CT · Pulmonary nodules · Artificial intelligence]   \n",
      "74                                                                                                [chemical three-dimensional conformers, convolutional neural network, COVID medicine development, negative medicine reactions, transfer-learning]   \n",
      "75                                                                                                                           [재난취약시설, 요구조자, 자동계수기, 인공지능, 영상인식, Disaster-vulnerable Facilities, Victims, Auto People Counter, AI, Image Recognition]   \n",
      "76                                                                                                                                    [Agricultural Artificial Intelligence, Crop Leaf Disease Identification, Plant Protection, Transfer Learning]   \n",
      "77                                                                                                      [구조 진단, 유도초음파, 심층학습, 합성곱 신경망, 객체검출, Structural Health Monitoring, Guided Wave, Deep Learning, Convolution Neural Network, Object Detection]   \n",
      "78                                                                                                                                               [Arabic Sign Language, Deep Neural Networks, ResNet, CNN, Automatic Recognition, Hearing Impaired]   \n",
      "79                                                                                                                                                                             [Human Posture, Physical Education Teaching, Deep Learning, ResNet.]   \n",
      "80   [Deep Learning, Convolutional Neural Networks, Channel-wise Attention, Depthwise Separable Convolution, Attention-Enhanced ResNet, Plant Health State Classification, 딥러닝, 합성곱 신경망, 채널 어텐션, 깊이 분리 합성곱, Attention-Enhanced ResNet, 작물 건강 상태 분류]   \n",
      "81                                                                  [문장 분류, 문장 유사도, Sentence BERT, CNN, ResNet, Transformer, Classification, Sentence classification, Sentence similarity, Sentence BERT, CNN, ResNet, Transformer, Classification]   \n",
      "82                                                                                           [CNN (convolutional neural network), Motion blur, Dataset, Crack detection, F1 score, CNN (convolutional neural network), 모션블러, 데이터셋, 균열 검출, F1 score]   \n",
      "83                                                                                                                     [졸음 인식, 운전자 행동 특징, SERN, 다중 특징, 딥러닝, drowsiness detection, driver behavior features, SERN, Multiple features, deap learning]   \n",
      "84                                                                              [레이어 중요도 평가, 전이 학습 모델, 합성곱 신경망 최적화, 실시간 화재 감지, 기여도, Layer Importance Evaluation, Transfer Learning Model, CNN Optimization, Real-Time Fire Detection, Contribution]   \n",
      "85                                                                                                                            [전이학습, 신발 스타일 분류, 신발 아웃솔, 신발 갑피, Transfer Learning, Shoes Style Classification, Shoes Outsole, Shoes Upper, ConvNeXt]   \n",
      "86                                                                                                                                                                                                                                               []   \n",
      "87                                                                                                                         [Automated construction equipment, Ground surface, Unmanned aerial vehicle, Multi-label classification, Computer vision]   \n",
      "88                                                                                                                                                  [Radar signal modulation identification, Composite modulated radar signal, Deep learning model]   \n",
      "89                                                                                                                                                               [Wire harness, Crimp harness inspection, Multi class semantic segmentation, U-Net]   \n",
      "90                                                                                                                                                                                       [DBSCAN, Deep Learning, FMCW Radar, Object Detection, UAV]   \n",
      "91                                                                                                                                                                                  [Speaker recognition, CNN, Resnet, Lightweight, Classification]   \n",
      "92                                                                                                                                                                                  [Speaker recognition, CNN, Resnet, Lightweight, Classification]   \n",
      "93                                                                                                                                                                                         [Smart Farm, Few-Shot, Classification, ResNet, DenseNet]   \n",
      "94                                                                                                                                   [패션 이미지, 의류 속성 분류, 딥러닝, Fashion Image, Clothing Attribute Classification, Deep Learning, ResNet, EfficientNet]   \n",
      "95                                                                                                                                                            [CNN, Deep Learning, Image Processing, Auto Defect Classification, Wafer Edge Defect]   \n",
      "96                                                                                                                                        [fire detection, neural network, attention mechanism, bi-directional pyramid, feature pyramid network, .]   \n",
      "97                                                                                                                         [딥러닝, 전이 학습, 인공신경망, 이미지 인식, 모델 합성, Deep learning, Transfer learning, Neural network, Image classification, Model fusion]   \n",
      "98                                                                                                                                                       [Cancer · Medical segmentation · Histology · Convolutional neural networks · Augmentation]   \n",
      "99                                                                                                                                 [딥러닝, 디지털 농업, 3D 객체 검출, 과수 꼭지 검출, Deep learning, Digital agriculture, 3D Object detection, Fruit stem detection]   \n",
      "100                                                                                                                                    [Image Classification, Time Series Data Analysis, Time Series Forecasting, Disease, XAI, EfficientNet, LSTM]   \n",
      "101                                                                                                                                                                        [Computer vision, deep learning, head gesture, human-robot interaction.]   \n",
      "102                                                                                                                                                        [remote sensing, attention, image segmentation, inception, 원격 감지, 어텐션, 영상 분할, Inception]   \n",
      "103                                                                                                                                          [Convolutional neural networks, Pavement cracks, Deep learning, Benchmark study, Crack classification]   \n",
      "104                                                                                                               [conscious sedation, deep learning, heart rate, machine learning, patient monitoring, pediatric intensive care unit, vital signs]   \n",
      "105                                                                                                                                                  [Alzheimer's disease (AD), Magnetic Resonance Imaging (MRI), Convolution Neural Network (CNN)]   \n",
      "106                                                                                                            [Deep Learning(딥러닝), Image Classification(이미지 분류), AI(인공지능), XAI(설명 가능한 인공지능), Inception Network(인셉션 네트워크), LIME Algorithm(라임 알고리즘)]   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   abstract  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                   특허 문헌의 유사성 평가 및 검색 연구는 특허 문헌의 효율적인 관리뿐 아니라 산업 및 기술 분야에서효율적이고 빠른 정보 수집을 위해 중요한 주제로 다뤄지고 있다. 특히 특허 도면은 산업 기술의 발전과 혁신의 결과물을 시각적으로 표현해왔으나 지금까지 텍스트에 비해 중요하게 다뤄지지 못한 측면이 있다.본 연구는 효과적인 특허 도면의 검색을 위해 딥러닝의 대표 모델 ResNet-50과 전통적인 컴퓨터비전알고리즘 SIFT를 이용하여 유사성을 평가하는 연구이다. 먼저 시각적 유형의 유사성을 평가하기 위해총 10,827개의 특허도면을 이용한 유형 분류 실험을 진행했으며 분류성능은 95%가 넘는 Accuracy를나타냈다. 두 번째로 기술도면 5,000개를 사용하여 ResNet과 SIFT를 이용한 검색 실험을 진행하여 각모델의 유사성을 평가하는 특징을 살펴보았다. 마지막으로 원본 데이터 50개와 원본 데이터를 다양한형태로 증강한 데이터 4,800개를 이용하여 편집 유형별로 검색 및 매칭한 결과, ResNet은 72.54%, SIFT는 86.71%의 평균적인 매칭 결과를 나타냈다.연구 수행 결과, 이미지 전체의 정보를 이용하여 유사도를 비교하는 ResNet-50과 달리 SIFT는 이미지내 특징점 등 속성 정보를 이용하여 유사도를 판단하므로 시각적으로 유사한 이미지를 찾는 일에는ResNet이, 같은 이미지를 찾는 일에는 SIFT가 더 강점이 있는 것으로 평가할 수 있다.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       백혈구 연구는 면역학, 세포생물학, 유전학 분야에서 수 세기에 걸쳐 이뤄졌으며, 백혈구의 기능, 발달, 면역 반응, 건강 영향을 이해하는데 기여하였다. 이를 토대로 감염병, 암, 면역 질환의 치료와 예방 방법이 발전하였다. 특히 림프구와 T세포 연구는 백신 개발과 암 치료에 혁명적인 영향을 미쳤다. 백혈구 연구는 혈액 관련 질병 예방과 치료, 호중구, 단핵구, 호산구 등의 세포 연구로도 자가면역 질환, 알레르기, 염증성 질환의 치료를 개선하였다. 그러나 백혈구의 정확한 기능, 상호작용, 면역 조절 연구는 아직 미흡하다. 최근 디지털 기술 발전으로 대규모 이미지 데이터 생성이 가능해졌으며, AI 알고리즘을 훈련하는 데 활용되고 있다. 특히 딥러닝 기술은 이미지 분석 분야에서 중요한 역할을 하며, 백혈구 연구에도 응용되고 있다. 이 연구는 ResNet-50 모델의 실제 응용 가능성을 확인하고자 하였으며, 모델의 성능을 확인한 결과 ResNet-50모델에 Epoch를 30으로 설정했을 때 성능이 균형적이고 안정적으로 나타나는 것을 확인하였다.   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               가상공간 환경에서 사용자의 몰입과 경험을 극대화하기 위해서는 시각적, 음향적 효과가 핵심적인 역할을 해야 한다. 하지만 온라인상에서 현장감이 부족하거나, 음질이 떨어져 시각 및 음향의 질적 수준이 오프라인에 비해 떨어진다는 의견이 존재한다. 가상 공간 내에서 실내외를 정확히 식별하여 사용자의 움직임에 따라 상호작용을 통해 현실감을 높이기 위해 가상환경 시스템에서 현실적인 효과를 제공할 필요가 있다. 이를 위해서는 가상공간에서의 실내외 환경 식별 정확도를 높이기 위한 연구가 진행되어야 한다. COCO DataSet 내 객체 수에 따른 실내외 식별 방법과 기존 알고리즘을 수정한 실내외 식별 방법에 따른 실내외 식별 방법을 통해 결과값에 대한 정확도를 향상할 수 있다. 이에 본 연구에서는 객체 수 및 기존 알고리즘 수정을 통한 가상공간 특성의 정확도 문제를 해결한다. 이는 향후 객체의 특징을 학습하여 데이터화 시키고 사용 DataSet 이외의 객체들을 식별할 수 있게 하여 새로운 공간 구축을 위한 변화의 방향을 제시하고자 한다.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    수면무호흡증(Sleep Apnea)은 전 세계적으로 심각한 건강 문제를 일으키는 질환으로, 주로 수면 중 상부 기도의 폐쇄로 인해 발생한다. 현재 수면무호흡증의 표준 진단 방법인 수면다원검사(Polysomnography, PSG)는 높은 비용과 복잡성 등의 한계가 있다. 본 연구에서는 수면 중 발생하는 소리 데이터를 활용하여 수면무호흡증을 탐지하는 비침습적 방법을 제안하였다. 소리 데이터는 MFCC(Mel-Frequency Cepstral Coefficients)로 특징을 추출하였으며, 1D CNN 기반의 ResNet(Residual Network) 모델을 통해 분류를 수행하였다. 실험 결과, 제안된 모델은 5겹 교차 검증을 통해 평균 정확도(Accuracy) 97.8%, 재현율(Recall) 97.7%, 민간도(Precision) 97.9%, AUC 0.978의 성능을 달성하였다. 향후 연구에서는 데이터셋을 확장하고 다양한 딥러닝 모델을 실험하여 모델의 성능을 더욱 향상시킬 계획이다. 본 연구는 수면무호흡증 탐지의 정확성을 높이고, 효율적인 건강관리 시스템 구축에 기여할 수 있을 것으로 기대한다.   \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      None   \n",
      "7                                                                                                                                                                                                                                                                                                         3D 모델링은 게임, AR, VR, 메타버스 등 다양한 분야에서 활용되고 있다. 최근 컴퓨터 하드웨어의 성능 향상으로 3D 공간에서의 시각화와 연산이 가속화되고 있으며, GAN 기술의 진보로 3D 객체를 생성하는 방법이 연구되고 있다. GAN 기반 네트워크는 이미지를 입력으로 받아 복셀(Voxel)을 생성하고, Wasserstein 손실 함수 도입 및 그래디언트 패널티 적용을 통해 학습하는 3D-VAE-IWGAN 방식을 제안하였다. GAN은 훈련에 포함되지 않은 여러 모델을 생성할 수 있지만, 아티팩트가 생기는 문제가 있다. 또 다른 방식으로는 2D에서 지도 학습하고 3D에서는 비지도 학습을 통해 3D 레이블 생성 비용을 줄인 DIB-R과 같은 네트워크가 제안되었다. DIB-R은 아티팩트를 줄일 수 있지만, 오토인코더 기반 네트워크로는 다양한 모델을 생성하기 어렵다. 본 논문은 3D-VAE-IWGAN에서 성능을 높인 Variational Autoencoder(VAE)와 Generative Adversarial Network(GAN)을 결합한 VAE-GAN에 잔차블록(Residual block)을 적용하는 방법을 제안하며 이미지 생성자와 판별자에 더 많은 특징을 추출하여 고품질 이미지 생성 및 잠재 공간 보간 성능이 향상된 시스템을 제안한다. 기존 네트워크와 비교한 결과는 의자 클래스에서 137.15로 116.33% 더 나은 결과를 보였고 침대에서도 137.24로 130.4%로 향상된 결과를 보였다.   \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          본 연구의 목적은 YOLOv8, ResNet50, Faster R-CNN 모델을 활용하여 고추 작물의 질병을 진단하고, 각 모델의 성능을 비교하는 것이다. 첫 번째 모델은 YOLOv8을 사용하여 질병을 진단하였고, 두 번째 모델은 ResNet50을 단독으로 사용하였다. 세 번째 모델은 YOLOv8과 ResNet50을 결합하여 질병을 진단하였으며, 네 번째 모델은 Faster R-CNN을 사용하여 질병을 진단하였다. 각 모델의 성능은 정확도, 정밀도, 재현율, F1-Score 지표로 평가된다. 연구 결과, YOLOv8과 ResNet50을 결합한 모델이 가장 높은 성능을 보였으며, YOLOv8 단독모델도 높은 성능을 나타냈다.   \n",
      "9                                                                                                                                                                                       핵의학 영상은 다양한 보간법을 적용하여 획득 영상의 축소 및 확대를 통하여 영상의 질을 개선한다. 또한, 입력 데이터셋과 정답 데이터셋 간의 특징을 추출하는 딥러닝 알고리즘 기반의 영상처리 기술도 영상의 질 향상에 크게 작용하고 있다. 이에 본 연구의 목적은 갑상샘 팬텀 영상을 이용하여 다양한 보간 방법을 적용하여 획득한 입력 데이터에 따른 딥러닝 알고리즘의 성능을 정량적 분석 인자를 이용하여 평가하고자 한다. 256 × 256 크기의 매트릭스로 갑상샘 팬텀 영상을 99mTcO₄- 37M㏃로 총 200장을 각각 1분씩 획득하여 정답 영상을 생성하였다. 최근접 이웃, 선형, 이차, 삼차, 오차 보간 방법을 다운샘플링과 업샘플링에서 적용하여 입력 데이터셋을 구축하였다. 학습률 0.0001, 학습횟수 300회로 설정된 초고분해능 잔차학습네트워크 (super resolution residual network, SRResNet) 구조를 구현하였으며, 데이터셋은 8:1:1 비율로 훈련, 검증 및 테스트 세트로 분할하여 학습하였다. 생성된 출력 영상은 변동계수 (coefficient of variationi, COV) 및 신호대잡음비 (contrast to noise ratio, CNR)를 사용하여 분석하였다. 그 결과 SRResNet 네트워크는 저분해능 영상을 생성하는데 삼차 보간법을 적용했을 때 가장 우수한 COV 및 CNR 값을 보였다. 그러므로 본 연구는 보간법 기반 딥러닝 알고리즘을 적용한 핵의학 갑상샘 영상의 질화질 개선 측면에서 입력 데이터로써의 적절한 보간법 적용이 생성 영상에 미치는 영상이 크다는 것을 확인하였으며, 영상의 질 향상을 위한 검사에 따른 적절한 보간법이 설정되어야 함을 증명하였다.   \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "13                                                                                                                                                                                                                                                                                                                         본 연구에서는 보안과 범죄 예방 강화의 하나로 공공 CCTV와 보안 카메라의 영상 데이터를 사용하여주거 및 공용 공간에서 이상행동을 탐지하기 위한 AI 학습 데이터 세트의 구축과 이를 활용한 모델을 시범 개발하였다. AI 학습 데이터 세트와 모델은 민간 기업의 AI 기술 발전과 AI 프로젝트 개발을 촉진하기 위해 설계되었다.데이터 세트 구축 시 비디오 프레임에서 특징을 추출하기 위하여 ResNet50을, JSON 파일에서 스켈레톤 포인트를처리하기 위하여 3D-CNN을 사용하여 모듈화 하였다. 이 데이터를 사전에 정의된 이상행동에 따라 Labeling 하였다. 또한 GridCV를 사용하여 SVM 분류기와 비디오 시퀀스 처리를 위한 GRU를 활용하였다. 모델의 학습 성능평가에서는 주요 정확도(main accuracy)가 지속해서 향상되었으며, 상세 손실(detailed loss) 또한 감소하는 추세를 보였다. 이를 바탕으로 학습된 모델은 주어진 비디오 시퀀스에서 나타나는 행동의 범주를 예측할 수 있다. 본연구에서 구축된 AI 학습 데이터 세트와 모델 시범 개발로 즉각적인 이상행동 감지를 통한 범죄예방 및 범인 검거를 위해 인공지능 학습에 필요한 데이터 확보, 구축 및 배포하여 민간기업의 AI 기술 발전 및 인공지능 사업의발전을 도모하고자 이상행동 탐지 기능 개발의 실용성에 대한 귀중한 인사이트를 제공하여 공공 안전 분야에서AI 애플리케이션의 발전에 기여할 것으로 기대된다.   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         전 세계는 기상이변의 영향으로 산불 등 자연 재해가 끊이지 않고 있으며 이로 인한 사회 안전에 심각한 위협이 되고 있다. 특히 대한민국 동해안 지역은 매년 산불 피해로 인한 막대한 재산 피해가 발생하고 있다. 초기 화재 감지 모델의 필수적인 개발은 훈련을 위한 제한된 이미지 데이터와 관련된 도전을 극복해야 하며, 이는 과적합의 위험을 증가시킨다. 이를 해결하기 위해, 랜덤하게 50% 범위까지 회전, 랜덤하게 20% 범위까지 축소 및 확대, 랜덤하게 50%까지 가로 및 세로 뒤집기를 적용하였다. 성능 평가에서 6층 신경망이 7층 신경망보다 더 우수한 성능을 보였으며, 이는 제한된 데이터셋을 가지고 계층 수를 늘리는 것이 바람직하지 않음을 의미한다. 또한, 화재 이미지 분류를 위한 딥러닝 기반 CNN 모델과 ResNet50 전이 학습 모델의 평가는 전이 학습의 우수한 효과를 확인하였다. 이러한 발견은 초기 화재 감지 모델 개발에 도움이 될 것으로 기대하며, 미래 시스템을 위한 귀중한 통찰력을 제공할 것이다.   \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                        본 논문은 한국에서 서식하는 매미과 속인 말매미 등 12종의 국내 자생종 매미의 음성 데이터를 활용하여 딥러닝기법을 통해 매미 종을 분류하는 새로운 접근 방식을 제시하였다. 표준화된 데이터 전처리 및 시간에 따른 주파수 변화를 시각적으로 나타내는 그래픽 표현방식인 스펙트로그램(Spectrogram)의 적용을 통해 주파수 변화와 시간적 변화를동시에 시각화하여 데이터의 특성을 파악하고 활용하며 딥러닝 모델인 ResNet34, ResNet50, AlexNet 모델을 적용하였다. 드롭아웃 기법을 적용하여 과적합(Overfitting)을 방지하며, 다양한 학습률(Learning Rates)을 적용하여 모델의학습 및 검증 과정을 최적화하였다. 이러한 접근을 통해 98% 이상의 높은 정확도로 매미 종을 식별을 검증하였다. 본연구는 인공지능 기술인 CNN(Convolutional Neural Network)를 활용하여 생물 다양성 보존과 종 식별의 정확성을높이기 위해 수행하였으며, 음성 데이터 기반의 딥러닝 시스템이 생태학적 연구와 환경 모니터링에 크게 기여할 수 있음을 시사한다. 나아가 본 연구는 생태계 보존 및 관리에 중요한 도구로 활용될 수 있음을 보여주며, 인공지능 기술과 생물분류학을 결합하여 향후 생물 다양성 연구와 환경 보호를 위한 새로운 방법을 제시할 수 있다   \n",
      "19                                                                                                                                                                                                                                                                                                     최근, 컴퓨터 기술이 발달하면서 CNN 기반의 객체 탐지 네트워크와 관련된 연구가 활발히 진행되고 있다. 하지만 많은 수의 CNN은 제한된 메모리와 연산량을 가지는 임베디드 환경에서의 추론을 어렵게 하는 원인이 된다. 이 문제의 대표적인 해결 방법으로 네트워크 가지치기 기법이 있다. 네트워크 가지치기 기법은 중복된 역할을 하는 파라미터를 제거하여 추론 시 요구되는 메모리와 연산량을 감소시켜 임베디드 보드에서의 추론을 용이하게 할 수 있다. 하지만 대부분의 가지치기 기법은 두 단계의 학습을 요구하여 많은 시간과 자원이 소모되고 가지치기에 따른 채널의 관계 변화를 반영하지 못해 최적의 경량 네트워크를 보장할 수 없다. 따라서 본 논문에서는 최적의 경량 네트워크를 얻기 위한 중요도 탐색 기법을 제안하고 가지치기의 과정을 단순화하여 한 단계의 훈련만으로 가지치기가 가능한 중요도 탐색 필터 가지치기 기법을 제안한다. 본 논문에서는 VGG-16과 ResNet-50을 백본 네트워크로 가지는 SSD 네트워크에 가지치기를 적용하고 Jetson Xavier NX에서 추론 속도를 측정하였다. ResNet-50을 이용하는 네트워크에서 실험 결과 mAP(0.5)는 가지치기 비율에 따라 0.5 %, 0.7 %, 1.0 % 감소하였지만, 추론 시간은 12.75 %, 16.03 %, 21.66 % 향상되었다. 또한 학습 시간은 다른 기법보다 최대 43.85 % 빠르며 유사한 가지치기 비율의 네트워크와 비교할 때 높은 성능을 가진다.   \n",
      "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 최근 전장에서의 드론 활용이 정찰뿐만 아니라 화력 지원까지 확장됨에 따라, 드론을 조기에 자동으로 식별하는 기술의 중요성이 더욱 증가하고 있다. 본 연구에서는 드론과 크기 및 외형이 유사한 다른 공중 표적들인 새와 풍선을 구분할 수 있는 효과적인 이미지 분류 모델을 확인하기 위해, 인터넷에서 수집한 3,600장의 이미지 데이터셋을 사용하고, 세 가지 사전 학습된 합성곱 신경망 모델(VGG16, ResNet50, InceptionV3)의 특징 추출기능과 추가 분류기를 결합한 전이 학습 접근 방식을 채택하였다. 즉, 가장 우수한 모델을 확인하기 위해 세 가지 사전 학습된 모델(VGG16, ResNet50, InceptionV3)의 성능을 비교 분석하였으며, 실험 결과 InceptionV3 모델이 99.66%의 최고 정확도를 나타냄을 확인하였다. 본 연구는 기존의 합성곱 신경망 모델과 전이 학습을 활용하여 드론을 식별하는 새로운 시도로써, 드론 식별 기술의 발전에 크게 기여 할 것으로 기대된다.   \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          최근 정적분석 기반의 시그니처 및 패턴 탐지 기술은 고도화되는 IT 기술에 따라 한계점이 드러나고 있다. 이는 여러 아키텍처에 대한 호환 문제와 시그니처 및 패턴 탐지의 본질적인 문제이다. 악성코드는 자신의 정체를 숨기기 위하여 난독화, 패킹 기법 등을 사용하고 있으며 또한, 코드 재정렬, 레지스터 변경, 분기문 추가 등 기존 정적분석 기반의 시그니처 및 패턴 탐지 기법을 회피하고 있다. 이에 본 논문에서는 이러한 문제를 해결할 수 있는 머신러닝을 통한 LLVM IR 코드 이미지 기반 악성코드 정적분석 자동화 기술을 제안한다. 바이너리가 난독화되거나 패킹된 사실에 불구하고 정적 분석 및 최적화를 위한 중간언어인 LLVM IR로 디컴파일한다. 이후 LLVM IR 코드를 이미지로 변환하여 CNN을 이용한 알고리즘 중 전이 학습 및 Keras에서 지원하는 ResNet50v2으로 학습하여 악성코드를 탐지하는 모델을 제시한다.   \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        엣지 장치에서 딥 러닝 기반 추론을 위해 추론 가속기가 탑재되고 있다. 딥 러닝 추론 가속기를 통해 연산 성능과 에너지 효율을 증가시킬 수 있다. 하지만 가속기에 최적화되지 않은 모델 구조와 설정을 사용하면 메모리 접근 등의 오버헤드로 인해 최적 성능을 낼 수 없다. 본 논문에서는 사전 학습된 MobileNet v2, ResNet50 v1 모델을 사용해 NVIDIA Jetson에서 Graphic Processing Unit (GPU)와 Deep Learning Accelerator (DLA)의 추론 성능을 분석하였다. 실험을 통해 DLA에 최적화되지 않은 모델을 실행하면 GPU보다 최대 5.1배 추론 시간이 증가함을 보였다. 특히, 프로파일링을 통해 DLA에서 지원하지 않는 연산을 GPU로 폴백 (fallback)하는 과정의 오버헤드로 추론 시간이 증가함을 보였다.   \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        본 연구는 딥러닝 알고리즘을 이용해 어깨 초음파 영상에서 이두근 건의 정상 및 병변을 구분하고, 영상 속 삼출액 병변을 탐지하는 컴퓨터 자동 진단 성능을 평가하고자 한다. D 병원에서 진료받은 어깨 통증 환자들의 초음 파 영상 증례 260건을 사용하였다. 딥러닝으로서 구분 알고리즘에는 ResNet-50, 탐지 알고리즘에는 DeepLabV3+ 를 적용하였으며 성능 평가 지표로 ROC 곡선, AUC, F1-Score 등을 사용하였다. 결과로 구분 알고리즘에서 정확 도 95%, 정밀도 100%, 재현율 91%, AUC 94%를 탐지 알고리즘에서 전역 정확도 97%, 평균 IOU 85%, F1-Score 66% 등을 나타냈다. 본 논문의 제시 모델을 바탕으로 추가 데이터 획득 및 여러 알고리즘을 적용한다면 임상에서 초음파 자동 진단 시스템으로의 응용이 가능하다고 판단된다.   \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     반려동물의 안구 질환은 늦은 진단과 치료로 인해 실명 등의 중대한 결과를 초래할 수 있다. 이는 반려동물 양육 가정의 증가에 따라 점점 더 중요한 문제로 부상하고 있다. 본 논문은 이 문제에 대응하기 위해 인공지능 기반의 조기 진단과 치료 방향을 제시하는 시스템을 설계하고 구현한다. 제안하는 시스템은 AIHUB의 라벨링된 데이터셋을 활용하여 ResNet과 EfficientNet 모델을 최적화한다. 또한 고정밀 질병 분류가 가능하도록 하여 전문가의 진단을 보조하고, 진료가 어려운 지역의 사용성을 높인다. 결과적으로 제안하는 시스템은 반려동물의 안구 건강을 효과적으로 관리하고 보호자의 부담을 경감할 수 있다. 성능평가를 통해, 제안하는 모델은 반려동물의 안구 질환 분류에서 90% 이상의 높은 정확도를 나타냄을 보인다.   \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                          본 연구는 엑스선 검사 과정에서 환자와 코드를 정확히 확인하지 않아 발생할 수 있는 실수나 오류를 예방하는 것을 목표로 하고 있다. 이를 통해 방사선사의 작업 효율성을 높이고 의료 사고를 방지하며, 합성곱 신경망 기반 이미지 분류 기술을 활용한 실질적인 임상 적용 방안을 제안하고자 한다. 연구는 19,381개의 상지 근골격계 엑스선 이미지를 7개의 영역, 19개의 class로 분류하였으며, 학습, 검증, 평가 세트 비율을 8:1:1로 분할하였다. 딥러닝 모델은 VGG-16, DenseNet-121, ResNet-152v2와 같은 심층 학습 모델을 사용하여 정확도, 정밀도, 재현율, F1 스코어 및 혼동 행렬을 기반으로 모델 성능을 평가하였다. 학습결과 DenseNet-121의 전체 정확도에서 87.77%, 평균 class 정확도에서 98.71%, 정밀도에서 91.78%, 재현율에서 86.93%, F1스코어에서 86.71%를 보였다. 모든지표에서 DenseNet-121이 가장 높은 성능을 보였다. 본 연구는 상지 X선 이미지를 활용한 다양한 심층 학습 모델의 성능을 평가하였으며, 충분한 성능을 보여주었다. 이를 통해 작업 효율성을 높이고 의료 사고 방지가능성을 확인하였다.   \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         이 논문에서는 비휘발성 메모리 기반 In-Memory Computing(IMC)를 활용한 단일 코어/다중 레이어 CNN 가속기의 설계 초기단계에서 가속기의 성능과 면적을 예측하여 최적 메모리 데이터 플로우 및 인터페이스를 탐색할 수 있는 새로운 설계 공간 탐색기를제안한다. 이를 위해 다양한 메모리 레이아웃, 인터페이스, 매핑 방법을 탐색 공간에 포함하였다. 설계 옵션들을 완전 탐색 방식으로탐색하고, 성능과 면적을 예측하여 최적 메모리 데이터 플로우 및 인터페이스를 탐색했다. ResNet-18를 목표 네트워크로 설정하고,제안하는 설계 공간 탐색기를 통해 찾아낸 최적 메모리 데이터 플로우 및 인터페이스는 baseline 대비 면적 효율 측면에서 약 132배 향상이 가능함을 확인했다.   \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             이미지 캡셔닝은 이미지의 특징을 추출하여 이미지를 인식하고 자연어 처리와 결합하여 이미지에 대한 설명을 생성하는 작업이다. 이미지 캡셔닝 결과는 때때로 부자연스러운 텍스트를 생성한다. 이러한 문제의 원인을 정확하게 파악하기 위해 인코더들의 성능을 비교 실험한다. 이미지 캡션 생성 과정은 인코더, 디코더 구조를 가진다. 인코더에서 얻어지는 이미지 특징 추출 결과에 따라 디코더에서 생성되는 텍스트에 많은 영향을 미친다. 그에 따라 CNN 계열의 Resnet50, VGG-16과 트랜스포머 계열의 비전 트랜스포머, 스윈 트랜스포머 인코더의 성능을 비교하여 캡션 생성에 있어서 결정적인 영향을 주는지를 분석한다. 정성 및 정량 평가한 결과를 수치화하고 그래프 및 표로 제시하여 CNN 계열과 트랜스포머 계열의 인코딩 결과를 비교 분석하였다.   \n",
      "41   영상에서 잡음은 시각적인 왜곡이나 불편을 주는 것 외에도 영상 시스템에서 성능 저하를 가져옴으로써 영상에서 잡음제거는 영상처리에서 중요한 전처리 과정이다. 본 논문에서는 영상에서 잡음제거를 위해 GAN 모델에서 파생된 DCGAN 기반의 deformable RED and transfer learning based generative adversarial networks (dRED-TL-GAN)모델을 제안하고자 한다. 제안된 dRED-TL-GAN 모델에서 생성자는 인코더-디코더 구조로 이루어진 deformable RED 구조이고, 판별자는 전이학습 기반 구조이다. 여기서 deformable RED 구조는 인코더의 컨볼루션 층에서 표준 컨볼루션 대신 deformable 컨볼루션을 사용하여 영상의 특징을 고려하였고, 판별자는 ResNet-18 모델을 사용하여 학습 속도가 분류 정확도를 높혔다. 본 논문에서 제안된 dRED-TL-GAN 모델의 성능 평가를 위해 전통적인 Mean 필터, Median 필터와 BM3D 필터, 그리고 기존 딥러닝의 DnCNN 모델, RED-CNN 모델 그리고 DCGAN 모델을 고려하였으며, 다양한 잡음, 즉, 가우시안 잡음 (Caussian noise), 포아송 잡음 (Poisson noise) 그리고 스팩클 잡음 (Speckle noise)으로 훼손된 얼굴 영상을 대상으로 실험하였다. 성능 실험은 정성적인 평가와 정량적인 평가로 구성되며, 정성적인 평가 결과, Mean 필터, Median 필터, 그리고 BM3D 필터를 포함한 공간 필터들은 대체로 잡음이 남아있고, 또한 호릿한 영상을 얻었고, 제안된 dRED-TL-GAN 모델은 다른 딥러닝 모델보다 에지있는 선명한 영상을 얻었다. 또한, 정량적인 평가 척도인 Peak signal-to-noise ratio (PSNR), Mean squared error (MSE) 그리고 Structural similarity index measure (SSIM) 면에서 dRED-TL-GAN 모델은 모든 잡음과 모든 평가 척도에서 가장 좋은 성능 수치를 얻었다.   \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  자율주행 차량에 대한 연구가 활발하게 이뤄지고 있다. 자율주행 차량이 등장함에 따라 기존의 차량과 자율주행 차량이 공존하는 과도기가 올 것이며, 이러한 과도기에는 사고율이 더욱 높아질 것이라 예상된다. 현재 교통사고 발생 시 손해보험협회의 ‘자동차 사고 과실 비율 인정기준’에 따라서 과실 비율을 측정한다. 그러나, 발생한 사고가 어떠한 유형의 사고인지 조사하는 데 소모되는 비용이 매우 크다. 또한 이미 과실 비율 책정이 완료된 사례에 대해서도 재심의를 요구하는 과실 비율 분쟁도 늘어나는 추세이다. 이러한 시간적, 물적 비용을 줄이기 위해 자동으로 과실 비율을 판단하는 딥러닝 모델을 제안하고자 한다. 본 논문에서는  ResNet-18 이미지 분류 모델과 TSN을 통한 비디오 행동 인식을 통해 사고 영상을 바탕으로 과실 비율을 판단하고자 한다. 모델이 상용화된다면, 과실 비율을 측정하는데 소요되는 시간을 획기적으로 단축할 수 있다. 또한 피의자에게 제공할 수 있는 과실 비율에 대한 객관적인 지표가 생기므로 과실 비율 분쟁도 완화될 것으로 기대된다.   \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      스마트 기기의 발달과 고해상도 이미징 기술의 대중화로 인해, 지문이 노출되거나, 화상 회의, 화상 통화 등 고화질 얼굴 사진에서 홍채 정보가 노출되고 있다. 스마트기기의 대중화는 일상적인 디지털 활동에서 무분별한 사진 공유로 인해, 개인의 생체정보인 지문이나 홍채가 노출되어 위조 및 악용될 가능성이 높아지고 있다. 이러한 문제를 해결하기 위해서 본 논문에서는 원본 이미지로부터 생체정보를 보호하고 보안을 강화할 목적으로 CNN의 영역 분할기법을 활용하여 이미지 내 지문과 홍채를 식별 보호하는 방안을 제안한다. 제안된 모델은 입력 이미지에서 지문 및홍채를 식별한 후 해당 영역에 블러 처리를 적용하며, 이를 원본 이미지와 결합하여 보안성을 강화한다. U-Net 및ResNet-34를 백본으로 사용한 모델 구조를 통해 학습 시간 및 성능을 비교한다.   \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                본 논문에서는 고정밀 얼굴 통증 인식을 위한 새로운 가상현실(VR) 기반 급속 안구 운동 검출 알고리즘을 제안한다. 어린이는 종종 예방 접종 및 치과 시술에 대한 두려움과 거부감을 나타내어 치료행위를 어렵게 한다. 그러므로 제안 방법은 VR을 통해 만화를 제공하여 어린이의 주의를 딴 데로 돌리는 한편, 무의식적인 눈의 움직임을 통해 통증 수준을 즉시 평가할 수 있다. 본 시스템은 VR 기술을 진보된 안구 운동 탐지 및 얼굴 표정 인식 알고리즘과 통합하여, 객관적인 통증 평가를 제공한다. 정밀한 안구 추적은 CamShift와 AdaBoost 알고리즘을 통해 구현되며, 통증 분류의 정확도는 ResNet18 및 Swin Transformer 아키텍처를 통합한 얼굴 인식 시스템을 통해 향상된다. 공개적으로 사용이 가능한 데이터 세트을 이용한 실험 결과는 제안된 방법이 통증 인식에서 높은 정확도를 달성하는 데 효과적임을 보여준다. 향후, 다양한 분야에서 VR 기반의 통증 평가 시스템을 적용하여 그 정확도를 높일 것으로 기대된다.   \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                      최근 DNN이 다양한 산업에 확산되면서 IoT 기기 및 엣지 컴퓨팅에 적합한 경량 모델에 관한 연구가 급증하고 있 다. 본 논문에서는 기존에 없던 딥러닝 모델의 파라미터를 1 비트 단위로 조작할 수 있는 자동화 프레임워크를 개발 하며 파라미터 비트와 모델 정확도 사이의 관계를 실험 및 연구한다. 본 연구는 제안된 프레임워크를 사용하여 ImageNet 데이터셋으로 사전 학습된 DNN 모델 중 CNN 모델들의 파라미터를 하위 n-bit를 0, 1 또는 랜덤한 값으로 치환하는 3가지 방법을 통해 각각 정보 손실 발생시키면서 파라미터와 정확도 간의 강인성을 비트 단위로 실험하였다. 주요 모델로는 InceptionV3, InceptionResnetV2, ResNet50, Xception, DenseNet121, Mobile NetV1, MobileNetV2 을 사용하였다. 실험 결과, 성능이 낮은 모델일수록 하위 비트의 정보 손실에 민감하여 성 능이 좋은 모델보다 정확도를 유지하는 비트 수가 적다는 것을 실험적으로 확인했고, 파라미터와 정확도 간의 강인 성이 높다는 것을 확인하였다. 이러한 실험을 바탕으로 모델별 유효 파라미터 비트를 설정하여 파라미터를 줄이며 정확도를 유지할 수 있다.   \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                            생체 인식은 사람의 생체적, 행동적 특징 정보를 특정 장치로 추출하여 본인 여부를 판별하는 기술이다. 생체 인식 분야에서 생체 특성 위조, 복제, 해킹 등 사이버 위협이 증가하고 있다. 이에 대응하여 보안 시스템이 강화되고 복잡해지며, 개인이 사용하기 어려워지고 있다. 이를 위해 다중 생체 인식 모델이 연구되고 있다. 기존 연구들은 특징 융합 방법을 제시하고 있으나, 특징 융합 방법 간의 비교는 부족하다. 이에 본 논문에서는 지문, 얼굴, 홍채 영상을 이용한 다중 생체 인식 모델의 융합 방법을 비교평가했다. 특징 추출을 위해 VGG-16, ResNet-50, EfficientNet-B1, EfficientNet-B4, EfficientNet-B7, Inception-v3를 사용했으며, 특성 융합을 위해 ‘Sensor-Level’, ‘Feature-Level’, ‘Score-Level’, ‘Rank-Level’ 융합 방법을 비교 평가했다. 비교 평가 결과 ‘Feature-Level’ 융합 방법에서 EfficientNet-B7 모델이 98.51%의 정확도를 보이며 높은 안정성을 보였다. 그러나 EfficietnNet-B7모델의 크기가 크기 때문에 생체 특성 융합을 위한 모델 경량화 연구가 필요하다.   \n",
      "50                                                                                                                                                                                                                                                                                                                                    만성 폐쇄성 폐질환은 만성적인 기도 폐쇄를 특징으로 하는 호흡기 질환이다. 만성 폐쇄성폐질환은 초기에 자각 증상이 거의 없어, 대부분 중증 상태로 악화된다. 또한, 인종, 성별, 키,몸무게 등 다양한 요인을 포함한 폐 질환 분류 회귀식은 복잡하고, 정확한 판별을 위해서는지속적인 갱신을 필요로 한다. 따라서 폐질환의 초기 진단이 용이하도록 간편한 휴대형 페기능 검사기를 통해 산출 가능한 볼륨-플로우 그래프 이미지 기반 분류 모델이 요구된다.본 논문에서는 폐질환 조기 진단을 위해 볼륨-플로우 그래프 이미지의 전처리 및 합성곱 신경망 기반 앙상블 딥러닝 모델을 구현하였고, 이를 검증했다. 합성곱 신경망 기반 앙상블 딥러닝 모델은 VGG16, VGG19, ResNet50, 그리고 MobileNet 구조 기반 4개의 모델로 구성되며, 전부 전이학습 및 미세조정하여 사용하였다. 세부적으로는 부족한 수의 학습 데이터를볼륨-플로우 그래프 이미지의 특성을 고려하여 적합한 데이터 증강기법을 적용하였고, 4개의 모델들은 가중치 기반의 간접투표 방식을 사용했다. 최종 앙상블 모델은 단순히 폐질환유무를 판별하는 것이 아닌 정상, 제한성 폐질환, 폐쇄성 폐질환, 그리고 혼합성 폐질환과 같이 총 4개의 클래스로 분류하는 모델임에도 불구하고, 테스트 데이터를 통한 성능은 정확도90.91%, 가중치 평균 정밀도 91.11%, 가중치 평균 재현율 90.91%로 높은 수치를 보였다.   \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                          손동작 인식은 이미지나 비디오 데이터로부터 인간의 동작 및 제스처를 식별하는 행동인식기술의 한 형태이다. 디지털 기술의 발전으로 제품에 스마트 기능이 추가되는 사례가 많아지면서 동작인식의 편리성과 효율성도 강조 되고 있다. 본 연구는 손동작 인식을 시도하기 위한 과정으로, 손동작을 기반으로 클래스를 나누어 각각의 클래스를 분류해내는 비디오 분류 연구를 진행한다. 비디오 영상 자체로 딥러닝 분류를 하게 되면 정확도도 높으며, 이미지를 통한 비디오 분류보다 다양한 분야에서 활용이 가능하다는 장점이 있다. 제시된 알고리즘은 3D CNN(Convolutional neural network)과 LSTM(Long Short-Term Memory)이 결합된 형태로 이루어져 있다. 개발한 3D CNN은 이미지나 비디오의 특징 추출에 주로 사용하는 2D CNN 중 ResNet-18의 구조에서 고안하였다. LSTM은 순차 데이터를 학습, 처리, 분류하는 데 주로 사용되고 있는 RNN(Recurrent Neural Network)중의 한 종류이다. 3D CNN을 통해 비디오의 특징을 추출하고, LSTM을 통해 추출된 특징의 시퀀스를 학습 후 각 비디오 시퀀스를 손동작의 변화를 기준으로 하는 다섯 가지 클래스로 분류하였으며 비디오 분류 결과 정확도 평균 87%를 보여 주었다.   \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "54                                                                                                                                                                                                                                                                                                                                                                       최근 딥러닝 기술은 다양한 분야의 분류 시스템에 활용됨에 따라 점차 딥러닝 모델의 성능을 극대화하기 위한 연구가 활발하게 진행되고 있다. 딥러닝 모델의 성능은 학습 데이터의 양과 품질에 따라 많은 영향을 받으며, 딥러닝 모델은 깊고 복잡하게 설계될수록 더 많은 학습 데이터가 요구된다. 또한 학습 데이터가 부족하거나 클래스 간의 데이터 불균형이 존재할 경우, 과 적합 현상이 발생하며 성능이 저하되는 문제가 발생한다. 음성 및 오디오를 활용하는 분야에서 분류 성능을 높이기 위해서 학습 데이터 확장 및 클래스 간 불균형 문제는 중요한 이슈이며, 이를 해결하기 위한 연구는 반드시 필요하다. 본 논문에서는 2차원 이미지 증강을 위해 고안된 WGAN 모델을 개선하여 1차원 오디오 신호를 효과적으로 증강하는 W2GAN-GP 증강 모델을 제안한다. 원 신호 데이터를 입력 받아 시간-주파수 변환 기법을 이용하여 1차 증강을 수행하고, 제안된 W2GAN-GP 모델을 이용하여 2차 증강을 통한 듀얼 오디오 신호 증강 기법을 제안한다. 또한 제안한 증강기법으로 생성된 오디오 데이터의 유효성을 검증하기 위해서 ResNet50 및 DenseNet 분류 모델을 이용하여 분류 정확도를 측정하였다. 분류 모델을 통한 검증 결과, 증강을 수행하지 않은 경우보다 약 27~30% 의 정확도가 높아진 것을 확인할 수 있었다.   \n",
      "55                                                                                                                                                                                 도로 교통 사고와 교통 위반 행동은 현대 사회에서 급증하는 문제로, 이에 대한 효과적인 대응이 필요하다. 이러한 사고와 위반 행동은 세계적으로 증가하는 추세를 보이며, 그로 인한 사회 및 경제적 영향은 상당히 심각하다. 주로 운전자의 부주의로 발생하는 도로 교통 사고를 예방하기 위해, 딥러닝과 머신러닝을 활용한 시스템이 구축되고 있다. 이전의 연구 들은 주로 운전자의 이미지를 기반으로 한 모델을 사용하여 운전자의 이상행동을 감지하는 데 초점을 맞추었다. 그러나 이러한 기존 연구들은 대부분 컨볼루션 기반의 모델을 사용하여 운전자의 이상행동을 감지하고 분류하는 데 중점을 두고 있다. 컨볼루션 기반 모델은 초기 학습 단계에서 이미지에서 특정 패턴 및 특징을 학습하고, 이를 고정된 크기의 필터로 추출하는 특징이 있다. 이는 다양한 운전 상황에 대한 적응성이 제한된다는 한계가 있다. 따라서 본 논문은 컨볼루션 기반 모델의 한계를 극복하고자, Vision Transformer 모델을 활용한 운전자 이상행동 분류 모델을 구축하였다. 해당 모델의 우수성을 확인하기 위해 기존 연구에서 사용된 ResNet-101, VGG19, Xception, ConvNeXt 등의 모델과 분류 성능 평가 지표를 기반으로 비교 분석을 실시하였다. 비교 분석 결과, Vision Transformer 모델이 기존의 컨볼루션 기반 모델들보다 탁월한 성능을 보여주었다. 이러한 결과는 Vision Transformer의 학습 방식이 다양한 특징 및 패턴을 효과적으로 학습하고 이를 활용할 수 있음을 시사한다. 본 연구는 도로 교통 안전성 향상을 위한 혁신적인 모델의 가능성을 제시하며, 더 나아가 안전 운전 문화의 정착과 사회적 이익을 증진시킬 수 있다.   \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "81                                                                                                                                                                                                                                                                                                                           본 연구는 생활 속 존재하는 다양한 상품들의 명칭을 BERT를 통해 임베딩 벡터화한 다음 이를 기반으로 상품 카테고리 예측을 수행하는 아키텍처에 대한 연구이다. 아키텍처의 성능은 상품 명칭으로부터 임베딩 추출을 수행하는 BERT 모델과, 추출된 임베딩으로 카테고리 예측을 수행하는 분류기에 의해 결정된다. 따라서 본 연구는 우선 상품 명칭 분류에 적합한 BERT 모델을 선정하고, 선정된 BERT 모델에 다양한 분류기를 적용하여 가장 높은 성능을 달성하는 BERT-분류기 조합을 찾고자 하였다. 최초 적합한 BERT 모델 선정에는 단순한 CNN 분류기를 사용하였으며 이를 baseline으로 다른 분류기와 성능을 비교하였다. 아키텍처의 성능은 카테고리 정답에 대한 precision, recall, f1 score, accuracy로 정량화하여 평가하였다. 실험 결과 BERT 측면에서는, Sentence BERT 모델이 비교 대상인 일반 BERT 모델보다 적합함을 확인하였다. 그리고 분류기 측면에서는, Sentence BERT와 CNN으로 구성된 baseline 대비하여 Residual Block이 추가 적용된 분류기가 더 높은 성능을 보였다. 본 연구에 사용된 Sentence BERT 모델의 경우 한국어 데이터가 학습되지 않은 단순 모델로, 향후 추가적 연구를 통해 다양한 한국어 데이터를 학습시켜 Domain Adaptation을 수행할 경우 추가적 성능 향상이 기대된다.   \n",
      "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         최근 교통사고의 주요한 원인 중 하나인 운전자 졸음으로 인한 교통사고를 예방하기 위해 졸음 인식 연구가 활발히 진행되는 중이다. 기존 졸음 인식 시스템은 운전자의 신체적 특징을 이용하여 졸음 상태를 인식하지만 신체 부위 폐 색에 의한 가려짐으로 제한되는 한계가 있다. 본 논문에서는 운전자의 다중 행동 특징을 이용한 SERN(Squeeze and Excitation Resnet Network) 기반 졸음 인식 시스템을 제안한다. 제안한 시스템은 다중 행동적 특징 기반 특징 추출 과정, 데이터의 계층적 레이블링 세분화 과정, SERN 모델에 의한 졸음 인식 과정으로 구성된다. 공개 DB인 NTHU-DDD를 사용한 실험 결과, 제안하는 SERN 모델 기반 운전자 졸음 검출 성능이 기존 네트워크 모델 보다 정확도 1.03% 우수함을 확인했다.   \n",
      "84                                                                                                                                                                                                                                                                                                                                                                       본 연구는 Layer Importance Evaluation을 통해 도출된 화재 감지에 최적화된 딥러닝 아키텍처를 제안한다. 기존의 합성곱 신경망(Convolutional Neural Network, CNN) 기반 화재 감지 시스템의 불필요한 복잡성과 연산을 초래하는 문제점을 해결하기 위해, Layer Importance Evaluation 기법을 통해 가중치 및 활성화 값에 근거한 모델의 내부 레이어의 동작을 분석하고, 화재 감지에 기여도가 높은 레이어를 식별한 뒤, 식별한 레이어만으로 모델을 재구성하여, 기존 모델과의 성능 지표를 비교 분석하였다. Xception, VGG19, ResNet, EfficientNetB5 등 네 가지 전이 학습 모델을 사용하여 화재 데이터를 학습시킨 후, Layer Importance Evaluation기법을 적용하여 각 레이어의 가중치와 활성화 값을 분석한 뒤 기여도가 가장 높은 상위 랭크 레이어들을 선별하여 새로운 모델을 구축하였다. 연구 결과, 구현된 아키텍처는 기존 모델 대비 약 80% 가량 경량화 된 파라미터로도 동등한 성능을 유지하며, 약 3~5배가량 신속한 학습 속도를 가지면서도 기존의 복잡한 전이학습 모델에 비해 정확도, 손실, 혼동행렬 지표에서 동등한 성능을 출력함으로써, 화재 감시 장비의 효율성을 높이는 데 기여할 수 있음을 확인하였다.   \n",
      "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 전자상거래의 성장과 4차산업혁명 기술 발전에 따라 패션 산업에서 인공지능을 접목한 서비스가 활발히 도입되고 있으나 신발 산업은 아직 관련 연구가 깊게 되어 있지 않아 활용 사례 및 데이터셋이 부족하다. 본 논문에서는 웹크롤링으로 운동화, 스니커즈 이미지를 수집하고 디자인 및 제조 관점을 반영하여 신발 스타일과 아웃솔, 갑피에 대해서 라벨링하였다. 구축한 약 2만건의 데이터셋을 대상으로 ResNet, ConvNeXt, ViT, Swin Transfomer를 전이학습하고 각 모델의 결과를 비교하였다. 그 결과 ConvNeXt 모델에서 가장 우수한 결과를 얻었고 과적합을 방지하기 위해 추가로 파인튜닝하여 테스트 데이터셋 대상으로 정확도 87%의 결과를 얻었다. 본 연구를 바탕으로 신발 산업에서 디자인 및 제조 기술에 딥러닝 모델을 활용하여 생산성을 향상시킬 수 있을 것이라 기대한다.   \n",
      "86                                                                                                                                                                                                                                                                                                                                                                       본 연구는 Layer Importance Evaluation을 통해 도출된 화재 감지에 최적화된 딥러닝 아키텍처를 제안한다. 기존의 합성곱 신경망(Convolutional Neural Network, CNN) 기반 화재 감지 시스템의 불필요한 복잡성과 연산을 초래하는 문제점을 해결하기 위해, Layer Importance Evaluation 기법을 통해 가중치 및 활성화 값에 근거한 모델의 내부 레이어의 동작을 분석하고, 화재 감지에 기여도가 높은 레이어를 식별한 뒤, 식별한 레이어만으로 모델을 재구성하여, 기존 모델과의 성능 지표를 비교 분석하였다. Xception, VGG19, ResNet, EfficientNetB5 등 네 가지 전이 학습 모델을 사용하여 화재 데이터를 학습시킨 후, Layer Importance Evaluation기법을 적용하여 각 레이어의 가중치와 활성화 값을 분석한 뒤 기여도가 가장 높은 상위 랭크 레이어들을 선별하여 새로운 모델을 구축하였다. 연구 결과, 구현된 아키텍처는 기존 모델 대비 약 80% 가량 경량화 된 파라미터로도 동등한 성능을 유지하며, 약 3~5배가량 신속한 학습 속도를 가지면서도 기존의 복잡한 전이학습 모델에 비해 정확도, 손실, 혼동행렬 지표에서 동등한 성능을 출력함으로써, 화재 감시 장비의 효율성을 높이는 데 기여할 수 있음을 확인하였다.   \n",
      "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "88                                                                                                                                                                                                                                                                                                                                 상대방에 대한 사전 정보 없이 수집된 레이더 신호의 변조를 식별하는 기술은 전자기전에서 매우 중요한 역할을 수행하며, 이를 통해 획득한 변조 방식에 관한 정보는 전자기전에서 전략 수립과 우위 확보를 위해 활용될 수 있다. 본 논문에서는 딥러닝 모델을 이용하여 37종의 단일 및 복합 변조 레이더 신호를 식별하는 시퀀스 데이터 기반의 레이더 신호 변조 식별 기법을 제안하고 시퀀스 데이터와 딥러닝 모델에 따른 변조 식별 성능을 분석한다. 제안하는 기법은 수신 레이더 신호를 시간 또는 주파수 영역에서 분석하여 시퀀스 데이터를 생성한 후, 이를 딥러닝 모델에 입력하여 변조 방식을 식별한다. 이때, 모델에 입력되는 시퀀스 데이터로 레이더 신호의 실수부 및 허수부로 구성한 시간 영역 데이터와 이산 푸리에 변환의 실수부 및 허수부로 구성한 주파수 영역 데이터를 고려하며, 딥러닝 모델로는 ResNet, DenseNet, Inception-v3를 고려한다. 컴퓨터 모의실험을 통해 주파수 영역 데이터를 모델 입력으로 사용하는 것이 시간 영역 데이터를 사용하는 것보다 변조 식별 성능이 우수함을 보이며, 본 논문에서 고려한 딥러닝 모델이 기존에 제안된 시퀀스 데이터 기반 레이더 변조 식별 딥러닝 모델보다 우수한 변조 식별 성능을 가짐을 확인한다. 또한, 딥러닝 모델별 변조 식별 성능을 비교하여 DenseNet201을 사용하는 경우에 가장 높은 변조 식별 성능을 보임을 입증한다.   \n",
      "89                                                                                                                                                                                                                                                                                                                                                              와이어 하네스는 가전제품, 전기자동차, 자율주행 자동차에서 혈류의 역할을 하는 임의의 전기적회로의 상호 연결을 제공하는 와이어의 그룹으로 정의되며 품질이 가장 중요한 척도이다. 와이어 하네스 품질 불량은 제품 고장, 화재, 인명사고와 직결되기 때문이다. 본 논문은 와이어 하네스 압착공정 결과물인 크림프 하네스의 불량을 판정하기 위해, 다중 시멘틱 세그멘테이션 기법을 활용하는 방법을 제안하였다. 크림프 하네스의 불량 판정 문제는 전처리된 하네스 데이터로부터 연속된 5개 세그먼트들의 높이 및 폭의 길이를 정확히 측정하면 가능한 것으로 판단되었다. 이에 착안하여 대표적인 시멘틱 세그멘테이션 모델인 U-Net을 기반으로 다중 세그먼트 식별에 적합한 AI 모델의 개발을 목표로 하였다. 다중 세그먼트 식별을 위해 U-Net의 인코더 부분을 ResNet 34, EfficientNet B1 및 Mix Transformer B0로 변형한 모델을 제안하였다. 인공지능 모델 개발을 위해, 데이터셋 구축에는 와이어 하네스 제조공장에서 수집된 크림프 하네스 이미지들을 사용하였다. 개발된 다중 시멘틱 세그멘테이션 AI 모델은 테스트 데이터셋에 대해 95.14%의 판별 정확도를 나타내었다. 제안된 방법은 종래의 방법(수작업, 압착센서 측정값 및 규칙 기반의 영상처리)의 단점을 개선한, 균일한 고품질 유지와 인건비 절감이 가능하다.   \n",
      "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            화자인식은 개개인마다 다른 음성 주파수를 분석하여 미리 저장된 음성과 비교해 본인 여부를 판단하는 하나의 기술을 의미한다.딥러닝 기반의 화자인식은 여러 분야에 적용되고 있으며, 펫 로봇도 그 중 하나이다. 하지만 펫 로봇의 하드웨어 성능은 딥러닝 기술의 많은 메모리 공간과 연산에 있어 매우 제한적인 상황이다. 이는 펫 로봇이 사용자와 실시간 상호작용에 있어 해결해야 할 중요한 문제점이다. 딥러닝 모델의 경량화는 위와 같은 문제를 해결하기 위한 하나의 중요한 방법으로 자리하였으며, 최근 많은 연구가진행되고 있다. 이 논문에서는 특정한 명령어 형태인 펫 로봇용 음성 데이터 세트를 구축하고 잔차(Residual)를 활용한 모델들의 결과를 비교해 펫 로봇용 화자인식의 경량화 연구의 결과를 서술하며, 결론에서는 제안한 방법에 대한 결과와 향후 연구방안에 대해서술한다.   \n",
      "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     최근 빈번하게 발생하는 화재를 인공지능 및 머신러닝 기법을 활용하여 조기에 감지하고 효과적인 대응에 초점을 맞춘 연구들이 활발히 진행되고 있다. 화재 영상에서 복잡한 배경 및 환경 요인의 불확실성으로 인해 불꽃 또는 연기 영역을 더욱 정확하게 검출할 수 있는 기법이 요구된다. 본 논문에서는 Attention 기반 BiFPN 구조의 ResNet 알고리즘을 제안한다. 화재영역을 정확하게 검출하기 위하여 어텐션 메커니즘과 척도 불변 BiFPN 구조를 유기적으로 결합하여 객체와 배경정보를 분리함과 동시에 서로간의 연관성 유지를 통해 불필요한 정보를 제거하고, 필요한 정보만을 강조함으로써 불꽃 및 연기 영역검출 결과가 객체 크기와 상관없이 일관된 정확도를 유지하는 방법을 제안한다. 실험을 통해 불꽃은 98%의 정확도와 98.67%의 정밀도, 연기는 96.67%의 정확도와 96.67%의 정밀도의 화재 검출에 대한 평가 결과를 확인하였다.   \n",
      "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     전이 학습은 이미 학습된 딥러닝 모델을 초기 모델로 활용하여, 다른 데이터에서 높은 성능 을 발휘하는 기술이다. 특히, 학습에 사용할 데이터의 질과 양이 충분하지 않을 때 전이 학습은 매우 유용한 것으로 알려져 있기에 높은 성능 안정성과 많은 데이터를 필요로 하는 자율주행 차 인식 분야에 응용될 수 있다. 그러나 많은 선행 연구들은 전이 학습 알고리즘 자체의 성능 향상에 초점을 맞추었다. 본 연구는 기존의 알고리즘 중심 접근 방식에서 벗어나, 여러 데이터 셋의 특징 추출기 출력을 융합한 심층 모델 융합 기법을 전이 학습에 적용하고자 한다. 실험 결과, 이러한 심층 모델 융합 기법이 전이 학습의 성능을 향상시킬 수 있음을 확인했다. 이 결 과는 앞으로 데이터가 부족한 자율주행 자동차 분야에서 전이 학습에 활용되어 물체 인지 성 능의 향상을 달성할 수 있을 것으로 기대된다.   \n",
      "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     None   \n",
      "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
      "101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
      "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
      "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
      "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
      "105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
      "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    None   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              multilingual_abstract  \n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The study of the similarity evaluation and retrieval of patent documents is critical not only for the efficient management of patent literature, but also for the rapid and effective collection of information in industrial and technological fields. Patent drawings visually represent the outcomes of technological advancements and innovations, but have not been given as much importance as texts in the past.This study evaluated the similarity of patent drawings for effective retrieval using the representative deep-learning model, ResNet-50, and the traditional computer vision algorithm, scale-invariant feature transform (SIFT). First, a classification experiment using 10,827 patent drawings was conducted to evaluate the similarity of the visual types, achieving a classification performance with an accuracy exceeding 95%. Second, a retrieval experiment using 5,000 technical drawings was conducted to compare the features of ResNet and SIFT based on their similarity. Finally, the retrieval and matching performances of ResNet and SIFT were evaluated using 50 original data samples and 4,800 augmented data samples created by various forms of editing. ResNet demonstrated an average matching performance of 72.54%, whereas SIFT achieved an average matching performance of 86.71%.The findings reveal that, unlike ResNet-50, which compares similarity using the entire image information, SIFT evaluates similarity based on attribute information, such as key points within the image. Consequently, ResNet is advantageous for identifying visually similar images, whereas SIFT excels in identifying identical images.  \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  White blood cell research spans centuries in the fields of immunology, cell biology, and genetics and has contributed to our understanding of white blood cell function, development, immune responses, and health effects. Based on this, treatment and prevention methods for infectious diseases, cancer, and immune diseases have been developed. In particular, lymphocyte and T cell research has had a revolutionary impact on vaccine development and cancer treatment. White blood cell research has been used to prevent and treat blood-related diseases, and research on cells such as neutrophils, monocytes, and eosinophils has also improved the treatment of autoimmune diseases, allergies, and inflammatory diseases. However, research on the exact function, interaction, and immune regulation of white blood cells is still insufficient. Recent advances in digital technology have made it possible to generate large-scale image data, which is being used to train AI algorithms. In particular, deep learning technology plays an important role in the field of image analysis and is also applied to white blood cell research. This study sought to confirm the actual applicability of the ResNet-50 model, and as a result of checking the model's performance, it was confirmed that when Epoch was set to 30 in the ResNet-50 model, the performance was balanced and stable.  \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       This paper proposes a method to improve the search functionality for website posts and product reviews by using a ResNet-Transformer model in conjunction with the BM25 ranking algorithm. BM25 is a widely used algorithm in text-based search that ranks documents by evaluating their relevance to user queries. However, it has limitations in capturing local features of words and understanding the context of a sentences. To address these issues, this study applies a classification approach that combines the ResNet model, which excels at extracting local features, with the Transformer model, known for its strong contextual understanding, as weights for BM25. Experimental results demonstrate that the proposed method improves the nDCG metric by 9.38% and the aP@5 metric by 11.82% compared to BM25 alone. This suggests that implementing this method in search engines across various websites can provide more accurate results for post and review searches.  \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                To maximize the user immersion and experience in a virtual space environment, visual and acoustic effects must play a pivotal role. However, there are opinions that the quality level of visuals and sound is lower than offline due to a lack of realism online or poor sound quality. It is necessary to provide realistic effects in a virtual environment system to accurately identify the indoor and outdoor space within the virtual space and increase the sense of reality through interaction according to the user movement. To this end, research should be conducted to increase the accuracy of identifying the indoor and outdoor environment in the virtual space. The accuracy of the obtained results can be enhanced by utilizing indoor and outdoor identification methods based on the object count within the Common Objects in Context (COCO) DataSet and the existing algorithm. Therefore, this study solves the problem of the accuracy of virtual space characteristics through the number of objects and the revision of the existing algorithm. This aims to suggest the direction of change for the construction of a new space by learning the characteristics of objects in the future, making them data, and allowing the identification of objects other than the DataSet used.  \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In recent years, there has been active research on emotion recognition based on speech data that can be utilized in various platforms. Despite the significant progress in emotion recognition research based on the Korean language within the country, the main issue remains the lack of Korean language databases. Due to the absence of such data, there are cases where overfitting issues arise in models proposed in previous studies. Therefore, this study proposes a ResNet model using the data augmentation with saturation (DA-S) method to improve the performance of speech emotion recognition using the existing model. In this study, the number of data was increased from 5,596 to 11,192 by applying DA-S with the AI-HUB database. Consequently, the proposed model successfully addressed the overfitting issue, resulting in a 31.76% improvement in the accuracy of speech emotion recognition.Furthermore, experiments were conducted using a total of 11,192 data samples, including both the original data and the data with DA-S applied to demonstrate the effects of data augmentation techniques in transforming and expanding data, as well as performance improvements due to the increase in data volume. The result showed that there was a 23.4% improvement when DA-S was applied.  \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Sleep Apnea is a serious global health issue caused primarily by the obstruction of the upper airway during sleep. The current standard diagnostic method for sleep apnea, Polysomnography (PSG), has limitations such as high cost and complexity. In this study, we propose a non-invasive method to detect sleep apnea by utilizing sound data recorded during sleep. The sound data features were extracted using Mel-Frequency Cepstral Coefficients (MFCC), and classification was performed using a 1D CNN-based ResNet (Residual Network) model. The experimental results show that the proposed model achieved an average accuracy of 97.8%, a recall of 97.7%, a precision of 97.9%, and an AUC of 0.978 through 5-fold cross-validation. Future research will focus on expanding the dataset and experimenting with various deep learning models to further improve the model's performance. This study is expected to contribute to improving the accuracy of sleep apnea detection and the development of an efficient healthcare management system.  \n",
      "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In the field of computer vision, models such as You Look Only Once (YOLO) and ResNet are widely used due to their real-time performance and high accuracy. However, to apply these models in real-world environments, factors such as runtime compatibility, memory usage, computing resources, and real-time conditions must be considered. This study compares the characteristics of three deep model runtimes: ONNX Runtime, TensorRT, and OpenCV DNN, and analyzes their performance on two models. The aim of this paper is to provide criteria for runtime selection for practical applications. The experiments compare runtimes based on the evaluation metrics of time, memory usage, and accuracy for vehicle license plate recognition and classification tasks. The experimental results show that ONNX Runtime excels in complex object detection performance, OpenCV DNN is suitable for environments with limited memory, and TensorRT offers superior execution speed for complex models.  \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               3D modeling is used in various fields such as games, AR, VR, and metaverse. Recently, visualization and computation in 3D space are accelerating due to improvements in the performance of computer hardware, and methods for generating 3D objects are being researched due to advances in GAN technology. The GAN-based network proposed a 3D-VAE-IWGAN method that receives images as input, generates voxels, and learns by introducing a Wasserstein loss function and applying a gradient penalty. GAN can generate multiple models that are not included in training, but there is a problem with artifacts occurring. As another method, a network such as DIB-R, which reduces the cost of 3D label generation through supervised learning in 2D and unsupervised learning in 3D, has been proposed. DIB-R can reduce artifacts, but it is difficult to generate diverse models with an autoencoder-based network. This paper proposes a method of applying residual blocks to VAE-GAN, which combines Variational Autoencoder (VAE) and Generative Adversarial Network (GAN), which improves performance in 3D-VAE-IWGAN, and further improves the image generator and discriminator. We propose a system that extracts many features to generate high-quality images and improve latent space interpolation performance.  The results of index compared to the existing network showed a better result of 116.33% at 137.15 in the chair class, and an improvement of 130.4% at 137.24 in the bed class.  \n",
      "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The purpose of this study is to diagnose diseases in pepper crops using YOLOv8, ResNet50, and Faster R-CNN models and compare their performance. The first model utilizes YOLOv8 for disease diagnosis, the second model uses ResNet50 alone, the third model combines YOLOv8 and ResNet50, and the fourth model uses Faster R-CNN. The performance of each model was evaluated using metrics such as accuracy, precision, recall, and F1-Score. The results show that the combined YOLOv8 and ResNet50 model achieved the highest performance, while the YOLOv8 standalone model also demonstrated high performance.  \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The nuclear medicine imaging can improve the image quality through the application of various interpolation techniques. Additionally, deep learning algorithm, which perform feature extraction between input and label datasets, is widely utilized to improve image quality in nuclear medicine. Thus, the purpose of this study was to confirm the performance of deep learning algorithm according to applied various interpolation methods as input data using the thyroid phantom images. A total of 200 thyroid phantom images, each sized 256 × 256, were acquired at an activity of 37 M㏃ for 1 minute to generate the label images. Interpolation methods including nearest neighbor, bilinear, biquadratic, bicubic, biquartic, and biquintic were applied during both downsampling and upsampling processes. The super-resolution residual network (SRResNet) architecture was implemented with a learning rate of 0.0001 and 300 epochs, using an 8:1:1 ratio for train, validation, and test sets, respectively. The generated output images analyzed using coefficient of variation (COV) and contrast to noise ratio (CNR). Consequently, the SRResNet algorithm, which used low-resolution images generated with the bicubic interpolation method, showed the highest performance. This study demonstrates the importance of selecting appropriate interpolation methods for generating input images to improve the accuracy of the SRResNet algorithm in nuclear medicine thyroid imaging and even in other medical fields for diagnosis.  \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Largescale waste similarity analysis is crucial for automating waste management on a large scale. It involvesconfirming the match between waste discharged from homes and that collected by agencies, which is essentialfor a stable automated system. This paper compares feature extraction methods for similarity measurement,including the scaleinvariant feature transform (SIFT) algorithm with added HSV color features, convolutionalneural networkbased encoders, and a modified 6channel (6CH) ResNet for endtoend learning. The resultsdemonstrate that the 6CH ResNet achieves up to 4.9% higher accuracy than both the basic SIFT method andencoders, as well as the SIFT algorithm with HSV color features. Implementing the 6CH ResNet in automatedwaste management systems can enhance object similarity measurement while using fewer computing resources.  \n",
      "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None  \n",
      "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 This study deals with a method of combining image generation using Semi Supervised Learning based on GAN (Generative Adversarial Network) and image classification using ResNet50. Through this, a new approach was proposed to obtain more accurate and diverse results by integrating image generation and classification.The generator and discriminator are trained to distinguish generated images from actual images, and image classification is performed using ResNet50. In the experimental results, it was confirmed that the quality of the generated images changes depending on the epoch, and through this, we aim to improve the accuracy of industrial accident prediction. In addition, we would like to present an efficient method to improve the quality of image generation and increase the accuracy of image classification through the combination of GAN and ResNet50.  \n",
      "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               This study presents the construction of an AI learning dataset and the prototypical development of a model for detecting anomalous behaviors in residential and public spaces, as part of an effort to enhance security and crime prevention. The AI learning dataset and model were designed to stimulate the advancement of AI technology and the development of AI projects in private companies. During the dataset construction, ResNet50 was modularized to extract features from video frames, and 3D-CNN was used to process skeleton points from JSON files. This data was then labeled according to predefined anomalous behaviors.Furthermore, GridCV was employed to utilize the SVM classifier and GRUs for processing video sequences. The learning performance evaluation of the model demonstrated a continuous improvement in main accuracy and a decreasing trend in detailed loss.. The trained model can predict the category of behavior appearing in a given video sequence. The AI learning dataset and model prototyped in this study provide valuable insights into the practicality of developing anomaly detection functions. It is expected to contribute to the advancement of AI applications in the field of public safety by securing, constructing, and distributing data necessary for AI learning for immediate anomaly detection, crime prevention, and offender apprehension.  \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Natural disasters, such as wildfires, due to climate change are a constant and serious threat to the world and societal safety. Every year, the eastern coastal region of South Korea experiences significant property damage due to wildfires. The imperative development of early fire detection models necessitates overcoming challenges associated with limited image data for training, which elevates the risk of overfitting. To address this, data augmentation techniques, including random rotation (up to 50%), random scaling (up to 20%), and random horizontal and vertical flipping (up to 50%), were employed to augment the training dataset. Performance evaluation indicated that the 6-layer neural network outperformed its 7-layer counterpart, highlighting the impracticality of increasing layer count with a limited dataset. Furthermore, an assessment of deep learning-based CNN models and ResNet50 transfer learning models for fire image classification underscored the superior efficacy of transfer learning. These findings hold promise for advancing early fire detection model development, offering valuable insights for future systems.  \n",
      "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Purpose: In this study, we aimed to classify an implant system by comparing the types of implant thread shapes shown on radiographs using various Convolutional Neural Networks (CNNs), particularly Xception, InceptionV3, ResNet50V2, and ResNet101V2. The accuracy of the CNN based on the implant site was compared.Materials and Methods: A total of 1000 radiographic images, consisting of eight types of implants, were preprocessed by resizing and CLAHE filtering, and then augmented. CNNs were trained and validated for implant thread shape prediction. Grad-CAM was used to visualize class activation maps (CAM) on the implant threads shown within the radiographic image.Results: Averaged over 10 validation folds, each model achieved an AUC of over 0.96: AUC of 0.961 (95% CI 0.952–0.970) with Xception, 0.973 (95% CI 0.966-0.980) with InceptionV3, 0.980 (95% CI 0.974-0.988) with ResNet50V2, and 0.983 (95% CI 0.975-0.992) with ResNet101V2. Accuracy was higher in the posterior region than in the anterior area in all four models. Most CAMs highlighted the implant surface where the threads were present; however, some showed responses in other areas.Conclusion: The CNN models accurately classified implants in all areas of the oral cavity according to the thread shape, using radiographic images.  \n",
      "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Purpose: The current study was used to propose a model for identifying cracks in concrete structures by comparing and analyzing various models using Convolutional Neural Networks (CNNs).Methods: Two CNN-based classification models, VGG-16 and ResNet-50, were developed, compared, and analyzed. A confusion matrix was employed as a performance indicator to evaluate their performance in individual instances.Results: The comparative analysis indicated that ResNet-50 outperformed VGG-16 in performance metrics. Additionally, the inference speed based on test data revealed a significant difference, with ResNet-50 requiring 35 seconds compared to VGG-16's 77 seconds.Conclusion: The ResNet-50 showed excellent performance in confusion matrix-based performance indicators and inference speed. It shows strong potential for practical applications in identifying concrete crack structures in real-world scenarios.  \n",
      "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Violence can be committed anywhere, even in crowded places. It is hence necessary to monitor human activities for public safety. Surveillance cameras can monitor surrounding activities but require human assistance to continuously monitor every incident. Automatic violence detection is needed for early warning and fast response. However, such automation is still challenging because of low video resolution and blind spots. This paper uses ResNet50v2 and the gated recurrent unit (GRU) algorithm to detect violence in the Movies, Hockey, and Crowd video datasets. Spatial features were extracted from each frame sequence of the video using a pretrained model from ResNet50V2, which was then classified using the optimal trained model on the GRU architecture. The experimental results were then compared with wavelet feature extraction methods and classification models, such as the convolutional neural network and long short-term memory. The results show that the proposed combination of ResNet50V2 and GRU is robust and delivers the best performance in terms of accuracy, recall, precision, and F1-score. The use of ResNet50V2 for feature extraction can improve model performance.  \n",
      "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This paper presents a novel approach to classifying cicada species by using deep learning techniques that utilize acoustic data of 12 cicada species found in Korea, including Meimuna opalifera.Standardized data preprocessing and the application of spectrograms, which visually represent frequency changes over time, were used to simultaneously visualize both frequency and temporal changes, allowing for species identification from data characteristics. Deep learning models such as ResNet34, ResNet50, and AlexNet were applied. Dropout techniques were employed to prevent overfitting, and various learning rates were applied to optimize the training and validation processes of the models. The approach successfully identified cicada species with an accuracy of over 98%. This study enhances the accuracy of species identification and conservation of biodiversity by using the artificial intelligence technology of a convolutional neural network. It suggests that deep learning systems based on acoustic data can significantly contribute to ecological research and environmental monitoring. Furthermore, this study has the potential for use as an essential tool in ecosystem conservation and management, combining AI and taxonomy to propose new methods for future biodiversity research and environmental protection.  \n",
      "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In recent years, with the development of computer technology, research on CNN-based object detection networks has been actively conducted. However, a large number of CNNs can make inference difficult in embedded environments with limited memory and computation. A typical solution to this problem is network pruning. Network pruning can facilitate inference on embedded boards by reducing the amount of memory and computation required by removing redundant parameters. However, most pruning methods require two stages of training, which consumes a lot of time and resources, and cannot guarantee an optimal lightweight network because they cannot reflect the changes in channel relationships due to pruning. Therefore, this paper proposes an importance search method to obtain an optimal lightweight network, and simplifies the pruning process to propose an importance search filter pruning method that can be pruned with only one stage of training. In this paper, we apply pruning to the SSD network with VGG-16 and ResNet-50 as the backbone network, and measure the inference speed on Jetson Xavier NX. In the network using ResNet-50, the experimental results showed that mAP(0.5) decreased by 0.5%, 0.7%, and 1.0% depending on the pruning ratio, but inference time improved by 12.75%, 16.03%, and 21.66%. In addition, the learning time is up to 43.85% faster than other methods and has high performance when compared to networks with similar pruning ratios.  \n",
      "20                                                                                                                                                                                                                                                                                                    Objective: Osteoporosis is highly prevalent among older adults and women. This condition leads to a deterioration in bone mineral density and microarchitecture, significantly increasing the risk of fractures. Additionally, osteoporosis commonly results in complications such as screw loosening and non-union during spinal surgery. Deep-learning algorithms have now achieved an accuracy comparable to the current human margin of error. Therefore, this study explored the potential of using transfer learning in deep learning algorithms to predict, diagnose, and screen for osteoporosis using commonly obtained sagittal spine X-rays from patients with spinal conditions.Methods: We retrospectively evaluated 2,300 consecutive patients who underwent dual energy X-ray absorptiometry (DXA) and lumbar sagittal plain X-ray exams between 2013 and 2021. The exclusion criteria included: (1) a gap of more than 1 year between the DXA and X-ray exams; (2) vertebrae that had undergone vertebroplasty; (3) lack of spine anterior-posterior DXA; and (4) images that were unassessable. Ultimately, 256 patients (images) were included in the study. Transfer learning was applied using convolutional neural network (CNN) techniques, specifically visual geometry group (VGG) 16, VGG 19, ResNet50, and Xception.Results: The most accurate CNN model in the training group was ResNet50, with an accuracy of 0.95. ResNet50 showed the best performance, with an accuracy of 0.82, precision of 0.80, recall of 0.86, and F1-score of 0.83. Additionally, its area under the curve (0.76) was higher than that other CNN models. The confusion matrix for ResNet50’s performance displayed the outcomes for images predicted as osteoporosis (n=12) among the test data osteoporosis images (n=14)Conclusion: Artificial intelligence (AI) technology employing deep learning techniques is significantly nearing human capabilities in the role of diagnostic assistance. The diagnosis of osteoporosis using bone mineral density is expected to evolve into a comprehensive diagnostic aid or decision-making tool with the integration of AI in the future.  \n",
      "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Recently, research on lightweight deep learning has been applied to various fields due to issues such as cost reduction, security, and power consumption due to decentralization. The lightweight deep learning model provides distributed processing of data and various services through a mobile environment. In this study, we compare two lightweight facial recognition deep learning models suitable for the mobile environment and propose a more suitable model. MobileFaceNet is a model optimized for deployment in an embedding environment, and we sought to find a more suitable model by comparing it with the ResNet model that has been recently studied. WebFace42M was used as the dataset, and landmarks were extracted using RetinaFace as a face alignment technique, and faces were aligned using opencv's affine transformation. As a result of applying the two models, ResNet-100 showed better performance in the same embedding environment.  \n",
      "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Cervical cancer is the fourth most common cancer in women worldwide, and more than 604,000 new cases were reported in 2020alone, resulting in approximately 341,831 deaths. The Cox regression model is a major model widely adopted in cancer research, butconsidering the existence of nonlinear associations, it faces limitations due to linear assumptions. To address this problem, this paperproposes ResSurvNet, a new model that improves the accuracy of cervical cancer mortality prediction using ResNet's residual learningframework. This model showed accuracy that outperforms the DNN, CPH, CoxLasso, Cox Gradient Boost, and RSF models comparedin this study. As this model showed accuracy that outperformed the DNN, CPH, CoxLasso, Cox Gradient Boost, and RSF models comparedin this study, this excellent predictive performance demonstrates great value in early diagnosis and treatment strategy establishment inthe management of cervical cancer patients and represents significant progress in the field of survival analysis.  \n",
      "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The purpose of this study is to propose a method for evaluating the similarity of Show gardens using Deep Learning models, specifically VGG-16 and ResNet50. A model for judging the similarity of show gardens based on VGG-16 and ResNet50 models was developed, and was referred to as DRG (Deep Recognition of similarity in show Garden design). An algorithm utilizing GAP and Pearson correlation coefficient was employed to construct the model, and the accuracy of similarity was analyzed by comparing the total number of similar images derived at 1st (Top1), 3rd (Top3), and 5th (Top5) ranks with the original images. The image data used for the DRG model consisted of a total of 278 works from the Le Festival International des Jardins de Chaumont-sur-Loire, 27 works from the Seoul International Garden Show, and 17 works from the Korea Garden Show. Image analysis was conducted using the DRG model for both the same group and different groups, resulting in the establishment of guidelines for assessing show garden similarity. First, overall image similarity analysis was best suited for applying data augmentation techniques based on the ResNet50 model. Second, for image analysis focusing on internal structure and outer form, it was effective to apply a certain size filter (16cm × 16cm) to generate images emphasizing form and then compare similarity using the VGG-16 model. It was suggested that an image size of 448 × 448 pixels and the original image in full color are the optimal settings. Based on these research findings, a quantitative method for assessing show gardens is proposed and it is expected to contribute to the continuous development of garden culture through interdisciplinary research moving forward.  \n",
      "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Recent developments in the use of drones on battlefields, extending beyond reconnaissance to firepower support, have greatly increased the importance of technologies for early automatic drone identification. In this study, to identify an effective image classification model that can distinguish drones from other aerial targets of similar size and appearance, such as birds and balloons, we utilized a dataset of 3,600 images collected from the internet. We adopted a transfer learning approach that combines the feature extraction capabilities of three pre-trained convolutional neural network models (VGG16, ResNet50, InceptionV3) with an additional classifier. Specifically, we conducted a comparative analysis of the performance of these three pre-trained models to determine the most effective one. The results showed that the InceptionV3 model achieved the highest accuracy at 99.66%. This research represents a new endeavor in utilizing existing convolutional neural network models and transfer learning for drone identification, which is expected to make a significant contribution to the advancement of drone identification technologies.  \n",
      "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Background: The acquisition of single-lead electrocardiogram (ECG) from mobile devices offers a more practical approach to arrhythmia detection. Using artificial intelligence for atrial fibrillation (AF) identification enhances screening efficiency. However, the potential of singlelead ECG for AF identification during normal sinus rhythm (NSR) remains under-explored.This study introduces a method to identify AF using single-lead mobile ECG during NSR.Methods: We employed three deep learning models: recurrent neural network (RNN), long short-term memory (LSTM), and residual neural networks (ResNet50). From a dataset comprising 13,509 ECGs from 6,719 patients, 10,287 NSR ECGs from 5,170 patients were selected. Single-lead mobile ECGs underwent noise filtering and segmentation into 10-second intervals. A random under-sampling was applied to reduce bias from data imbalance. The final analysis involved 31,767 ECG segments, including 15,157 labeled as masked AF and 16,610 as Healthy.Results: ResNet50 outperformed the other models, achieving a recall of 79.3%, precision of 65.8%, F1-score of 71.9%, accuracy of 70.5%, and an area under the receiver operating characteristic curve (AUC) of 0.79 in identifying AF from NSR ECGs. Comparative performance scores for RNN and LSTM were 0.75 and 0.74, respectively. In an external validation set, ResNet50 attained an F1-score of 64.1%, recall of 68.9%, precision of 60.0%, accuracy of 63.4%, and AUC of 0.68.Conclusion: The deep learning model using single-lead mobile ECG during NSR effectively identified AF at risk in future. However, further research is needed to enhance the performance of deep learning models for clinical application.  \n",
      "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Purpose: The purpose of this study was to classify mandibular molar furcation involvement (FI) in periapical radiographs using a deep learning algorithm.Materials and Methods: Full mouth series taken at East Carolina University School of Dental Medicine from 2011-2023 were screened. Diagnostic-quality mandibular premolar and molar periapical radiographs with healthy or FI mandibular molars were included. The radiographs were cropped into individual molar images, annotated as “healthy” or “FI,” and divided into training, validation, and testing datasets. The images were preprocessed by PyTorch transformations. ResNet-18, a convolutional neural network model, was refined using the PyTorch deep learning framework for the specific imaging classification task. CrossEntropyLoss and the AdamW optimizer were employed for loss function training and optimizing the learning rate, respectively. The images were loaded by PyTorch DataLoader for efficiency. The performance of ResNet-18 algorithm was evaluated with multiple metrics, including training and validation losses, confusion matrix, accuracy, sensitivity, specificity, the receiver operating characteristic (ROC) curve, and the area under the ROC curve.Results: After adequate training, ResNet-18 classified healthy vs. FI molars in the testing set with an accuracy of 96.47%, indicating its suitability for image classification.Conclusion: The deep learning algorithm developed in this study was shown to be promising for classifying mandibular molar FI. It could serve as a valuable supplemental tool for detecting and managing periodontal diseases.  \n",
      "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In this study, we present a novel approach for enhancing chest X-ray image classification (normal, Covid-19, edema, massnodules, and pneumothorax) by combining contrastive learning and machine learning algorithms. A vast amount of unlabeleddata was leveraged to learn representations so that data efficiency is improved as a means of addressing the limited availabilityof labeled data in X-ray images. Our approach involves training classification algorithms using the extracted features from alinear fine-tuned Momentum Contrast (MoCo) model. The MoCo architecture with a Resnet34, Resnet50, or Resnet101backbone is trained to learn features from unlabeled data. Instead of only fine-tuning the linear classifier layer on the MoCopretrainedmodel, we propose training nonlinear classifiers as substitutes for softmax in deep networks. The empirical resultsshow that while the linear fine-tuned ImageNet-pretrained models achieved the highest accuracy of only 82.9% and the linearfine-tuned MoCo-pretrained models an increased highest accuracy of 84.8%, our proposed method offered a significantimprovement and achieved the highest accuracy of 87.9%.  \n",
      "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Recently, research on the convergence of drones and artificial intelligence technologies have been conducted in various industrial fields. In this paper, we propose an illegal parking vehicle recognition model using deep learning-based object recognition and classification algorithms. The model of object recognition and classification consist of YOLOv8 and ResNet18, respectively. The proposed model was trained using image data collected in general road environment, and the trained model showed high accuracy in determining illegal parking. From simulation results, it was confirmed that the proposed model has generalization performance to identify illegal parking vehicles from various images.  \n",
      "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None  \n",
      "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Cu-Cu bonding, one of the key technologies in advanced packaging, enhances semiconductor chip performance, miniaturization, and energy efficiency by facilitating rapid data transfer and low power consumption. However, the quality of the interface bonding can significantly impact overall bond quality, necessitating strategies to quickly detect and classify in-process defects. This study presents a methodology for detecting defects in wafer junction areas from Scanning Acoustic Microscopy images using a ResNet-50 based deep learning model. Additionally, the use of the defect map is proposed to rapidly inspect and categorize defects occurring during the Cu-Cu bonding process, thereby improving yield and productivity in semiconductor manufacturing.  \n",
      "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Blind Spot Detection (BSD) is critical for enhancing vehicle safety and is an integral component of many Advanced Driver Assistance Systems (ADAS). In this work, we propose a novel, cost-effective BSD solution utilizing a deep learning approach with the vehicle’s existing rearview camera. While previous works such as Histogram of Oriented Gradients (HOG) combined with Support Vector Machine (SVM) have been used for BSD, these approaches often struggle to maintain high accuracy, particularly in diverse real-world environments. To address these limitations, we employed ResNet-18, a deep learning network, to improve both recall and precision in detecting vehicles in the blind spot. Our approach was evaluated across multiple hardware platforms, including CPU, GPU, and Field Programmable Gate Arrays (FPGA). We conducted comparative analyses in terms of detection performance, processing speed, energy consumption, and cost efficiency. The results showed that our deep learning-based BSD system achieved a 100% recall rate on all hardware platforms, ensuring no critical events were missed, thereby greatly enhancing safety. Among the platforms, FPGA demonstrated superior energy efficiency. Over time, FPGA emerged as the most cost-effective platform due to its low operational costs. This work contributes to the development of more reliable and efficient BSD systems by leveraging deep learning and identifying the optimal hardware platforms for real-time vehicle detection in ADAS applications.  \n",
      "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Cu-Cu bonding, one of the key technologies in advanced packaging, enhances semiconductor chip performance, miniaturization, and energy efficiency by facilitating rapid data transfer and low power consumption. However, the quality of the interface bonding can significantly impact overall bond quality, necessitating strategies to quickly detect and classify in-process defects. This study presents a methodology for detecting defects in wafer junction areas from Scanning Acoustic Microscopy images using a ResNet-50 based deep learning model. Additionally, the use of the defect map is proposed to rapidly inspect and categorize defects occurring during the Cu-Cu bonding process, thereby improving yield and productivity in semiconductor manufacturing.  \n",
      "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In this research, we propose a Semi-Supervised learning based railroad surface defect detection method. The Resnet50 model, pretrained on ImageNet, was employed for the training. Data without labels are randomly selected, and then labeled to train the ResNet50 model. The trained model is used to predict the results of the remaining unlabeled training data. The predicted values exceeding a certain threshold are selected, sorted in descending order, and added to the training data. Pseudo-labeling is performed based on the class with the highest probability during this process. An experiment was conducted to assess the overall class classification performance based on the initial number of labeled data. The results showed an accuracy of 98% at best with less than 10% labeled training data compared to the overall training data.  \n",
      "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Inference accelerators are currently being utilized for deep learning inference on edge devices. Deep learning inference accelerators can enhance computational performance and energy efficiency. However, it is important to note that optimal performance cannot be achieved if the model structure and settings (e.g. hyperparameters) are not optimized for the accelerator, which can result in overheads, such as frequent memory access. This study analyses the inference performance of the Graphic Processing Unit (GPU) and the Deep Learning Accelerator (DLA) on NVIDIA Jetson with pre-trained MobileNet v2 and ResNet50 v1 models. The results of our experiments show that running non-optimized models on the DLA results in up to 5.1 times longer inference time compared to the GPU. This paper showed through profiling that the increase in inference time is due to the overhead of GPU fallback to perform operations not supported by DLA.  \n",
      "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This study aims to evaluate the computer's automatic diagnosis performance to distinguish normal and lesions of biceps from shoulder ultrasound images using a deep learning algorithm and to detect exudate lesions in the images. 260 cases of ultrasound imaging of shoulder pain patients treated at D hospital were used. As deep learning, ResNet-50 was applied to the classification algorithm and DeepLabV3+ was applied to the detection algorithm, and ROC curves, AUC, and F1-Score were used as performance evaluation indicators. As a result, 95% accuracy, 100% precision, 91% reproduction rate, and 94% AUC in the classification algorithm showed 97% global accuracy, 85% average IOU, and 66% F1-Score in the detection algorithm. Based on the model presented in this paper, it is judged that the automatic ultrasound diagnosis system can be applied in clinical practice if additional data is acquired and several algorithms are applied.  \n",
      "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Pet eye diseases can have serious consequences, including blindness, if not diagnosed and treated promptly. This issue is becoming increasingly important as more households own pets. In this paper, we present a system that uses artificial intelligence to provide early diagnosis and treatment recommendations for pet eye diseases. We use labeled data sets from AIHUB to optimize ResNet and EfficientNet models for diagnosing these diseases. The proposed system helps experts classify diseases with high precision and makes it more accessible in areas with limited medical services. As a result, the system effectively manages and protects the ocular health of cats and dogs, reducing the burden on their caregivers. Performance evaluations demonstrate that the proposed model achieves over 90% accuracy in classifying eye diseases.  \n",
      "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Background: Evaluating embryo quality is crucial for the success of in vitro fertilization procedures. Traditional methods, such as the Gardner grading system, rely on subjective human assessment of morphological features, leading to potential inconsistencies and errors. Artificial intelligence-powered grading systems offer a more objective and consistent approach by reducing human biases and enhancing accuracy and reliability.Methods: We evaluated the performance of five convolutional neural network architectures—EfficientNet-B0, InceptionV3, ResNet18, ResNet50, and VGG16— in grading blastocysts into five quality classes using only embryo images, without incorporating clinical or patient data. Transfer learning was applied to adapt pretrained models to our dataset, and data augmentation techniques were employed to improve model generalizability and address class imbalance.Results: EfficientNet-B0 outperformed the other architectures, achieving the highest accuracy, area under the receiver operating characteristic curve, and F1-score across all evaluation metrics. Gradient-weighted Class Activation Mapping was used to interpret the models’ decision-making processes, revealing that the most successful models predominantly focused on the inner cell mass, a critical determinant of embryo quality.Conclusions: Convolutional neural networks, particularly EfficientNet-B0, can significantly enhance the reliability and consistency of embryo grading in in vitro fertilization procedures by providing objective assessments based solely on embryo images. This approach offers a promising alternative to traditional subjective morphological evaluations.  \n",
      "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This study aims to prevent errors that may occur during the radiography examination process, such as misinterpretation of images, by utilizing artificial intelligence, a core technology of the Fourth Industrial Revolution. Through this, we sought to enhance the work efficiency of radiologic technologists, prevent medical accidents. We labeled 19,381 upper ex- tremity musculo-skeletal X-ray images into 7 regions and 19 classes, and divided them into training, validation, and test sets at a ratio of 8:1:1. We used deep learning models such as VGG-16, DenseNet-121, and ResNet-152v2 to evaluate model performance based on accuracy, precision, recall, F1-score, and confusion matrix. The results showed that DenseNet-121 achieved an overall accuracy of 87.77%, an average class accuracy of 98.71%, a precision of 91.78%, a recall of 86.93%, and an F1 score of 86.71%. DenseNet-121 demonstrated the highest performance across all metrics. This study evaluated the performance of various deep learning models using upper extremity radiographic image and demonstrated sufficient performance. Through this, the potential to improve work efficiency and prevent medical accidents was confirmed.  \n",
      "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           This paper presents a novel Design Space Explorer(DSE) that can predict the performance and area of asingle-core/multi-layer CNN accelerator using non-volatile memory-based In-Memory Computing(IMC) at an earlystage of design to explore the optimal memory data flow and interface. To achieve this, we include variousmemory layouts, interfaces, and mapping methods in the exploration space. Design options were explored in anexhaustive search manner, and the optimal memory data flow and interfaces were explored by predictingperformance and area. Using ResNet-18 as the target network, we found that the optimal memory data flow andinterface found by the proposed DSE can improve the area efficiency by about 132 times compared to thebaseline.  \n",
      "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Image captioning involves extracting features from an image to recognize its content and combining them with natural language processing to generate a description of the image. However, the results of image captioning sometimes generate unnatural text. To accurately identify the cause of this issue, a comparative experiment of various encoders’ performance is conducted. The image caption generation process employs an encoder-decoder architecture. Since the text generated by the decoder is heavily influenced by the results of image feature extraction obtained from the encoder. This study compares the performance of CNN-based encoders, such as ResNet50 and VGG-16, with Transformer-based encoders, like Vision Transformer and Swin Transformer, to analyze whether they have a decisive impact on caption generation. This study quantified the results of the qualitative and quantitative evaluation and presented them in graphs and tables to compare and analyze the encoding performance between CNN-based and Transformer-based models.  \n",
      "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Noise in images not only causes visual distortion or inconvenience, but also reduces performance in the imaging system, so image denoising is an important preprocessing process in image processing. In this paper, we propose a dRED-TL-GAN model based on DCGAN, derived from GAN, to remove noise from images. The generator of the dRED-TL-GAN model is a deformable RED structure consisting of an encoder-decoder structure, and the discriminator is a transfer learning-based structure. Here, the deformable RED structure used deformable convolutin in the encoder’s convolution layer to remove noise by considering the characteristics of the image, and used the ResNet-18 model in the discriminator to increase learning speed and classification accuracy. To evaluate the performance of the proposed dRED-TL-GAN model, traditional filters including Mean filter, Median filter, and BM3D filter, and existing deep learning models including DnCNN model, RED-CNN model, and DCGAN model were considered. An performance experiment was conducted on face images damaged by various noises, namely Gaussian noise, Poisson noise, and Speckle noise. The performance experiment consists of qualitative and quantitative evaluations. First, in the qualitative evaluation, spatial filters including the Mean filter, Median filter, and BM3D filter generally remained noisy and resulted in blurry results, and the propose dRED-TL-GAN model obtained clearer images with edges than other deep learning models. Additionally, in a quantitative evaluation using PSNR, MSE, and SSIM metrics, the dRED-TL-GAN model performs well under all noises considered and on all evaluation metrics.  \n",
      "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The various structures of artificial neural networks, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have been extensively studied and served as the backbone of numerous models. Among these, a transformer architecture has demonstrated its potential for natural language processing and become a subject of in-depth research. Currently, the techniques can be adapted for image processing through the modifications of its internal structure, leading to the development of Vision transformer (ViT) models. The ViTs have shown high accuracy and performance with large data-sets. This study aims to develop a ViT-based model for detecting pneumonia using chest X-ray images and quantitatively evaluate its performance. The various architectures of the ViT-based model were constructed by varying the number of encoder blocks, and different patch sizes were applied for network training. Also, the performance of the ViT-based model was compared to the CNN-based models, such as VGGNet, GoogLeNet, and ResNet. The results showed that the traninig efficiency and accuracy of the ViT-based model depended on the number of encoder blocks and the patch size, and the F1 scores of the ViT-based model ranged from 0.875 to 0.919. The training effeciency of the ViT-based model with a large patch size was superior to the CNN-based models, and the pneumonia detection accuracy of the ViT-based model was higher than that of the VGGNet. In conclusion, the ViT-based model can be potentially used for pneumonia detection using chest X-ray images, and the clinical availability of the ViT-based model would be improved by this study.  \n",
      "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Objectives: Telemedicine is firmly established in the healthcare landscape of many countries. Acute respiratory infections arethe most common reason for telemedicine consultations. A throat examination is important for diagnosing bacterial pharyngitis,but this is challenging for doctors during a telemedicine consultation. A solution could be for patients to upload imagesof their throat to a web application. This study aimed to develop a deep learning model for the automated diagnosis ofexudative pharyngitis. Thereafter, the model will be deployed online. Methods: We used 343 throat images (139 with exudativepharyngitis and 204 without pharyngitis) in the study. ImageDataGenerator was used to augment the training data. Theconvolutional neural network models of MobileNetV3, ResNet50, and EfficientNetB0 were implemented to train the dataset,with hyperparameter tuning. Results: All three models were trained successfully; with successive epochs, the loss and trainingloss decreased, and accuracy and training accuracy increased. The EfficientNetB0 model achieved the highest accuracy(95.5%), compared to MobileNetV3 (82.1%) and ResNet50 (88.1%). The EfficientNetB0 model also achieved high precision(1.00), recall (0.89) and F1-score (0.94). Conclusions: We trained a deep learning model based on EfficientNetB0 that candiagnose exudative pharyngitis. Our model was able to achieve the highest accuracy, at 95.5%, out of all previous studies thatused machine learning for the diagnosis of exudative pharyngitis. We have deployed the model on a web application that canbe used to augment the doctor’s diagnosis of exudative pharyngitis.  \n",
      "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Research on autonomous vehicles is being actively conducted. As autonomous vehicles emerge, there will be a transitional period in which traditional and autonomous vehicles coexist, potentially leading to a higher accident rate. Currently, when a traffic accident occurs, the fault ratio is determined according to the criteria set by the General Insurance Association of Korea. However, the time required to investigate the type of accident is substantial. Additionally, there is an increasing trend in fault ratio disputes, with requests for reconsideration even after the fault ratio has been determined. To reduce these temporal and material costs, we propose a deep learning model that automatically determines fault ratios. In this study, we aimed to determine fault ratios based on accident video through a image classification model based on ResNet-18 and video action recognition using TSN. If this model commercialized, could significantly reduce the time required to measure fault ratios. Moreover, it provides an objective metric for fault ratios that can be offered to the parties involved, potentially alleviating fault ratio disputes.  \n",
      "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            With the development of smart devices and the popularization of high-resolution imaging technology, fingerprints arebeing exposed, and iris information is being exposed in high-definition facial photos such as video conferences and videocalls. The popularization of smart devices has increased the possibility of personal biometric information such asfingerprints and irises being exposed and forged and misused due to indiscriminate photo sharing in everyday digitalactivities. To solve this problem, this paper proposes a method to identify and protect fingerprints and irises in imagesby utilizing CNN's region segmentation technique for the purpose of protecting biometric information from the originalimage and enhancing security. The proposed model identifies fingerprints and irises in the input image, applies blurringto the corresponding regions, and combines them with the original image to enhance security. The learning time andperformance are compared through model structures using U-Net and ResNet-34 as backbones.  \n",
      "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    This paper proposes a novel VR-based rapid eye movement detection algorithm for high-precision facial pain recognition. Children often show fear and resistance to vaccinations and dental procedures, making treatment difficult. Therefore, the proposed method provides cartoons through VR to divert children's attention, while pain levels can be immediately assessed through eye movements. By integrating VR technology with advanced eye movement detection and facial expression recognition algorithms, the system provides objective pain assessment. Precise eye tracking is achieved using CamShift and AdaBoost algorithms, while pain classification accuracy is enhanced through a facial recognition system integrating ResNet18 and Swin Transformer architectures. Experimental results using publicly available datasets demonstrate the effectiveness of the proposed method in achieving high accuracy in pain recognition. In the future, it is expected that the accuracy of the VR-based pain assessment system will be improved by applying it in various fields.  \n",
      "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Recently, with the proliferation of DNNs in various industries, there has been a surge in research on lightweight models suitable for IoT devices and edge computing. In this paper, we propose an automated framework that enables manipulation of deep learning model parameters at a 1-bit level, a capability not previously available. We investigate the relationship between parameter bits and model accuracy. Using the developed framework, we systematically experimented with the parameters of CNN models pre-trained on the ImageNet dataset by setting the lower n-bit to 0, 1, or a random value while each method inducing distinct information loss. The primary models evaluated include InceptionV3, InceptionResnetV2, ResNet50, Xception, DenseNet121, MobileNetV1, and MobileNetV2. Experimental results show that models with lower performance are more sensitive to information loss in the lower bits, requiring fewer bits to maintain accuracy compared to high-performing models. This concludes a high robustness between parameters and accuracy.  \n",
      "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Most object detection methods use a horizontal bounding box that causes problems between adjacent objects with arbitrary directions, resulting in misaligned detection. Hence, the horizontal anchor should be replaced by a rotating anchor to determine oriented bounding boxes. A two-stage process of delineating a horizontal bounding box and then converting it into an oriented bounding box is inefficient. To improve detection, a box-boundary-aware vector can be estimated based on a convolutional neural network. Specifically, we propose a ResNeXt101 encoder to overcome the weaknesses of the conven-tional ResNet, which is less effective as the network depth and complexity increase. Owing to the cardinality of using a homogeneous design and multi-branch architecture with few hyperparameters, ResNeXt captures better information than ResNet. Experimental results demonstrate more accurate and faster oriented object detection of our proposal compared with a baseline, achieving a mean average precision of 89.41% and inference rate of 23.67 fps.  \n",
      "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Biometric recognition is a technology that determines whether a person is identified by extracting information on a person's biometric and behavioral characteristics with a specific device. Cyber threats such as forgery, duplication, and hacking of biometric characteristics are increasing in the field of biometrics. In response, the security system is strengthened and complex, and it is becoming difficult for individuals to use. To this end, multiple biometric models are being studied. Existing studies have suggested feature fusion methods, but comparisons between feature fusion methods are insufficient. Therefore, in this paper, we compared and evaluated the fusion method of multiple biometric models using fingerprint, face, and iris images. VGG-16, ResNet-50, EfficientNet-B1, EfficientNet-B4, EfficientNet-B7, and Inception-v3 were used for feature extraction, and the fusion methods of 'Sensor-Level', 'Feature-Level', 'Score-Level', and 'Rank-Level' were compared and evaluated for feature fusion. As a result of the comparative evaluation, the EfficientNet-B7 model showed 98.51% accuracy and high stability in the 'Feature-Level' fusion method. However, because the EfficietnNet-B7 model is large in size, model lightweight studies are needed for biocharacteristic fusion.  \n",
      "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Chronic Obstructive Pulmonary Disease (COPD) is a respiratory disease characterized bychronic airway obstruction. COPD often progresses to a severe stage, since thereare few noticeable symptoms in the early stages. Also regression equations involingvarious factors such as race, gender, height, and weight to determine whether ornot there is pulmonary disease is complex and needs to be updated periodically.Therefore, there is a demand for a system that can easily analze the presence orabsence of the pulmonary disease, even for non-experts. In this paper, aCNN-based flow volume loops classification model using ensemble learning andappropriate data pre-processing algorithms was proposed and validated to diagnosepulmonary disease in the early stages. The ensemble model was organized by fourCNN models based on VGG16, VGG19, Resnet50, and MobileNet and used transferlearning and fine-tuning for each pre-trained model. Specifically, to overcome asmall amount of data, several data augmentation techniques that took into accountthe characteristics of flow volume loops were used, and soft voting was employedfor the ensemble model. The proposed ensemble model not only could diagnose thepresence or absence of pulmonary disease but could also classify into a total of fourcategories: normal, restrictive, obstructive, and combined pulmonary diseases. As aresult of the experiment, the performance of the proposed ensemble model showedan accuracy of 90.91%, precision of 91.11%, and recall of 90.91%.  \n",
      "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   With the increase in agricultural exports, pest and disease quarantine measures have been strengthened globally. Upon detection of pests or diseases in agricultural products, the entire shipment must be recalled or discarded. Therefore, detecting pests during the post-harvest sorting process is critical. This study aims to identify the optimal deep-learning model for classifying healthy and pest-infested pears during sorting. To achieve this, a dataset was created by collecting images of pest-infested pears under conditions similar to publicly available healthy pear images. The study compares CNN-based models (ResNet, MobileNet, EfficientNet, ConvNext) and a transformer-based model (ViT) using the dataset. Standard learning parameters and data augmentation techniques were also evaluated. Accuracy and Grad-CAM were used to analyze model performance. The results indicate that ResNet101 achieved the best performance based on accuracy and Grad-CAM.  \n",
      "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Hand gesture recognition is a subset of motion recognition technology that identifies human actions from image or video data. With the advancement of digital technology and the increasing integration of smart functions into products, the convenience and efficiency of motion recognition have become more prominent. This research aims to explore the process of hand gesture recognition by classifying video sequences based on hand gestures. The research focuses on video classification using deep learning techniques, which offer higher accuracy and broader applicability compared to image-based video classification. The proposed algorithm combines a 3D Convolutional Neural Network (3D CNN) with a Long Short-Term Memory network (LSTM). The developed 3D CNN is based on the ResNet-18 architecture, which is commonly used for feature extraction in images and videos. The LSTM, an extension of the Recurrent Neural Network (RNN), is employed to learn, process, and classify sequential data. The 3D CNN extracts features from video sequences, and the LSTM learns these feature sequences to classify each video sequence into one of five classes based on variations in hand gestures. The combined network, utilizing 3D CNN for feature extraction and LSTM for sequence learning, provides a robust approach to classify hand gestures in video sequences, demonstrating potential for diverse applications in various fields. The video classification accuracy reached approximately 87%.  \n",
      "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In strawberry cultivation, maturity classification plays an important role in ensuring the efficiency and quality of harvesting. In this study, we propose an Improved Faster R-CNN model to address these challenges, using MobileNetV3-Large as the backbone network to achieve a lightweight model, and introducing RoI Align to improve the spatial accuracy of the feature map. Experiments are conducted using the KGCV_Strawberry dataset, with precision, recall, F1 score, and mean average precision (mAP) measured for performance evaluation. The experimental results show that the proposed model achieves an average precision of 71.35%, recall of 71.07%, and F1 score of 71.21% across all classes. In particular, the proposed model achieves 63% performance on mAP0.5 and 58% performance on mAP0.5:0.95, which is comparable to existing ResNet-based models while achieving faster inference speed. The proposed model achieves a processing speed of 27.6543 ms, which is about 2 ms faster than existing ResNet-based models. This indicates that the goal of creating a lightweight model with improved image processing capability was achieved with minimal performance degradation. This research is expected to contribute to the development of automated strawberry cultivation systems in greenhouse environments and has the potential to be applied to various agricultural environments in the future.  \n",
      "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Recently, as deep learning technology is being utilized in classification systems in various fields, research is being actively conducted to maximize the performance of deep learning models. The performance of deep learning models is greatly affected by the amount and quality of training data, and the deeper and more complex the design of a deep learning model, the more training data is required. Additionally, if training data is insufficient or there is data imbalance between classes, over-fitting occurs and performance deteriorates. In order to improve classification performance in fields that utilize voice and audio, learning data expansion and imbalance between classes are important issues, and research to resolve these issues is essential. In this paper, we propose a W2GAN-GP augmentation model that effectively augments one-dimensional audio signals by improving the WGAN model designed for two-dimensional image augmentation. We propose a dual audio signal augmentation technique by performing the first augmentation on the original signal data using the time-frequency transform technique, and then performing the second augmentation using the proposed W2GAN-GP model. To verify the validity of the audio data generated by the proposed augmentation technique, classification accuracy was measured using ResNet50 and DenseNet classification models. As a result of verification through the classification model, it was confirmed that the accuracy increased by about 27 to 30% compared to the case where augmentation was not performed.  \n",
      "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The surge in road traffic accidents and traffic violations is a pressing issue in modern society, demanding effective responses. These incidents display a global upward trend, with significant societal and economic repercussions. To mitigate road accidents, primarily caused by driver negligence, systems leveraging deep learning and machine learning are being developed. Previous research has predominantly focused on models based on driver images for detecting abnormal driving behavior, with a predominant emphasis on convolutional models. Convolutional models learn specific patterns and features from images during the initial stages of training, extracting them using fixed-size filters, thereby limiting adaptability to diverse driving scenarios. This paper addresses the limitations of convolutional models by introducing a driver abnormal behavior classification model using the Vision Transformer. To validate the superiority of this model, a comparative analysis was conducted with well-established models such as ResNet-101, VGG19, Xception, and ConvNeXt, employing classification performance metrics from previous studies. The results of the comparative analysis demonstrate that the Vision Transformer model outperforms traditional convolutional models. This outcome indicates the effectiveness of Vision Transformer’s learning approach in efficiently capturing and utilizing various features and patterns. This research not only presents the potential for an innovative model to enhance road traffic safety but also pledges to contribute to the establishment of a safety-oriented driving culture and the enhancement of societal benefits.  \n",
      "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Objectives Breast cancer poses a significant global health challenge, characterized by complex origins and the potential for life-threatening metastasis. The critical need for early and accurate detection is underscored by the 685,000 lives claimed by the disease worldwide in 2020. Deep learning has made strides in advancing the prompt diagnosis of breast cancer. However, obstacles persist, such as dealing with high-dimensional data and the risk of overfitting, necessitating fresh approaches to improve accuracy and real-world applicability.Methods In response to these challenges, we propose BCED-Net, which stands for Breast Cancer Ensemble Diagnosis Network. This innovative framework leverages transfer learning and the extreme gradient boosting (XGBoost) classifier on the Breast Cancer RSNA dataset. Our methodology involved feature extraction using pre-trained models—namely, Resnet50, EfficientnetB3, VGG19, Densenet121, and ConvNeXtTiny—followed by the concatenation of the extracted features. Our most promising configuration combined features extracted from deep convolutional neural networks—namely Resnet50, EfficientnetB3, and ConvNeXtTiny—that were classified using the XGBoost classifier.Results The ensemble approach demonstrated strong overall performance with an accuracy of 0.89. The precision, recall, and F1-score values, which were all at 0.86, highlight a balanced trade-off between correctly identified positive instances and the ability to capture all actual positive samples.Conclusion BCED-Net represents a significant leap forward in addressing persistent issues such as the high dimensionality of features and the risk of overfitting.  \n",
      "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None  \n",
      "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Orchid is a kind of terrestrial herb and it has elegant fower posture, quiet fower fragrance, rich colors and noble moral, therefore it has high ornamental value and is deeply loved by people. There are many kinds of orchids, and some of them are similar in shape, texture and color, which make people difcult to quickly and correctly distinguish them. As the existing classifcation model of orchid species have the problems of low accuracy rate and long classifcation time because of the inter species similarities and intra species diferences in orchid species, thus infuencing its wide application. In order to solve the problem above, in this paper, an improved classifcation model based on feature fusion is proposed for orchid species. The achievement of the paper lies in the fact that we successfully developed a classifcation model based on feature fusion to realize the high-efcient classifcation for orchid species. Specifcally, in our scheme, frstly we obtained 12 orchid image sets with number of 12,227 images by network and feld photography; Secondly we analyzed and studied the semantic relationship of diferent scale features from acquired orchid images above; Thirdly we designed an improved classifcation model based on feature fusion on the basis of the semantic relationship above; At last, we used the classifcation model above to realize the high-efcient classifcation for 12 orchid species. The experimental results showed that our proposed classifcation model based on feature fusion in this paper can realize 92.98% classifcation accuracy rate compared with classifcation models without using feature fusion technology, which can greatly improve the classifcation efciency for orchid species.  \n",
      "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None  \n",
      "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Purpose: To develop a deep learning model classifying the laterality of optical coherence tomography (OCT) images.Methods: The study included two-dimensional OCT images (horizontal/vertical macular section) from Seoul National University Hospital.A deep learning model based on ResNet-18 was developed and trained to classify whether OCT images were horizontal or vertical sections and to predict the laterality of the images. Analysis of the results included calculating a mean area under the receiver operating characteristic curve (AUROC) and evaluating accuracy, specificity, and sensitivity. Gradient-weighted class activation for mapping visualization highlighted critical regions for classification.Results: A total of 5,000 eyes of 2,500 patients (10,000 images) was included in the development process. The test dataset consisted of 1,000 eyes of 500 patients (590 eyes without macular abnormalities, 208 epiretinal membranes, 111 age-related macular degenerations, 56 central macular edemas, 23 macular holes, and 12 other macular abnormalities). The deep learning model predicted the OCT section of the eyes in the test dataset with a mean AUROC of 0.9967. The accuracy, sensitivity, and specificity were 0.9835, 0.9870, and 0.9800, respectively.The model predicted the laterality of the eyes in horizontal OCT images with a mean AUROC of 1.0000. The accuracy, sensitivity, and specificity were 0.9970, 1.0000, and 0.9940, respectively. Using vertical OCT images, deep learning models failed to demonstrate any predictive performance in laterality classification.Conclusions: We developed a deep learning model to classify the horizontal/vertical sections of OCT images and predict the laterality of horizontal OCT images with high accuracy, sensitivity, and specificity.  \n",
      "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Early prostate cancer diagnosis by pathologists remains challenging. Recent advances in computer-aided detection (CAD), artificial intelligence (AI), and machine learning (ML) allow prostate cancer grading. This study explored the accuracy of prostate cancer detection by deep learning techniques, particularly convolutional neural networks (CNNs). We performed three-way binary classification based on images cropped to 256 × 256 and 512 × 512 pixels using an ensemble deep CNN model. Six pre-trained CNN models (MobileNet, VGG-16, ResNet-50, DenseNet-121, Inception-V3, and EfficientNet-B0) were integrated to classify histopathological features. The overall accuracy for the combined 256 × 256 and 512×512 pixel images was 94.9%. Additionally, in separate classifications of 256×256 and 512×512 images, we achieved overall accuracies of 90.8% and 94.3%, respectively. Consequently, our method effectively distinguishes benign from malignant samples, approaching near-perfect accuracy.  \n",
      "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The World Health Organization (WHO) has designated the COVID-19 pandemic a global health emergency, prompting responses all over the world. The fatality rate is between 2% and 5%, and millions of people around the world have been infected. While the WHO recommends tests, resource-intensive testing has motivated the development of CNN technology for automated identification. Research employing machine learning models shows great accuracy in classifying X-ray and CT images for COVID-19 detection. These models include denseNet201, resnet50V2, inceptionv3, mobile net, and custom CNNs. The interpretation of chest X-rays has come a long way, yet there are still obstacles to overcome. In this paper, we present a way for using a machine learning model to categorize chest X-ray pictures into normal, COVID-19, viral pneumonia, and lung opacity, demonstrating the model's efficacy in assisting medical diagnosis, especially in time-sensitive situations like COVID-19.  \n",
      "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    This study proposes a soiling detection algorithm to identify and locate contamination in vehicle camera lenses. Research on AI applications that utilize cameras and distance sensors in driving systems is directly related to the advancement of autonomous driving systems., Detecting soiling, such as mud and water droplets, is a particularly critical issue. Traditional methods using piezoelectric, ultrasonic, and thermal sensors can introduce additional design and maintenance complexity. Therefore, this study aims to reduce this complexity by utilizing existing surround-view cameras installed on vehicles without additional sensors. The proposed algorithm employs image-processing techniques and a lightweight neural network architecture based on ResNet18 to detect lens contamination in real-time across various driving environments. Experiments were conducted using 5,000 images from the WoodScape Soiling Dataset. The input images were divided into a 16 × 16 grid and classified into four labels: opaque, semi-transparent, transparent, and clean. The proposed grid-level multiclass soiling classification model demonstrated effective performance in detecting contamination, including mud, water droplets, and foggy dust. This study is expected to enhance the safety and convenience of driving systems.  \n",
      "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In the field of agriculture, conducting research on neural network models for image classification is necessary to accurately categorize crops based on their types and health conditions and distinguish them from other species, to minimize crop losses. This study aimed to compare multiple neural network models to select the optimal model that can classify the images of nine vegetable seedlings, such as carrot, Kimchi cabbage, kohlrabi, lettuce, mallow, mustard, pak-choi, spinach, and sweet pepper. The best model was selected based on its accuracy (precision, recall, and F1 score) from eight trained models, namely DenseNet201, InceptionResNetV2, InceptionV3, MobileNetV2, ResNet152V2, VGG16, VGG19, and Xception. To train the models, a 9-class dataset, 20 epochs, 32 batch sizes, Adam optimizer, and a learning rate of 0.001 were used. The DenseNet201 model exhibited the highest accuracy and was, therefore, selected as the optimal model. With a batch size of 128, Adam optimizer, and a learning rate of 0.001, this model exhibited high precision, recall, and F1 score, and its superiority was confirmed using a confusion matrix. As a result, the DenseNet201 model is expected to improve the recognition performance of the model by using images of various plant species, exploring more networks, and optimizing the hyperparameters to achieve higher recognition accuracy.  \n",
      "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Purpose: Bone quality is one of the most important clinical factors for the primary stability and successful osseointegration of dental implants. This preliminary pilot study aimed to evaluate the clinical applicability of deep learning (DL) for assessing bone quality using panoramic (PA) radiographs compared with an implant surgeon’s subjective tactile sense and cone-beam computed tomography (CBCT) values.Methods: In total, PA images of 2,270 edentulous sites for implant placement were selected, and the corresponding CBCT relative gray value measurements and bone quality classification were performed using 3-dimensional dental image analysis software. Based on the pre-trained and fine-tuned ResNet-50 architecture, the bone quality classification of PA images was classified into 4 levels, from D1 to D4, and Spearman correlation analyses were performed with the implant surgeon’s tactile sense and CBCT values.Results: The classification accuracy of DL was evaluated using a test dataset comprising 454 cropped PA images, and it achieved an area under the receiving characteristic curve of 0.762 (95% confidence interval [CI], 0.714–0.810). Spearman correlation analysis of bone quality showed significant positive correlations with the CBCT classification (r=0.702; 95% CI, 0.651–0.747; P<0.001) and the surgeon’s tactile sense (r=0.658; 95% CI, 0.600–0.708, P<0.001) versus the DL classification.Conclusions: DL classification using PA images showed a significant and consistent correlation with CBCT classification and the surgeon’s tactile sense in classifying the bone quality at the implant placement site. Further research based on high-quality quantitative datasets is essential to increase the reliability and validity of this method for actual clinical applications.  \n",
      "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Purpose: Bone quality is one of the most important clinical factors for the primary stability and successful osseointegration of dental implants. This preliminary pilot study aimed to evaluate the clinical applicability of deep learning (DL) for assessing bone quality using panoramic (PA) radiographs compared with an implant surgeon's subjective tactile sense and cone-beam computed tomography (CBCT) values. Methods: In total, PA images of 2,270 edentulous sites for implant placement were selected, and the corresponding CBCT relative gray value measurements and bone quality classification were performed using 3-dimensional dental image analysis software. Based on the pre-trained and fine-tuned ResNet-50 architecture, the bone quality classification of PA images was classified into 4 levels, from D1 to D4, and Spearman correlation analyses were performed with the implant surgeon's tactile sense and CBCT values. Results: The classification accuracy of DL was evaluated using a test dataset comprising 454 cropped PA images, and it achieved an area under the receiving characteristic curve of 0.762 (95% confidence interval [CI], 0.714-0.810). Spearman correlation analysis of bone quality showed significant positive correlations with the CBCT classification (r=0.702; 95% CI, 0.651-0.747; P<0.001) and the surgeon's tactile sense (r=0.658; 95% CI, 0.600-0.708, P<0.001) versus the DL classification. Conclusions: DL classification using PA images showed a significant and consistent correlation with CBCT classification and the surgeon's tactile sense in classifying the bone quality at the implant placement site. Further research based on high-quality quantitative datasets is essential to increase the reliability and validity of this method for actual clinical applications.  \n",
      "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"The methods of classifying malware family through malware visualization generate malware images and then classify malware family using artificial intelligence models such as Convolutional Neural Networks(CNNs). However, such methods are vulnerable to malware obfuscation techniques. In this paper, we propose a malware classification method based on the Separable Vision Transformer(SepViT) that is robust against obfuscation techniques. The proposed method performs malware family classification using a SepViT model enhanced with Sharpness-Aware Minimization(SAM) after visualizing malware as grayscale images. From the experimental results using the Microsoft Malware Classification Challenge dataset, we show that SAM Optimizer-based SepViT used in the proposed method can classify malware family more accurately than four methods(ResNet18, ViT, CrossViT, SepViT). We also analyze the basis for classification of the proposed method using Grad-Cam. In addition, from the experiments using AndroDex dataset, we show that the proposed method shows good detection performance even in the presence of obfuscation in malware.\"  \n",
      "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this study, a method is proposed for diagnosing the normal operation and faults of an on-board charger (OBC), the battery charging device of a vehicle, using the convolutional neural network algorithm. In the conducted experiments, faults were defined as short and open states of the switching component in the power factor correction (PFC) stage of the OBC topology. To achieve this, the current data of the PFC boost inductor collected through PSIM simulations were reconstructed into images using continuous wavelet transform. In the MATLAB environment, transfer learning was implemented using pre-trained models such as GoogLeNet, ResNet50, ShuffleNet, and MobileNetV2 for diagnosing faults in the OBC. Experimental results revealed that MobileNetV2 achieved a validation accuracy of 95.91%. Subsequently, the performance of the model was analyzed using a confusion matrix on the test set, yielding an accuracy of 96.53%. These findings underscore the effectiveness of the proposed approach in classifying normal and faulty data.  \n",
      "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Optimization of multi-layer, multi-element systems such as deep learning is an NP-hard problem that requires determining the number of layers, the number of elements in a layer, and the types of elements. In such a system, deleting redundant elements to reduce the size while maintaining performance is crucial to conserve resources and improve the efficiency of the system. This is a very complex and challenging problem because it consists of a large number of multi-layered and multi-element systems with a huge search space. Evolutionary computation is widely used for large-scale optimization problems due to its high efficiency, but it is difficult to apply due to the characteristics of evolutionary computation when the calculation of the fitness function is complex. To solve this problem, we propose a technique that dramatically reduces the search space by improving the representation of the gene.We verify its feasibility by applying it to a case study, CNN pruning. We use the ResNet56 model for the CIFAR10 dataset and compare it with existing pruning approaches.  \n",
      "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Purpose: Recently, deep learning techniques have been introduced for age estimation, with automated methods based on radiographic analysis demonstrating high accuracy. In this study, we applied convolutional neural network (CNN) techniques to the lower dentition area on orthopantomograms (OPGs) of children to develop an automated age estimation model and evaluate its accuracy for use in forensic dentistry.Methods: In this study, OPGs of 2,856 subjects aged 3-14 years were analyzed. The You Only Look Once (YOLO) V8 object detection technique was applied to extract the mandibular dentition area on OPGs, designating it as the region of interest (ROI). First, 200 radiographs were randomly selected, and were used to train a model for extracting the ROI. The trained model was then applied to the entire dataset. For the CNN image classification task, 80% of OPGs were allocated to the training set, while the remaining 20% were used as the test set. A transfer learning approach was employed using the ResNet50 and VGG19 backbone models, with an ensemble technique combining these models to improve performance. The mean absolute error (MAE) on the test set was used as the validation metric, and the model with the lowest MAE was selected.Results: In this study, the age estimation model developed using mandibular dentition region from OPGs achieved MAE and root mean squared error (RMSE) values of 0.501 and 0.742, respectively, on the test set, and MAE and RMSE values of 0.273 and 0.354, respectively, on the training set.Conclusions: The automated age estimation model developed in this study demonstrated accuracy comparable to that of previous research and shows potential for applications in forensic investigations. Increasing the sample size and incorporating diverse deep learning techniques are expected to further enhance the accuracy of future age estimation models.  \n",
      "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Recently, there has been a growing emphasis on identifying both known and unknown diseases in plant disease recognition. In this task, a model trained only on images of known classes is required to classify an input image into either one of the known classes or into an unknown class. Consequently, the capability to recognize unknown diseases is critical for model deployment. To enhance this capability, we are considering three factors. Firstly, we propose a new logits-based scoring function for unknown scores. Secondly, initial experiments indicate that a compact feature space is crucial for the effectiveness of logits-based methods, leading us to employ the AM-Softmax loss instead of Cross-entropy loss during training. Thirdly, drawing inspiration from the efficacy of transfer learning, we utilize a large plant-relevant dataset, PlantCLEF2022, for pre-training a model. The experimental results suggest that our method outperforms current algorithms. Specifically, our method achieved a performance of 97.90 CSA, 91.77 AUROC, and 90.63 OSCR with the ResNet50 model and a performance of 98.28 CSA, 92.05 AUROC, and 91.12 OSCR with the ConvNext base model. We believe that our study will contribute to the community.  \n",
      "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Due to the global COVID-19 pandemic, distinct medicines have been devel-oped for treating the coronavirus disease (COVID). However, predicting and identifying potential adverse reactions to these medicines face significant chal-lenges in producing effective COVID medication. Accurate prediction of adverse reactions to COVID medications is crucial for ensuring patient safety and medicine success. Recent advancements in computational models used in pharmaceutical production have opened up new possibilities for detecting such adverse reactions. Due to the urgent need for effective COVID medication development, this research presents a multi-label Inceptionv3 and long short-term memory methodology for COVID (Inceptionv3-LSTM-COV) medicine development. The presented experimental evaluations were conducted using the chemical conformer image of COVID medicine. The features of the chemi-cal conformer are denoted utilizing the RGB color channel, which is extracted using Inceptionv3, GlobalAveragePooling2D, and long short-term memory (LSTM) layers. The results demonstrate that the efficiency of the Inceptionv3-LSTM-COV model outperformed the previous study’s perfor-mance and achieved better results compared to MLCNN-COV, Inceptionv3, ResNet50, MobileNetv2, VGG19, and DenseNet201 models. The proposed model reported the highest accuracy value of 99.19% in predicting adverse reactions to COVID medicine.  \n",
      "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Purpose 2-[18F]FDG PET/CT plays an important role in the management of pulmonary nodules. Convolutional neuralnetworks (CNNs) automatically learn features from images and have the potential to improve the discrimination betweenmalignant and benign pulmonary nodules. The purpose of this study was to develop and validate a CNN model for classificationof pulmonary nodules from 2-[18F]FDG PET images.Methods One hundred thirteen participants were retrospectively selected. One nodule per participant. The 2-[18F]FDG PETimages were preprocessed and annotated with the reference standard. The deep learning experiment entailed random datasplitting in five sets. A test set was held out for evaluation of the final model. Four-fold cross-validation was performed fromthe remaining sets for training and evaluating a set of candidate models and for selecting the final model. Models of threetypes of 3D CNNs architectures were trained from random weight initialization (Stacked 3D CNN, VGG-like and Inceptionv2-like models) both in original and augmented datasets. Transfer learning, from ImageNet with ResNet-50, was also used.Results The final model (Stacked 3D CNN model) obtained an area under the ROC curve of 0.8385 (95% CI: 0.6455–1.0000)in the test set. The model had a sensibility of 80.00%, a specificity of 69.23% and an accuracy of 73.91%, in the test set, foran optimised decision threshold that assigns a higher cost to false negatives.Conclusion A 3D CNN model was effective at distinguishing benign from malignant pulmonary nodules in 2-[18F]FDGPET images.  \n",
      "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            To treat the novel COronaVIrus Disease (COVID), comparatively fewer medicines have been approved. Due to the global pandemic status of COVID, several medicines are being developed to treat patients. The modern COVID medicines development process has various challenges, including predicting and detecting hazardous COVID medicine responses. Moreover, correctly pre-dicting harmful COVID medicine reactions is essential for health safety. Significant developments in computational models in medicine development can make it possible to identify adverse COVID medicine reactions. Since the beginning of the COVID pandemic, there has been significant demand for developing COVID medicines. Therefore, this paper presents the transfer-learning methodology and a multilabel convolutional neural network for COVID (MLCNN-COV) medicines development model to identify negative responses of COVID medicines. For analysis, a framework is proposed with five multilabel transfer-learning models, namely, MobileNetv2, ResNet50, VGG19, DenseNet201, and Inceptionv3, and an MLCNN-COV model is designed with an image augmentation (IA) technique and validated through experiments on the image of three-dimensional chemical conformer of 17 number of COVID medicines. The RGB color channel is utilized to represent the feature of the image, and image features are extracted by employing the Convolution2D and MaxPooling2D layer. The findings of the current MLCNN-COV are promising, and it can identify individual adverse reactions of medicines, with the accuracy ranging from 88.24% to 100%, which outper-formed the transfer-learning model’s performance. It shows that three-dimensional conformers adequately identify negative COVID medicine responses.  \n",
      "75   Purpose: The purpose of this study is to improve the recognition rate of APC (Auto People Counting), which accurately identifies the remaining claimants and provides them to response agencies such as fire departments when a disaster occurs in a disaster-vulnerable facility such as a nursing hospital. Currently, when a disaster occurs, response agencies arrive at the disaster site and ask building officials directly to determine the status of those in need within the building. This may be inaccurate information about the rescuer, which may expand the scope of work of the response agency and pose a risk to the safety of the rescuer. APC automatically counts the number of people entering and leaving the building and provides real-time information on the number of people remaining, making it possible to accurately determine the status of those in need in the event of a disaster. The purpose of this study is to select the optimal artificial intelligence algorithm so that APC can more accurately count the number of people entering.Method: In this study, baseline modeling was performed using a CNN model to improve the algorithm that recognizes images of entering personnel through cameras targeting APCs installed and operated in actual disaster-vulnerable facilities. The study was conducted by analyzing the performance of various algorithms to select the top seven candidates and using a transfer learning model to select the optimal algorithm with the best performance.Research Results: As a result of the experiment, the precision and recall of the Densenet201 and Resnet152v2 models, which had the best time and performance, were confirmed to show 100% accuracy for all labels. Among these, the Densenet201 model showed higher performance.Conclusion: Among various artificial intelligence algorithms, the optimal algorithm that can be applied to APC was selected. This will improve the recognition rate of APC and enable quick and safe rescue operations by accurately identifying the information of rescuers in the event of a disaster. This is expected to contribute to ensuring the safety of rescuers performing rescue operations as well as the safe rescue of rescuers. In the future, additional research on algorithm analysis and learning is required to accurately identify the number of people entering disaster-vulnerable facilities in various disaster situations such as haze.  \n",
      "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Traditional manual identification of crop leaf diseases is challenging. Owing to the limitations in manpower and resources, it is challenging to explore crop diseases on a large scale. The emergence of artificial intelligence technologies, particularly the extensive application of deep learning technologies, is expected to overcome these challenges and greatly improve the accuracy and efficiency of crop disease identification. Crop leaf disease identification models have been designed and trained using large-scale training data, enabling them to predict different categories of diseases from unlabeled crop leaves. However, these models, which possess strong feature representation capabilities, require substantial training data, and there is often a shortage of such datasets in practical farming scenarios. To address this issue and improve the feature learning abilities of models, this study proposes a deep transfer learning adaptation strategy. The novel proposed method aims to transfer the weights and parameters from pre-trained models in similar large-scale training datasets, such as ImageNet. ImageNet pre-trained weights are adopted and fine-tuned with the features of crop leaf diseases to improve prediction ability. In this study, we collected 16,060 crop leaf disease images, spanning 12 categories, for training. The experimental results demonstrate that an impressive accuracy of 98% is achieved using the proposed method on the transferred ResNet-50 model, thereby confirming the effectiveness of our transfer learning approach.  \n",
      "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None  \n",
      "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Arabic Sign Language (ArSL) is used by individuals who are hard of hearing or deaf in Arab countries, as well as others around the world who use it for religious purposes. for the need for automated systems to facilitate the learning and communication of ArSL is therefore significant. Such systems would allow people to learn Arabic Sign Language and use it to communicate among themselves and with the surrounding community. This paper presents the development of an automatic recognition system capable of accurately identifying Arabic signs through hand gestures. In this paper, two Residual Network (ResNet) Configurations, Version 1 (V1) and Version 2 (V2), are proposed and detailed. The proposed ResNet V1 achieved an average accuracy of 98.83%, while ResNet V2 achieved an average accuracy of 98.84%. The results described in this paper far exceed those reported in the extant literature. The high accuracy of the proposed system shows the potential for integrating the system with education tools and assistive technologies for people with special needs.  \n",
      "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The study of human posture is widely applied in physical education teaching, human motion recognition, and other aspects. With the rise of online teaching, the lack of convenient physical education teaching methods has been able to improve. However, due to the complex structure of human body, the study of human posture is a hard problem of consciousness problem in the area of computer vision. This article mainly studies human posture research algorithms based on deep learning. It uses 101-layer network of ResNet to detect the key points of human body in the image and obtains the categories and coordinates of these key points. In this article, a 101-layer network of ResNet model is constructed to fully learn the visual features of key points in human posture. Secondly, the key point location loss function is improved, and the human posture research is realized by using huber loss function instead of mean square error (MSE) loss function. Finally, experimental analysis shows that compared to traditional integral pose regression (IPR) and location adaptive integral pose regression (LAIPR), the use of ResNet based human posture estimation method for human posture recognition improves precision. It has practical significance for physical education teaching applications.  \n",
      "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Tomatoes are rich in nutrients like lycopene, β-carotene, and vitamin C. However, they often suffer from biological and environmental stressors, resulting in significant yield losses. Traditional manual plant health assessments are error-prone and inefficient for large-scale production. To address this need, we collected a comprehensive dataset covering the entire life span of tomato plants, annotated across 5 health states from 1 to 5. Our study introduces an Attention-Enhanced DS-ResNet architecture with Channel-wise attention and Grouped convolution, refined with new training techniques. Our model achieved an overall accuracy of 80.2% using 5-fold cross-validation, showcasing its robustness in precisely classifying the health states of tomato plants  \n",
      "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      This research focuses on an architecture that vectorizes the names of various products found in daily life using BERT, followed by predicting product categories based on these embeddings. The architecture's performance is determined by the BERT model, which extracts embeddings from product names, and the classifier that predicts categories from these embeddings. Consequently, this research initially aimed to identify a BERT model suitable for classifying product names and then find the most efficient combination of BERT model and classifier by applying various classifiers to the chosen BERT model. A simple CNN classifier was employed for the initial selection of a suitable BERT model, serving as a baseline for performance comparison with other classifiers. The architecture's effectiveness was quantified using precision, recall, f1 score, and accuracy for category predictions. Experimental results showed that the Sentence BERT model was more suitable for this task than a conventional BERT model. Additionally, classifiers enhanced with Residual Blocks demonstrated superior performance compared to the baseline combination of Sentence BERT and CNN. The Sentence BERT model used in this study, not trained on Korean data, suggests that further improvements could be achieved through Domain Adaptation by training with diverse Korean datasets.  \n",
      "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this study, we analyzed the effect of motion blur on images used for detecting cracksin concrete tunnel linings on the performance of CNN models. Motion-blurred imageswith intensities ranging from 10 to 50 were generated on the Kaggle and KICT datasets.A semantic segmentation model with ResNet 18, ResNet 34, VGG 11, and Alex-Net as backbones for feature extraction was employed, all pre-trained on the U-Netarchitecture. The performance of these models in crack detection was then assessed.It was observed that detection accuracy decreased across all models as the intensity ofmotion blur increased for each dataset. Within the same model, the F1-score on theKICT dataset showed over 20% higher performance than on the Kaggle dataset. Thisstudy demonstrates that CNN-based crack detection performance is affected by thequality of the image data and that the crack detection accuracy of CNN models canvary depending on the quality of the dataset used in training.  \n",
      "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Recently, driver drowsiness has been one of the major causes of traffic accidents, and study on drowsiness detection has been actively conducted to prevent drowsiness-related accidents. Existing drowsiness detection systems recognize the drowsy state of the driver using the driver's physical features, but they have limitations due to occlusion caused by obstructed body parts. In this paper, we propose a drowsiness detection system based on the squeeze and excitation resnet network (SERN) using the driver's multi-behavioral features. The proposed system consists of a multibehavioral feature extraction process, a hierarchical data labeling refinement process, and a drowsiness detection process using the SERN model. As a result of an experiment using public DB’s NTHU-DDD, it was confirmed that the proposed SERN model based driver drowsiness detection performance was 1.03% better than the existing network model.  \n",
      "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    This study proposes a deep learning architecture optimized for fire detection derived through Layer Importance Evaluation. In order to solve the problem of unnecessary complexity and operation of the existing Convolutional Neural Network (CNN)-based fire detection system, the operation of the inner layer of the model based on the weight and activation values was analyzed through the Layer Importance Evaluation technique, the layer with a high contribution to fire detection was identified, and the model was reconstructed only with the identified layer, and the performance indicators were compared and analyzed with the existing model. After learning the fire data using four transfer learning models: Xception, VGG19, ResNet, and EfficientNetB5, the Layer Importance Evaluation technique was applied to analyze the weight and activation value of each layer, and then a new model was constructed by selecting the top rank layers with the highest contribution. As a result of the study, it was confirmed that the implemented architecture maintains the same performance with parameters that are about 80% lighter than the existing model, and can contribute to increasing the efficiency of fire monitoring equipment by outputting the same performance in accuracy, loss, and confusion matrix indicators compared to conventional complex transfer learning models while having a learning speed of about 3 to 5 times faster.  \n",
      "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                With the growth of e-commerce and the development of the 4th Industrial Revolution technologies, services using AI are being actively introduced in the fashion industry. However, the shoes industry has not yet been studied in-depth on AI and there is a lack of datasets and use cases. In this paper, we collected images of sneakers and running shoes by web crawling and labeled shoe styles, outsoles, and uppers from design and manufacturing perspectives. We trained ResNet, ConvNeXt, ViT and Swin Transformer on our dataset and compared the results of each model. As a result, the ConvNeXt model obtained the best results and was further fine-tuned to prevent overfitting, resulting in 87% accuracy on the test dataset. Based on this study, We expect that deep learning will be used in design and manufacturing process to improve productivity in the shoe industry.  \n",
      "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    This study proposes a deep learning architecture optimized for fire detection derived through Layer Importance Evaluation. In order to solve the problem of unnecessary complexity and operation of the existing Convolutional Neural Network (CNN)-based fire detection system, the operation of the inner layer of the model based on the weight and activation values was analyzed through the Layer Importance Evaluation technique, the layer with a high contribution to fire detection was identified, and the model was reconstructed only with the identified layer, and the performance indicators were compared and analyzed with the existing model. After learning the fire data using four transfer learning models: Xception, VGG19, ResNet, and EfficientNetB5, the Layer Importance Evaluation technique was applied to analyze the weight and activation value of each layer, and then a new model was constructed by selecting the top rank layers with the highest contribution. As a result of the study, it was confirmed that the implemented architecture maintains the same performance with parameters that are about 80% lighter than the existing model, and can contribute to increasing the efficiency of fire monitoring equipment by outputting the same performance in accuracy, loss, and confusion matrix indicators compared to conventional complex transfer learning models while having a learning speed of about 3 to 5 times faster.  \n",
      "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The construction industry is introducing autonomous heavy equipment to overcome labor shortages and improve productivity. For autonomous heavy equipment to work on earthmoving at sites, the equipment needs to recognize and understand ground surface types. However, the ground surface types are manually inspected in practice, and related studies are lacking. To address this issue, the authors developed and tested models that automatically classify ground surface types from images acquired by an unmanned aerial vehicle using a deep learning-based multi-label classification method that applies Binary Relevance (BR) and Label Powerset (LP) methods with Residual Neural Network (ResNet) and Vision Transformer classification network (VIT). The model performances were comparatively evaluated through experiments conducted on actual construction sites. The results showed that the BR model with ResNet is the best model in terms of automated ground surface type identification during earthmoving. The results are expected to broaden the understanding of complex and expansive construction sites for autonomous vehicles and thus facilitate deployment of autonomous heavy equipment by helping them to understand working areas and any obstacles on construction sites quickly and effectively, which will reduce the cost and time needed for on-site ground surface management.  \n",
      "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Identifying the modulation scheme of collected radar signals without prior information plays an important role in electromagnetic warfare, and the information on the modulation scheme obtained through modulation identification of radar signal can be used to establish strategies and secure superiority in electromagnetic warfare. In this paper, using a deep learning model, we propose a sequence data-based modulation identification method for 37 types of single and composite modulated radar signals. In addition, we analyze the modulation identification performance according to the sequence data and the deep learning model. The proposed method generates sequence data by analyzing received radar signals in the time or frequency domain, and then using this data as input to the deep learning model, it identifies the modulation scheme. As sequence data, we consider the time domain data consisting of real and imaginary parts of the radar signal and the frequency domain data consisting of real and imaginary parts of the discrete Fourier transform. For deep learning models, we consider ResNet, DenseNet, and Inception-v3. Through computer simulations, we show that using the frequency domain data as the input of the model, the modulation identification performance is better compared to using the time domain data. It is also confirmed that the deep learning models considered in this paper show better performance than the existing deep learning model of the sequence data-based modulation identification method. Furthermore, by comparing the performance of each deep learning model, we demonstrate that the DenseNet201 exhibits the best modulation identification performance.  \n",
      "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Wire harness is defined as groups of wires providing interconnections for arbitrary electrical circuits that serve as the bloodstream in consumer electronics, electric vehicles, and autonomous cars. Poor wire harness quality is directly related to produce product failures, fires, and human casualties. This paper proposes a method that utilizes multi-class semantic segmentation techniques to determine the defects of the crimp harness, which is the result of a product of the wire harness crimp process. The problem of defect detection of crimp harness can be solved by accurately measuring the height and width of five consecutive segments from the preprocessed harness data. With this insight, we aimed to develop an AI model suitable for multi-segment identification based on U-Net, a representative semantic segmentation model. To identify multiple segments, we proposed a model that modifies the encoder part of U-Net using Resnet 34, EfficientNet B1, and Mix Transformer B0. For AI model development, images of crimp harnesses collected from a wire harness manufacturing plant were used to build the dataset. The developed multi-class semantic segmentation AI model showed a discernment accuracy of 95.14% on the test dataset. Through the method proposed in this paper, it is possible to maintain uniform high quality and reduce labor costs, which improves the shortcomings of the existing crimp harness quality inspection(manual, crimp sensor measurements, and rule-based image processing).  \n",
      "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Drones have found extensive utility in both public and personal places. Consequently, the accurate detection and tracking of drones have emerged as pivotal endeavors in terms of ensuring their optimal performance. This research paper introduces a novel application for discerning the movements of humans and drones from cloud points through the utilization of frequency-modulated continuous wave radar. The dynamic density-based spatial clustering of applications with noise (Dynamic-DBSCAN) algorithm was employed to classify cloud points into separate groups corresponding to the number of objects within the tracking area. Compared to the original DBSCAN algorithm, this method increased accuracy by about 16.8%, achieving an accuracy of up to 93.99%. Subsequently, a trio of deep learning algorithms-long short-term memory, deep neural network, and residual network (ResNet)—were harnessed to ascertain the categorization of each group as either human or drone. According to the results, ResNet achieved the best accuracy rate of 97.72%. Overall, this study underscores the efficacy of the proposed method in accurately and efficiently distinguishing between human and drone entities for effective monitoring and management.  \n",
      "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None  \n",
      "92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Speaker recognition refers to a technology that analyzes voice frequencies that are different for each individualand compares them with pre-stored voices to determine the identity of the person. Deep learning-based speakerrecognition is being applied to many fields, and pet robots are one of them. However, the hardware performance ofpet robots is very limited in terms of the large memory space and calculations of deep learning technology. This isan important problem that pet robots must solve in real-time interaction with users. Lightening deep learning modelshas become an important way to solve the above problems, and a lot of research is being done recently. In thispaper, we describe the results of research on lightweight speaker recognition for pet robots by constructing a voicedata set for pet robots, which is a specific command type, and comparing the results of models using residuals. Inthe conclusion, we present the results of the proposed method and Future research plans are described.  \n",
      "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         This study introduces a model utilizing Few-shot Learning to effectively classify and discern weather-induced damages in crops, focusing on cold and heat damages affecting peaches, apples, and pears. It addresses the challenge of limited data availability in agriculture by leveraging Few-shot Learning, offering a promising solution for data scarcity issues. The model demonstrates robust classification capabilities under constrained data conditions, highlighting the potential of AI and machine learning technologies to tackle significant challenges in modern agriculture related to weather damage. The research suggests avenues for future work, including model performance enhancement, integration with real-time monitoring systems, and broader application across various crops and weather conditions, aiming to contribute to sustainable agriculture and food security.  \n",
      "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None  \n",
      "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 This paper proposes an automatic defect classification (ADC) system to detect and classify bare wafer edge defects that occur during the extreme wafer thinning process required for advanced chip stacking technologies such as TSV and HBM. The proposed system combines a convolutional neural network (ResNet) with traditional image processing techniques (OpenCV-based frequency domain filtering) to effectively classify and visualize wafer edge defects. Experimental results demonstrate that the system achieves high accuracy in defect classification and detection, providing an efficient solution to prevent wafer damage and yield reduction.  \n",
      "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recent research has been actively focused on utilizing artificial intelligence and machine learning techniques to detect fires early and respond effectively, given the frequent occurrences of fires. There is a demand for techniques that can accurately detect flame or smoke areas in fire images due to the uncertainty of complex backgrounds and environmental factors. In this paper, to accurately detect fire areas, we propose a method that seamlessly combines attent ion mechanisms and scale-invariant Bidirectional Feature Pyramid Network(BiFPN) structures to separate object and background information while maintaining their correlations. It removes unnecessary information and emphasizes only the necessary information to maintain consistent accuracy in flame and smoke areas regardless of their sizes. Evaluation results showed that fires could be detected with 98% accuracy and 98.67% precision for flames and 96.67% accuracy and 96.67% precision for smoke.  \n",
      "97                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Transfer learning is a technique that leverages a deep learning model trained on a specific dataset as an initial model that allows fast training of a high-performing model on another dataset. Because a pre-trained model already learns how to extract the features from previously trained data, it allows for faster and better performance on new datasets compared to models that are initialized randomly. Transfer learning is particularly useful when the quality and quantity of the data to be learned are insufficient. On the other hand, most studies focused on improving the performance of transfer learning algorithms themselves. This study departs from the existing algorithm-centric research approach and aims to incorporate deep model fusion techniques that combine the outputs of feature extractors from different datasets into transfer learning. These experiments show that the application of deep model fusion improves the performance of transfer learning. These findings will be applicable to transfer learning in various domains with limited data.  \n",
      "98                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Cancer is one of the most common health problems aff ecting individuals worldwide. In the fi eld of biomedical engineering, one of the main methods for cancer diagnosis is the analysis of histological images of tissue structures and cell nuclei using artifi cial intelligence. Here, we compared the performance of 15 deep learning methods viz: UNet, Deep-UNet, UNet-CBAM, RA-UNet, SA-Unet and Nuclei-SegNet, UNet-VGG2016, UNet-Resnet-101, TransResUNet, Inception-UNet, Att-UNet++ , FF-UNet, Att-UNet, Res-UNet and a new model, DanNucNet, in pathological nuclei segmentation on tissue slices from different organs on fi ve open datasets: MoNuSeg, CoNSeP, CryoNuSeg, Data Science Bowl, and NuInsSeg. Before training on the data, the pixel intensity and color distribution were analyzed, and diff erent augmentation techniques were applied. The results showed that the UNet-based model with 34.57 million Deep-UNet parameters performed the best, outperforming all models in terms of the Dice coeffi cient from 3.13 to 22.91%. The implementation of Deep-UNet in this context provides a valuable tool for accurate extraction of cancer cell nuclei from histological images, which in turn will contribute to further developments in cancer pathology and digital histology.  \n",
      "99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     This paper proposes a fruit stem detection system using Anchor-Free-based 3D object detection model for fruit stem removal. The fruit stem is an important part that affects the hygiene, food safety, quality, and freshness of fruits. It can save manpower and time by automating existing manual-dependent tap removal operations, and increase efficiency in related agricultural fields. The FCAF3D model is a 3D object detection algorithm that predicts the skin and stem of the fruit, respectively, showing high detection performance even in the small size of the fruit stem. The network structure of the model consists of ResNet, GSDN, and FCOS networks, which handle various object scales through the FPN structure. In this paper, model training was performed using apples as an example, and the learned model showed high accuracy in the apple dataset, and the bounding box coordinate value of the test results can be used in the fruit stem removal system.Experimental results showed that the FCAF3D model showed high performance in detecting fruit stem.  \n",
      "100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Modern agriculture leverages various smart farming technologies to manage crops efficiently, but ensuring optimal growth for each crop remains challenging. This paper proposes a novel approach that integrates an image classification model with a time series analysis model to address this issue. The proposed system uses EfficientNet to classify crop diseases and LSTM (Long Short-Term Memory) to analyze time series data to predict the progression of the disease based on its presence. The model performance was evaluated using Accuracy, Precision, Recall, and F1 Score metrics. The image classification model achieved 91% Precision, 92% Recall, 97% Accuracy, and 94% F1 Score, outperforming ResNet, DenseNet, and SENet. Additionally, the model's reliability was verified using Grad-CAM (Gradient-weighted Class Activation Map), an XAI technique. The time series analysis model demonstrated a Recall of 88% and Precision and F1 Score of 86%. The proposed system is accessible via a web page, allowing easy access for users, and includes features for writing posts and comments to facilitate a user community. This system aims to advance smart farming technology, increase crop yields, and minimize agricultural losses, ultimately contributing to improved crop management efficiency.  \n",
      "101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Head gesture is a natural and non-verbal communication method for human-computer and human-robot interaction, conveying attitudes and intentions. However, the existing vision-based recognition methods cannot meet the precision and robustness of interaction requirements. Due to the limited computational resources, applying most high-accuracy methods to mobile and onboard devices is challenging. Moreover, the wearable device-based approach is inconvenient and expensive. To deal with these problems, an end-to-end two-stream fusion network named TSIR3D is proposed to identify head gestures from videos for analyzing human attitudes and intentions. Inspired by Inception and ResNet architecture, the width and depth of the network are increased to capture motion features sufficiently. Meanwhile, convolutional kernels are expanded from the spatial domain to the spatiotemporal domain for temporal feature extraction. The fusion position of the two-stream channel is explored under an accuracy/complexity trade-off to a certain extent. Furthermore, a dynamic head gesture dataset named DHG and a behavior tree are designed for human-robot interaction. Experimental results show that the proposed method has advantages in real-time performance on the remote server or the onboard computer. Furthermore, its accuracy on the DHG can surpass most state-of-the-art vision-based methods and is even better than most previous approaches based on head-mounted sensors. Finally, TSIR3D is applied on Pepper Robot equipped with Jetson TX2.  \n",
      "102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Remote sensing applications play a vital role in various areas such as urban planning, agriculture, and environmental monitoring. Remote sensing image segmentation, in particular, is a prominent domain that aims to address the challenges in these applications. Deep learning has significantly improved the efficiency and accuracy of remote sensing image segmentation by automating the identification of regions of interest. However, most existing methods struggle with capturing both global and local information in the images, which is crucial for accurate pixel classification. To overcome this limitation, this paper presents an enhanced version of the U-Net architecture that incorporates the InceptionResNetV2-Attention based encoder. This proposed method effectively combines the strengths of the Inception and ResNet architectures, along with the attention mechanism. The efficacy of the proposed network is verified using two publicly available datasets. The Semantic Drone Dataset consists of satellite images, while the NITRDrone dataset comprises images captured from Unmanned Aerial Vehicles (UAVs). The results demonstrate that the proposed architecture performs well on imagery obtained from different platforms, achieving  a dice-coefficient of 85.04% and 88.70% for each dataset respectively, outperforming other networks.  \n",
      "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Highway agencies and practitioners expect to have the most efficient method with adequate accuracy when choosing a deep learning-based model for pavement crack classification. However, many works are implemented on their own dataset, making them hard to compare with each other, and also less persuasive and robust. Therefore, a Road Cracks Classification Dataset is proposed to serve as a standard and open-source dataset. Based on this dataset, a benchmark study of fourteen deep learning classification methods is evaluated. Two parameters, the Ratio of F1 and Training Time (RFT) and Ratio of F1 and Prediction Time (RFP), are proposed to quantify the efficiency of networks. The results show that ConvNeXt_base reaches the highest accuracy among all models but requires the longest training time. AlexNet takes the least training time among all models, but gains the lowest accuracy. Of the four crack types, the block crack has the lowest accuracy, which means it is the most difficult to detect. SqueezeNet1_0 has the highest efficiency among all models in converting the computing power to accuracy. Wide ResNet 50_2 consumes the longest prediction time among CNN models, while the ConvNeXt_base has the highest feasibility on real-time tasks. To implement a suitable deep learning-based pavement crack inspection, we recommend a good balance between computational cost and accuracy. Based on this, we provide practical recommendations according to different user groups.  \n",
      "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Background:Optimal sedation assessment in critically ill children remains challenging due to the subjective nature of behavioral scales and intermittent evaluation schedules. This study aimed to develop a deep learning model based on heart rate variability (HRV) parameters and vital signs to predict effective and safe sedation levels in pediatric patients.Methods: This retrospective cross-sectional study was conducted in a pediatric intensive care unit at a tertiary children’s hospital. We developed deep learning models incorporating HRV parameters extracted from electrocardiogram waveforms and vital signs to predict Richmond Agitation-Sedation Scale (RASS) scores. Model performance was evaluated using the area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). The data were split into training, validation, and test sets (6:2:2), and the models were developed using a 1D ResNet architecture.Results: Analysis of 4,193 feature sets from 324 patients achieved excellent discrimination ability, with AUROC values of 0.867, 0.868, 0.858, 0.851, and 0.811 for whole number RASS thresholds of −5 to −1, respectively. AUPRC values ranged from 0.928 to 0.623, showing superior performance in deeper sedation levels. The HRV metric SDANN2 showed the highest feature importance, followed by systolic blood pressure and heart rate.Conclusions: A combination of HRV parameters and vital signs can effectively predict sedation levels in pediatric patients, offering the potential for automated and continuous sedation monitoring in pediatric intensive care settings. Future multi-center validation studies are needed to establish broader applicability.  \n",
      "105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              This paper focuses on detecting Alzheimer’s Disease (AD). The most usual form of dementia is Alzheimer's disease, which causes permanent cause memory cell damage. Alzheimer's disease, a neurodegenerative disease, increases slowly over time. For this matter, early detection of Alzheimer's disease is important.  The purpose of this work is using Magnetic Resonance Imaging (MRI) to diagnose AD.  A Convolution Neural Network (CNN) model, Reset, and VGG the pre-trained learning models are used.  Performing analysis and validation of layers affects the effectiveness of the model. T1-weighted MRI images are taken for preprocessing from ADNI. The Dataset images are taken from the Alzheimer's Disease Neuroimaging Initiative (ADNI). 3D MRI scans into 2D image slices shows the optimization method in the training process while achieving 96% and 94% accuracy in VGG 16 and ResNet 18 respectively. This study aims to classify AD from brain 3D MRI images and obtain better results.  \n",
      "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In the last 10 years, AI has made rapid progress, and image classification, in particular, are showing excellentperformance based on deep learning. Nevertheless, due to the nature of deep learning represented by a black box,it is difficult to actually use it in critical decision-making situations such as national defense, autonomous driving,medical care, and finance due to the lack of explainability of judgement results. In order to overcome theselimitations, in this study, a model description algorithm capable of local interpretation was applied to the inceptionnetwork-derived AI to analyze what grounds they made when classifying national defense data. Specifically, weconduct a comparative analysis of explainability based on confidence values by performing LIME analysis from theInception v2_resnet model and verify the similarity between human interpretations and LIME explanations.Furthermore, by comparing the LIME explanation results through the Top1 output results for Inception v3,Inception v2_resnet, and Xception models, we confirm the feasibility of comparing the efficiency and availabilityof deep learning networks using XAI.  \n",
      "\n",
      "=== 주제어 빈도수 ===\n",
      "                                                                                                                            keyword  \\\n",
      "43                                                                                                                    Deep learning   \n",
      "16                                                                                                                    Deep Learning   \n",
      "96                                                                                                                              CNN   \n",
      "24                                                                                                                           ResNet   \n",
      "12                                                                                                                              딥러닝   \n",
      "67                                                                                                                         ResNet50   \n",
      "52                                                                                                             Image Classification   \n",
      "103                                                                                                                  Classification   \n",
      "141                                                                                                                         합성곱 신경망   \n",
      "107                                                                                                                   deep learning   \n",
      "137                                                                                                            Image classification   \n",
      "15                                                                                                          Artificial Intelligence   \n",
      "66                                                                                                                           YOLOv8   \n",
      "193                                                                                                    convolutional neural network   \n",
      "11                                                                                                                             인공지능   \n",
      "215                                                                                                                               .   \n",
      "3                                                                                                                         ResNet-50   \n",
      "86                                                                                                          Artificial intelligence   \n",
      "51                                                                                                                 Object Detection   \n",
      "49                                                                                                                           이미지 분류   \n",
      "229                                                                                                                 Computer vision   \n",
      "57                                                                                                                              GAN   \n",
      "100                                                                                                   Convolutional Neural Networks   \n",
      "101                                                                                                               Transfer Learning   \n",
      "102                                                                                                   Convolutional neural networks   \n",
      "39                                                                                                                Data Augmentation   \n",
      "275                                                                                                                            LSTM   \n",
      "25                                                                                                                      Transformer   \n",
      "391                                                                                                                        F1 score   \n",
      "327                                                                                                    Separable Vision Transformer   \n",
      "92                                                                                                                              GRU   \n",
      "91                                                                                                                           GridCV   \n",
      "288                                                                                                              Vision Transformer   \n",
      "90                                                                                                                           3D-CNN   \n",
      "59                                                                                                                           스마트 농업   \n",
      "60                                                                                                                         작물 질병 진단   \n",
      "61                                                                                                                             욜로v8   \n",
      "62                                                                                                                            레스넷50   \n",
      "63                                                                                                                        딥러닝 모델 비교   \n",
      "68                                                                                                   Deep Learning Model Comparison   \n",
      "64                                                                                                                Smart Agriculture   \n",
      "65                                                                                                           Crop Disease Diagnosis   \n",
      "344                                                                                                      COVID medicine development   \n",
      "85                                                                                                                   Neural Network   \n",
      "203                                                                                                    Convolutional Neural Network   \n",
      "322                                                                                                                    Bone density   \n",
      "387                                                                                              CNN (convolutional neural network)   \n",
      "323                                                                                                                  Dental implant   \n",
      "324                                                                                                              Dental radiography   \n",
      "77                                                                                               Convolutional Neural Network (CNN)   \n",
      "435                                                                                                                          Resnet   \n",
      "436                                                                                                                     Lightweight   \n",
      "160                                                                                                                   Cu-Cu Bonding   \n",
      "140                                                                                                                           전이 학습   \n",
      "30                                                                                                                           YOLOv5   \n",
      "397                                                                                                                            SERN   \n",
      "106                                                                                                      Convolution Neural Network   \n",
      "4                                                                                                                              SIFT   \n",
      "335                                                                                                               transfer learning   \n",
      "36                                                                                                                              정확도   \n",
      "97                                                                                                                             전이학습   \n",
      "384                                                                                                                   Sentence BERT   \n",
      "151                                                                                                               Chest X-ray image   \n",
      "444                                                                                                                    EfficientNet   \n",
      "377                                                                                                       Attention-Enhanced ResNet   \n",
      "162                                                                                                                      Defect Map   \n",
      "23                                                                                                                             BM25   \n",
      "161                                                                                                                Defect Detection   \n",
      "136                                                                                                               Transfer learning   \n",
      "434                                                                                                             Speaker recognition   \n",
      "134                                                                                                            Deep Neural Networks   \n",
      "98                                                                                                                           데이터 증강   \n",
      "345                                                                                                                     Inceptionv3   \n",
      "376                                                                                                 Depthwise Separable Convolution   \n",
      "346                                                                                                                     multi-label   \n",
      "378                                                                                               Plant Health State Classification   \n",
      "342                                                                                                     unknown disease recognition   \n",
      "341                                                                                                       plant disease recognition   \n",
      "347  Convolutional neural networks · Positron emission tomography · 2-[18F]FDG PET/CT · Pulmonary nodules · Artificial intelligence   \n",
      "343                                                                                                      adverse medicine reactions   \n",
      "370                                                                                                           Automatic Recognition   \n",
      "348                                                                                           chemical three-dimensional conformers   \n",
      "358                                                                                                                              AI   \n",
      "368                                                                                                                     Guided Wave   \n",
      "367                                                                                                    Structural Health Monitoring   \n",
      "366                                                                                                                            객체검출   \n",
      "365                                                                                                                            심층학습   \n",
      "364                                                                                                                           유도초음파   \n",
      "363                                                                                                                           구조 진단   \n",
      "362                                                                                                                Plant Protection   \n",
      "361                                                                                                Crop Leaf Disease Identification   \n",
      "371                                                                                                                Hearing Impaired   \n",
      "360                                                                                            Agricultural Artificial Intelligence   \n",
      "359                                                                                                               Image Recognition   \n",
      "372                                                                                                                   Human Posture   \n",
      "373                                                                                                     Physical Education Teaching   \n",
      "349                                                                                                     negative medicine reactions   \n",
      "357                                                                                                             Auto People Counter   \n",
      "356                                                                                                                         Victims   \n",
      "339                                                                                                                  Filter Pruning   \n",
      "355                                                                                                  Disaster-vulnerable Facilities   \n",
      "354                                                                                                                            영상인식   \n",
      "374                                                                                                                         ResNet.   \n",
      "375                                                                                                          Channel-wise Attention   \n",
      "369                                                                                                            Arabic Sign Language   \n",
      "353                                                                                                                           자동계수기   \n",
      "352                                                                                                                            요구조자   \n",
      "351                                                                                                                          재난취약시설   \n",
      "350                                                                                                               transfer-learning   \n",
      "340                                                                                                           Dental age estimation   \n",
      "329                                                                                                                            악성코드   \n",
      "338                                                                                                        Gene Expression by Rules   \n",
      "337                                                                                                        Evolutionary Computation   \n",
      "303                                                                                                                             Eye   \n",
      "302                                                                Orchid species · Classifcation model · Feature fusion · ResNet34   \n",
      "301                                                                                                                         패션 카테고리   \n",
      "300                                                                                                                         욜로 버전 8   \n",
      "299                                                                                                                           레즈넷50   \n",
      "298                                                                                                                           오버샘플링   \n",
      "297                                                                                                                        컨볼루션 신경망   \n",
      "296                                                                                                                Fashion category   \n",
      "295                                                                                                                    Oversampling   \n",
      "294                                                                                                              XGBoost classifier   \n",
      "293                                                                                                          Performance evaluation   \n",
      "292                                                                                            Feature extraction and concatenation   \n",
      "291                                                                                                    Breast cancer classification   \n",
      "290                                                                                                        Driver abnormal behavior   \n",
      "289                                                                                                                    Road traffic   \n",
      "287                                                                                                                        운전자 이상행동   \n",
      "286                                                                                                                           도로 교통   \n",
      "285                                                                                                                 Imbalanced data   \n",
      "284                                                                                                           Speech classification   \n",
      "283                                                                                                   Time-frequency transformation   \n",
      "282                                                                                                                         WGAN-GP   \n",
      "281                                                                                                              Audio augmentation   \n",
      "280                                                                                                             Strawberry Maturity   \n",
      "279                                                                                                                       RoI Align   \n",
      "278                                                                                                                    Faster R-CNN   \n",
      "304                                                                                                    Optical coherence tomography   \n",
      "305                                                                                                               Multi-dimensional   \n",
      "306                                                                                                                    Histological   \n",
      "320                                                                                                                     DenseNet201   \n",
      "336                                                                                               Multilayers Multi-elements System   \n",
      "334                                                                                                               wavelet transform   \n",
      "333                                                                                                                 fault diagnosis   \n",
      "332                                                                                                                on-board charger   \n",
      "331                                                                                                    Sharpness-Aware Minimization   \n",
      "330                                                                                                                        유형 분류 기법   \n",
      "380                                                                                                                       깊이 분리 합성곱   \n",
      "328                                                                                                     SharpnessAware Minimization   \n",
      "326                                                                                                           Classification Method   \n",
      "325                                                                                                                         Malware   \n",
      "321                                                                                                                      Vegetables   \n",
      "319                                                                                                    Convolutional neural network   \n",
      "307                                                                                                                 Prostate Cancer   \n",
      "318                                                                                                                Confusion matrix   \n",
      "317                                                                                                              autonomous driving   \n",
      "316                                                                                                            image classification   \n",
      "315                                                                                                               woodscape dataset   \n",
      "314                                                                                                               soiling detection   \n",
      "313                                                                                                               Medical diagnosis   \n",
      "312                                                                                                        Automated identification   \n",
      "311                                                                                                      Chest X-ray classification   \n",
      "310                                                                                                         Machine learning models   \n",
      "309                                                                                                               COVID-19 pandemic   \n",
      "308                                                                                                 Ensemble Deep Transfer Learning   \n",
      "379                                                                                                                          채널 어텐션   \n",
      "0                                                                                                                             특허 도면   \n",
      "381                                                                                                                     작물 건강 상태 분류   \n",
      "457                                                                                                                    Model fusion   \n",
      "468                                                                                                                             XAI   \n",
      "467                                                                                                                         Disease   \n",
      "466                                                                                                         Time Series Forecasting   \n",
      "465                                                                                                       Time Series Data Analysis   \n",
      "464                                                                                                            Fruit stem detection   \n",
      "463                                                                                                             3D Object detection   \n",
      "462                                                                                                             Digital agriculture   \n",
      "461                                                                                                                        과수 꼭지 검출   \n",
      "460                                                                                                                        3D 객체 검출   \n",
      "459                                                                                                                          디지털 농업   \n",
      "458                                        Cancer · Medical segmentation · Histology · Convolutional neural networks · Augmentation   \n",
      "456                                                                                                                  Neural network   \n",
      "382                                                                                                                           문장 분류   \n",
      "455                                                                                                                           모델 합성   \n",
      "454                                                                                                                          이미지 인식   \n",
      "453                                                                                                                           인공신경망   \n",
      "452                                                                                                         feature pyramid network   \n",
      "451                                                                                                          bi-directional pyramid   \n",
      "450                                                                                                             attention mechanism   \n",
      "449                                                                                                                  neural network   \n",
      "448                                                                                                                  fire detection   \n",
      "447                                                                                                               Wafer Edge Defect   \n",
      "446                                                                                                      Auto Defect Classification   \n",
      "445                                                                                                                Image Processing   \n",
      "469                                                                                                                    head gesture   \n",
      "470                                                                                                        human-robot interaction.   \n",
      "471                                                                                                                  remote sensing   \n",
      "472                                                                                                                       attention   \n",
      "495                                                                                                     Inception Network(인셉션 네트워크)   \n",
      "494                                                                                                                XAI(설명 가능한 인공지능)   \n",
      "493                                                                                                                        AI(인공지능)   \n",
      "492                                                                                                    Image Classification(이미지 분류)   \n",
      "491                                                                                                              Deep Learning(딥러닝)   \n",
      "490                                                                                                Convolution Neural Network (CNN)   \n",
      "489                                                                                                Magnetic Resonance Imaging (MRI)   \n",
      "488                                                                                                        Alzheimer's disease (AD)   \n",
      "487                                                                                                                     vital signs   \n",
      "486                                                                                                   pediatric intensive care unit   \n",
      "485                                                                                                              patient monitoring   \n",
      "484                                                                                                                machine learning   \n",
      "483                                                                                                                      heart rate   \n",
      "482                                                                                                              conscious sedation   \n",
      "481                                                                                                            Crack classification   \n",
      "480                                                                                                                 Benchmark study   \n",
      "479                                                                                                                 Pavement cracks   \n",
      "478                                                                                                                       Inception   \n",
      "477                                                                                                                           영상 분할   \n",
      "476                                                                                                                             어텐션   \n",
      "475                                                                                                                           원격 감지   \n",
      "474                                                                                                                       inception   \n",
      "473                                                                                                              image segmentation   \n",
      "443                                                                                               Clothing Attribute Classification   \n",
      "442                                                                                                                   Fashion Image   \n",
      "441                                                                                                                        의류 속성 분류   \n",
      "411                                                                                                        Real-Time Fire Detection   \n",
      "409                                                                                                         Transfer Learning Model   \n",
      "408                                                                                                     Layer Importance Evaluation   \n",
      "407                                                                                                                             기여도   \n",
      "406                                                                                                                       실시간 화재 감지   \n",
      "405                                                                                                                     합성곱 신경망 최적화   \n",
      "277                                                                                                     Hand Gesture Classification   \n",
      "403                                                                                                                      레이어 중요도 평가   \n",
      "402                                                                                                                   deap learning   \n",
      "401                                                                                                               Multiple features   \n",
      "400                                                                                                        driver behavior features   \n",
      "399                                                                                                            drowsiness detection   \n",
      "398                                                                                                                           다중 특징   \n",
      "396                                                                                                                       운전자 행동 특징   \n",
      "395                                                                                                                           졸음 인식   \n",
      "394                                                                                                                           균열 검출   \n",
      "393                                                                                                                            데이터셋   \n",
      "392                                                                                                                            모션블러   \n",
      "390                                                                                                                 Crack detection   \n",
      "389                                                                                                                         Dataset   \n",
      "388                                                                                                                     Motion blur   \n",
      "386                                                                                                             Sentence similarity   \n",
      "385                                                                                                         Sentence classification   \n",
      "383                                                                                                                          문장 유사도   \n",
      "410                                                                                                                CNN Optimization   \n",
      "412                                                                                                                    Contribution   \n",
      "440                                                                                                                          패션 이미지   \n",
      "413                                                                                                                       신발 스타일 분류   \n",
      "439                                                                                                                        DenseNet   \n",
      "438                                                                                                                        Few-Shot   \n",
      "437                                                                                                                      Smart Farm   \n",
      "433                                                                                                                             UAV   \n",
      "432                                                                                                                      FMCW Radar   \n",
      "431                                                                                                                          DBSCAN   \n",
      "430                                                                                                                           U-Net   \n",
      "429                                                                                               Multi class semantic segmentation   \n",
      "428                                                                                                        Crimp harness inspection   \n",
      "427                                                                                                                    Wire harness   \n",
      "426                                                                                                             Deep learning model   \n",
      "425                                                                                                Composite modulated radar signal   \n",
      "424                                                                                          Radar signal modulation identification   \n",
      "423                                                                                                      Multi-label classification   \n",
      "422                                                                                                         Unmanned aerial vehicle   \n",
      "421                                                                                                                  Ground surface   \n",
      "420                                                                                                Automated construction equipment   \n",
      "419                                                                                                                        ConvNeXt   \n",
      "418                                                                                                                     Shoes Upper   \n",
      "417                                                                                                                   Shoes Outsole   \n",
      "416                                                                                                      Shoes Style Classification   \n",
      "415                                                                                                                           신발 갑피   \n",
      "414                                                                                                                          신발 아웃솔   \n",
      "404                                                                                                                        전이 학습 모델   \n",
      "248                                                                                                             Deep Neural Network   \n",
      "276                                                                                                            Video Classification   \n",
      "111                                                                                                                   Deep-learning   \n",
      "122                                                                                                       lightweight deep learning   \n",
      "121                                                                                                           embedding environment   \n",
      "120                                                                                                                          X-rays   \n",
      "119                                                                                                                           Spine   \n",
      "118                                                                                                                    Osteoporosis   \n",
      "117                                                                                                                Lumbar vertebrae   \n",
      "116                                                                                                                  Inference time   \n",
      "115                                                                                                                         Pruning   \n",
      "114                                                                                                             Network compression   \n",
      "113                                                                                                                Object detection   \n",
      "112                                                                                                                      Spectogram   \n",
      "110                                                                                                              violence detection   \n",
      "124                                                                                                                   MobileFaceNet   \n",
      "109                                                                                                                video processing   \n",
      "108                                                                                                          deep transfer learning   \n",
      "105                                                                                                                  Concrete Crack   \n",
      "104                                                                                                                  Implant system   \n",
      "99                                                                                                               Wildfire Detection   \n",
      "95                                                                                                                            산불 감지   \n",
      "94                                                                                                                        AI 학습 데이터   \n",
      "93                                                                                                                          이상행동 탐지   \n",
      "89                                                                                                                 AI Learning Data   \n",
      "88                                                                                                                Anomaly Detection   \n",
      "87                                                                                                   Generative Adversarial Network   \n",
      "123                                                                                                                face recognition   \n",
      "125                                                                                                                           자궁경부암   \n",
      "274                                                                                                                      Quarantine   \n",
      "144                                                                                                             Electrocardiography   \n",
      "156                                                                                                                         LLVM IR   \n",
      "155                                                                                                                        ResNet18   \n",
      "154                                                                                                                 Illegal parking   \n",
      "153                                                                                                        Self-supervised learning   \n",
      "152                                                                                                            Contrastive learning   \n",
      "150                                                                                                                     Radiography   \n",
      "149                                                                                                                   Periodontitis   \n",
      "148                                                                                                                           Molar   \n",
      "147                                                                                                                        Mandible   \n",
      "146                                                                                                            Probability Learning   \n",
      "145                                                                                                             Mobile Applications   \n",
      "143                                                                                                             Atrial Fibrillation   \n",
      "126                                                                                                                        생존 예측 모델   \n",
      "142                                                                                                                            공중표적   \n",
      "139                                                                                                                              드론   \n",
      "138                                                                                                                  Aerial targets   \n",
      "135                                                                                                                          Drones   \n",
      "133                                                                                                                Machine Learning   \n",
      "132                                                                                                        Cox Proportional Hazards   \n",
      "131                                                                                                       Survival Prediction Model   \n",
      "130                                                                                                                 Cervical Cancer   \n",
      "129                                                                                                                          심층 신경망   \n",
      "128                                                                                                                           기계 학습   \n",
      "127                                                                                                                        콕스 비례 위험   \n",
      "84                                                                                                                      Korean Menu   \n",
      "83                                                                                                              Attention Mechanism   \n",
      "82                                                                                                                 Text Recognition   \n",
      "20                                                                                                                        레즈넷-트랜스포머   \n",
      "35                                                                                                                           실내와 실외   \n",
      "34                                                                                                                            가상 공간   \n",
      "33                                                                                                                        Immersive   \n",
      "32                                                                                                                         Accuracy   \n",
      "31                                                                                                               Indoor and Outdoor   \n",
      "29                                                                                                                    Virtual Space   \n",
      "28                                                                                                               Rangking algorithm   \n",
      "27                                                                                                                   Website search   \n",
      "26                                                                                                               ResNet-Transformer   \n",
      "22                                                                                                                          랭킹 알고리즘   \n",
      "21                                                                                                                          웹사이트 검색   \n",
      "19                                                                                                                            트랜스포머   \n",
      "81                                                                                               OCR(Optical Character Recognition)   \n",
      "18                                                                                                                              레즈넷   \n",
      "17                                                                                                                   Image analysis   \n",
      "14                                                                                                                White blood cells   \n",
      "13                                                                                                                           이미지 분석   \n",
      "10                                                                                                                              백혈구   \n",
      "9                                                                                                             similarity Comparison   \n",
      "8                                                                                                      classification and retrieval   \n",
      "7                                                                                                                     binary images   \n",
      "6                                                                                                                   Patent drawings   \n",
      "5                                                                                                                            유사도 비교   \n",
      "2                                                                                                                           분류 및 검색   \n",
      "37                                                                                                                              몰입감   \n",
      "38                                                                                                       Speech Emotion Recognition   \n",
      "40                                                                                                                      Sleep apnea   \n",
      "41                                                                                                                  Polysomnography   \n",
      "80                                                                                                           Online to Offline(O2O)   \n",
      "79                                                                                                                      Large Waste   \n",
      "78                                                                                                                 Image Similarity   \n",
      "76                                                                                                            Quantitative analysis   \n",
      "75                                                                                                                     ResNet model   \n",
      "74                                                                                                         Interpolation techniques   \n",
      "73                                                                                                                 Nuclear medicine   \n",
      "72                                                                                                                             정량분석   \n",
      "71                                                                                                                         ResNet모델   \n",
      "70                                                                                                                              보간법   \n",
      "69                                                                                                                              핵의학   \n",
      "58                                                                                                                   Residual block   \n",
      "56                                                                                                                              VAE   \n",
      "55                                                                                                             3D object generation   \n",
      "54                                                                                                                             Mesh   \n",
      "53                                                                                                                            Voxel   \n",
      "50                                                                                                                       Deep Model   \n",
      "48                                                                                                                            객체 감지   \n",
      "47                                                                                                                Inference Runtime   \n",
      "46                                                                                                                             딥 모델   \n",
      "45                                                                                                                             ONNX   \n",
      "44                                                                                                                             MFCC   \n",
      "42                                                                                                                      Sleep sound   \n",
      "157                                                                                                                     Image based   \n",
      "158                                                                                                               Malware Detection   \n",
      "159                                                                                                                      ResNet50V2   \n",
      "235                                                                                                                           영역 분할   \n",
      "246                                                                                                                           비트 연산   \n",
      "245                                                                                                                            파라미터   \n",
      "244                                                                                                                       딥 뉴럴 네트워크   \n",
      "243                                                                                                     Pediatric Medical Treatment   \n",
      "242                                                                                                          Eye Movement Detection   \n",
      "241                                                                                                                Pain Recognition   \n",
      "240                                                                                                                 Virtual Reality   \n",
      "239                                                                                                                      합성곱 신경망 모델   \n",
      "238                                                                                                                           생체 정보   \n",
      "237                                                                                                                         인공지능 학습   \n",
      "236                                                                                                                              감지   \n",
      "234                                                                                                           Biometric Information   \n",
      "220                                                                                                                 image denoising   \n",
      "233                                                                                                Artificial Intelligence Learning   \n",
      "232                                                                                                                       Detection   \n",
      "231                                                                                                                    Segmentation   \n",
      "230                                                                                                                  Video analysis   \n",
      "228                                                                                                              Action recognition   \n",
      "227                                                                                                                    Telemedicine   \n",
      "226                                                                                                                     Pharyngitis   \n",
      "225                                                                                                                       Diagnosis   \n",
      "224                                                                                                             Pneumonia detection   \n",
      "223                                                                                                              Vision transformer   \n",
      "222                                                                                                                       흉부 X-선 영상   \n",
      "247                                                                                                                             강인성   \n",
      "1                                                                                                                            이진 이미지   \n",
      "249                                                                                                                      parameters   \n",
      "250                                                                                                             bit-wise operations   \n",
      "273                                                                                                            Deep Learning Models   \n",
      "272                                                                                                         Pear Pests and Diseases   \n",
      "271                                                                                                                              검역   \n",
      "270                                                                                                                          딥러닝 모델   \n",
      "269                                                                                                                           배 병해충   \n",
      "268                                                                                                               Flow Volume Loops   \n",
      "267                                                                                                               Pulmonary Disease   \n",
      "266                                                                                                    Ensemble Deep Learning Model   \n",
      "265                                                                                                                      볼륨-플로우 그래프   \n",
      "264                                                                                                                             폐질환   \n",
      "263                                                                                                                      앙상블 딥러닝 모델   \n",
      "262                                                                                                                  Feature Fusion   \n",
      "261                                                                                                                      Multimodal   \n",
      "260                                                                                                                      Biometrics   \n",
      "259                                                                                                                           특징 융합   \n",
      "258                                                                                                                            멀티모달   \n",
      "257                                                                                                                           생체 인식   \n",
      "256                                                                                                               satellite imagery   \n",
      "255                                                                                                                      ResNeXt101   \n",
      "254                                                                                                       oriented object detection   \n",
      "253                                                                                                       box-boundary-aware vector   \n",
      "252                                                                                                                      robustness   \n",
      "251                                                                                                                        accuracy   \n",
      "221                                                                                                                           폐렴 진단   \n",
      "219                                                                                                                     dRED-TL-GAN   \n",
      "163                                                                                                            Blind Spot Detection   \n",
      "176                                                                                                                      selection.   \n",
      "187                                                                                                         Deep learning algorithm   \n",
      "186                                                                                                  Computer Automated Diagnostics   \n",
      "185                                                                                                                        영상 병변 탐지   \n",
      "184                                                                                                                           영상 구분   \n",
      "183                                                                                                                          초음파 영상   \n",
      "182                                                                                                                        딥러닝 알고리즘   \n",
      "181                                                                                                                       컴퓨터 자동 진단   \n",
      "180                                                                                                            Performance analysis   \n",
      "179                                                                                                                     Edge device   \n",
      "178                                                                                                                       Inference   \n",
      "177                                                                                                       Deep learning accelerator   \n",
      "175                                                                                                                         sorting   \n",
      "218                                                                                                          deformable convolution   \n",
      "174                                                                                                                    thresholding   \n",
      "173                                                                                                                 Semi-supervised   \n",
      "172                                                                                                                    Rail surface   \n",
      "171                                                                                                    Model Parameter Quantization   \n",
      "170                                                                                                               Vehicle Detection   \n",
      "169                                                                                                               Automotive Safety   \n",
      "168                                                                                                               Energy Efficiency   \n",
      "167                                                                                                                       ResNet-18   \n",
      "166                                                                                                                            FPGA   \n",
      "165                                                                                                       Deep Learning Accelerator   \n",
      "164                                                                                              Advanced Driver Assistance Systems   \n",
      "188                                                                                                               Ultrasound  image   \n",
      "189                                                                                                          Image Lesion Detection   \n",
      "190                                                                                                         artificial intelligence   \n",
      "191                                                                                                                 ocular diseases   \n",
      "217                                                                                                                 deformable 컨볼루션   \n",
      "216                                                                                                                            잡음제거   \n",
      "214                                                                                                      shifted window transformer   \n",
      "213                                                                                                             vision transformers   \n",
      "212                                                                                                        visual geometry group-16   \n",
      "211                                                                                                             residual network 50   \n",
      "210                                                                                                                image captioning   \n",
      "209                                                                                                             Non-volatile memory   \n",
      "208                                                                                                        accelerator optimization   \n",
      "207                                                                                                                             IMC   \n",
      "206                                                                                                        Design space exploration   \n",
      "205                                                                                                               Examination Error   \n",
      "204                                                                                                                   Medical Image   \n",
      "202                                                                                                                           검사 오류   \n",
      "201                                                                                                                           영상 분류   \n",
      "200                                                                                                                           의료 영상   \n",
      "199                                                                                                          in vitro fertilization   \n",
      "198                                                                                                                          embryo   \n",
      "197                                                                                                   convolutional neural networks   \n",
      "196                                                                                                                      blastocyst   \n",
      "195                                                                                                                             시스템   \n",
      "194                                                                                                                           안구 질환   \n",
      "192                                                                                                                          system   \n",
      "496                                                                                                         LIME Algorithm(라임 알고리즘)   \n",
      "\n",
      "     count  \n",
      "43      17  \n",
      "16      16  \n",
      "96      15  \n",
      "24      14  \n",
      "12      13  \n",
      "67       9  \n",
      "52       8  \n",
      "103      8  \n",
      "141      6  \n",
      "107      6  \n",
      "137      6  \n",
      "15       5  \n",
      "66       4  \n",
      "193      4  \n",
      "11       4  \n",
      "215      4  \n",
      "3        4  \n",
      "86       4  \n",
      "51       3  \n",
      "49       3  \n",
      "229      3  \n",
      "57       3  \n",
      "100      3  \n",
      "101      3  \n",
      "102      3  \n",
      "39       3  \n",
      "275      3  \n",
      "25       3  \n",
      "391      2  \n",
      "327      2  \n",
      "92       2  \n",
      "91       2  \n",
      "288      2  \n",
      "90       2  \n",
      "59       2  \n",
      "60       2  \n",
      "61       2  \n",
      "62       2  \n",
      "63       2  \n",
      "68       2  \n",
      "64       2  \n",
      "65       2  \n",
      "344      2  \n",
      "85       2  \n",
      "203      2  \n",
      "322      2  \n",
      "387      2  \n",
      "323      2  \n",
      "324      2  \n",
      "77       2  \n",
      "435      2  \n",
      "436      2  \n",
      "160      2  \n",
      "140      2  \n",
      "30       2  \n",
      "397      2  \n",
      "106      2  \n",
      "4        2  \n",
      "335      2  \n",
      "36       2  \n",
      "97       2  \n",
      "384      2  \n",
      "151      2  \n",
      "444      2  \n",
      "377      2  \n",
      "162      2  \n",
      "23       2  \n",
      "161      2  \n",
      "136      2  \n",
      "434      2  \n",
      "134      2  \n",
      "98       2  \n",
      "345      1  \n",
      "376      1  \n",
      "346      1  \n",
      "378      1  \n",
      "342      1  \n",
      "341      1  \n",
      "347      1  \n",
      "343      1  \n",
      "370      1  \n",
      "348      1  \n",
      "358      1  \n",
      "368      1  \n",
      "367      1  \n",
      "366      1  \n",
      "365      1  \n",
      "364      1  \n",
      "363      1  \n",
      "362      1  \n",
      "361      1  \n",
      "371      1  \n",
      "360      1  \n",
      "359      1  \n",
      "372      1  \n",
      "373      1  \n",
      "349      1  \n",
      "357      1  \n",
      "356      1  \n",
      "339      1  \n",
      "355      1  \n",
      "354      1  \n",
      "374      1  \n",
      "375      1  \n",
      "369      1  \n",
      "353      1  \n",
      "352      1  \n",
      "351      1  \n",
      "350      1  \n",
      "340      1  \n",
      "329      1  \n",
      "338      1  \n",
      "337      1  \n",
      "303      1  \n",
      "302      1  \n",
      "301      1  \n",
      "300      1  \n",
      "299      1  \n",
      "298      1  \n",
      "297      1  \n",
      "296      1  \n",
      "295      1  \n",
      "294      1  \n",
      "293      1  \n",
      "292      1  \n",
      "291      1  \n",
      "290      1  \n",
      "289      1  \n",
      "287      1  \n",
      "286      1  \n",
      "285      1  \n",
      "284      1  \n",
      "283      1  \n",
      "282      1  \n",
      "281      1  \n",
      "280      1  \n",
      "279      1  \n",
      "278      1  \n",
      "304      1  \n",
      "305      1  \n",
      "306      1  \n",
      "320      1  \n",
      "336      1  \n",
      "334      1  \n",
      "333      1  \n",
      "332      1  \n",
      "331      1  \n",
      "330      1  \n",
      "380      1  \n",
      "328      1  \n",
      "326      1  \n",
      "325      1  \n",
      "321      1  \n",
      "319      1  \n",
      "307      1  \n",
      "318      1  \n",
      "317      1  \n",
      "316      1  \n",
      "315      1  \n",
      "314      1  \n",
      "313      1  \n",
      "312      1  \n",
      "311      1  \n",
      "310      1  \n",
      "309      1  \n",
      "308      1  \n",
      "379      1  \n",
      "0        1  \n",
      "381      1  \n",
      "457      1  \n",
      "468      1  \n",
      "467      1  \n",
      "466      1  \n",
      "465      1  \n",
      "464      1  \n",
      "463      1  \n",
      "462      1  \n",
      "461      1  \n",
      "460      1  \n",
      "459      1  \n",
      "458      1  \n",
      "456      1  \n",
      "382      1  \n",
      "455      1  \n",
      "454      1  \n",
      "453      1  \n",
      "452      1  \n",
      "451      1  \n",
      "450      1  \n",
      "449      1  \n",
      "448      1  \n",
      "447      1  \n",
      "446      1  \n",
      "445      1  \n",
      "469      1  \n",
      "470      1  \n",
      "471      1  \n",
      "472      1  \n",
      "495      1  \n",
      "494      1  \n",
      "493      1  \n",
      "492      1  \n",
      "491      1  \n",
      "490      1  \n",
      "489      1  \n",
      "488      1  \n",
      "487      1  \n",
      "486      1  \n",
      "485      1  \n",
      "484      1  \n",
      "483      1  \n",
      "482      1  \n",
      "481      1  \n",
      "480      1  \n",
      "479      1  \n",
      "478      1  \n",
      "477      1  \n",
      "476      1  \n",
      "475      1  \n",
      "474      1  \n",
      "473      1  \n",
      "443      1  \n",
      "442      1  \n",
      "441      1  \n",
      "411      1  \n",
      "409      1  \n",
      "408      1  \n",
      "407      1  \n",
      "406      1  \n",
      "405      1  \n",
      "277      1  \n",
      "403      1  \n",
      "402      1  \n",
      "401      1  \n",
      "400      1  \n",
      "399      1  \n",
      "398      1  \n",
      "396      1  \n",
      "395      1  \n",
      "394      1  \n",
      "393      1  \n",
      "392      1  \n",
      "390      1  \n",
      "389      1  \n",
      "388      1  \n",
      "386      1  \n",
      "385      1  \n",
      "383      1  \n",
      "410      1  \n",
      "412      1  \n",
      "440      1  \n",
      "413      1  \n",
      "439      1  \n",
      "438      1  \n",
      "437      1  \n",
      "433      1  \n",
      "432      1  \n",
      "431      1  \n",
      "430      1  \n",
      "429      1  \n",
      "428      1  \n",
      "427      1  \n",
      "426      1  \n",
      "425      1  \n",
      "424      1  \n",
      "423      1  \n",
      "422      1  \n",
      "421      1  \n",
      "420      1  \n",
      "419      1  \n",
      "418      1  \n",
      "417      1  \n",
      "416      1  \n",
      "415      1  \n",
      "414      1  \n",
      "404      1  \n",
      "248      1  \n",
      "276      1  \n",
      "111      1  \n",
      "122      1  \n",
      "121      1  \n",
      "120      1  \n",
      "119      1  \n",
      "118      1  \n",
      "117      1  \n",
      "116      1  \n",
      "115      1  \n",
      "114      1  \n",
      "113      1  \n",
      "112      1  \n",
      "110      1  \n",
      "124      1  \n",
      "109      1  \n",
      "108      1  \n",
      "105      1  \n",
      "104      1  \n",
      "99       1  \n",
      "95       1  \n",
      "94       1  \n",
      "93       1  \n",
      "89       1  \n",
      "88       1  \n",
      "87       1  \n",
      "123      1  \n",
      "125      1  \n",
      "274      1  \n",
      "144      1  \n",
      "156      1  \n",
      "155      1  \n",
      "154      1  \n",
      "153      1  \n",
      "152      1  \n",
      "150      1  \n",
      "149      1  \n",
      "148      1  \n",
      "147      1  \n",
      "146      1  \n",
      "145      1  \n",
      "143      1  \n",
      "126      1  \n",
      "142      1  \n",
      "139      1  \n",
      "138      1  \n",
      "135      1  \n",
      "133      1  \n",
      "132      1  \n",
      "131      1  \n",
      "130      1  \n",
      "129      1  \n",
      "128      1  \n",
      "127      1  \n",
      "84       1  \n",
      "83       1  \n",
      "82       1  \n",
      "20       1  \n",
      "35       1  \n",
      "34       1  \n",
      "33       1  \n",
      "32       1  \n",
      "31       1  \n",
      "29       1  \n",
      "28       1  \n",
      "27       1  \n",
      "26       1  \n",
      "22       1  \n",
      "21       1  \n",
      "19       1  \n",
      "81       1  \n",
      "18       1  \n",
      "17       1  \n",
      "14       1  \n",
      "13       1  \n",
      "10       1  \n",
      "9        1  \n",
      "8        1  \n",
      "7        1  \n",
      "6        1  \n",
      "5        1  \n",
      "2        1  \n",
      "37       1  \n",
      "38       1  \n",
      "40       1  \n",
      "41       1  \n",
      "80       1  \n",
      "79       1  \n",
      "78       1  \n",
      "76       1  \n",
      "75       1  \n",
      "74       1  \n",
      "73       1  \n",
      "72       1  \n",
      "71       1  \n",
      "70       1  \n",
      "69       1  \n",
      "58       1  \n",
      "56       1  \n",
      "55       1  \n",
      "54       1  \n",
      "53       1  \n",
      "50       1  \n",
      "48       1  \n",
      "47       1  \n",
      "46       1  \n",
      "45       1  \n",
      "44       1  \n",
      "42       1  \n",
      "157      1  \n",
      "158      1  \n",
      "159      1  \n",
      "235      1  \n",
      "246      1  \n",
      "245      1  \n",
      "244      1  \n",
      "243      1  \n",
      "242      1  \n",
      "241      1  \n",
      "240      1  \n",
      "239      1  \n",
      "238      1  \n",
      "237      1  \n",
      "236      1  \n",
      "234      1  \n",
      "220      1  \n",
      "233      1  \n",
      "232      1  \n",
      "231      1  \n",
      "230      1  \n",
      "228      1  \n",
      "227      1  \n",
      "226      1  \n",
      "225      1  \n",
      "224      1  \n",
      "223      1  \n",
      "222      1  \n",
      "247      1  \n",
      "1        1  \n",
      "249      1  \n",
      "250      1  \n",
      "273      1  \n",
      "272      1  \n",
      "271      1  \n",
      "270      1  \n",
      "269      1  \n",
      "268      1  \n",
      "267      1  \n",
      "266      1  \n",
      "265      1  \n",
      "264      1  \n",
      "263      1  \n",
      "262      1  \n",
      "261      1  \n",
      "260      1  \n",
      "259      1  \n",
      "258      1  \n",
      "257      1  \n",
      "256      1  \n",
      "255      1  \n",
      "254      1  \n",
      "253      1  \n",
      "252      1  \n",
      "251      1  \n",
      "221      1  \n",
      "219      1  \n",
      "163      1  \n",
      "176      1  \n",
      "187      1  \n",
      "186      1  \n",
      "185      1  \n",
      "184      1  \n",
      "183      1  \n",
      "182      1  \n",
      "181      1  \n",
      "180      1  \n",
      "179      1  \n",
      "178      1  \n",
      "177      1  \n",
      "175      1  \n",
      "218      1  \n",
      "174      1  \n",
      "173      1  \n",
      "172      1  \n",
      "171      1  \n",
      "170      1  \n",
      "169      1  \n",
      "168      1  \n",
      "167      1  \n",
      "166      1  \n",
      "165      1  \n",
      "164      1  \n",
      "188      1  \n",
      "189      1  \n",
      "190      1  \n",
      "191      1  \n",
      "217      1  \n",
      "216      1  \n",
      "214      1  \n",
      "213      1  \n",
      "212      1  \n",
      "211      1  \n",
      "210      1  \n",
      "209      1  \n",
      "208      1  \n",
      "207      1  \n",
      "206      1  \n",
      "205      1  \n",
      "204      1  \n",
      "202      1  \n",
      "201      1  \n",
      "200      1  \n",
      "199      1  \n",
      "198      1  \n",
      "197      1  \n",
      "196      1  \n",
      "195      1  \n",
      "194      1  \n",
      "192      1  \n",
      "496      1  \n",
      "데이터가 저장되었습니다: resnet_2024_academic_riss.csv\n",
      "데이터가 저장되었습니다: k_resnet_2024_academic_riss.csv\n",
      "모든 데이터 크롤링 및 저장 완료.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "urllib3.disable_warnings()  # Warning message 제거\n",
    "\n",
    "# 검색어 설정\n",
    "search_query = 'resnet'\n",
    "\n",
    "# 연도 범위 설정\n",
    "years = range(2015, 2025)\n",
    "\n",
    "# 연도별 크롤링 및 CSV 파일 생성\n",
    "for search_year in years:\n",
    "    data = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "        'Referer': 'https://www.riss.kr'\n",
    "    }\n",
    "\n",
    "    # 페이지 순회\n",
    "    for i in range(0, 1000):  # 페이지 계산\n",
    "        param = {\n",
    "            'isDetailSearch': 'N',\n",
    "            'searchGubun': 'true',\n",
    "            'viewYn': 'OP',\n",
    "            'strQuery': search_query,\n",
    "            'exQuery': f'regnm:KCI등재◈regnm:KCI우수등재◈regnm:KCI등재후보◈pyear:{search_year}◈',\n",
    "            'exQueryText': f'등재정보 [KCI등재]@@regnm:KCI등재◈등재정보 [KCI우수등재]@@regnm:KCI우수등재◈등재정보 [KCI등재후보]@@regnm:KCI등재후보◈발행연도 [{search_year}]@@pyear:{search_year}◈',\n",
    "            'order': '/DESC',\n",
    "            'onHanja': 'false',\n",
    "            'strSort': 'RANK',\n",
    "            'iStartCount': i * 100,  # 페이지 계산 (iStartCount는 0, 100, 200, ... 형식으로 증가)\n",
    "            'fsearchMethod': 'search',\n",
    "            'sflag': 1,\n",
    "            'isFDetailSearch': 'N',\n",
    "            'icate': 're_a_kor',\n",
    "            'colName': 're_a_kor',\n",
    "            'pageScale': 100,\n",
    "            'isTab': 'Y',\n",
    "            'query': search_query\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # 메인 검색 결과 페이지 요청\n",
    "            response = requests.get(\"https://www.riss.kr/search/Search.do\", params=param, headers=headers, timeout=10, verify=False)\n",
    "            response.raise_for_status()  # HTTP 오류 발생 시 예외\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            articles = soup.select(\".srchResultListW>ul>li\")  # 검색 결과 항목들 선택\n",
    "\n",
    "            if not articles:  # 더 이상 결과가 없으면 종료\n",
    "                print(f\"{search_year}년 데이터 크롤링 완료.\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    title = article.select_one(\".title > a\").text.strip()\n",
    "                    link = 'https://www.riss.kr/' + article.select_one(\".title > a\").attrs['href']\n",
    "\n",
    "                    # 상세 페이지 요청\n",
    "                    response_detail = requests.get(link, headers=headers, timeout=10, verify=False)\n",
    "                    response_detail.raise_for_status()\n",
    "                    html_detail = response_detail.text\n",
    "                    soup_detail = BeautifulSoup(html_detail, 'html.parser')\n",
    "                    year = soup_detail.find('span', string='발행연도').find_next_sibling().text.strip()\n",
    "\n",
    "                    # 주제어 수집\n",
    "                    if soup_detail.find('span', string='주제어'):\n",
    "                        keywords = soup_detail.find('span', string='주제어').find_next_sibling().text.split(';')\n",
    "                        keywords = [keyword.strip() for keyword in keywords]\n",
    "                    else:\n",
    "                        keywords = None\n",
    "\n",
    "                    # 초록 내용 수집\n",
    "                    abstract = None  # 국문 초록 초기화\n",
    "                    multilingual_abstract = None  # 다국어 초록 초기화\n",
    "\n",
    "                    abstract_sections = soup_detail.select(\".content > div\")\n",
    "                    for section in abstract_sections:\n",
    "                        title_tag = section.select_one(\"p.title\")\n",
    "                        if title_tag:\n",
    "                            title_text = title_tag.text.strip()\n",
    "                            # 국문 초록\n",
    "                            if \"국문 초록\" in title_text:\n",
    "                                text_off = section.select_one(\".text.off > p\")\n",
    "                                if text_off:\n",
    "                                    abstract = text_off.text.strip()\n",
    "                            # 다국어 초록\n",
    "                            elif \"다국어 초록\" in title_text or \"Multilingual\" in title_text:\n",
    "                                text_off = section.select_one(\".text.off > p\")\n",
    "                                if text_off:\n",
    "                                    multilingual_abstract = text_off.text.strip()\n",
    "\n",
    "                    print(title, year, keywords, abstract, multilingual_abstract)\n",
    "                    data.append([title, year, keywords, abstract, multilingual_abstract])\n",
    "                except Exception as e:\n",
    "                    print(f\"세부 데이터 수집 중 오류 발생: {e}\")\n",
    "\n",
    "            # 페이지 간 지연 시간 추가\n",
    "            time.sleep(2)  # 2초 대기\n",
    "\n",
    "        except RequestException as e:\n",
    "            print(f\"페이지 요청 중 오류 발생: {e}. 5초 후 재시도합니다.\")\n",
    "            time.sleep(5)  # 재시도 전 대기\n",
    "\n",
    "    # 결과 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(data, columns=['title', 'date', 'keywords', 'abstract', 'multilingual_abstract'])\n",
    "\n",
    "    # 주제어가 None인 경우 빈 리스트로 변환\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: x if x is not None else [])\n",
    "\n",
    "    # 주제어 빈도수 계산\n",
    "    all_keywords = [keyword for row in df['keywords'] for keyword in row]  # 모든 주제어를 하나의 리스트로 결합\n",
    "    keyword_counts = Counter(all_keywords)  # 빈도수 계산\n",
    "\n",
    "    # 주제어 빈도수 데이터프레임 생성\n",
    "    keyword_df = pd.DataFrame(keyword_counts.items(), columns=['keyword', 'count']).sort_values(by='count', ascending=False)\n",
    "\n",
    "    # 결과 출력 설정\n",
    "    pd.set_option('display.max_columns', None)  # 열을 모두 출력\n",
    "    pd.set_option('display.max_rows', None)  # 행을 모두 출력\n",
    "    pd.set_option('display.max_colwidth', None)  # 긴 데이터도 모두 출력\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"=== 데이터프레임 ===\")\n",
    "    print(df)\n",
    "    print(\"\\n=== 주제어 빈도수 ===\")\n",
    "    print(keyword_df)\n",
    "\n",
    "    # 검색어를 파일 이름에 안전하게 사용할 수 있도록 변환\n",
    "    safe_search_query = search_query.replace(' ', '_')\n",
    "\n",
    "    # 파일명 생성\n",
    "    csv_filename = f\"{safe_search_query}_{search_year}_academic_riss.csv\"\n",
    "    keyword_csv_filename = f\"k_{safe_search_query}_{search_year}_academic_riss.csv\"\n",
    "\n",
    "    # 폴더 경로 설정\n",
    "    academic_riss_folder = f\"{search_year}_academic_riss\"\n",
    "    keyword_riss_folder = \"k_academic_riss\"\n",
    "\n",
    "    os.makedirs(academic_riss_folder, exist_ok=True)\n",
    "    os.makedirs(keyword_riss_folder, exist_ok=True)\n",
    "\n",
    "    # 파일 저장\n",
    "    df.to_csv(f\"{academic_riss_folder}/{csv_filename}\", index=False)\n",
    "    keyword_df.to_csv(f\"{keyword_riss_folder}/{keyword_csv_filename}\", index=False)\n",
    "\n",
    "    print(f\"데이터가 저장되었습니다: {csv_filename}\")\n",
    "    print(f\"데이터가 저장되었습니다: {keyword_csv_filename}\")\n",
    "\n",
    "print(\"모든 데이터 크롤링 및 저장 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
