title,date,keywords,abstract,multilingual_abstract
BERT와 지식 그래프를 이용한 한국어 문맥 정보 추출 시스템,2020,"['contextual information extraction', 'person extraction', 'relation extraction', 'sentiment extraction', 'BERT', 'knowledge graph', '문맥 정보 추출', '인물 추출', '관계 추출', '감정 추출', 'BERT', '지식 그래프']","인공지능 기술의 비약적 발전과 함께 사람의 언어를 다루는 자연어 처리 분야 역시 활발하게 연구가 진행되고 있다. 특히 최근에는 구글에서 공개한 언어 모델인 BERT는 대량의 코퍼스를 활용해 미리 학습시킨 모델을 제공함으로써 자연어 처리의 여러 분야에서좋은 성능을 보이고 있다. BERT에서 다국어 모델을 지원하고 있지만 한국어에 바로 적용했을 때는 한계점이 존재하기 때문에 대량의한국어 코퍼스를 이용해 학습시킨 모델을 사용해야 한다. 또한 텍스트는 어휘, 문법적인 의미만 담고 있는 것이 아니라 전후 관계, 상황과 같은 문맥적인 의미도 담고 있다. 기존의 자연어 처리 분야에서는 어휘나 문법적인 의미를 중심으로 연구가 주로 이루어졌다.텍스트에 내재되어 있는 문맥 정보의 정확한 파악은 맥락을 이해하는 데에 있어 중요한 역할을 한다. 단어들의 관계를 이용해 연결한 지식그래프는 컴퓨터에게 쉽게 문맥을 학습시킬 수 있는 장점이 있다. 본 논문에서는 한국어 코퍼스를 이용해 사전 학습된 BERT 모델과 지식 그래프를 이용해 한국어 문맥 정보를 추출하는 시스템을 제안하고자 한다. 텍스트에서 중요한 요소가 되는 인물, 관계, 감정, 공간, 시간 정보를 추출할 수 있는 모델을 구축하고 제안한 시스템을 실험을 통해 검증한다.","Along with the rapid development of artificial intelligence technology, natural language processing, which deals with human language, is also actively studied. In particular, BERT, a language model recently proposed by Google, has been performing well in many areas of natural language processing by providing pre-trained model using a large number of corpus. Although BERT supports multilingual model, we should use the pre-trained model using large amounts of Korean corpus because there are limitations when we apply the original pre-trained BERT model directly to Korean. Also, text contains not only vocabulary, grammar, but contextual meanings such as the relation between the front and the rear, and situation. In the existing natural language processing field, research has been conducted mainly on vocabulary or grammatical meaning. Accurate identification of contextual information embedded in text plays an important role in understanding context. Knowledge graphs, which are linked using the relationship of words, have the advantage of being able to learn context easily from computer. In this paper, we propose a system to extract Korean contextual information using pre-trained BERT model with Korean language corpus and knowledge graph. We build models that can extract person, relationship, emotion, space, and time information that is important in the text and validate the proposed system through experiments."
트랜스포머와 BERT로 구현한 한국어 형태소 분석기의 성능 분석,2020,"['sequence-to-sequence', 'Korean morphological analyzer', 'Transformer', 'BERT', 'attention mechanism', 'copying mechanism', '시퀀스-투-시퀀스', '한국어 형태소 분석기', '트랜스포머', 'BERT', '주의 메커니즘', '복사 메커니즘']","본 논문은 Transformer로 구현한 한국어 형태소 분석기를 다룬다. Transformer는 최근에 가장 널리 사용되는 sequence-to-sequence 모델 중 하나이다. Transformer는 인코더와 디코더로 구성되어 있는데 인코더는 원문을 고정된 크기의 벡터로 압축시키고 디코더는 이 벡터를 이용하여 형태소 분석 결과를 생성해 낸다. 본 논문에서는 또한 Transformer의 인코더를 BERT로 대체해 본다. BERT는 대용량의 학습데이터를 이용하여 미리 학습시켜 놓은 언어 표현 모델이다. 디코더에는 주의 메커니즘과 복사 메커니즘을 도입하였다. 인코더와 디코더에서의 처리 단위는 각각 어절 단위 WordPiece와 형태소 단위의 WordPiece를 사용하였다. 실험을 통해, BERT의 파라미터를 문제에 맞게 재조정했을 때의 성능이 Transformer를 임의의 값으로 초기화하여 사용했을 때에 비해 F1에서 2.9%의 성능 향상을 보임을 알 수 있었다. 또한 학습단계에서 충분히 학습되지 못한 WordPiece의 임베딩이 형태소 분석에 어떤 영향을 미치는지도 살펴보았다.","This paper introduces a Korean morphological analyzer using the Transformer, which is one of the most popular sequence-to-sequence deep neural models. The Transformer comprises an encoder and a decoder. The encoder compresses a raw input sentence into a fixed-size vector, while the decoder generates a morphological analysis result for the vector. We also replace the encoder with BERT, a pre-trained language representation model. An attention mechanism and a copying mechanism are integrated in the decoder. The processing units of the encoder and the decoder are eojeol-based WordPiece and morpheme-based WordPiece, respectively. Experimental results showed that the Transformer with fine-tuned BERT outperforms the randomly initialized Transformer by 2.9% in the F1 score. We also investigated the effects of the WordPiece embedding on morphological analysis when they are not fully updated in the training phases."
BERT 기반 End-to-end 신경망을 이용한 한국어 상호참조해결,2020,"['딥 러닝', '상호참조해결', 'BERT', '자연어처리', 'deep learning', 'coreference resolution', 'BERT', 'natural language processing']","상호참조해결은 주어진 문서에서 상호참조해결 대상이 되는 멘션(mention)을 식별하고, 같은 개체(entity)를 의미하는 멘션을 찾아 그룹화하는 자연어처리 태스크이다. 한국어 상호참조해결에서는 멘션 탐지와 상호참조해결을 동시에 진행하는 end-to-end 모델과 포인터 네트워크 모델을 이용한 방법이 연구되었다. 구글에서 공개한 BERT 모델은 자연어처리 태스크에 적용되어 많은 성능 향상을 보였다. 본 논문에서는 한국어 상호참조해결을 위한 BERT 기반 end-to-end 신경망 모델을 제안하고, 한국어 데이터로 사전 학습된 KorBERT를 이용하고, 한국어의 구조적, 의미적 특징을 반영하기 위하여 의존구문분석 자질과 개체명 자질을 적용한다. 실험 결과, ETRI 질의응답 도메인 상호참조해결 데이터 셋에서 CoNLL F1 (DEV) 71.00%, (TEST) 69.01%의 성능을 보여 기존 연구들에 비하여 높은 성능을 보였다.","Coreference resolution is a natural language task that identifies a mention that is a coreference resolution in a given document and finds and clusters the mention of the same entity. In the Korean coreference resolution, a method using the end-to-end model that simultaneously performs mention detection and mention clustering, and another method pointer network using the encoder-decoder model were used. The BERT model released by Google has been applied to natural language processing tasks and has demonstrated many performance improvements. In this paper, we propose a Korean end-to-end neural coreference resolution with BERT. This model uses the KorBERT pre-trained with the Korean data and applies dependency parsing results and the named entity recognition feature to reflect the structural and semantic characteristics of the Korean language.Experimental results show that the performance of the CoNLL F1 (DEV) 71.00% and (TEST) 69.01% in the ETRI Q & A domain data set was higher than the previous studies."
BERT를 이용한 한국어 의미역 결정,2020,"['의미역 결정', '기계학습', '언어 모델', 'Bidirectional Encoder Representations from Transformers', 'semantic role labeling', 'machine learning', 'language model', 'bidirectional encoder representations from transformers']","의미역 결정은 문장 내에서 “누가, 무엇을, 어떻게, 왜” 등의 관계를 찾아내는 자연어처리의 한 응용이다. 최근 의미역 결정 연구는 주로 기계학습을 이용하고 자질 정보를 배제한 종단 대 종단(end-to-end) 방식의 연구가 이루어지고 있다. 최근 BERT(Bidirectional Encoder Representations from Transformers)라는 언어 모델이 자연어처리 분야에 등장하여 기존 자연어처리 분야의 최고 성능 모델들 보다 더 좋은 성능을 보이고 있다. 종단 대 종단 방식을 이용한 의미역 결정 연구의 성능은 주로 기계학습 모델의 구조나 사전에 학습된 언어 모델의 영향을 받는다. 따라서 본 논문에서는 한국어 의미역 결정 성능 향상을 위해 BERT를 한국어 의미역 결정에 적용한다. 실험 결과 BERT를 이용한 한국어 의미역 결정 모델의 성능이 85.77%로 기존 한국어 의미역 결정 모델들 보다 좋은 성능을 보였다.","Semantic role labeling is an application of natural language processing to identify relationships such as ""who, what, how and why"" with in a sentence. The semantic role labeling study mainly uses machine learning algorithms and the end-to-end method that excludes feature information.Recently, a language model called BERT (Bidirectional Encoder Representations from Transformers) has emerged in the natural language processing field, performing better than the stateof- the-art models in the natural language processing field. The performance of the semantic role labeling study using the end-to-end method is mainly influenced by the structure of the machine learning model or the pre-trained language model. Thus, in this paper, we apply BERT to the Korean semantic role labeling to improve the Korean semantic role labeling performance. As a result, the performance of the Korean semantic role labeling model using BERT is 85.77%, which is better than the existing Korean semantic role labeling model."
BERT를 이용한 한국어 특허상담 기계독해,2020,[],"기계독해는(Machine reading comprehension) 사용자 질의와 관련된 문서를 기계가 이해한 후 정답을 추론하는 인공지능 자연어처리 태스크를 말하며, 이러한 기계독해는 챗봇과 같은 자동상담 서비스에 활용될 수 있다. 최근 자연어처리 분야에서 가장 높은 성능을 보이고 있는 BERT 언어모델은 대용량의 데이터를 pre-training 한 후에 각 자연어처리 태스크에 대해 fine-tuning하여 학습된 모델로 추론함으로써 문제를 해결하는 방식이다. 본 논문에서는 BERT기반 특허상담 기계독해 태스크를 위해 특허상담 데이터 셋을 구축하고 그 구축 방법을 소개하며, patent 코퍼스를 pre-training한 Patent-BERT 모델과 특허상담 모델학습에 적합한 언어처리 알고리즘을 추가함으로써 특허상담 기계독해 태스크의 성능을 향상시킬 수 있는 방안을 제안한다. 본 논문에서 제안한 방법을 사용하여 특허상담 질의에 대한 정답 결정에서 성능이 향상됨을 보였다.","MRC (Machine reading comprehension) is the AI NLP task that predict the answer for user's query by understanding of the relevant document and which can be used in automated consult services such as chatbots. Recently, the BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding) model, which shows high performance in various fields of natural language processing, have two phases. First phase is Pre-training the big data of each domain. And second phase is fine-tuning the model for solving each NLP tasks as a prediction. In this paper, we have made the Patent MRC dataset and shown that how to build the patent consultation training data for MRC task. And we propose the method to improve the performance of the MRC task using the Pre-trained Patent-BERT model by the patent consultation corpus and the language processing algorithm suitable for the machine learning of the patent counseling data. As a result of experiment, we show that the performance of the method proposed in this paper is improved to answer the patent counseling query."
한국어 기술문서 분석을 위한 BERT 기반의 분류모델,2020,"['문서분류', '딥러닝', '단어 임베딩', 'BERT', 'Document Classification', 'Deep Learning']","최근 들어 기술개발 현황, 신규기술 분야 출현, 기술융합과 학제 공동연구, 기술의 트렌드 변화 등을 파악하기 위해 R&D 과제정보, 특허와 같은 기술문서의 분류정보가 많이 활용되고 있다. 이러한 기술문서를 분류하기 위해 주로 텍스트마이닝 기법들이 활용되어 왔다. 그러나 기존 텍스트마이닝 방법들로 기술문서를 분류하기 위해서는 기술문서들을 대표하는 특징들을 직접 추출해야 하는 한계점이 있다. 따라서 본 연구에서는 딥러닝 기반의 BERT모델을 활용하여 기술문서들로부터 자동적으로 문서 특징들을 추출한 후, 이를 문서 분류에 직접 활용하는 모델을 제안하고, 이에 대한 성능을 검증하고자 한다. 이를 위해 텍스트 기반의 국가 R&D 과제 정보를 활용하여 BERT 기반 국가 R&D 과제의 중분류코드 예측 모델을 생성하고 이에 대한 성능을 평가한다.","It is necessary to classify technical documents such as patents, R&D project reports in order to understand the trends of technology convergence and interdisciplinary joint research, technology development and so on. Text mining techniques have been mainly used to classify these technical documents. However, in the case of classifying technical documents by text mining algorithms, there is a disadvantage that the features representing technical documents must be directly extracted. In this study, we propose a BERT-based document classification model to automatically extract document features from text information of national R&D projects and to classify them. Then, we verify the applicability and performance of the proposed model for classifying documents."
BERT를 활용한 뉴스 감성분석과 거시경제지표 조합을 이용한 주가지수 예측,2020,"['Natural Language', 'Sentiment Analysis', 'Stock Prediction', 'Macro Economy Index', 'Deep learning', '자연어처리', '감성분석', '주가예측', '거시경제지표', '딥러닝']","주가지수는 한 국가의 경제 지표뿐만 아니라 투자판단의 지표로도 활용되므로 이를 예측하는 연구가지속해서 진행되고 있다. 주가지수 예측을 하는 작업은 기술적, 경제적 및 심리적 요인 등이 반영된것으로 예측의 정확도를 위해서는 복합적 요인을 고려해야 한다. 따라서 지수의 변동에 영향을 미치는요인들을 선별하여 반영한 주가지수 예측모델연구가 필요하다. 이와 관련한 기존 연구에서는 시장의변동을 만들어 내는 뉴스 정보 또는 거시 경제 지표를 각각 이용하거나, 몇 가지의 지표 조합만을반영한 예측 연구가 대부분이었다. 따라서 본 연구에서는 미국 다우존스지수 예측을 위해 뉴스 정보의감성 분석과 다양한 거시경제지표를 고려하여 효과적인 지표 조합을 제시하고자 한다. 뉴스 정보의감성 분석은 최신 자연어처리 기법인 BERT와 NLTK VADER를 사용하고, 예측모델은 주가예측모델로적합하다고 알려진 딥러닝 예측모델 LSTM을 적용하여 가장 효과적인 지표 조합을 제시했다.","The stock index is used not only as an economic indicator for a country, but also as an indicator for investment judgment, which is why research into predicting the stock index is ongoing. The task of predicting the stock price index involves technical, basic, and psychological factors, and it is also necessary to consider complex factors for prediction accuracy. Therefore, it is necessary to study the model for predicting the stock price index by selecting and reflecting technical and auxiliary factors that affect the fluctuation of the stock price according to the stock price. Most of the existing studies related to this are forecasting studies that use news information or macroeconomic indicators that create market fluctuations, or reflect only a few combinations of indicators. In this paper, this we propose to present an effective combination of the news information sentiment analysis and various macroeconomic indicators in order to predict the US Dow Jones Index. After Crawling more than 93,000 business news from the New York Times for two years, the sentiment results analyzed using the latest natural language processing techniques BERT and NLTK, along with five macroeconomic indicators, gold prices, oil prices, and five foreign exchange rates affecting the US economy Combination was applied to the prediction algorithm LSTM, which is known to be the most suitable for combining numeric and text information. As a result of experimenting with various combinations, the combination of DJI, NLTK, BERT, OIL, GOLD, and EURUSD in the DJI index prediction yielded the smallest MSE value."
소수 클래스 데이터 증강을 통한 BERT기반의 유형 분류 모델 성능 개선,2020,[],자연어처리 분야에서 딥러닝 기반의 분류 모델은 획기적인 성능을 보여주고 있다. 특히 2018 년 발표된 구글의 BERT 는 다양한 태스크에서 높은 성능을 보여준다. 본 논문에서는 이러한 BERT 가 클래스 불균형이 심한 데이터에 대해 어느 정도 성능을 보여주는지 확인하고 이를 해결하는 방법으로 EDA 를 선택해 성능을 개선하고자 한다. BERT 에 알맞게 적용하기 위해 다양한 방법으로 EDA를 구현했고 이에 대한 성능을 평가하였다.,다국어 초록 정보 없음
사전 학습된 한국어 BERT의 전이학습을 통한 한국어 기계독해 성능개선에 관한 연구,2020,"['Language Model', 'Masked Language Model', 'Question Answering', 'BERT', 'Fine-Tuning']",국문 초록 정보 없음,"Language Models such as BERT has been an important factor of deep learning-based natural language processing. Pre-training the transformer-based language models would be computationally expensive since they are consist of deep and broad architecture and layers using an attention mechanism and also require huge amount of data to train. Hence, it became mandatory to do fine-tuning large pre-trained language models which are trained by Google or some companies can afford the resources and cost. There are various techniques for fine tuning the language models and this paper examines three techniques, which are data augmentation, tuning the hyper paramters and partly re-constructing the neural networks. For data augmentation, we use no-answer augmentation and back-translation method. Also, some useful combinations of hyper parameters are observed by conducting a number of experiments. Finally, we have GRU, LSTM networks to boost our model performance with adding those networks to BERT pre-trained model. We do fine-tuning the pre-trained korean-based language model through the methods mentioned above and push the F1 score from baseline up to 89.66. Moreover, some failure attempts give us important lessons and tell us the further direction in a good way."
BERT를 활용한 문장 감정 분석 연구,2020,[],"소셜 네트워크 서비스 등의 발전으로 인해 개인이 다수에게 의견을 표출하는 통로가 활성화되었다. 게시물에 드러난 감정을 통해 특정 주제에 대한 여론을 도출할 수 있다. 본 논문에서는 BERT를 통한 문장 분석 기술, 그 중에서도 감정 분석을 하는 방법을 분석하고, 이를 일반화된 문장에 적용시키기 위한 데이터 셋 구성에 관하여 연구를 진행하였다.",다국어 초록 정보 없음
BERT 임베딩과 선택적 OOV 복사 방법을 사용한 문서요약,2020,"['BERT', 'random masked OOV', 'morpheme-to-sentence converter', 'text summarization', 'recognition of unknown word', 'deep-learning', 'generative summarization', 'BERT', 'OOV 랜덤 마스킹', '형태소-문장 변환기', '문서요약', '미등록 단어 인식', '딥러닝', '생성요약']","문서 자동 요약은 주어진 문서로부터 주요 내용을 추출하거나 생성하는 방식으로 짧게 줄이는 작업이다. 생성 요약은 미리 생성된 워드 임베딩 정보를 사용한다. 하지만, 전문 용어와 같이 저빈도 핵심 어휘는 임베딩 사전에서 누락되는 문제가 발생한다. 문서 자동 요약에서 미등록 어휘의 출현은 요약 성능을 저하시킨다. 본 논문은 Selectively Pointing OOV(Out of Vocabulary) 모델에 BERT(Bidirectional Encoder Representations from Transformers) 형태소 임베딩, Masked OOV, 형태소-to-문장 변환기를 적용하여 미등록 어휘에 대한 선택적 복사 및 요약 성능을 높였다. 기존 연구와 달리 정확한 포인팅 정보와 선택적 복사 지시 정보를 명시적으로 제공하는 선택적 OOV 포인팅 복사 방법과 함께 BERT 임베딩과 OOV 랜덤 마스킹, 형태소-문장 변환기를 추가하였다. 제안한 OOV 모델을 통해서 자동 생성 요약을 수행한 결과 단어 재현 기반의 ROUGE-1이 54.97 나타났으며, 또한 어순 기반의 ROUGE-L이 39.23으로 향상되었다.","Automatic text summarization is a process of shortening a text document via extraction or abstraction. Abstractive text summarization involves using pre-generated word embedding information. Low-frequency but salient words such as terminologies are seldom included in dictionaries, that are so called, out-of-vocabulary (OOV) problems. OOV deteriorates the performance of the encoder-decoder model in the neural network. To address OOV words in abstractive text summarization, we propose a copy mechanism to facilitate copying new words in the target document and generating summary sentences. Different from previous studies, the proposed approach combines accurately pointing information, selective copy mechanism, embedded by BERT, randomly masking OOV, and converting sentences from morpheme. Additionally, the neural network gate model to estimate the generation probability and the loss function to optimize the entire abstraction model was applied. Experimental results demonstrate that ROUGE-1 (based on word recall) and ROUGE-L (longest used common subsequence) of the proposed encoding-decoding model have been improved at 54.97 and 39.23, respectively."
BERT 언어 모델을 이용한 감정 분석 시스템,2020,[],"감정 분석은 문서의 주관적인 감정, 의견, 기분을 파악하기 위한 방법으로 소셜 미디어, 온라인 리뷰 등 다양한 분야에서 활용된다. 문서 내 텍스트가 나타내는 단어와 문맥을 기반으로 감정 수치를 계산하여 긍정 또는 부정 감정을 결정 한다. 2015년에 구축된 네이버 영화평 데이터 20 만개에 12 만개를 추가 구축하여 감정 분석 연구를 진행하였으며 언어 모델로는 최근 자연어처리 분야에서 높은 성능을 보여주는 BERT 모델을 이용하였다. 감정 분석 기법으로는 LSTM(Long Short-Term Memory) 등 기존의 기계학습 기법과 구글의 다국어 BERT 모델, 그리고 KoBERT 모델을 이용하여 감정 분석의 성능을 비교하였으며, KoBERT 모델이 89.90%로 가장 높은 성능을 보여주었다.",다국어 초록 정보 없음
DeNERT: DQN과 BERT를 이용한 개체명 인식 모델,2020,"['Natural language processing', 'Named entity recognition', 'Reinforcement learning', 'BERT', 'DQN', 'Language model', '자연어처리', '개체명 인식', '강화학습', '언어모델']","본 논문에서는 새로운 구조의 개체명 인식 DeNERT 모델을 제안한다. 최근 자연어처리 분야는 방대한 양의 말뭉치로 사전 학습된 언어 표현 모델을 활용하는 연구가 활발하다. 특히 자연어처리 분야 중 하나인 개체명인식은 대부분 지도학습 방식을 사용하는데, 충분히 많은 양의 학습 데이터 세트와 학습 연산량이 필요하다는 단점이 있다. 강화학습은 초기 데이터 없이 시행착오 경험을 통해 학습하는 방식으로 다른 기계학습 방법론보다 조금 더 사람이 학습하는 과정에 가까운 알고리즘으로 아직 자연어처리 분야에는 많이 적용되지 않은 분야이다. 아타리 게임이나 알파고 등 시뮬레이션 가능한 게임 환경에서 많이 사용된다. BERT는 대량의 말뭉치와 연산량으로 학습된 구글에서 개발한 범용 언어 모델이다. 최근 자연어 처리 연구 분야에서 높은 성능을 보이고 있는 언어 모델이며 많은 자연어처리 하위분야에서도 높은 정확도를 나타낸다. 본 논문에서는 이러한 DQN, BERT 두가지 딥러닝 모델을 이용한 새로운 구조의 개체명 인식 DeNERT 모델을 제안한다. 제안하는 모델은 범용 언어 모델의 장점인 언어 표현력을 기반으로 강화학습 모델의 학습 환경을 만드는 방법으로 학습된다. 이러한 방식으로 학습된 DeNERT 모델은 적은 양의 학습 데이터세트로 더욱 빠른 추론시간과 높은 성능을 갖는 모델이다. 마지막으로 제안하는 모델의 개체명 인식 성능평가를 위해 실험을 통해서 검증한다.","In this paper, we propose a new structured entity recognition DeNERT model. Recently, the field of natural language processing has been actively researched using pre-trained language representation models with a large amount of corpus. In particular, the named entity recognition, which is one of the fields of natural language processing, uses a supervised learning method, which requires a large amount of training dataset and computation. Reinforcement learning is a method that learns through trial and error experience without initial data and is closer to the process of human learning than other machine learning methodologies and is not much applied to the field of natural language processing yet. It is often used in simulation environments such as Atari games and AlphaGo. BERT is a general-purpose language model developed by Google that is pre-trained on large corpus and computational quantities. Recently, it is a language model that shows high performance in the field of natural language processing research and shows high accuracy in many downstream tasks of natural language processing. In this paper, we propose a new named entity recognition DeNERT model using two deep learning models, DQN and BERT. The proposed model is trained by creating a learning environment of reinforcement learning model based on language expression which is the advantage of the general language model. The DeNERT model trained in this way is a faster inference time and higher performance model with a small amount of training dataset. Also, we validate the performance of our model’s named entity recognition performance through experiments."
BERT 기반 한국어 개방형 정보 추출,2020,"['BERT', '개방형 정보 추출', '딥러닝', 'Sequence Labeling', 'open information extraction', 'deep learning', 'sequence labeling']","개방형 정보 추출은 자연어로 된 문장에서 구조화된 정보인 트리플을 추출하는 기술이다. 기존의 개방형 정보 추출은 입력 문장에서 관계 정보를 추출해야 하는 특성 때문에 품사 패턴, 의존 구문 분석 정보, 의미역 결정 정보 등을 이용한 복잡한 방법을 사용하였다. 본 논문에서는 한국어 개방형 정보 추출을 순차열 분류 문제로 보고 사전학습 된 BERT 모델을 적용하는 방법을 제안한다. 실험 결과 본 논문에서 제안한 모델이 정답이 아닌 자동으로 구축된 학습데이터 만을 사용했음에도 기존의 규칙기반의 방법 보다 F-1 measure 2～3% 정도의 성능향상을 보였다.","Open information extraction is a difficult technique for extracting triples, which are structured information from natural language sentences. Because the open information extraction task has the feature of extracting complex relation information from input sentences, traditionary open information extraction embraces a complicated method using parts-of-speech pattern information, dependency parsing information, and semantic role labeling information. In this paper, we propose a method to apply the pre-trained BERT model to the Korean open information extraction as a sequence labeling problem. The proposed model shows the F-1 measure 2% - 3% better than the rule-based method even though it uses only automatically constructed noisy training data."
A Small-Scale Korean-Specific BERT Language Model,2020,"['언어 모델링', '임베딩 모델', '한국어 모델', '사전', '토크나이저', 'BERT', 'language modeling', 'embedding model', 'Korean language modeling', 'vocabulary', 'tokenizer']",국문 초록 정보 없음,다국어 초록 정보 없음
BERT를 활용한 속성기반 감성분석: 속성카테고리 감성분류 모델 개발,2020,"['속성기반 감성분석', 'ABSA', '속성카테고리 감성분류', 'BERT', 'NLP', 'Aspect-Based Sentiment Analysis', 'Aspect Category Sentiment Classification']",국문 초록 정보 없음,"Sentiment Analysis (SA) is a Natural Language Processing (NLP) task that analyzes the sentiments consumers or the public feel about an arbitrary object from written texts. Furthermore, Aspect-Based Sentiment Analysis (ABSA) is a fine-grained analysis of the sentiments towards each aspect of an object. Since having a more practical value in terms of business, ABSA is drawing attention from both academic and industrial organizations. When there is a review that says “The restaurant is expensive but the food is really fantastic”, for example, the general SA evaluates the overall sentiment towards the ‘restaurant’ as ‘positive’, while ABSA identifies the restaurant’s aspect ‘price’ as ‘negative’ and ‘food’ aspect as ‘positive’. Thus, ABSA enables a more specific and effective marketing strategy.  In order to perform ABSA, it is necessary to identify what are the aspect terms or aspect categories included in the text, and judge the sentiments towards them. Accordingly, there exist four main areas in ABSA; aspect term extraction, aspect category detection, Aspect Term Sentiment Classification (ATSC), and Aspect Category Sentiment Classification (ACSC). It is usually conducted by extracting aspect terms and then performing ATSC to analyze sentiments for the given aspect terms, or by extracting aspect categories and then performing ACSC to analyze sentiments for the given aspect category.  Here, an aspect category is expressed in one or more aspect terms, or indirectly inferred by other words. In the preceding example sentence, ‘price’ and ‘food’ are both aspect categories, and the aspect category ‘food’ is expressed by the aspect term ‘food’ included in the review. If the review sentence includes ‘pasta’, ‘steak’, or ‘grilled chicken special’, these can all be aspect terms for the aspect category ‘food’. As such, an aspect category referred to by one or more specific aspect terms is called an explicit aspect. On the other hand, the aspect category like ‘price’, which does not have any specific aspect terms but can be indirectly guessed with an emotional word ‘expensive,’ is called an implicit aspect. So far, the ‘aspect category’ has been used to avoid confusion about ‘aspect term’. From now on, we will consider ‘aspect category’ and ‘aspect’ as the same concept and use the word ‘aspect’ more for convenience. And one thing to note is that ATSC analyzes the sentiment towards given aspect terms, so it deals only with explicit aspects, and ACSC treats not only explicit aspects but also implicit aspects.  This study seeks to find answers to the following issues ignored in the previous studies when applying the BERT pre-trained language model to ACSC and derives superior ACSC models. First, is it more effective to reflect the output vector of tokens for aspect categories than to use only the final output vector of [CLS] token as a classification vector? Second, is there any performance difference between QA (Question Answering) and NLI (Natural Language Inference) types in the sentence-pair configuration of input data? Third, is there any performance difference according to the order of sentence including aspect category in the QA or NLI type sentence-pair configuration of input data?  To achieve these research objectives, we implemented 12 ACSC models and conducted experiments on 4 English benchmark datasets. As a result, ACSC models that provide performance beyond the existing studies without expanding the training dataset were derived. In addition, it was found that it is more effective to reflect the output vector of the aspect category token than to use only the output vector for the [CLS] token as a classification vector. It was also found that QA type input generally provides better performance than NLI, and the order of the sentence with the aspect category in QA type is irrelevant with performance. There may be some differences depending on the characteristics of the dataset, but when using NLI type sentence-pair"
BERT를 이용한 한국어 질의응답 데이터 셋에서의 기계 독해,2020,[],국문 초록 정보 없음,"Machine reading comprehension (MRC) in multi-language datasets has not been explored as much as in English datasets. In this work, we demonstrate BERT-based question answering system on Korean question and answering dataset called KorQuAD 2.0. This paper explores KorQuAD 2.0 with BERT-multilingual model released by Google and Larva-base which is a pre-trained language model on Korean corpus released by Naver using an additional tokenizer. We also adopt negative sampling during training to balance the ratio of positive and negative data samples and a different size of window stride during inference to increase the inference speed. As a result, we achieve 58.21% of exact match (EM) score and 77.33% of F1 score which largely outperforms the previous baseline of 30.24% EM and 45.96% of F1 score."
BERT기반의 Context Sentence정보를 활용한 계약서 조항 분류 연구,2020,"['Legal Tech', 'Contract Analysis', 'BERT', 'Sentence Classification']",국문 초록 정보 없음,다국어 초록 정보 없음
BERT를 사용하는 호텔을 위한 다 기준 관광객 추천 시스템,2020,"['추천시스템', 'BERT', '다기준 추천시스템']",국문 초록 정보 없음,다국어 초록 정보 없음
BERT와 UDA를 이용한 철자오류에 강건한 한국어 자동 띄어쓰기,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
BERT를 활용한 비지도학습기반 한국어 문장 축약,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
BERT를 이용한 알츠하이머병 치매와 조현병 진단,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
한국어 개체명 분석을 위한 BERT-Tiny 기반 모델 연구,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
Fine-tuning BERT Models for Keyphrase Extraction in Scientific Articles,2020,"['keyphrase extraction', 'BERT', 'fine-tuning', 'embedding', 'scientific articles']",국문 초록 정보 없음,다국어 초록 정보 없음
Long-distant Coreference Resolution by Clustering-extended BERT for Korean and English Document,2020,"['딥러닝', '자연언어처리', '상호참조해결', 'deep learning', 'natural language processing', 'coreference resolution', 'BERT']",국문 초록 정보 없음,다국어 초록 정보 없음
BERT 기반의 인프라 피해산정,2020,"['BERT', '딥러닝', '자연어처리', '자연재해예방', '피해산정']",국문 초록 정보 없음,다국어 초록 정보 없음
Co-BERT: 도메인 특화 지식 그래프 생성 방법론,2020,"['Deep-Learning', 'Knowledge Graph', 'Text Analytics', 'Pre-Trained Language Model', 'BERT']",국문 초록 정보 없음,다국어 초록 정보 없음
BERT 및 Multi-Head Attention을 활용한 다국어 개방형 정보 추출 시스템,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
BERT 기반의 문장 의미 정보를 반영한 문장 내 정보 추출 연구,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
BERT 모델을 이용한 비교과 프로그램 문의 챗봇 시스템 설계,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
BERT 파생 모델의 한국어에 대한 성능 비교,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
BERT 언어 모델을 사용한 상담 매칭 시스템,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
빈도 분석과 BERT 단어 임베딩을 통한 뉴스 기사 제목에서의 용어 사용 변화 관측,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
한국어 의미역 결정을 위한 BERT 기반 데이터 증축 기법,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
데이터 증강을 활용한 BERT 언어모델 사전학습,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
감정 분석을 위한 BERT 사전학습모델과 추가 자질 모델의 결합,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
한국어 문장 유사도 측정을 위한 Sentence BERT,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
약물 부작용 시그널 탐지를 위한 BERT 기반의 약물 리뷰 레이블링 기법,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
다중 지문 기계 독해를 위한 경량화 BERT 모델의 비교,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
감정 분석을 위한 그래프 순위화 기반 강인한 한국어 BERT 모델,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
A Pre-trained Language Model for Chinese Pinyin-to-Character Task Based on BERT,2020,[],국문 초록 정보 없음,다국어 초록 정보 없음
그래프 구조를 이용한 악성 댓글 분류 시스템 설계 및 구현,2020,"['그래프', '텍스트 분류', '자연어처리', '악성댓글', '댓글분류', 'GCN', 'GAT', 'Text classification', 'NLP', 'BERT']","인터넷상의 소통을 위해 댓글 시스템은 필수적이다. 하지만 온라인상의 익명성을 악용하여 타인에 대한 부적절한 표현 등의 악성 댓글 또한 존재한다. 악성 댓글로부터 사용자를 보호하기 위해 악성/정상 댓글의 분류가 필요하고 이는 텍스트 분류로 구현할 수 있다. 자연어 처리에서 텍스트 분류는 중요한 주제 중 하나이고 최근 BERT 등 pretrained model을 활용한 연구와 GCN, GAT 등의 그래프 구조를 활용한 연구가 활발히 진행되고 있다. 본 연구에서는 실제 공개된 댓글에 대해 BERT, GCN, GAT 을 활용하여 댓글 분류 시스템을 구현하고 성능을 비교하였다. 본 연구에서는 그래프 기반 모델을 사용한 시스템이 BERT 대비 높은 성능을 보여주었다.","A comment system is essential for communication on the Internet. However, there are also malicious comments such as inappropriate expression of others by exploiting anonymity online. In order to protect users from malicious comments, classification of malicious / normal comments is necessary, and this can be implemented as text classification. Text classification is one of the important topics in natural language processing, and studies using pre-trained models such as BERT and graph structures such as GCN and GAT have been actively conducted. In this study, we implemented a comment classification system using BERT, GCN, and GAT for actual published comments and compared the performance. In this study, the system using the graph-based model showed higher performance than the BERT."
사전학습 언어모델의 기술 분석 연구,2020,"['pre-trained language model', 'fine-tuning', 'attention', 'self-supervised learning', 'BERT']",국문 초록 정보 없음,"The pre-training language model, BERT has achieved a great success in natural language processing by transferring knowledge from rich-resource pre-training task to the low-resource downstream tasks. The model was recognized as a breakthrough or an innovative technology that changed the paradigm of natural language processing. In this paper, a number of studies have been analyzed to classify and compare the research directions. We examined a technical challenges after BERT. In the pre-training process, self-supervised learning is performed, which relies entirely on training data. If we introduce the linguistic knowledge in the course of the training, it would be possible to get better result more effectively. Therefore, it is necessary to develop a method to insert external knowledge such as linguistic information in the training process. The mask language model and the next sentence prediction are being used in BERT's pre-training tasks. Though, to get much deeper understanding of the natural language, some other effective methods are to be studied and developed. Lastly, we should aim to develop eXplainable Artificial Intelligence (XAI) technology in natural language processing, helping us look into the transparent processing. The pre-trained language model focuses on the development of skills that can be used for all the tasks of natural language understanding. A lot of researches are focused on how to adapt language model effectively to downstream tasks based on common language models, even with the case with little data. It is also hoped that the technical analysis reviewed in this study will provide linguists and computer researchers with an opportunity to understand recent technological  achievements in the field of natural language processing and to seek joint research."
사전학습 언어모델을 이용한 기계번역시스템,2020,"['Deep Neural network', 'Denoising Autoencoder', 'Unsupervised Learning', 'BERT', 'Transfer learning', 'Machine Translation']",국문 초록 정보 없음,다국어 초록 정보 없음
안전기준의 검색과 분석을 위한 기계독해 기반 질의응답 시스템,2020,"['Q&A System', 'Machine Reading Comprehension', 'BERT', 'KorQuAd', 'Safety Standard']",국문 초록 정보 없음,"If various unreasonable safety standards are preemptively and effectively readjusted, the risk of accidents can be reduced. In this paper, we proposed a machine reading comprehension-based safety standard Q&A system to secure supporting technology for effective search and analysis of safety standards for integrated and systematic management of safety standards. The proposed model finds documents related to safety standard questions in the various laws and regulations, and then divides these documents into provisions. Only those provisions that are likely to contain the answer to the question are selected, and then the BERT-based machine reading comprehension model is used to find answers to questions related to safety standards. When the proposed safety standard Q&A system is applied to KorQuAD dataset, the performance of EM 40.42% and F1 55.34% are shown."
MASS와 상대 위치 표현을 이용한 영어-한국어 신경망 기계 번역,2020,"['machine translation', 'NMT', 'MASS', 'relative position representation', '기계 번역', 'NMT', 'MASS', '상대 위치 표현']","신경망 기계 번역(Neural Machine Translation)은 주로 지도 학습(supervised learning)을 이용하는 Sequence-to-Sequence 모델에 대한 연구가 이루어지고 있다. 그러나, 지도 학습 방법은 데이터가 부족한 경우에는 낮은 성능을 보이기 때문에, 최근에는 BERT와 MASS 같은 대량의 단일 언어 데이터 기반 사전학습(pre-training) 모델을 이용하여 미세조정(fine-tuning)을 하는 전이 학습(transfer learning) 방법이 자연어 처리 분야에서 주로 연구되고 있다. 본 논문에서는 언어 생성(language generation) 작업을 위한 사전학습 방법을 사용하는 MASS 모델을 영어-한국어 기계 번역에 적용하였다. 실험 결과, MASS 모델을 이용한 영어-한국어 기계 번역 모델의 성능이 기존 모델들보다 좋은 성능을 보였고, 추가로 MASS 모델에 상대 위치 표현 방법을 적용하여 기계 번역 모델의 성능을 개선하였다.","Neural Machine Translation has been mainly studied for a Sequence-to-Sequence model using supervised learning. However, since the supervised learning method shows low performance when the data is insufficient, recently, a transfer learning method of fine-tuning using the pre-training model based on a large amount of monolingual data such as BERT and MASS has been mainly studied in the field of natural language processing. In this paper, MASS using the pre-training method for language generation, was applied to the English-Korean machine translation. As a result of the experiment, the performance of the English-Korean machine translation model using MASS showed better performance than the existing models, and the performance of the machine translation model was further improved by applying the relative position representation method to MASS."
소셜미디어와 학술DB에 나타난 놀이의 교육적 의미 탐색: 의미연결망분석 및 감성분석을 중심으로,2020,"['놀이(play)', '소셜미디어(social media)', '의미연결망 분석(semantic network analysis)', '감성분석(sentimental analysis)']","본 연구는 소셜미디어 및 학술DB에 나타난 놀이의 교육적 의미 및 사회적 인식을 살펴보고 놀이 중심의 누리과정을 수행하는데 방향성을 제시하고자 수행되었다. 이를 위하여 누리과정의 놀이에 대한 선행연구를 고찰하고 텍스톰을 통해 자료를 검색하였다. 그 결과 총 8384개의 자료를 수집하여 2회의 클리닝 과정을 거쳤으며, 유아교육전문가 3인에게 내용타당도를 검증받아 총 7448개의 데이터와 학술DB에 있는 484개 논문 초록을 추출하였다. 추출한 데이터는 R을 활용하여 빈도분석 및 중심성 분석을 실시하였고, Ucinet6을 활용하여 concor 분석을 실시하였다. 분석 결과 유아, 누리과정, 유치원, 놀이중심, 교육과정, 놀이, 책, 교육 등의 키워드가 상위빈도를 차지하였고 놀이에 대한 3개의 군집은 키워드를 고려하여 ‘놀이 중심 교육’, 놀이지원방식’, ‘누리과정 적용’으로 명명하였다. 또한 구글 BERT를 활용하여 실시한 감성분석 결과 놀이는 긍정적인 감성이 더 높은 것으로 나타났다. 이러한 분석 결과를 기반으로 누리과정 운영에 있어 놀이중심의 지향성과 이를 성공적으로 실행하기 위한 실행 방향성에 대해 논의하였다.","The purpose of this study is to explore the educational meaning and social perception of play in social media and academic DB and to suggest direction in performing play-oriented Nuri curricula. For this the prior research on the play of the Nuri process were examined and the data were searched through textom. As a result, a total of 8,384 data were collected, on which two cleaning processes were performed, followed by feasibility verification by three early childhood education experts, and a total of 7448 data and abstracts from 484 papers in the academic DB were extracted. The text mining and network analysis were performed using R version 3.5.2 on extracted data, and concor analysis was performed using Ucinet6. The results were as the following, The key words such as young children, Nuri curricula, kindergarten, play center, curriculum, play, book, and education ranked the top in frequency. As a result of network and Concor analysis, three major clusters were formed and the clusters were named ""play-centered education"", ""play support method"", and ""application of Nuri curricula"" in consideration of the key words. Based on the results of this analysis, this study discusses the direction of play-oriented and successful implementation the Nuri curricula."
자연어처리 모델을 이용한 이커머스 데이터 기반 감성 분석 모델 구축,2020,"['NLP', 'BERT', 'KoBERT', 'ELMo', 'LSTM', 'CNN', '자연어처리', '감성 분석', '이커머스', '워드 임베딩', '센텐스 임베딩', '전이학습']","자연어 처리 분야에서 번역, 형태소 태깅, 질의응답, 감성 분석등 다양한 영역의 연구가 활발히 진행되고 있다. 감성 분석 분야는 Pretrained Model을 전이 학습하여 단일 도메인 영어 데이터셋에 대해 높은 분류 정확도를 보여주고 있다. 본 연구에서는 다양한 도메인 속성을 가지고 있는 이커머스 한글 상품평 데이터를 이용하고 단어 빈도 기반의 BOW(Bag Of Word), LSTM[1], Attention, CNN[2], ELMo[3], KoBERT[4] 모델을 구현하여 분류 성능을 비교하였다. 같은 단어를 동일하게 임베딩하는 모델에 비해 문맥에 따라 다르게 임베딩하는 전이학습 모델이 높은 정확도를 낸다는 것을 확인하였고, 17개 카테고리 별, 모델 성능 결과를 분석하여 실제 이커머스 산업에서 적용할 수 있는 감성 분석 모델 구성을 제안한다. 그리고 모델별 용량에 따른 추론 속도를 비교하여 실시간 서비스가 가능할 수 있는 모델 연구 방향을 제시한다.","In the field of Natural Language Processing, Various research such as Translation, POS Tagging, Q&A, and Sentiment Analysis are globally being carried out. Sentiment Analysis shows high classification performance for English single-domain datasets by pretrained sentence embedding models. In this thesis, the classification performance is compared by Korean E-commerce online dataset with various domain attributes and 6 Neural-Net models are built as BOW (Bag Of Word), LSTM[1], Attention, CNN[2], ELMo[3], and BERT(KoBERT)[4]. It has been confirmed that the performance of pretrained sentence embedding models are higher than word embedding models. In addition, practical Neural-Net model composition is proposed after comparing classification performance on dataset with 17 categories. Furthermore, the way of compressing sentence embedding model is mentioned as future work, considering inference time against model capacity on real-time service."
논문 및 특허 데이터를 활용한 자연어 처리 기반 ICT 동향 분석 연구,2020,"['동향 분석', '자연어 처리', 'BERT', '네트워크 분석', '정보통신기술', 'trend analysis', 'natural language processing', 'BERT', 'network analysis', 'information and communication technology']",국문 초록 정보 없음,다국어 초록 정보 없음
신경망기계번역 기술 진화와 번역품질 분석,2020,"['신경망기계번역', '트랜스포머', '언어모델', 'BERT', '비정상적 변동성', 'neural machine translation', 'translation quality', 'transformer', 'language model', 'BERT', 'unreasonable volatility']",국문 초록 정보 없음,"There was a big technical progress in the research field of machine translation: the main approach has switched from statistical machine translation (SMT) to neural machine translation (NMT), leading to dramatic improvements in translation quality. Recently, another progress has been taking place from recurrent neural network(RNN)-based NMT to transformer-based NMT (T-NMT). As the performance of NMT has evolved, a lot of research papers for machine translation have been published in the field of interpretation and translation. Their main focus is on whether machine translation can replace human translation, and analyzing the quality of translation results. In this paper, we briefly explain the history of the machine translation research and review the mechanism of NMT. NMT is basically composed of three parts: encoder, attention mechanism, and decoder. Further we discuss the new transformer structure based on the encoder-decoder model. We also discuss the challenges in NMT and explain the research direction or solutions to the problems. Particular attention is given to the mistranslation of NMT, quality of the translation, and robustness against the noises in the training dataset as well as in the testing sentences. In order to test the performance of transformer-based NMT, we used the Google NMT (GNMT) service for 4 languages – Korean, English, German, and Japanese. We confirmed the robustness against sentences with noises. However, we found unexpected volatility of NMT models where the input sentence is semantically and syntactically correct, resulting in critical degradation of translation quality."
딥러닝 사전학습 언어모델 기술 동향,2020,[],국문 초록 정보 없음,"Recently, a technique for applying a deep learning language model pretrained from a large corpus to fine-tuning for each application task has been widely used as a language processing technology. The pretrained language model shows higher performance and satisfactory generalization performance than existing methods. This paper introduces the major research trends related to deep learning pretrained language models in the field of language processing. We describe in detail the motivations, models, learning methods, and results of the BERT language model that had significant influence on subsequent studies. Subsequently, we introduce the results of language model studies after BERT, focusing on SpanBERT, RoBERTa, ALBERT, BART, and ELECTRA. Finally, we introduce the KorBERT pretrained language model, which shows satisfactory performance in Korean language. In addition, we introduce techniques on how to apply the pretrained language model to Korean (agglutinative) language, which consists of a combination of content and functional morphemes, unlike English (refractive) language whose endings change depending on the application."
PC-SAN: Pretraining-Based Contextual Self-Attention Model for Topic Essay Generation,2020,"['Natural language generation', 'Essay generation', 'Pretraining-based method', 'Self-attention network', 'Deep learning']",국문 초록 정보 없음,"Automatic topic essay generation (TEG) is a controllable text generation task that aims to generate informative, diverse, and topic-consistent essays based on multiple topics. To make the generated essays of high quality, a reasonable method should consider both diversity and topic-consistency. Another essential issue is the intrinsic link of the topics, which contributes to making the essays closely surround the semantics of provided topics. However, it remains challenging for TEG to fill the semantic gap between source topic words and target output, and a more powerful model is needed to capture the semantics of given topics. To this end, we propose a pretraining-based contextual self-attention (PC-SAN) model that is built upon the seq2seq framework. For the encoder of our model, we employ a dynamic weight sum of layers from BERT to fully utilize the semantics of topics, which is of great help to fill the gap and improve the quality of the generated essays. In the decoding phase, we also transform the target-side contextual history information into the query layers to alleviate the lack of context in typical self-attention networks (SANs). Experimental results on large-scale paragraph-level Chinese corpora verify that our model is capable of generating diverse, topic-consistent text and essentially makes improvements as compare to strong baselines. Furthermore, extensive analysis validates the effectiveness of contextual embeddings from BERT and contextual history information in SANs."
설명자의 영화평 감성 분석 결과 해석에 대한 고찰,2020,"['sentiment classification', 'BERT', 'Explainable AI', 'Explainer', 'IG', 'LIME', 'SHAP']",국문 초록 정보 없음,"This study examines how explainers interpret the results of sentiment analysis on movie reviews. Deep leaning is a state-of-art technique to deal with complex and very large volume of data. However, it is known as a black box model by which human cannot predict or trace its predictions. In order to provide explainable connection between its input and output, this study has applied a model, so-called an explainer. Two Sentiment analyzers are constructed utilizing a Multi-Layered Perceptrons model and a Gradient Boosting model, respectively. Three types of explainer models are adopted to interpret the sentiment classification; IG, LIME and SHAP. The interpretation result is examined through extracting highly relevant top N features and applying Heatmap. In addition, Exaplantion Component Agreement, an evaluation method is suggested to measure how the explanations are agreeable between the models and human annotators."
A Study on the Performance Analysis of Entity Name Recognition Techniques Using Korean Patent Literature,2020,"['Entity name recognition', 'word embedding', 'BiLSTM-CRF', 'BERT', 'NLP']",국문 초록 정보 없음,다국어 초록 정보 없음
The lumbar multifidus is characterised by larger type I muscle fibres compared to the erector spinae,2020,"['Paraspinal muscles', 'Skeletal muscle fibers']",국문 초록 정보 없음,"The metabolic capacity of a muscle is one of the determinants of muscle function. Muscle fiber type characteristics give an indication about this metabolic capacity. Therefore it might be expected that the lumbar multifidus (MF) as a local stabilizer contains higher proportions of slow type I fibers, compared to the erector spinae (ES) as a global mobilizer. The aim of this study is to determine the muscle fiber characteristics of the ES and MF to provide insight into their structural and metabolic characteristics, and thereby the functional capacity of both muscles. Muscle fiber type characteristics in the ES and MF were investigated with an immunofluorescence staining of the myosin heavy chain isoforms. In both the ES and MF, type I muscle fibers are predominantly present. The cross-sectional area (CSA) of type I muscle fibers is significantly larger in the lumbar MF compared to the ES. However, the mean muscle fiber type percentage for type I was not significantly different, which resulted in an insignificant difference in relative cross-sectional area (RCSA) for type I. No significant differences were found for all other muscle fiber types. This may indicate that the MF displays muscle fiber type characteristics that tend to be more appropriate to maintain stability of the spine. However, because we could not demonstrate significant differences in RCSA between ES and MF, we cannot firmly state that there are functional differences between the ES an MF based only on structural characteristics."
Data-driven models and computational tools for neurolinguistics: a language technology perspective,2020,"['neurolinguistics', 'neuroimaging data', 'EEG', 'fMRI', 'natural language representations', 'word embeddings', 'distributional semantics models', 'word2vec', 'GloVe', 'BERT', 'brain-aware embeddings']",국문 초록 정보 없음,"In this paper, our focus is the connection and influence of language technologies on the research in neurolinguistics. We present a review of brain imaging-based neurolinguistic studies with a focus on the natural language representations, such as word embeddings and pre-trained language models. Mutual enrichment of neurolinguistics and language technologies leads to development of brain-aware natural language representations. The importance of this research area is emphasized by medical applications."
Hidden Grief behind the Burnt-cork Makeup: Racial Melancholia in Caryl Phillips’s Dancing in the Dark,2020,"['Blackface', 'Caryl Phillips', 'Melancholia', 'Plantation Minstrelsy', 'In Dahomey']",국문 초록 정보 없음,"This article investigates the nature of the artistic and aesthetic innovations highlighted by Caryl Phillips’s Dancing in the Dark, a biological fiction about two Broadway vaudeville stars, Bert Williams and George Walker. Because the Williams and Walker Company features a black coon figure, the duo are fully aware of their audience’s expectations to see conventional racial travesty and stock plantation minstrel figures. With new dancing repertories and humanized characters, however, they attempt to advance past the bugbear of conventional racial phrenology and seek a radical break from earlier African American show business productions as well as from plantation minstrelsy. Their new American characters are daringly experimented with in a musical titled In Dahomey. My argument is that their critical innovations In Dahomey are a product of a type of melancholic subject formation that leads to the emergence of their self-critical agency. Williams and Walker’s melancholic reflections on white minstrelsy, indeed, trigger off some critical understanding of their own black coon performance, and the experience of being forced to play racial stereotypes causes another melancholia, from whose critical resources they start to design their musical experiment, In Dahomey."
Hidden Grief behind the Burnt-cork Makeup: Racial Melancholia in Caryl Phillips’s Dancing in the Dark,2020,"['Blackface', 'Caryl Phillips', 'Melancholia', 'Plantation Minstrelsy', 'In Dahomey']",국문 초록 정보 없음,"This article investigates the nature of the artistic and aesthetic innovations highlighted by Caryl Phillips’s Dancing in the Dark, a biological fiction about two Broadway vaudeville stars, Bert Williams and George Walker. Because the Williams and Walker Company features a black coon figure, the duo are fully aware of their audience’s expectations to see conventional racial travesty and stock plantation minstrel figures. With new dancing repertories and humanized characters, however, they attempt to advance past the bugbear of conventional racial phrenology and seek a radical break from earlier African American show business productions as well as from plantation minstrelsy. Their new American characters are daringly experimented with in a musical titled In Dahomey. My argument is that their critical innovations In Dahomey are a product of a type of melancholic subject formation that leads to the emergence of their self-critical agency. Williams and Walker’s melancholic reflections on white minstrelsy, indeed, trigger off some critical understanding of their own black coon performance, and the experience of being forced to play racial stereotypes causes another melancholia, from whose critical resources they start to design their musical experiment, In Dahomey."
딥러닝 기반 특허의 종속 청구항 인식 개선,2020,[],"특허를 통해 기술의 권리를 정의하고 보호하는 일이 매우 중요해짐에 따라 특허 문서를 분석하는 연구 또한 중요해지고 있다. 특히 특허의 청구항을 종속항과 독립항을 구분하고, 관련된 인용을 찾아내는 일은 관련 특허들을 분석하는데 매우 중요하다. 본 연구는 최근 텍스트 분석 분야에 획기적 성능 개선을 이끈 BERT(Bidirectional Encoder Representations From Transformers) 언어 모델을 사용하고 Neural Network 의 파인 튜닝 과정을 통해 청구항의 독립과 종속을 구분하였고, 인용하는 항의 번호와 인용 문구로 이루어진 인용 패턴을 통해 종속항의 인용 항을 찾아내었다. 이 방법을 2003 년 이후의 xml 형식의 미국 특허 데이터에 사용한 결과, 정확도 99% 의 성능을 확보하였다.",다국어 초록 정보 없음
딥러닝을 활용한 감정 분석 과정에서 필요한 데이터 전처리 및 형태 변형,2020,"['data preprocessing', 'transformation', 'sentiment analysis', 'deep learning']",국문 초록 정보 없음,"This study examined how to preprocess and transform data efficiently in order to use deep learning techniques in analyzing linguistic data. Researchers’ interests in deep learning techniques have explosively increased worldwide; however, it is not easy for them to link linguistics to deep learning techniques or algorithms because linguists do not know how and where to begin in using them. Thus, this study provides the general procedure to train data using deep learning algorithms in practice. In particular, for instance, we focused on how to preprocess and transform Tweet data for a sentiment analysis by using deep learning techniques. In addition, we introduced the latest deep learning algorithm, so-called BERT, in the data preprocessing and transformation procedure. The data preprocessing is particularly important because the result from deep learning can significantly vary depending on it. Even though the data preprocessing procedure can differ according to the aim of research, this study tries to introduce the general way that advanced researchers frequently use for deep learning algorithms. This study is expected to lower the barriers in applying deep learning techniques to linguistic data and make it easier for researchers to conduct deep learning research related to linguistics."
사전훈련 된 모델을 통한 한국어 임베딩 성능 비교,2020,"['Natural Language Processing', '자연어 처리', 'Pre-trained Model', '사전훈련 된 모델', 'RNN', '순환신경망', 'Attention', '어텐션', 'Transformer', '트랜스포머']",국문 초록 정보 없음,"The field of natural language processing achieved rapid growth as a deep learning-based method, which is a neural network, was applied from a statistical-based method. The deep learning model uses an end-to-end technique that induces the model to understand itself from start to finish without human intervention. However, in order to train a natural language processing model, a lot of hardware resources are required. In order to overcome hardware limitations, various pretrained models are used and fine tuning is applied to downstream tasks. Among the pretrained models, the most representative model is BERT (Bidirectional Encoder Representations from Transformer) technology, which is applied to various tasks. In this paper, we compare the performance with the existing neural network model by applying the pretrained Korean model to the subtractive classification data."
