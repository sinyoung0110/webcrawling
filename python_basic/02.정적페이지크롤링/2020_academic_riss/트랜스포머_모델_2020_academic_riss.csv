title,date,keywords,abstract,multilingual_abstract
사전학습 언어모델 기반 트랜스포머를 활용한 의미유사도기반 자연어이해 의도파악 방법,2020,"['심층신경망', '자연어이해', '의도분석', '의도파악', '의미유사도', '트랜스포머', 'deep neural network', 'natural language understanding', 'intention analysis', 'semantic similarity', 'transformer']","자연어이해는 로봇, 메신저, 자연어 인터페이스 등에 활용되는 근간 기술 중 하나이다. 본 연구에서는 자연어이해 문제 중 문장의 의도를 파악하는 의도파악기술에 있어, 전통적인 분류기술을 활용하는 것이 아닌, 문장의 의미를 벡터 형태로 가공할 수 있는 문장 및 의미틀 읽기장치를 학습시키고, 훈련문장과 질의문장의 벡터 공간상의 의미거리를 측정하여, 가장 가까운 훈련문장의 의도를 질의문장의 의도로 부착하는 방법을 제안한다. 이를 위해, 사전학습 언어모델 기반 트랜스포머를 활용하여 기호 형태의 문장 및 의미틀을 벡터 형태로 변환하는 방법을 소개한다. 한국어 기반 날씨 및 내비게이션 영역의 말뭉치와 영어 기반 항공교통 예약 영역, 음성 언어 이해 시스템 영역의 자연어 말뭉치등을 활용한 다양한 실험을 통하여 제안한 방법이 성공적으로 의미벡터를 배움을 보이고, 기존 의도파악 기술 대비 높은 성능을 가짐을 보인다.","Natural language understanding (NLU) is a central technique applied to developing robot, smart messenger, and natural interface. In this study, we propose a novel similarity-based intent analysis method instead of the typical classification methods for intent analysis problems in the NLU.To accomplish this, the neural network-based text and semantic frame readers are introduced to learn semantic vectors using pairwise text-semantic frame instances. The text to vector and the semantic frame to vector projection methods using the pre-trained transformer are proposed. Then, we propose a method of attaching the intention tag of the nearest training sentence to the query sentence by measuring the semantic vector distances in the vector space. Four experiments on the natural language learning suggest that the proposed method demonstrates superior performance compared to the existing intention analysis techniques. These four experiments use natural language corpora in Korean and English. The two experiments in Korean are weather and navigation language corpora, and the two English-based experiments involve air travel information systems and voice platform language corpora."
다중 인코더 Transformer 기반 번역문 자동 사후 교정 모델의 디코더 주의 구조 연구,2020,"['machine translation', 'automatic post-editing', 'transformer', 'multi-encoder architecture', 'attention mechanism', '기계 번역', '번역문 자동 사후 교정', '트랜스포머', '다중 인코더 구조', '주의 집중 기법']","번역문 자동 사후 교정은 기계 번역 시스템의 결과물을 자동으로 교정하여 더 나은 번역문을 만들어내는 과정으로, 기계 번역 시스템 외적으로 기계 번역의 품질 향상을 수행하기 위해 제안된 연구 분야이다. 본 논문에서는 번역문 자동 사후 교정 문제에 사용되는 다중 인코더 Transformer 기반 교정 모델의 기본 구조를 살펴본 뒤, 디코더에서 인코더 출력과의 상호 의존성을 담당하는 주의 구조를 다양하게 구성하고 적용해 보았다. WMT18 사후 교정 말뭉치를 이용한 실험에서는 다중 인코더 Transformer를 이용한 모델 전부가 기계 번역 시스템의 결과물에 비해 더 나은 품질의 문장을 생성하였으며, 디코더 내에 번역문에 원문의 문맥 정보를 반영하는 구조를 적용하는 것이 사후 교정의 성능 향상에 크게 기여함을 확인할 수 있었다.","Automatic Post-Editing (APE) is a study on the correction for the output of machine translation (MT) system for the purpose of improving its translation quality independent of the MT system itself. In this paper, we examine the basic architecture of a multi-encoder transformer-based APE model, and implement several variants of the system’s encoder-decoder attention layer which takes the outputs of the multi-encoder as its inputs. In experiments with the WMT18 APE data, all variations on our model successfully improve the translation quality of the original MT outputs. In particular, we find modeling the attention to incorporate source sentence context into the translated sentence improves post-editing performance."
Attention에 기반한 한국어 언어모델 연구,2020,"['Language model', 'Language model based on DNN', 'Transformer for language model', 'Sentence-piece tokenizer']","본 논문에서는 어텐션(atention)에 기반을 둔 한국어 언어모델에 관한 연구를 수행하였다. 대표적인 어텐션 모델로 셀프 어텐션(self-attention)이 가능한 트랜스포머(transformer)가 있다. 트랜스포머는 인코더와 디코더로 구성이 되는데 언어모델로는 디코더를 일반적으로 사용한다. 한국어에 적용 실험을 하기 위해서 기본 토큰 단위로 센텐스피스(SentencePiece)를 사용하여 구하였다. AI-Hub 한국어 평가 코퍼스 60만 문장을 이용하여 성능 비교를 한 결과 5,000개의 센텐스피스 토큰을 사용한 것이 10,000개의 센텐스피스 토큰을 사용한 것과 비교하였을 경우 33.4%의 복잡도가 감소하였다. 또한 한국어 음성인식 실험을 통하여 복잡도 성능이 우수한 5,000개의 센텐스피스 토큰을 갖는 언어모델의 성능이 우수하다는 것을 보였다.","In this paper, we make a study on the language model based on attention. The representative attention model is a transformer model, which enables a self-attention. Even though the transformer model consists of encoder and decoder, decoder is usually used for language model. We build a sentence-piece model for tokenizing. The experimental result yields that the token unit number of 5000 gets the perplexity(ppl) reduction of 33.4% compared with that of 10,000 when AI-Hub corpus (https://www.aihub.org.kr) is used in the sentence-piece model. In order to prove the performance of language model with regard to perplexity, we make an experiment of speech recognition that the model with low perplexity yields better performance than that with high perplexity."
트랜스포머와 BERT로 구현한 한국어 형태소 분석기의 성능 분석,2020,"['sequence-to-sequence', 'Korean morphological analyzer', 'Transformer', 'BERT', 'attention mechanism', 'copying mechanism', '시퀀스-투-시퀀스', '한국어 형태소 분석기', '트랜스포머', 'BERT', '주의 메커니즘', '복사 메커니즘']","본 논문은 Transformer로 구현한 한국어 형태소 분석기를 다룬다. Transformer는 최근에 가장 널리 사용되는 sequence-to-sequence 모델 중 하나이다. Transformer는 인코더와 디코더로 구성되어 있는데 인코더는 원문을 고정된 크기의 벡터로 압축시키고 디코더는 이 벡터를 이용하여 형태소 분석 결과를 생성해 낸다. 본 논문에서는 또한 Transformer의 인코더를 BERT로 대체해 본다. BERT는 대용량의 학습데이터를 이용하여 미리 학습시켜 놓은 언어 표현 모델이다. 디코더에는 주의 메커니즘과 복사 메커니즘을 도입하였다. 인코더와 디코더에서의 처리 단위는 각각 어절 단위 WordPiece와 형태소 단위의 WordPiece를 사용하였다. 실험을 통해, BERT의 파라미터를 문제에 맞게 재조정했을 때의 성능이 Transformer를 임의의 값으로 초기화하여 사용했을 때에 비해 F1에서 2.9%의 성능 향상을 보임을 알 수 있었다. 또한 학습단계에서 충분히 학습되지 못한 WordPiece의 임베딩이 형태소 분석에 어떤 영향을 미치는지도 살펴보았다.","This paper introduces a Korean morphological analyzer using the Transformer, which is one of the most popular sequence-to-sequence deep neural models. The Transformer comprises an encoder and a decoder. The encoder compresses a raw input sentence into a fixed-size vector, while the decoder generates a morphological analysis result for the vector. We also replace the encoder with BERT, a pre-trained language representation model. An attention mechanism and a copying mechanism are integrated in the decoder. The processing units of the encoder and the decoder are eojeol-based WordPiece and morpheme-based WordPiece, respectively. Experimental results showed that the Transformer with fine-tuned BERT outperforms the randomly initialized Transformer by 2.9% in the F1 score. We also investigated the effects of the WordPiece embedding on morphological analysis when they are not fully updated in the training phases."
사전훈련 된 모델을 통한 한국어 임베딩 성능 비교,2020,"['Natural Language Processing', '자연어 처리', 'Pre-trained Model', '사전훈련 된 모델', 'RNN', '순환신경망', 'Attention', '어텐션', 'Transformer', '트랜스포머']",,"The field of natural language processing achieved rapid growth as a deep learning-based method, which is a neural network, was applied from a statistical-based method. The deep learning model uses an end-to-end technique that induces the model to understand itself from start to finish without human intervention. However, in order to train a natural language processing model, a lot of hardware resources are required. In order to overcome hardware limitations, various pretrained models are used and fine tuning is applied to downstream tasks. Among the pretrained models, the most representative model is BERT (Bidirectional Encoder Representations from Transformer) technology, which is applied to various tasks. In this paper, we compare the performance with the existing neural network model by applying the pretrained Korean model to the subtractive classification data."
제한된 학습 데이터를 사용하는 End-to-End 음성 인식 모델,2020,"['speech recognition', 'end-to-end model', 'small-data speech recognition', '음성 인식', '종단간 음성 인식', '적은 데이터 음성 인식']","음성 인식은 딥러닝 및 머신러닝 분야에서 활발히 상용화 되고 있는 분야 중 하나이다. 그러나, 현재 개발되고 있는 음성 인식 시스템은 대부분 성인 남녀를 대상으로 인식이 잘 되는 실정이다. 이것은 음성 인식 모델이 대부분 성인남녀 음성 데이터베이스를 학습하여 구축된 모델이기 때문이다. 따라서, 노인, 어린이 및 사투리를 갖는 화자의 음성을 인식하는데 문제를 일으키는 경향이 있다. 노인과 어린이의 음성을 잘 인식하기 위해서는 빅데이터를 구축하는 방법과 성인 대상 음성 인식 엔진을 노인 및 어린이 데이터로 적응하는 방법 등이 있을 수 있지만, 본 논문에서는 음향적 데이터 증강에 기반한 재귀적 인코더와 언어적 예측이 가능한 transformer 디코더로 구성된 새로운 end-to-end 모델을 제안한다. 제한된 데이터셋으로 구성된 한국어 노인 및 어린이 음성 인식을 통해 제안된 방법의 성능을 평가한다.","Speech recognition is one of the areas actively commercialized using deep learning and machine learning techniques. However, the majority of speech recognition systems on the market are developed on data with limited diversity of speakers and tend to perform well on typical adult speakers only. This is because most of the speech recognition models are generally learned using a speech database obtained from adult males and females. This tends to cause problems in recognizing the speech of the elderly, children and people with dialects well. To solve these problems, it may be necessary to retain big database or to collect a data for applying a speaker adaptation. However, this paper proposes that a new end-to-end speech recognition method consists of an acoustic augmented recurrent encoder and a transformer decoder with linguistic prediction. The proposed method can bring about the reliable performance of acoustic and language models in limited data conditions. The proposed method was evaluated to recognize Korean elderly and children speech with limited amount of training data and showed the better performance compared of a conventional method."
Transformer 네트워크를 이용한 음성신호 변환,2020,"['voice conversion', 'transformer network', 'signal-to-signal conversion', '음성 변환', '트랜스포머 네트워크', '신호 대 신호 변환']","음성 변환은 다양한 음성 처리 응용에 적용될 수 있으며, 음성 인식을 위한 학습 데이터 증강에도 중요한 역할을 할 수 있다. 기존의 방법은 음성 합성을 이용하여 음성 변환을 수행하는 구조를 사용하여 멜 필터뱅크가 중요한 파라미터로 활용된다. 멜 필터뱅크는 뉴럴 네트워크 학습의 편리성 및 빠른 연산 속도를 제공하지만, 자연스러운 음성파형을 생성하기 위해서는 보코더를 필요로 한다. 또한, 이 방법은 음성 인식을 위한 다양한 데이터를 얻는데 효과적이지 않다. 이 문제를 해결하기 위해 본 논문은 원형 스펙트럼을 사용하여 음성 신호 자체의 변환을 시도하였고, 어텐션 메커니즘으로 스펙트럼 성분 사이의 관계를 효율적으로 찾아내어 변환을 위한 자질을 학습할 수 있는 transformer 네트워크 기반 딥러닝 구조를 제안하였다. 영어 숫자로 구성된 TIDIGITS 데이터를 사용하여 개별 숫자변환 모델을 학습하였고, 연속 숫자 음성 변환 디코더를 통한 결과를 평가하였다. 30명의 청취 평가자를 모집하여 변환된 음성의 자연성과 유사성에 대해 평가를 진행하였고, 자연성 3.52±0.22 및 유사성 3.89±0.19 품질의 성능을 얻었다.","Voice conversion can be applied to various voice processing applications. It can also play an important role in data augmentation for speech recognition. The conventional method uses the architecture of voice conversion with speech synthesis, with Mel filter bank as the main parameter. Mel filter bank is well-suited for quick computation of neural networks but cannot be converted into a high-quality waveform without the aid of a vocoder. Further, it is not effective in terms of obtaining data for speech recognition. In this paper, we focus on performing voice-to-voice conversion using only the raw spectrum. We propose a deep learning model based on the transformer network, which quickly learns the voice conversion properties using an attention mechanism between source and target spectral components. The experiments were performed on TIDIGITS data, a series of numbers spoken by an English speaker. The conversion voices were evaluated for naturalness and similarity using mean opinion score (MOS) obtained from 30 participants. Our final results yielded 3.52±0.22 for naturalness and 3.89±0.19 for similarity."
Numerical Analysis of Surface and Internal Discharge Phenomena at the Interface of Hetero‑Dielectric Composites Based on the Migration–Migration Model,2020,['BCT model · Migration–migration model · Nanocomposites · Surface discharge'],,"Nanocomposites have been actively studied for understanding the superior electrical characteristics and for utilizing in the electric devices of HVDC systems. Nanocomposites in electrical devices are not used unilaterally, but rather as the heterodielectric composites, which form an interface with other insulators. In this study, the hetero-dielectric composites comprised of nanocomposites and transformer oil were analyzed using the migration–migration model incorporating the fnite element method. Until now, hetero-dielectric composites have been usually studied by using the Migration–Ohmic model, where the specifc space charge behavior cannot be analyzed. Although interface conditions are signifcant for the transport and local accumulation of charge, the BCT models, including conditions for interface with other insulators, have not been reported.To investigate the space charge behavior of the hetero-dielectric composites, charge carriers of positive, negative ions and electrons are considered in the liquid region and electrons and holes in the nanocomposites. These hetero-dielectric composites are applied to the needle-bar electrode system to analyze the surface discharge phenomena at the interface. The BCT model was applied to the nanocomposites, and the drift dominated charge continuity model was applied to the transformer oil. These numerical results were compared with those from the previous research works validated with the experimental results. We analyzed the propagation of the streamer with an equivalent parallel current circuit and adopted the efective mobility of the electrons according to the feld strength to improve the applicability of the numerical analysis model."
Four-port Hydraulic Transformer with Inlet and Outlet Equal Flow and Its Efficiency Characteristics,2020,"['Four distribution ports', 'Hydraulic transformer', 'Efficiency characteristics', 'Theoretical and Experimental', 'Energy saving']",,"In order to avoid throttling loss and recover differential pressure energy, braking energy and gravitational potential energy without changing the original hydraulic system, the four-port hydraulic transformer (FHT) is proposed. Its theoretical and experimental efficiency characteristics are researched. The basic structure and principle of the FHT are explained. Besides, the mathematical model of its efficiency is established. The results show that the pressure ratio can be changed by adjusting the control angle of valve plate, and the efficiency characteristics of the FHT match the efficiency characteristics of the hydraulic pump/motor. The total efficiency increases first and decreases afterwards with the increment of cylinder speed, and increases with the control angle of valve plate, while decreases with the recycling pressure difference."
BERT를 이용한 한국어 의미역 결정,2020,"['의미역 결정', '기계학습', '언어 모델', 'Bidirectional Encoder Representations from Transformers', 'semantic role labeling', 'machine learning', 'language model', 'bidirectional encoder representations from transformers']","의미역 결정은 문장 내에서 “누가, 무엇을, 어떻게, 왜” 등의 관계를 찾아내는 자연어처리의 한 응용이다. 최근 의미역 결정 연구는 주로 기계학습을 이용하고 자질 정보를 배제한 종단 대 종단(end-to-end) 방식의 연구가 이루어지고 있다. 최근 BERT(Bidirectional Encoder Representations from Transformers)라는 언어 모델이 자연어처리 분야에 등장하여 기존 자연어처리 분야의 최고 성능 모델들 보다 더 좋은 성능을 보이고 있다. 종단 대 종단 방식을 이용한 의미역 결정 연구의 성능은 주로 기계학습 모델의 구조나 사전에 학습된 언어 모델의 영향을 받는다. 따라서 본 논문에서는 한국어 의미역 결정 성능 향상을 위해 BERT를 한국어 의미역 결정에 적용한다. 실험 결과 BERT를 이용한 한국어 의미역 결정 모델의 성능이 85.77%로 기존 한국어 의미역 결정 모델들 보다 좋은 성능을 보였다.","Semantic role labeling is an application of natural language processing to identify relationships such as ""who, what, how and why"" with in a sentence. The semantic role labeling study mainly uses machine learning algorithms and the end-to-end method that excludes feature information.Recently, a language model called BERT (Bidirectional Encoder Representations from Transformers) has emerged in the natural language processing field, performing better than the stateof- the-art models in the natural language processing field. The performance of the semantic role labeling study using the end-to-end method is mainly influenced by the structure of the machine learning model or the pre-trained language model. Thus, in this paper, we apply BERT to the Korean semantic role labeling to improve the Korean semantic role labeling performance. As a result, the performance of the Korean semantic role labeling model using BERT is 85.77%, which is better than the existing Korean semantic role labeling model."
신경망기계번역 기술 진화와 번역품질 분석,2020,"['신경망기계번역', '트랜스포머', '언어모델', 'BERT', '비정상적 변동성', 'neural machine translation', 'translation quality', 'transformer', 'language model', 'BERT', 'unreasonable volatility']",,"There was a big technical progress in the research field of machine translation: the main approach has switched from statistical machine translation (SMT) to neural machine translation (NMT), leading to dramatic improvements in translation quality. Recently, another progress has been taking place from recurrent neural network(RNN)-based NMT to transformer-based NMT (T-NMT). As the performance of NMT has evolved, a lot of research papers for machine translation have been published in the field of interpretation and translation. Their main focus is on whether machine translation can replace human translation, and analyzing the quality of translation results. In this paper, we briefly explain the history of the machine translation research and review the mechanism of NMT. NMT is basically composed of three parts: encoder, attention mechanism, and decoder. Further we discuss the new transformer structure based on the encoder-decoder model. We also discuss the challenges in NMT and explain the research direction or solutions to the problems. Particular attention is given to the mistranslation of NMT, quality of the translation, and robustness against the noises in the training dataset as well as in the testing sentences. In order to test the performance of transformer-based NMT, we used the Google NMT (GNMT) service for 4 languages – Korean, English, German, and Japanese. We confirmed the robustness against sentences with noises. However, we found unexpected volatility of NMT models where the input sentence is semantically and syntactically correct, resulting in critical degradation of translation quality."
기계번역 사후교정(Automatic Post Editing) 연구,2020,"['기계번역', '기계번역 사후교정', '딥러닝', '인공신경망 기계번역', '트랜스포머', 'Machine Translation', 'Automatic Post Editing', 'Deep Learning', 'Neural Machine Translation', 'Transformer']",기계번역이란 소스문장(Source Sentence)을 타겟문장(Target Sentence)으로 컴퓨터가 번역하는 시스템을 의미한다. 기계번역에는 다양한 하위분야가 존재하며 APE(Automatic Post Editing)이란 기계번역 시스템의 결과물을 교정하여 더 나은 번역문을 만들어내는 기계번역의 하위분야이다. 즉 기계번역 시스템이 생성한 번역문에 포함되어 있는 오류를 수정하여 교정문을 만드는 과정을 의미한다. 기계번역 모델을 변경하는 것이 아닌 기계번역 시스템의 결과 문장을 교정하여 번역품질을 높이는 연구분야이다. 2015년부터 WMT 공동 캠페인 과제로 선정되었으며 성능 평가는 TER(Translation Error Rate)을 이용한다. 이로 인해 최근 APE에 모델에 대한 다양한 연구들이 발표되고 있으며 이에 본 논문은 APE 분야의 최신 동향에 대해서 다루게 된다.,"Machine translation refers to a system where a computer translates a source sentence into a target sentence. There are various subfields of machine translation. APE (Automatic Post Editing) is a subfield of machine translation that produces better translations by editing the output of machine translation systems. In other words, it means the process of correcting errors included in the translations generated by the machine translation system to make proofreading. Rather than changing the machine translation model, this is a research field to improve the translation quality by correcting the result sentence of the machine translation system. Since 2015, APE has been selected for the WMT Shaed Task. and the performance evaluation uses TER (Translation Error Rate). Due to this, various studies on the APE model have been published recently, and this paper deals with the latest research trends in the field of APE."
BERT를 이용한 한국어 특허상담 기계독해,2020,[],"기계독해는(Machine reading comprehension) 사용자 질의와 관련된 문서를 기계가 이해한 후 정답을 추론하는 인공지능 자연어처리 태스크를 말하며, 이러한 기계독해는 챗봇과 같은 자동상담 서비스에 활용될 수 있다. 최근 자연어처리 분야에서 가장 높은 성능을 보이고 있는 BERT 언어모델은 대용량의 데이터를 pre-training 한 후에 각 자연어처리 태스크에 대해 fine-tuning하여 학습된 모델로 추론함으로써 문제를 해결하는 방식이다. 본 논문에서는 BERT기반 특허상담 기계독해 태스크를 위해 특허상담 데이터 셋을 구축하고 그 구축 방법을 소개하며, patent 코퍼스를 pre-training한 Patent-BERT 모델과 특허상담 모델학습에 적합한 언어처리 알고리즘을 추가함으로써 특허상담 기계독해 태스크의 성능을 향상시킬 수 있는 방안을 제안한다. 본 논문에서 제안한 방법을 사용하여 특허상담 질의에 대한 정답 결정에서 성능이 향상됨을 보였다.","MRC (Machine reading comprehension) is the AI NLP task that predict the answer for user's query by understanding of the relevant document and which can be used in automated consult services such as chatbots. Recently, the BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding) model, which shows high performance in various fields of natural language processing, have two phases. First phase is Pre-training the big data of each domain. And second phase is fine-tuning the model for solving each NLP tasks as a prediction. In this paper, we have made the Patent MRC dataset and shown that how to build the patent consultation training data for MRC task. And we propose the method to improve the performance of the MRC task using the Pre-trained Patent-BERT model by the patent consultation corpus and the language processing algorithm suitable for the machine learning of the patent counseling data. As a result of experiment, we show that the performance of the method proposed in this paper is improved to answer the patent counseling query."
BERT 임베딩과 선택적 OOV 복사 방법을 사용한 문서요약,2020,"['BERT', 'random masked OOV', 'morpheme-to-sentence converter', 'text summarization', 'recognition of unknown word', 'deep-learning', 'generative summarization', 'BERT', 'OOV 랜덤 마스킹', '형태소-문장 변환기', '문서요약', '미등록 단어 인식', '딥러닝', '생성요약']","문서 자동 요약은 주어진 문서로부터 주요 내용을 추출하거나 생성하는 방식으로 짧게 줄이는 작업이다. 생성 요약은 미리 생성된 워드 임베딩 정보를 사용한다. 하지만, 전문 용어와 같이 저빈도 핵심 어휘는 임베딩 사전에서 누락되는 문제가 발생한다. 문서 자동 요약에서 미등록 어휘의 출현은 요약 성능을 저하시킨다. 본 논문은 Selectively Pointing OOV(Out of Vocabulary) 모델에 BERT(Bidirectional Encoder Representations from Transformers) 형태소 임베딩, Masked OOV, 형태소-to-문장 변환기를 적용하여 미등록 어휘에 대한 선택적 복사 및 요약 성능을 높였다. 기존 연구와 달리 정확한 포인팅 정보와 선택적 복사 지시 정보를 명시적으로 제공하는 선택적 OOV 포인팅 복사 방법과 함께 BERT 임베딩과 OOV 랜덤 마스킹, 형태소-문장 변환기를 추가하였다. 제안한 OOV 모델을 통해서 자동 생성 요약을 수행한 결과 단어 재현 기반의 ROUGE-1이 54.97 나타났으며, 또한 어순 기반의 ROUGE-L이 39.23으로 향상되었다.","Automatic text summarization is a process of shortening a text document via extraction or abstraction. Abstractive text summarization involves using pre-generated word embedding information. Low-frequency but salient words such as terminologies are seldom included in dictionaries, that are so called, out-of-vocabulary (OOV) problems. OOV deteriorates the performance of the encoder-decoder model in the neural network. To address OOV words in abstractive text summarization, we propose a copy mechanism to facilitate copying new words in the target document and generating summary sentences. Different from previous studies, the proposed approach combines accurately pointing information, selective copy mechanism, embedded by BERT, randomly masking OOV, and converting sentences from morpheme. Additionally, the neural network gate model to estimate the generation probability and the loss function to optimize the entire abstraction model was applied. Experimental results demonstrate that ROUGE-1 (based on word recall) and ROUGE-L (longest used common subsequence) of the proposed encoding-decoding model have been improved at 54.97 and 39.23, respectively."
탭 변환 단권변압기 기반 LVRT/HVRT 시험장비의 임피던스 설계,2020,"['Low Voltage Ride Through(LVRT)', 'High Voltage Ride Through(HVRT)', 'Grid connected wind turbines', 'Grid code', 'LVRT/HVRT test device']","본 논문은 계통 연계 기준인 Low Voltage Ride Through(LVRT) 및 High Voltage Ride Through(HVRT) 기능을 평가하기 위한 시험 장비의 임피던스 설계 방법을 제안한다. LVRT/HVRT 시험 장비는 계통 연계 규정에 명시되어 있는 계통 사고전압을 일정시간 동안 발생시킬 수 있어야 하며 설계 사양에 맞게 사고전류의 크기를 제한해야 한다. 본 논문에서는 LVRT/HVRT 동작 시 탭 변환 단권변압기 시험 장비의 등가 모델을 기반으로 계통 연계 규정을 만족하기 위한 단권변압기의 임피던스를 설계한다. 제안하는 설계 방법을 이용하여 LVRT/HVRT 시험 시 요구되는 다양한 사고전압을 출력할 수 있는 시험장비의 설계를 위한 탭 간의 임피던스 설계 과정을 설명한다. 제안하는 설계 방법의 타당성을 검증하기 위하여, 10MVA급 LVRT/HVRT 시험 장비의 설계 과정을 설명하고 시뮬레이션을 통하여 확인하였다.","This paper proposes an impedance design method of the test device for evaluating Low Voltage Ride Through(LVRT) and High Voltage Ride Through(HVRT) functions. The LVRT/HVRT test device should have ability to generate the fault voltage specified in the grid code for a certain period and to limit the magnitude of the fault current with the design specification. In this paper, the impedance design method for auto transformer is proposed based on a equivalent model of a tap-change auto-transformer during LVRT/HVRT operation. In addition, to generate various fault voltages required the LVRT/HVRT test, tap impedance design in the auto transformer is considered. To verify the validity of the proposed design method, the design process of the 10MVA LVRT/HVRT test device was conducted and the design results was verified through simulation models."
Coronavirus Disease-19(COVID-19)에 특화된 인공신경망 기계번역기,2020,['-19'],"최근 세계보건기구(WHO)의 Coronavirus Disease-19(COVID-19)에 대한 팬데믹 선언으로 COVID-19는 세계적인 관심사이며 많은 사망자가 속출하고 있다. 이를 극복하기 위하여 국가 간 정보 교환과 COVID-19 관련 대응 방안 등의 공유에 대한 필요성이 증대되고 있다. 하지만 언어적 경계로 인해 원활한 정보 교환 및 공유가 이루어지지 못하고 있는 실정이다. 이에 본 논문은 COVID-19 도메인에 특화 된 인공신경망 기반 기계번역(Neural Machine Translation(NMT)) 모델을 제안한다. 제안한 모델은 영어를 중심으로 프랑스어, 스페인어, 독일어, 이탈리아어, 러시아어, 중국어 지원이 가능한 Transformer 기반 양방향 모델이다. 실험결과 BLEU 점수를 기준으로 상용화 시스템과 비교하여 모든 언어 쌍에서 유의미한 높은 성능을 보였다.",
내부 구조물과 외함의 재질에 따른 배전용 변압기의 표류부하손 영향,2020,"['Distribution transformer', 'Stray load loss', 'Clamp', 'Tank', 'high manganese steel']",,"In this paper, the influence of the material of the clamp and the tank is analyzed for reducing the stray load loss for 100kVA transformer. In order to select the appropriate materials for the clamp and the tank of the transformer, analysis was carried out through FEM. SS400, Aluminum, and high manganese steel are used for the comparison and analysis. A total of nine models were analyzed. The use of high manganese steel, which has relatively low conductivity and relative permeability, showed a 31.55% reduction effect compared with SS400."
사전 학습된 한국어 BERT의 전이학습을 통한 한국어 기계독해 성능개선에 관한 연구,2020,"['Language Model', 'Masked Language Model', 'Question Answering', 'BERT', 'Fine-Tuning']",,"Language Models such as BERT has been an important factor of deep learning-based natural language processing. Pre-training the transformer-based language models would be computationally expensive since they are consist of deep and broad architecture and layers using an attention mechanism and also require huge amount of data to train. Hence, it became mandatory to do fine-tuning large pre-trained language models which are trained by Google or some companies can afford the resources and cost. There are various techniques for fine tuning the language models and this paper examines three techniques, which are data augmentation, tuning the hyper paramters and partly re-constructing the neural networks. For data augmentation, we use no-answer augmentation and back-translation method. Also, some useful combinations of hyper parameters are observed by conducting a number of experiments. Finally, we have GRU, LSTM networks to boost our model performance with adding those networks to BERT pre-trained model. We do fine-tuning the pre-trained korean-based language model through the methods mentioned above and push the F1 score from baseline up to 89.66. Moreover, some failure attempts give us important lessons and tell us the further direction in a good way."
외철형 배전용 변압기의 손실 감소를 위한 철심 최적설계,2020,"['Shell type transformer', 'Stray load loss', 'Core loss', 'Optimum design']",,"In this paper, we propose the design of a transformer with four cores arranged radially to reduce the stray load loss of a 100㎸A shell type transformer. Also, the design of the four cores used a magnetic equivalent circuit method taking into account the nonlinear properties. The optimized design reduced no-load losses to 2.29% and stray load losses of the transformer tank to 77.30%."
Hybrid 전력망의 모델링 및 사고 해석,2020,"['Fault analysis', 'Hybrid power grid model', 'Renewable energy sources', 'PSCAD', 'Short circuit']",,"In order to increase an acceptance of renewable energy sources and to lay out an efficient power grid, a hybrid power grid model design and faults analysis must be preceded. This paper is a basic study for a hybrid power grid configuration, fault analysis, and protection coordination. Using PSCAD software, modeling of a hybrid power grid and faults analysis through short circuit are performed. First, a hybrid power grid consisting of an AC 154[kV] power source, a 50[MVA] transformer, a MVDC and LVDC power grid through bidirectional power converters, PV, and load was constructed. Each components of the selected hybrid power grid was modeled with PSCAD software and the controllers were designed and simulated. And it is intended to analyze the faults characteristics by grasping the fault currents during the short circuit and +pole to -pole faults while changing each fault inception location. Finally, the simulation results of the faults analysis by the maximum value and the current fluctuation rate of the fault currents were compared."
Analysis of the Semantic Answer Types to Understand the Limitations of MRQA Models,2020,"['기계 독해 질의 응답', '질의 분석', '트랜스포머 언어 모델', '정답유형 분석', 'machine reading question answering', 'query analysis', 'transformer language models', 'answer type']",,"Recently, the performance of Machine Reading Question Answering (MRQA) models has surpassed humans on datasets such as SQuAD. For further advances in MRQA techniques, new datasets are being introduced. However, they are rarely based on a deep understanding of the QA capabilities of the existing models tested on the previous datasets. In this study, we analyze the SQuAD dataset quantitatively and qualitatively to demonstrate how the MRQA models answer the questions. It turns out that the current MRQA models rely heavily on the use of wh-words and Lexical Answer Types (LAT) in the questions instead of using the meanings of the entire questions and the evidence documents. Based on this analysis, we present the directions for new datasets so that they can facilitate the advancement of current QA techniques centered around the MRQA models."
복수의 변압기를 사용하여 독립 전력제어가 가능한 DC 배전용 다중포트 Dual-Active-Bridge 컨버터,2020,"['DC distribution', 'Decoupling power control', 'DAB(Dual-Active-Bridge) converter', 'Multi-port converter', 'Multiple transformers']",,"This study proposes a power decoupled multi-port dual-active-bridge (DAB) DC-DC converter employing multiple transformers. Conventional multiport DAB DC-DC converters experience a power coupling issue from the use of a single transformer, which essentially requires complex power decoupling control. To solve this issue, a multiport DAB DC-DC converter employing multiple transformers is proposed to decouple output power without additional complex control algorithms. The proposed converter uses multiple transformers that can expand output ports easily. Therefore, transformers and the proposed multi-port DAB converter can be designed simply. In addition, the number of coupling inductors can be reduced in the proposed three-port DAB converter compared with that in conventional multiport DAB converters. The power decoupling characteristics and equivalent circuit of the proposed converter are analyzed using theoretical model approaches. Finally, a 3-kW laboratory prototype is developed to verify the effectiveness of the proposed converter."
다출력 공진형 컨버터의 변압기 누설 인덕턴스를 고려한 특성 분석,2020,"['LLC resonant converter', 'Multi-winding', 'Cross-regulation', 'Voltage transfer ratio for multi-output structure', 'Leakage inductanc']",,"This paper presents analysis and experimental results of cross-regulation characteristics in multi-output LLC resonant converter focus on transformer leakage inductance. The detail leakage inductance is extracted using the multi-winding transformer equivalent model, and the effects of leakage inductance on the cross-regulation characteristics under two load conditions are investigated. The analytical results are verified through an experiment results using 240W 4-output LLC resonant converter. Furthermore, the solution for reducing the cross-regulation error is discussed."
다출력 LLC 공진형 컨버터의 변압기 누설 인덕턴스를 고려한 Cross-regulation 특성 분석,2020,"['LLC resonant converter', 'Multi-winding', 'Cross-regulation', 'Voltage transfer ratio for multi-output structure', 'Leakage inductanc']",,"This paper presents analysis and experimental results of cross-regulation characteristics in multi-output LLC resonant converter focus on transformer leakage inductance. The detail leakage inductance is extracted using the multi-winding transformer equivalent model, and the effects of leakage inductance on the cross-regulation characteristics under two load conditions are investigated. The analytical results are verified through an experiment results using 240W 4-output LLC resonant converter. Furthermore, the solution for reducing the cross-regulation error is discussed."
Observer-based load current sensorless control strategy of inverter circuit in three-phase UPS,2020,"['Inverter', 'Load current sensorless', 'Observer', 'Three-phase UPS']",,"A load current sensorless control strategy using an optimized observer is designed for a three-phase online transformer-based UPS. A closed-loop observer containing state feedback is constructed based on the improved state space model of the inverter circuit. A simple feedback matrix design method is then applied to the observer according to the dead-beat control law, and the load current feedforward control scheme is reconstructed using the proposed observer. This control strategy can save three load current sensors, reduce the THD of the output voltage, and improve dynamic performance. The effectiveness of this strategy is verified by conducting simulation and experiment in a 10 kVA UPS."
Magnetostrictive Characteristics of the Grain‑Oriented Electrical Steel in an Epstein Frame Magnetized with a DC Biased Magnetic Field,2020,['DC bias · Epstein frame · Grain-oriented electrical steel · Magnetostriction'],,"The presence of direct current (dc) biased magnetic feld enhances the vibration and noise in the transformer cores due to magnetostrictive efect in the grain-oriented electrical steel laminations. Based on an Epstein frame, an experimental setup was built to measure the in-plane magnetostriction, the principal magnetostriction was computed under a dc bias, and the infuence of dc bias magnetic feld on the magnetostrictive property of electrical steel sheet was investigated. A model based on a back propagation neural network (BPNN) method assisted with the Levenberg–Marquardt algorithm was proposed to describe the magnetostrictive behaviour in the presence of dc bias. Finally, the BPNN model was verifed by comparing the measured magnetostriction and the computed one. This research is helpful to make an accurate estimation to deformation of iron core under a dc bias."
비접지 시스템 선박의 지락 고장 발생에 따른 전위 특성,2020,"['Line-to-earth fault', 'Line-to-earth voltage', 'Symmetrical coordinates method', 'Unearthed system', 'Zero sequence voltage']",,"A unearthed system is applied in many ships for the stability of power in case of a line-to-earth fault. This system is difficult for ship engineers to recognize when the line-to-earth fault occurs. In order to monitor such fault at all times, ground potential transformer is used for high voltage(6.6kV) ships and permanent insulation monitor is used for low voltage(450V) ships. In this paper, we analyze the variation characteristics of line-to-earth voltage and zero sequence voltage in case of the line-to-earth fault. For this, mathematical model of line-to-earth voltage and zero sequence voltage was derived through the symmetric coordinate method, and a MATLAB simulation was performed. The simulation proves that this method can accurately judge the faulty phase, especially for the line-to-earth fault with unearthed system."
국내 배전계통에서의 CVR 계수 추정 및 경제적 효과 분석 적용 방안,2020,"['Conservation Voltage Reduction', 'Economic Analysis', 'Direct Method', 'Distribution System']",,"In this paper, the CVR estimation method is proposed using the direct method. This method calculates the voltage change and the load change for the same time period when the tap of a main transformer in a substation changes. Through these CVR estimation results, economic analysis is conducted by analying the CVR effect of the domestic distribution systems. Based on the estimated CVR factor, a representative model of the domestic distribution system is assumed, and economic analysis is evaluated through CVR effect analysis when CVR control is operated. Demand reduction of a substation and line loss reduction costs are calculated using the CVR factor and voltage reduction. Through this analysis, CVR in distribution systems can be expected to reduce the power loss cost and power plant construction cost."
국내 배전계통에서의 계수 추정 및 경제적 효과 분석 적용 방안,2020,"['Conservation Voltage Reduction', 'Economic Analysis', 'Direct Method', 'Distribution System']",,"In this paper, the CVR estimation method is proposed using the direct method. This method calculates the voltage change and the load change for the same time period when the tap of a main transformer in a substation changes. Through these CVR estimation results, economic analysis is conducted by analying the CVR effect of the domestic distribution systems. Based on the estimated CVR factor, a representative model of the domestic distribution system is assumed, and economic analysis is evaluated through CVR effect analysis when CVR control is operated. Demand reduction of a substation and line loss reduction costs are calculated using the CVR factor and voltage reduction. Through this analysis, CVR in distribution systems can be expected to reduce the power loss cost and power plant construction cost."
Rotor ground-fault diagnosis methods for synchronous condensers based on amplitude and phase-angle of voltage,2020,"['Fault diagnosis', 'Fault detection', 'Ground-fault resistance (GFR)', 'Grounding resistance (GR)', 'Rotor ground fault', 'Synchronous condensers']",,"A single ground fault of the rotor windings in a synchronous condenser can cause serious damage if the fault is not eliminated in time. This paper proposes a new rotor ground-fault diagnosis method for synchronous condensers based on the amplitude of the 150 Hz component of the voltage across a grounding resistance (GR) placed in the neutral of an excitation transformer. It can be seen that the amplitude of the 150 Hz component of the voltage across the GR increases with a decrease of the ground-fault resistance (GFR). This method is an improvement of existing algorithms for rotor ground-fault detection that requires less analyzing and can achieve online detection of the severity of a rotor ground fault at any point of the excitation winding. In addition, the influence of different excitation voltages on the algorithm based on phase-angle is analyzed considering actual working characteristics. Moreover, a model is built in the MATLAB/Simulink platform using the real parameters of a TTS-300-2 synchronous condenser to verify the effectiveness of the proposed method. Finally, a dual diagnostic criterion is given according to the results of simulations. The research conclusions can have a great significance on the healthy running of synchronous condensers and they can help to drastically reduce both cost and repair time."
