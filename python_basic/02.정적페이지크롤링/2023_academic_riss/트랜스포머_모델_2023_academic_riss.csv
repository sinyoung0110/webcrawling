title,date,keywords,abstract,multilingual_abstract
트랜스포머 오토인코더와 Diffusion 모델을 사용한 3D CAD 모델 생성 방법,2023,"['CAD', 'Deep learning', 'Transformer', 'Diffusion model', 'Autoencoder']","본 논문에서는 명령어 시퀀스로 표현되는 3D CAD 모델을 생성할 수 있는 새로운 딥러닝 모델을 제안한다. 제안하는 모델은 CAD 명령어 시퀀스를 생성하기 위하여 트랜스포머 기반 오토인코더와 diffusion 기반 생성 모델로 구성된다. 트랜스포머 기반 오토인코더는 CAD 명령어 시퀀스를 잘 생성하도록 잠재 표현을 학습하고, diffusion 기반 생성 모델은 새로운 명령어 시퀀스 생성을 위해서 잠재 변수를 생성하도록 학습한다. 실험을 통하여 제안하는 모델이 유효하고 새로운 3D CAD 모델을 성공적으로 생성할 수 있음을 보여주었다. 또한, CAD 모델에 대한 의미 있는 잠재 공간을 학습하였음을 보여주었다.","In this paper, we propose a novel deep learning model that generates 3D CAD models represented as command sequences. The proposed model consists of a transformer-based autoencoder and a diffusion-based generative model. The transformer-based autoencoder learns latent representations to generate CAD command sequences effectively, and the diffusion-based generative model learns to generate latent variables for creating new command sequences. Experimental results show that the proposed model is able to successfully generate valid and new 3D CAD models. Our experiments also show that the proposed model learns the meaningful latent space for CAD models."
신약후보 분자생성을 위한 새로운 트랜스포머 모델,2023,"['drug discovery', 'molecule generation', 'deep learning', 'Transformer', 'language model', '약물 발견', '분자 생성', '심층 학습', 'Transformer', '언어 모델']","다양한 생성모델 기반의 신약 후보 생성 방법 중, 회귀적 신경망 (RNNs) 기반의 모델이 최고 성능을 보여왔다. RNN의 장기 의존성 문제를 해결하기 위해 Transformer 기반의 모델이 제안되어왔으나, RNN 기반 모델에 비해서 낮은 성능을 보였는데, Transformer 모델의 과적합 문제가 그 원인일 수 있다. 해당문제를 완화하도록, 본 논문에서는, 큰 decoder 모델을 간단한 순방향 신경망으로 변환하는 모델을 제안한다. 실험결과, 제안된 모델이 기존 최고 성능 모델을 주요 지표들에서 앞서며, 다른 지표에서도 유사한 성능을 보이는 것을 확인했다. 또한, 제안하는 모델을 SARs-CoV-2 (COVID-19) 바이러스에 대항할 수 있는 신약 후보 생성에 적용하였고, 그렇게 생성된 신약 후보군들이 현재 시장에서 사용되는 약들인 Paxlovid, Molnupiravir, Remdesivir들 보다 더 효과적인 실험결과를 확인하였다.",다국어 초록 정보 없음
지식 증류 기법을 사용한 트랜스포머 기반 초해상화 모델 경량화 연구,2023,"['Knowledge Distillation', 'Super Resolution', 'Transformer', 'Deep Learning', 'Image Processing', '.']",최근 자연어 처리에서 사용되던 트랜스포머 모델이 이미지 초해상화 분야에서도 적용되면서 좋은 성능을 보여주고 있다. 그러나 이러한 트랜스포머 기반 모델들은 복잡하고 많은 학습 파라미터를 가지고 있어 많은 하드웨어 자원을 요구하기 때문에 작은 모바일 기기에서는 사용하기 어렵다는 단점을 가지고 있다. 따라서 본 논문에서는 트랜스포머 기반 초해상화 모델의 크기를 효과적으로 줄일 수있는 지식 증류 기법을 제안한다. 실험 결과 트랜스포머 블록의 개수를 줄인 학생 모델에서 제안 기법을 적용해 교사 모델과 비슷한성능을 내거나 더 높일 수 있음을 확인하였다.,다국어 초록 정보 없음
비전 트랜스포머를 통한 Full Transformer 비디오 캡셔닝 모델 제안,2023,"['비전 트랜스포머', '비디오 캡셔닝', '딥러닝', '유니버셜 트랜스포머', 'ViT', 'video captioning', 'deep learning', 'universal transformer']",국문 초록 정보 없음,"In the field of computer vision, a method to use a transformer model is emerging. Active research is underway on performance improvement using transformers not only for images but also for video data. ViViT proposed learning the temporal and spatial information of the video with two types of transformers. However, in the case of the ViViT and other using ViT models, including the first proposed ViT, only the learned CLS Token is used, and the remaining Patch sequence is not considered. In this study, various methods using the information of Patch Sequence learned through Self-attention are experimented. Thereafter, a video captioning task is performed using the corresponding method as a feature extraction network. Performance evaluation is conducted through four metrics, and the captioning results for the MSVD dataset of the final proposal network are close to those of the SOTA models in the BLEU-4, METEOR, and ROUGE-L metrics, even though only the appearance feature is used."
인버터블 트랜스포머를 이용한 플로우 기반 생성 모델,2023,"['Generative model', 'Transformer', 'Normalizing flow', 'Conditional probability']","플로우 기반 생성모델은 Variational AutoEncoder (VAE), Generative Adversarial Network(GAN)와 달리 잠재 벡터 z의 확률 분포에서 일련의 역변환을 통해 데이터 x의 분포를 명시적으로 학습한다. 이 논문에서는 시계열 데이터를 처리하는 인버터블 트랜스포머를 이용한 플로우 기반 생성모델을 제시한다. 이 모델을 통해 표준 정규 분포로부터 실제 데이터의 복잡한 조건부 확률분포를 추정할 수 있다. 학습된 모델에 조건을 넣어주면 실제 데이터 분포를 따르는 새로운 시계열 데이터를 만들 수 있다.","Unlike Variational AutoEncoder(VAE), Generative Adversarial Network(GAN), a flow-based generative model explicitly learns the distribution of data x by a sequence of invertible transformation. This paper proposes a flow-based generative model using invertible transformer, which processes time-series data. We can estimate the distribution of real data from standard normal distribution using the proposed model. A learned model with condition generates new time-series data which follows the distribution of real data."
트랜스포머 모델을 사용한 시맨틱 초해상화,2023,"['Super-Resolution', 'Decoder', 'Transformer', 'Semantic Information', 'SwinIR']",국문 초록 정보 없음,"This paper proposes an effective method to improve the performance of SwinIR, a vision Transformer-based super-resolution neural network model, by introducing a Transformer decoder with learnable category queries. The decoder allows to extract semantic information of each dataset belonging to different categories (e.g., text and face); the semantic information can improve category-specific texture reconstruction in the process of super-resolution. Experiments were conducted using decoders of different architectures to analyze the performance of the proposed method. The experimental results confirm that the use of decoder can improve the quality of super-resolution images produced by SwinIR qualitatively and quantitatively, although improvements may vary depending on the depth of the decoder and how semantic information is applied."
트랜스포머를 활용한 CAN 침입탐지모델,2023,"['Transformer', 'CAN', 'Network for Vehicle', 'Intrusion Detection', '트랜스포머', 'CAN', '차량용 네트워크', '침입탐지']",국문 초록 정보 없음,"CAN is the most widely used network protocol for in-vehicle networks and is used not only for civilian vehicles but also for military ground unmanned systems. However it has a weak structure in terms of cybersecurity because only the reliability and efficiency were considered when CAN was developed. Since various cyberattacks are possible using these vulnerabilities, a lot of researches about intrusion detection through CAN traffic analysis have been actively conducted in order to cope with this problem. Recently, research using various deep learning models is underway to improve detection performance, but these detection models analyze the CAN message in the unit of traffic groups with a certain size and identify only whether an attack message exists in the group, so there is a limitation in that the detection models cannot identify which traffics are related with attacks. In this paper, we propose a CAN intrusion detection model using the Transformer model. By applying this detection model, more precise detection results can be presented because it can identify the attack traffics as well as the existence of attack traffics by analyzing the traffic at the signal level."
PatentQ&A: 트랜스포머 모델을 이용한 신경망 검색 시스템 제안,2023,"['신경망 검색', '트랜스포머', '특허 도메인', '버트', 'neural search', 'transformer', 'patent domain', 'BERT']",최근 신경망 검색은 통계적 방법에 기반한 검색을 뛰어넘어 의미에 기반한 검색을 가능하게 하며 오타가 있어도 정확한 검색 결과를 찾을 수 있게 한다. 본 논문에서는 특허에 전문 지식이 없는 일반인이 일반 용어를 사용하여 특허 정보를 검색할 경우 사용자 질문 의도에 가장 근접한 답변을 보여주는 신경망 기반 특허 Q&A 검색 시스템을 제안한다. 특허청 홈페이지에 게시된 특허고객 상담 데이터로 특허 데이터 셋을 구축하였다. 사용자가 입력한 질문에 대한 유사한 질문을 추출하고 우선순위를 다시 지정하기 위해 특허 데이터 셋으로 미세조정한 Patent-KoBERT(Triplet)과 Patent-KoBERT(CrossEntropy)를 사용하였다. 실험 결과 Mean Reciprocal Rank(MRR)과 Mean Average Precision(MAP)의 수치는 0.96으로 사용자가 입력한 질문 의도와 가장 유사한 답변을 잘 선정한다는 것을 확인할 수 있다.,"Recent neural network search has enabled semantic search beyond search based on statistical methods, and finds accurate search results even with typos. This paper proposes a neural network-based patentQ&A search system that provides the closest answer to the user's question intention when a general public without patent expertise searches for patent information using general terms. A patent dataset was constructed using patent customer consultation data posted on the Korean Intellectual Property Office website. Patent-KoBERT (Triplet) and Patent-KoBERT (CrossEntropy) were fine-tuned as patent datasets were used to extract similar questions to questions entered by the user and re-rank them. As a result of the experiment, values of Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) were 0.96, confirming that answers most similar to the intention of the user input were well selected."
전이 학습 및 SHAP 분석을 활용한 트랜스포머 기반 감정 분류 모델,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델의 경량화,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
멀티헤드 어텐션 병렬화를 통한 트랜스포머 기반 객체추적 모델의 실행속도 향상,2023,"['Transformer', 'Multi-head attention', 'CSWinTT', 'Object tracking', 'Multi-threading']","최근 딥러닝 기반 객체추적 기술이 발전함에 따라 스포츠 경기 분석, 영상 보안, 증강현실 등 다양한 응용 분야에서 객체추적 기술이 사용되고 있다. 사용자들은 높은 객체추적 정확도뿐만 아니라 빠른 객체추적 속도에 따른 높은 QoS를 요구한다. 본 연구에서는 현재 객체추적 솔루션 중 최고로 꼽히는 트랜스포머기반 CSWinTT 모델의 객체추적 속도를 향상시킨다. 이 모델의 인코더 레이어 내 Multi-Head Attention(MHA)의 head연산들은 전체 트랜스포머 추론 과정에서 가장 많은 실행시간을 차지하고, 각 head들은 각각 다른 입력 값을 가지지만 직렬로 실행된다. 이를 개선하기 위하여 본 연구에서는 각각의 head연산들을 병렬적으로 실행시킨다. 병렬 실행을 위하여 하나의 모듈로 이루어진 MHA연산을 head 개수만큼 서브 모듈로 분리하고, 분리된 각각의 모듈을 멀티쓰레드 환경에서 실행한다. 이때 순수 Python 환경에서는 불가능한 멀티쓰레드 환경을 C++ 실행 환경으로 개선하여 가능하게 한다. 또한 각 쓰레드들이 비동기적으로 전달하는 커널들을 GPU 내부에서 최대한 동시에 실행될 수 있도록 한다. 다양한 실험을 통해 MHA 병렬실행의 효과를 확인한 결과, 추론정확도는 거의 동일하게 유지하면서 기존 실행환경에 비하여 인코더의 평균 실행시간은 56.8% 감소하였고, 평균 FPS는 63.3% 증가하였다.","With the recent advance of deep learning-based object tracking technology, it is being used in various application fields such as sports game analysis, video security, and augmented reality. Users require high object tracking accuracy as well as high QoS according to fast object tracking speed. In this study, we improve the object tracking speed of CSWinTT(transformer-based object-tracking model), which is currently considered as the best object tracking solution. The head operations of the Multi-Head Attention(MHA) in the encoder layer of this model occupy the most execution time in the entire inference procedure of the transformer. Each head has a different input value, but is executed in a serial manner. To overcome this, in this study, each head operation is executed in parallel. For parallel operation, the MHA consisting of one module is divided into sub-modules by the number of heads, and each separated sub-module is executed in a multi-threading environment. The pure Python environment does not guarantee a complete multi-threaded run. We thus improve to a C++ implementation environment to enable complete multi-threading. In addition, kernels transmitted asynchronously by each thread can be executed as concurrently as possible inside the GPU. As a result of checking the effect of MHA parallel execution through various experiments, the average execution time of the encoder decreased by 56.8% and the average FPS increased by 63.3% compared to the existing method while maintaining almost the same inference accuracy."
RNN과 트랜스포머 기반 모델들의 한국어 리뷰 감성분류 비교,2023,"['딥러닝', '트랜스포머', 'BERT', 'GPT', '감성 분석', '자연어 처리', 'Deep Learning', 'Transformer', 'BERT', 'GPT', 'Sentiment Analysis', 'Natural Language Processing']",국문 초록 정보 없음,다국어 초록 정보 없음
정보처리과정으로서 인간 주의에 대한 인지심리학적 모델과 트랜스포머 Attention 메커니즘 비교 분석,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 알고리즘을 활용한 모델예측제어기 심층모방학습 연구,2023,"['Transformer', 'Imitation Learning', 'Autonomous Driving', 'Attention Mechanisms', 'Model Predictive Controller']",국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델기반 전기자동차 리튬이온 배터리 SOH 예측 연구,2023,"['State of health(건강상태)', 'Transformer(트랜스포머)', 'Electric Vehicle(전기자동차)', 'Moving average(이동평균)', 'Wavelet Transform(웨이블릿변환)']",국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델의 전이학습을 이용한 중환자실 패혈증 환자의 생존 예측 모델,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델을 활용한 건강검진 결과지 정보 추출 연구,2023,"['정보추출', '딥러닝', '트랜스포머']",국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델의 확장 : 배열 데이터에 대한 새로운 위치 인코딩 기법,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머와 로스 개선을 이용한 향상된 백채널 카테고리 예측 모델,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델을 사용한 특징점 학습과 한국어 수어 인식,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
카테고리 쿼리를 사용한 트랜스포머 기반 영상 분류 모델의 성능 개선,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
AdapTex: 하이브리드 비전 트랜스포머 모델을 사용한 Image-to-LaTeX 수식 OCR 모델과 어댑터를 적용한 전이학습,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
사용자 소셜 정보를 활용한 트랜스포머모델 기반의 추천 시스템,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
ZeRO 최적화 기법을 고려한 트랜스포머 모델의 3차원 병렬적 분산딥러닝 통신 시간 예측,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
컨볼루션 필터를 이용한 트랜스포머 모델의 어텐션 연산 희소화 알고리즘,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
텍스트 기반 인간 동작 생성을 위한 트랜스포머 기반 모델의 positional encoding 영향 비교,2023,"['Automation', 'Human motion generation']",국문 초록 정보 없음,다국어 초록 정보 없음
비전 트랜스포머 모델 양자화 연구 동향,2023,[],국문 초록 정보 없음,"In recent years, the performance and applications of transform neural networks in vision tasks have grown phenomenally, outperforming convolutional neural network (CNN)-based models in many cases. However, ViT-based models often have larger model sizes and more parameters, making them challenging to deploy on resourceconstrained edge devices. This necessitates model optimization techniques, such as model quantization. In this paper, we cover model quantization among model optimization techniques, focusing on how it differs from CNN-based model quantization and how ViT-based model quantization has evolved."
다중 기상 예보를 활용한 트랜스포머 기반 태양광 발전량 예측 모델,2023,"['태양 에너지(Solar energy)', '발전량 예측(Power generation prediction)', '딥러닝(Deep learning)', '태양광 발전(Solar power generation)', '트랜스포머(Transformer)']",국문 초록 정보 없음,"Given multiple weather forecasts, how can we achieve higher photovoltaic (PV) prediction accuracy while maintaining prediction stability? As the significance of renewable energy sources increases, accurate prediction of PV generation has become increasingly important. However, the inherent volatility of the weather sets a challenge into achieving accurate PV generation predictions. To address this challenge, this study proposes a Transformer-based Photovoltaic power prediction model Utilizing multiple weather forecasts (TPU), which is a novel framework that utilizes multiple weather forecasts to enhance prediction accuracy while maintaining performance stability. TPU employs attentive Long Short-Term Memory (LSTM) to create embeddings of weather forecasts with different time horizons and cross-attention to fuse the features of different forecasts. Experimental results conducted using a six-month dataset comprising generation data from nine PV power plants spread across the whole country demonstrate that our model outperforms all baseline models in terms of mean absolute error (MAE) and root mean squared error (RMSE), while concurrently achieving a reduced mean absolute deviation (MAD)."
자동차 산업 데이터 분류 방법론 연구: 트랜스포머 기반 모델 간 비교 실험,2023,"['Open Data Portal(오픈 데이터 포털)', 'Automotive Industry Data(자동차 산업 데이터)', 'Data Classification(데이터 분류)', 'Natural Language Processing(자연어 처리)', 'Transformer-Based Models(트랜스포머 기반 모델)']",국문 초록 정보 없음,다국어 초록 정보 없음
직물 이미지 결함 탐지를 위한 딥러닝 기술 연구: 트랜스포머 기반 이미지 세그멘테이션 모델 실험,2023,"['Fabric Defect Detection', 'Deep Learning', 'Image Segmentation', 'ZJU-Leaper', 'Transformer', 'Segformer']",국문 초록 정보 없음,"Purpose In the textile industry, fabric defects significantly impact product quality and consumer satisfaction. This research seeks to enhance defect detection by developing a transformer-based deep learning image segmentation model for learning high-dimensional image features, overcoming the limitations of traditional image classification methods.Design/methodology/approach This study utilizes the ZJU-Leaper dataset to develop a model for detecting defects in fabrics. The ZJU-Leaper dataset includes defects such as presses, stains, warps, and scratches across various fabric patterns. The dataset was built using the defect labeling and image files from ZJU-Leaper, and experiments were conducted with deep learning image segmentation models including Deeplabv3, SegformerB0, SegformerB1, and Dinov2.Findings The experimental results of this study indicate that the SegformerB1 model achieved the highest performance with an mIOU of 83.61% and a Pixel F1 Score of 81.84%. The SegformerB1 model excelled in sensitivity for detecting fabric defect areas compared to other models. Detailed analysis of its inferences showed accurate predictions of diverse defects, such as stains and fine scratches, within intricated fabric designs."
트랜스포머 모델 기반의 반도체 공정 설비 이상 탐지,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델 기반의 반도체 공정 설비 이상 탐지,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
판결문 논증 분석을 위한 트랜스포머 모델 기반 논증 구조 추출 방안에 관한 연구,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
교차로 주행 환경 내 트랜스포머 모델 기반 차량 궤적 및 차량 거동 변경 예측 네트워크의 비교 연구,2023,"['Trajectory prediction(궤적 예측)', 'Maneuver classification(거동 변경)', 'Transformer(트랜스포머)', 'Intersection and Roundabout(교차로)']",국문 초록 정보 없음,다국어 초록 정보 없음
Graph Transformer SlowFast 모델 기반의 아동 이상 행동 분석 시스템,2023,"['Child behavior analysis', 'Real-time', 'CCTV', 'SlowFast', 'Graph transformer']","본 논문에서는 아동 안전사고를 사전에 방지하고자 4가지 유형의 아동 이상 행동 분석이 가능한 Graph Transformer SlowFast 모델을 제안한다. 기존 아동 행동 분석 모델의 경우 자세 추정 모델을 3D 차원 좌표계로 변환하여 RRT(Rapidly Explore Random Tree) 알고리즘 기반의 분류 기법을 사용한다. 하지만 RRT의 초기 입력에 해당하는 3D 좌푯값이 부족할 경우 RRT는 최적의 분류 결과를 얻을 수 없으며, 자세 추정과 분류 모델을 단일 스트림으로 처리하여 다중 인원 분석 시 지연이 발생한다. 제안하는 모델은 공간적 특징과 시간적 특징을 분리하여 특징 정보를 학습 가능토록 하며, 해당 특징 간의 연관성을 위해 Transformer 모델을 그래프 기반에서 동작하도록 변형한다. 그리고 분리된 특징의 학습은 GPU기반의 두 개의 스트림을 생성하여 병렬로 처리한다. 제안된 모델은 아동 이상 행동 분석에서 기존보다 F1-Score 10%의 정확도 향상을 보이며, 분석 속도에서 7FPS 빠른 결과를 나타낸다.","This paper proposes a Graph Transformer SlowFast model capable of analyzing 4 types of abnormal child behavior to prevent child safety accidents in advance. Previous child behavior analysis models transform the pose estimation model into a 3D coordinate system and use a classification technique based on the RRT(Rapidly Explore Random Tree) algorithm. However, when RRT initial 3D coordinate inputs are insufficient, we cannot obtain optimal classification results. Also, the pose estimation and classification model will be processed as a single stream, resulting in delays in simultaneous analysis of multiple people. The proposed model separates spatial and temporal features in order to enable the training of many feature information, and, for the association between those features, the Transformer model is changed to operate on graph basis. This training of separated features is processed in parallel by creating two GPU-based streams. Regarding the analysis of children's abnormal behavior, the proposed model shows a 10% improvement of accuracy based on F1-Score, and 7FPS improvement in analysis speed over the previous models."
Transformer 모델을 이용한 NGS 빅 데이터의 SNP/Indel 탐색,2023,"['NGS', 'Deep learning', 'Variant calling', 'Transformer model', '차세대 시퀀싱', '딥러닝', '변이 탐색', '트랜스포머 모델']","그 동안 NGS 데이터를 기반으로 SNP/indel을 찾는 수 많은 프로그램들이 개발되어 왔다. 하지만 사용자가 직접 매개 변수를 정해서 variant를 찾는 방식의 기존 프로그램들은 많은 true positive를 놓치거나 많은 false positive를 찾는다. 이는 특히 한 위치에 서로 다른 SNP/indel이 적용된 경우일 때 더욱 심하다. 이러한 문제점을 해결하기 위해 우리는 일반적인 알고리즘보다 좋은 성능을 보이는 딥 러닝 방법을 적용하여 NGS 데이터로부터 SNP/Indel을 찾는 방법을 시도하였다. 특히 텍스트 기반의 NGS 데이터에 적합하도록 read pileup data를 가공한 후 Transformer 모델을 기반으로 변형한 딥러닝 모델에 적용하였다. 이 방식은 SNP와 Indel이 혼용되어 나타나는 케이스에서 더 좋은 성능을 보였으며, 그 외의 경우에도 기존의 다른 프로그램들과 유사한 성능을 보였다.","Numerous programs for finding SNPs and short indels based on NGS data have been developed. However, existing programs in which users directly set parameters and call variants miss many true positives or call many false positives. This is particularly serious when different SNP/indel variants are applied to one location. In order to solve this problem, we conducted a research to find SNPs/Indels from NGS read data by applying a deep-learning method that showed better performance than the general state-of-the-art algorithm. After processing the read pileup data to be suitable for text-based NGS data, we applied it to a deep learning model modified based on the Transformer model. This method showed better performance, especially in the special case where SNPs and Indels were mixed, and showed similar performance to other existing programs in other cases."
Transformer 기반 한국어 영어 번역 경량화 모델,2023,"['Colloquial style', 'Natural language processing', 'Transformer', 'Translation', 'Visualization', 'Transformer', '구어체', '번역', '시각화', '자연어 처리']","Transformer 계열 신경망이 기존 자연어 처리 분야에서 활용된 RNN(Recurrent Neural Network) 대신에 많이 활용되고 있다. 최근에는 이러한 신경망들이 기업 내부에서 업무 중에 사용하는 것이 개인정보나 기밀 사항 유출 우려로 사용을 금지하고 있고 이에 따라 기업 내부에서 모델을 개발하고 있다. ChatGPT와 같은 LLM(Large Language Model)의 개발은 데이터의 수, 파라미터의 개수, 학습 시간 등에 따라 개발하기 어려운 실정이다. 본 논문에서는 한국지능정보사회진흥원(NIA)에서 공개한 한국어-영어 번역 구어체 데이터를 활용해 파라미터를 줄여 학습하고자 한다. 전체 데이터셋 40만 개 중 1,000개를 사용해 파라미터 값을 설정한 결과 훈련 손실 값은 2.76의 검증 손실 값은 7.96으로 다른 값에 비해 가장 좋은 결과를 보였으며 시각화를 통해 이를 확인하였다. 이를 활용해 다양한 산업 분야에 활용이 가능할 것으로 사료된다.","Transformer series neural networks have been widely used in place of RNNs(Recurrent Neural Networks) in the field of natural language processing. In recent years, these neural networks have been prohibited from being used in the workplace due to concerns about leaking personal information or confidentiality, so models are being developed in-house. The development of LLM(Large Language Model) such as ChatGPT is difficult to develop due to the number of data, number of parameters, and learning time. In this paper, we utilize Korean-English translation colloquial data released by the National Intelligence Agency(NIA) to learn with fewer parameters. As a result of setting the parameter values using 1,000 out of 400,000 total datasets, the training loss value is 2.76 and the validation loss value is 7.96, which is the best result compared to other values, and it is confirmed through visualization. It is expected that it can be used in various industries."
수중영상을 이용한 저서성 해양무척추동물의 실시간 객체 탐지: YOLO 모델과 Transformer 모델의 비교평가,2023,"['Benthic marine invertebrates', 'Deep learning', 'Real-time object detection', 'YOLO', 'DETR', '저서성 해양무척추동물', '딥러닝', '실시간 객체 탐지']",국문 초록 정보 없음,"Benthic marine invertebrates, the invertebrates living on the bottom of the ocean, are an essential component of the marine ecosystem, but excessive reproduction of invertebrate grazers or pirate creatures can cause damage to the coastal fishery ecosystem. In this study, we compared and evaluated You Only Look Once Version 7 (YOLOv7), the most widely used deep learning model for real-time object detection, and detection tansformer (DETR), a transformer-based model, using underwater images for benthic marine invertebrates in the coasts of South Korea. YOLOv7 showed a mean average precision at 0.5 (mAP@0.5) of 0.899, and DETR showed an mAP@0.5 of 0.862, which implies that YOLOv7 is more appropriate for object detection of various sizes. This is because YOLOv7 generates the bounding boxes at multiple scales that can help detect small objects. Both models had a processing speed of more than 30 frames per second (FPS), so it is expected that real-time object detection from the images provided by divers and underwater drones will be possible. The proposed method can be used to prevent and restore damage to coastal fisheries ecosystems, such as rescuing invertebrate grazers and creating sea forests to prevent ocean desertification."
효율적인 Transformer 모델 경량화를 위한 구조화된 프루닝,2023,"['Transformer', '-']",국문 초록 정보 없음,"With the recent development of Generative AI technology by IT giants, the size of the transformer model is increasing exponentially over trillion won. In order to continuously enable these AI services, it is essential to reduce the weight of the model. In this paper, we find a hardware-friendly structured pruning pattern and propose a lightweight method of the transformer model. Since compression proceeds by utilizing the characteristics of the model algorithm, the size of the model can be reduced and performance can be maintained as much as possible. Experiments show that the structured pruning proposed when pruning GPT-2 and BERT language models shows almost similar performance to fine-grained pruning even in highly sparse regions. This approach reduces model parameters by 80% and allows hardware acceleration in structured form with 0.003% accuracy loss compared to fine-tuned pruning."
Music Transformer 기반 음악 정보의 가중치 변형을 통한 멜로디 생성 모델 구현,2023,"['Feature Extraction', 'Muisc Transformer', 'Short Time Fourier Transform', 'Convolutional Neural Network', 'Generation Model']",국문 초록 정보 없음,"In this paper, we propose a new model for the conditional generation of music, considering key and rhythm, fundamental elements of music. MIDI sheet music is converted into a WAV format, which is then transformed into a Mel Spectrogram using the Short-Time Fourier Transform (STFT). Using this information, key and rhythm details are classified by passing through two Convolutional Neural Networks (CNNs), and this information is again fed into the Music Transformer. The key and rhythm details are combined by differentially multiplying the weights and the embedding vectors of the MIDI events. Several experiments are conducted, including a process for determining the optimal weights. This research represents a new effort to integrate essential elements into music generation and explains the detailed structure and operating principles of the model, verifying its effects and potentials through experiments. In this study, the accuracy for rhythm classification reached 94.7%, the accuracy for key classification reached 92.1%, and the Negative Likelihood based on the weights of the embedding vector resulted in 3.01."
객체 추적 성능향상을 위한 Heatmap Detection 및 Transformer 기반의 MOT 모델 설계,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
Vision Transformer기반 특징 가지치기 및 강화를 통한 추적 모델 성능 강화 기술,2023,[],"최근 단일 객체 추적 분야에서는 CNN 기반의 딥러닝 모델을 이용하여 객체 추적 성능을 향상시킬 수 있었지만, 시간적 정보를 강조하는 것에 한계가 있어왔다. 최근에는 트랜스포머 기반의 모델을 통하여 공간적 및 시간적 정보를 함께 강조함으로써 다양한 환경에서 state-of-the-art (SOTA)의 성능을 보여주고 있다. 하지만, 대부분의 트랜스포머 기반 객체 추적 연구는 영상에 대해서 모델이 추출한 특징을 기반으로 추적하고자 하는 target 객체에 대해 유사도 측정하는 부분을 강화하는 방식으로 성능을 향상시키고 있다. 하지만 트랜스포머 기반의 단일 객체 추적은 모델에서 추출된 특징에 의존적이라는 한계점이 존재한다. 본 논문은 모델에서 추출된 특징에 의존적이라는 한계점을 극복하기 위해 모델에서 나오는 특징을 개선하여 추적에 필요한 유사도 계산을 통하여 성능을 향상하는 방법을 제안한다. Target 객체를 더 정확하게 추적하기 위하여, target 객체가 다른 물체 혹은 백그라운드에 의해 일부분이 가려지는 폐색 현상이 발생하였을 때 장애물에 대하여 attention 된 특징을 제거하고 target에 대한 attention을 강조하기 위해 트랜스포머 모델의 Attention map에 adaptive threshold를 적용하여 불필요한 특징을 제거하고 target에 대한 특징을 강조하는 Pruning 방법을 제안한다. 본 논문은 baseline으로 채택한 ‘SwinTrack’ 아키텍쳐에 Vision Transformer 네트워크를 백본으로 사용하여 추적 모델의 Average Overlap (AO) 성능이 63.4%에서 Pruning을 적용하여 64.0%로, Success Rate_0.5 (SR) 성능이 73.9%에서 74.5%로 개선하여 부분 가림 등의 환경적 제약이 있는 상황에서도 target의 특징을 정확하게 검출하여 추적 성능을 향상할 수 있었다. 본 연구를 바탕으로 CCTV, 로봇, 드론과 같은 다양한 기기에서의 사물 추적 분야에 활용 가능할 것으로 기대된다.",다국어 초록 정보 없음
Transformer 모델을 활용한 공기압축기 이상탐지,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
Transformer 기반 OCR 모델을 이용한 강인한 캡차 영상 인식 알고리즘,2023,"['Transformer', 'OCR', 'Captcha']",국문 초록 정보 없음,다국어 초록 정보 없음
Transformer 모델을 통한 모바일 트래픽 분류,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
Swin Transformer 모델을 적용한 한우 발정 탐지,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
한국어 데이터셋을 활용한 Transformer 기반 모델에 관한 연구,2023,"['Korean Dataset', 'Natural Language Processing', 'Neural Network', 'Optimization', 'Transformer']",국문 초록 정보 없음,다국어 초록 정보 없음
3D CNN과 Vision Transformer 모델을 활용한 MRI 기반 알츠하이머병 환자 진단,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
수도 미터 AMI 데이터에서 Transformer 기반 모델들과 Linear 기반 모델들의 장기 시계열 예측 성능 분석,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
박자와 키 조건을 적용한 Conditional Music Transformer 모델에 관한 연구,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
MMF-GTO(Multi Modal Fusion - GRU and Transformer with Oversampling) 모델을 활용한 멀티모달 감정 인식,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
Transformer 기반 계층적 구조 및 인접발화간의 영향 반영 가능한 dialogue act classification 모델 연구,2023,"['DAC(Dialogue Act Classification)', 'Transformer']",국문 초록 정보 없음,다국어 초록 정보 없음
BAT-Transformer: 멀티모달 감정인식을 위한 오디오-텍스트-생리학적 신호 혼합 모델,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
transformer모델의 attention score를 활용한 개체별 유전체 데이터 핵심부분 연구,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
Transformer 아키텍처를 활용한 가상풍력터빈 데이터 모델 개발,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
로봇 경로계획을 위한 멀티모달 Transformer 모델,2023,"['Path planning', 'Multi-modal learning', 'Computer vision', 'Natural language processing']",국문 초록 정보 없음,다국어 초록 정보 없음
카메라-레이더 융합을 통한 Transformer 기반의 3차원 객체 검출 모델 개선,2023,"['3D Object Detection(3차원 객체검출)', 'Sensor Fusion(센서융합)', 'Deep Learning(딥러닝)', 'Bird’s Eye View(조감도)', 'Autonomous Vehicle(자율주행 자동차)']",국문 초록 정보 없음,다국어 초록 정보 없음
주가 예측을 위한 Deep Learning: LSTM 및 Transformer 모델,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
시간 간격 포지셔널 임베딩과 사용자 피드백 가중치를 사용한 Recommender-Transformer 모델,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
An Ensemble Model for Credit Default Discrimination: Incorporating BERT-based NLP and Transformer,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
한글 텍스트 감정 이진 분류 모델 생성을 위한 미세 조정과 전이학습에 관한 연구,2023,"['트랜스포머', '사전 학습된 버트', '미세조정', '전이학습', '한글 문장 감정분류', 'Transformer', 'pre-trained BERT', 'fine-tuning', 'transfer learning', 'Korean text sentiment classification']","근래에 트랜스포머(Transformer) 구조를 기초로 하는 ChatGPT와 같은 생성모델이 크게 주목받고 있다. 트랜스포머는 다양한 신경망 모델에 응용되는데, 구글의 BERT(bidirectional encoder representations from Transformers) 문장생성 모델에도 사용된다. 본 논문에서는, 한글로 작성된 영화 리뷰에 대한 댓글이 긍정적인지 부정적인지를 판단하는 텍스트 이진 분류모델을 생성하기 위해서, 사전 학습되어 공개된 BERT 다국어 문장생성 모델을 미세조정(fine tuning)한 후, 새로운 한국어 학습 데이터셋을 사용하여 전이학습(transfer learning) 시키는 방법을 제안한다. 이를 위해서 104개 언어, 12개 레이어, 768개 hidden과 12개의 집중(attention) 헤드 수, 110M 개의 파라미터를 사용하여 사전 학습된 BERT-Base 다국어 문장생성 모델을 사용했다. 영화 댓글을 긍정 또는 부정 분류하는 모델로 변경하기 위해, 사전 학습된 BERT-Base 모델의 입력 레이어와 출력 레이어를 미세 조정한 결과, 178M개의 파라미터를 가지는 새로운 모델이 생성되었다. 미세 조정된 모델에 입력되는 단어의 최대 개수 128, batch_size 16, 학습 횟수 5회로 설정하고, 10,000건의 학습 데이터셋과 5,000건의 테스트 데이터셋을 사용하여 전이 학습시킨 결과, 정확도 0.9582, 손실 0.1177, F1 점수 0.81인 문장 감정 이진 분류모델이 생성되었다. 데이터셋을 5배 늘려서 전이 학습시킨 결과, 정확도 0.9562, 손실 0.1202, F1 점수 0.86인 모델을 얻었다.","Recently, generative models based on the Transformer architecture, such as ChatGPT, have been gaining significant attention. The Transformer architecture has been applied to various neural network models, including Google's BERT(Bidirectional Encoder Representations from Transformers) sentence generation model. In this paper, a method is proposed to create a text binary classification model for determining whether a comment on Korean movie review is positive or negative. To accomplish this, a pre-trained multilingual BERT sentence generation model is fine-tuned and transfer learned using a new Korean training dataset. To achieve this, a pre-trained BERT-Base model for multilingual sentence generation with 104 languages, 12 layers, 768 hidden, 12 attention heads, and 110M parameters is used. To change the pre-trained BERT-Base model into a text classification model, the input and output layers were fine-tuned, resulting in the creation of a new model with 178 million parameters. Using the fine-tuned model, with a maximum word count of 128, a batch size of 16, and 5 epochs, transfer learning is conducted with 10,000 training data and 5,000 testing data. A text sentiment binary classification model for Korean movie review with an accuracy of 0.9582, a loss of 0.1177, and an F1 score of 0.81 has been created. As a result of performing transfer learning with a dataset five times larger, a model with an accuracy of 0.9562, a loss of 0.1202, and an F1 score of 0.86 has been generated."
트랜스포머 기반의 다중 시점 3차원 인체자세추정,2023,"['다중 시점 3차원 인체자세추정', '트랜스포머', '단안 시점 다중 프레임 모델', 'multi-view 3D human pose estimation', 'transformer', 'monocular multi-frame model']","3차원 인체자세추정은 스포츠, 동작인식, 영상매체의 특수효과 등의 분야에서 널리 활용되고 있는 기술이다. 이를 위한 여러 방법들 중 다중 시점 3차원 인체자세추정은 현실의 복잡한 환경에서도 정밀한 추정을 하기 위해 필수적인 방법이다. 하지만 기존 다중 시점 3차원 인체자세추정 모델들은 3차원 특징 맵을 사용함에 따라 시간 복잡도가 높은 단점이 있다. 본 논문은 계산 복잡도가 적은 트랜스포머 기반 기존 단안 시점 다중 프레임 모델을 다중 시점에 대한 3차원 인체자세추정으로 확장하는 방법을 제안한다. 다중 시점으로 확장하기 위하여 먼저 2차원 인체자세 검출자 CPN(Cascaded Pyramid Network)을 활용하여 획득한 4개 시점의 17가지 관절에 대한 2차원 관절좌표를 연결한 8차원 관절좌표를 생성한다. 그 다음 이들을 패치 임베딩 한 뒤 17×32 데이터로 변환하여 트랜스포머 모델에 입력한다. 마지막으로, 인체자세를 출력하는 MLP(Multi-Layer Perceptron) 블록을 매 반복 마다 사용한다. 이를 통해 4개 시점에 대한 3차원 인체자세추정을 동시에 수정한다. 입력 프레임 길이 27을 사용한 Zheng[5]의 방법과 비교했을 때 제안한 방법의 모델 매개변수의 수는 48.9%, MPJPE(Mean Per Joint Position Error)는 20.6mm(43.8%) 감소했으며, 학습 횟수 당 평균 학습 소요 시간은 20배 이상 빠르다.","The technology of Three-dimensional human posture estimation is used in sports, motion recognition, and special effects of video media. Among various methods for this, multi-view 3D human pose estimation is essential for precise estimation even in complex real-world environments. But Existing models for multi-view 3D human posture estimation have the disadvantage of high order of time complexity as they use 3D feature maps. This paper proposes a method to extend an existing monocular viewpoint multi-frame model based on Transformer with lower time complexity to 3D human posture estimation for multi-viewpoints. To expand to multi-viewpoints our proposed method first generates an 8-dimensional joint coordinate that connects 2-dimensional joint coordinates for 17 joints at 4-vieiwpoints acquired using the 2-dimensional human posture detector, CPN(Cascaded Pyramid Network). This paper then converts them into 17×32 data with patch embedding, and enters the data into a transformer model, finally. Consequently, the MLP(Multi-Layer Perceptron) block that outputs the 3D-human posture simultaneously updates the 3D human posture estimation for 4-viewpoints at every iteration. Compared to Zheng[5]'s method the number of model parameters of the proposed method was 48.9%, MPJPE(Mean Per Joint Position Error) was reduced by 20.6 mm (43.8%) and the average learning time per epoch was more than 20 times faster."
모델 내부/외부 특징량 상관 학습을 통한 지식 증류,2023,"['Knowledge distillation', 'model compression', 'transformer', 'correlation learning', 'Image classification', '지식 증류', '모델 압축', '트랜스포머', '상관관계 학습', '이미지 분류']","본 논문에서는 이종 모델의 특징맵 간 상관관계인 외부적 상관관계와 동종 모델 내부 특징맵 간 상관관계인 내부적 상관관계를 활용하여 교사 모델로부터 학생 모델로 지식을 전이하는 Internal/External Knowledge Distillation (IEKD)를 제안한다. 두 상관관계를 모두 활용하기 위하여 특징맵을 시퀀스 형태로 변환하고, 트랜스포머를 통해 내부적/외부적 상관관계를 고려하여 지식 증류에 적합한 새로운 특징맵을 추출한다. 추출된 특징맵을 증류함으로써 내부적 상관관계와 외부적 상관관계를 함께 학습할 수 있다. 또한 추출된 특징맵을 활용하여 feature matching을 수행함으로써 학생 모델의 정확도 향상을 도모한다. 제안한 지식 증류 방법의 효과를 증명하기 위해, CIFAR-100 데이터 셋에서 “ResNet-32×4/VGG-8” 교사/학생 모델 조합으로 최신 지식 증류 방법보다 향상된 76.23% Top-1 이미지 분류 정확도를 달성하였다.","In this paper, we propose an Internal/External Knowledge Distillation (IEKD), which utilizes both external correlations between feature maps of heterogeneous models and internal correlations between feature maps of the same model for transferring knowledge from a teacher model to a student model. To achieve this, we transform feature maps into a sequence format and extract new feature maps suitable for knowledge distillation by considering internal and external correlations through a transformer. We can learn both internal and external correlations by distilling the extracted feature maps and improve the accuracy of the student model by utilizing the extracted feature maps with feature matching. To demonstrate the effectiveness of our proposed knowledge distillation method, we achieved 76.23% Top-1 image classification accuracy on the CIFAR-100 dataset with the “ResNet-32×4/VGG-8” teacher and student combination and outperformed the state-of-the-art KD methods."
ViT 기반 모델 역전 공격 및 방어 기법들에 대한 연구,2023,[],"ViT(Vision Transformer)는 트랜스포머 구조에 이미지를 패치들로 나눠 한꺼번에 인풋으로 입력하는 모델이다. CNN 기반 모델보다 더 적은 훈련 계산량으로 다양한 이미지 인식 작업에서 SOTA(State-of-the-art) 성능을 보이면서 다양한 비전 작업에 ViT 를 적용하는 연구가 활발히 진행되고 있다. 하지만, ViT 모델도 AI 모델 훈련시에 생성된 그래디언트(Gradients)를 이용해 원래 사용된 훈련 데이터를 복원할 수 있는 모델 역전 공격(Model Inversion Attacks)에 안전하지 않음이 증명되고 있다. CNN 기반의 모델 역전 공격 및 방어 기법들은 많이 연구되어 왔지만, ViT 에 대한 관련 연구들은 이제 시작 단계이고, CNN 기반의 모델과 다른 특성이 있기에 공격 및 방어 기법도 새롭게 연구될 필요가 있다. 따라서, 본 연구는 ViT 모델에 특화된 모델 역전 공격 및 방어 기법들의 특징을 서술한다.",다국어 초록 정보 없음
열화상 영상 Super Resolution 모델 성능 비교 및 IR 영상 Fine-tuning,2023,[],"최근 Deep Learning Computer Vision 분야는 인간의 능력을 뛰어넘는 능력들을 보여주고, 다양한 분야의 영상처리에서 유용하게 사용되고 있다. Computer Vision의 한 분야로 Single image super resolution(SISR)이 있다. 이는 단일 이미지를 x2, x3, x4, x8 등의 Upscale 하여 저해상도의 이미지를 고해상도의 이미지로 재구성해준다. 위의 기술은 열화상 영상을 만들기 위한 적외선 센서의 비용적, 성능적 한계로 인해 주로 다루어지는 저해상도의 열화상 영상 데이터를 다양한 곳에 활용 가능하도록 고해상도 이미지로 변환하기에 굉장히 유용하다.  이에 본 논문은 다양한 Super resolution 모델들의 성능을 비교 분석한다. 모델의 종류로는 Deep convolution neural network(심층 합성곱 신경망)를 기본 구조로 이루어진 SRCNN [1]. 이를 기반으로 이미지 생성 모델인 적대적 신경망(GAN, Generative adversarial network)모델인 SRGAN [2]이 발전한 Real-ESRGAN [3] 그리고 트랜스포머(Transformer) 기반의 SwinIR [4], HAT [5] 등이 있다. 위 모델들의 육안으로 보이는 성능, 모델의 무게를 나타내는 파라미터의 수, 모델을 학습하는데 걸리는 시간 과 학습된 모델이 테스트 이미지를 처리하는 시간을 비교 분석한다.  또한 Real-ESRGAN 모델에 Thermal dog and people dataset을 fine-tuning하여 열화상(Thermal, Infrared) 데이터를 입력 데이터로 필요로 하는 분야에 기존 사전학습 모델보다 뛰어난 열화상 사전학습 모델을 제안한다.",다국어 초록 정보 없음
KoBERT 기반 일반상식 추출 및 반영한 KoBART 기반 대화생성모델,2023,"['KoBERT', 'KoBART', '일반상식', '대화생성모델']","대화 시스템은 사람과 기계 사이에 대화가 가능하도록 하게 해주는 인공지능 시스템으로 주로 인코더-디코더 구조의 시퀀스-투-시퀀스 모델을 이용하여 사용자 질의에 대한 답변을 생성한다. 이러한 답변을 생성할 때 일반상식을 반영하게 되면 흥미로운 답변을 생성할 수 있다. 따라서, 본 논문에서는 사전 학습된 Korean Bidirectional and Auto-Regressive Transformers (KoBART) 모델의 질의에 대한 답변을 생성하는 과정에서 사전 학습된 Korean Bidirectional Encoder Representations from Transformers(KoBERT) 모델을 사용하여, 해당 질의에 가장 관련 있는 일반상식을 선택하여 답변을 생성하는 모델을 제안한다. 이 모델은 일반상식을 반영할 수 있을 뿐만 아니라 사용자 질의와 가장 관련된 일반상식을 선택할 수 있다는 점에서 의의가 있다. 제안한 모델을 다양한 모델과 비교한 결과, 제안한 모델이 가장 좋은 성능을 나타내는 것을 정량적으로 확인하였다. 뿐만 아니라 질의에 대해 각 모델 별로 답변을 생성하여 그 성능을 비교 분석하였고, 가장 적절한 답변을 생성하는 것을 정성적으로 확인하였다.",다국어 초록 정보 없음
임상의학 언어이해 성능향상을 위한 Discharge Summary CRPT 모델 연구,2023,"['대조적 표상 사전훈련', '대조 손실', 'BERT-base', 'Contrastive Representation Pre-training', 'Contrastive Loss', 'Clinical BERT']","BERT 모델은 자연어이해 작업에서 높은 성능을 나타냈으며 의생명 분야를 포함한 다양한 도메인에서 적용되고 있는 사전학습 언어모델이다. 임상의학에서 환자의 의료기록과 임상 정보를 정확하게 이해하기 위해서는 전문용어를 포함한 자연어로 구성된 문장들 간의 의미론적 관계를 정확하게 추론하는 것이 매우 중요하다. 그러나 BERT 모델을 임상의학 분야의 대용량 데이터로 사전학습 시키고 문장 수준의 의미 이해 정확도를 향상시키기 위한 연구는 아직 활발하게 이루어지지 않고 있다. 이러한 문제를 해결하기 위해 본 연구에서는 대조적 표상 사전 훈련 방법을 이용해 BERT 모델의 성능을 향상시키는 방법을 제안하고자 한다. 제안된 Discharge Summary CRPT 모델은 퇴원요약 기록으로 BERT 모델을 사전 훈련시키는 과정에서 다음 2가지를 개선하여 의학언어이해 성능을 향상시켰다. 다음 문장 예측 단계의 교차 엔트로피 손실을 대조 손실로 대체함으로써 문장 간 문맥적 의미추론 정확도를 향상시켰다. 또한, 기존 무작위 마스킹을 단어 전체 마스킹 방법으로 개선해 임상의학 텍스트에 대한 자연어이해 성능이 향상되는지 확인하였다. 제안된 모델은 BLUE 벤치마크 데이터셋(MedNLI, BioSSES)으로 검증한 결과 임상의학 텍스트에 대한 자연어추론 정확도(Accuracy = 0.825) 및 문장 유사도(sentence similarity = 0.775)에서 기존 BERT 모델에 비해 성능 향상을 나타냈다.","Recently BERT(Bidirectional Encoder Representation from Transformer) has shown tremendous improvement in performance for various NLP tasks. BERT has been applied to many domains including biomedical field. Especially clinical domain, the semantic relationship between sentences is very important to understand patient’s medical record and health history in physical examination. However, in current Clinical BERT model, the pre-training method is difficult to capture sentence level semantics. To address this problem, we propose a Discharge Summary CRPT(contrastive representations pre-training) model, which can enhance contextual meanings between sentences by replacing cross-entropy loss to contrastive loss in next sentence prediction (NSP) task. Also we tried to improve the performance by changing random masking technique to whole word masking (WWM) for masked language model (MLM). Especially, we focus on enhancing language representations of BERT model by pre-training with Discharge Summaries Notes to optimize in clinical text understanding. We demonstrate that our Discharge Summary CRPT model yields improvements in performance of clinical NLP task with BLUE (Biomedical Language Understanding Evaluation) Benchmark dataset (MedNLI and BioSSES)."
트랜스포머 기반 시계열 데이터 분류작업을 위한 GASF와 CNN을 사용한 CLS 토큰 추가 임베딩 방법,2023,"['time series', 'deep learning', 'RNN', 'CNN', 'transformer', 'Gramian angular field', '시계열', '딥러닝', '순환 신경망', '컨볼루션 신경망', '트랜스포머', '그라미안 각도 필드']","시계열 데이터란 일정한 시간 동안 수집된 일련의 순차적으로 정해진 데이터 셋의 집합을 의미하며 예측, 분류, 이상치 탐지 등에 활용되고 있다. 기존 시계열 분야는 순환신경망으로 구성된 모델을 주로 활용하여 분석하였지만, 최근 트랜스포머의 개발로 인하여 연구 추세가 변화하고 있다. 하지만 트랜스포머는 시계열 데이터 예측에는 좋은 성능을 보이지만, 분류 작업에는 상대적으로 부족한 성능을 보인다. 본 논문에서는 시계열 데이터를 트랜스포머에 입력으로 활용하기 위해 그라미안 각도 합산 필드와 컨볼루션 신경망을 사용하여 생성한 분류 토큰을 추가하는 임베딩 방식을 제안하며, 사전 학습 기법을 활용하여 성능을 향상시킴을 보인다. 제안하는 모델 성능을 비교하기 위하여 12개의 서로 다른 모델들과 평균 정확도를 기준으로 성능 평가를 진행하였으며, 제안하는 모델은 다른 모델에 비해 최소 1.4% 최대 21.1%까지 성능 향상을 보인다.",다국어 초록 정보 없음
대규모 언어모델을 활용한 리걸 마인드 개발의 현황과 전망,2023,"['대규모 언어모델', '리걸 마인드', '법률 인공지능 서비스', '리걸테크', 'Large Language Model', 'Legal Mind', 'Legal AI Service', 'Legal Tech']","본 논문은 대규모 언어모델이 리걸 마인드(Legal Mind)를 가질 수 있는지에 관하여, 특히 변호사 시험 문제를 해결하기 위한 능력을 가질 수 있는지에 대한 물음을 통해 그 현황을 살펴보고 전망을 제시한다. 본 연구는 먼저, 자연어 인공지능 모델의 발전사를 소개한다. 순환신경망(Recurrent Neural Network, RNN)부터 임베딩(Embedding)을 위한 단어의 벡터 전환(Word to Vector) 기술, 트랜스포머(Transformer) 구조를 활용한 BERT(Bidirectional Encoder Representations from Transformers)를 거쳐, 1,750억 개의 파라미터를 가진 GPT 같은 대규모 언어모델들의 발전 과정을 서술하고, 최근 법률 분야에 자연어 인공지능 모델의 활용이 화두가 되고 있는 이유를 설명한다.다음으로 ChatGPT 프롬프터(Prompter)에 한국의 민법 규정과 대법원 판례를 입력함으로써 새로운 학습과정 없이도 ChatGPT에게 점진적으로 리걸 마인드를 형성시킬 수 있음을 보여준다.마지막으로 법률 인공지능 서비스의 발전 전망을 소개하고, 리걸 마인드를 탑재한 대규모 언어모델이 법률 시장에 미치는 영향을 논의한다. 대규모 언어모델에 리걸 마인드를 형성시키는 연구는 법률 시장에 혁신적인 변화를 가져올 것으로 기대되며, 본 논문에서는 기술 발전에 따른 앞으로의 법률 시장 변화를 제안한다.","This paper presents an assessment and outlook on whether large-scale language models can possess Legal Mind, the capability to solve attorney exam questions. Initially, it introduces the developmental history of natural language artificial intelligence models, traversing from Recurrent Neural Networks (RNNs) to Word to Vector techniques for Embedding, and the application of the Transformer architecture in BERT (Bidirectional Encoder Representations from Transformers). It delineates the developmental trajectory of large-scale language models like ChatGPT with 175 billion parameters and elucidates the recent focus on the utilization of natural language AI models in the legal domain.Subsequently, it demonstrates the incremental formation of Legal Mind within ChatGPT without the need for new learning processes by inputting South Korean civil law provisions and Supreme Court precedents into the ChatGPT Prompter.Finally, it outlines the developmental prospects of legal AI services and discusses the impact of integrating Legal Mind into large-scale language models on the legal market. Research aimed at instilling Legal Mind within large-scale language models is expected to bring innovative changes to the legal market, and this paper proposes future changes in the legal market in line with technological advancements."
Conformer 모델을 이용한 물체 표면 재료의 특성에 따른  가속도 신호 기반 햅틱 질감 인식,2023,"['햅틱 질감 인식', 'Conformer 모델', '합성곱 신경망', '트랜스포머', 'Haptic texture recognition', 'Conformer model', 'Convolutional neural network', 'Transformer']","본 논문에서는 합성곱 신경망과 트랜스포머의 장점을 결합한 Conformer 모델을 이용하여 물체 표면의 질감특성을 나타내는 햅틱 가속도 신호로부터 질감 인식 성능을 향상시키는 방식을 제안한다. 제안한 방식에서는 사람이스타일러스와 같은 도구를 이용하여 물체 표면과 접촉하는 동안 충격음과 진동에 의해 발생한 3축 가속도 신호를 1차원 가속도 데이터로 결합하고, 오디오 신호와 유사성을 갖는 햅틱 가속도 신호로부터 로그 멜-스펙트로그램을 추출한다. 그리고 추출된 로그 멜-스펙트로그램에 Conformer 모델을 적용하여 다양한 물체의 질감을 인식하는 데 있어 주요한 지역적 및 전역적인 주파수 특징을 학습한다. 제안된 모델의 성능 평가를 위해 60개의 재질로 구성된 Lehrstuhl für Medientechnik(LMT) 햅틱 질감 데이터세트를 실험한 결과 제안된 방식이 기존 방식들보다 물체 표면 재료의 질감을효과적으로 잘 인식할 수 있음을 보였다.","In this paper, we propose a method to improve texture recognition performance from haptic acceleration signals representing the texture characteristics of object surface materials by using a Conformer model that combines the advantages of a convolutional neural network and a transformer. In the proposed method, three-axis acceleration signals generated by impact sound and vibration are combined into one-dimensional acceleration data while a person contacts the surface of the object materials using a tool such as a stylus , and the logarithmic Mel-spectrogram is extracted from the haptic acceleration signal similar to the audio signal. Then, Conformer is applied to the extracted the logarithmic Mel-spectrogram to learn main local and global frequency features in recognizing the texture of various object materials. Experiments on the Lehrstuhl für Medientechnik (LMT) haptic texture dataset consisting of 60 materials to evaluate the performance of the proposed model showed that the proposed method can effectively recognize the texture of the object surface material better than the existing methods."
그래프 트랜스포머 기반 농가 사과 품질 이미지의 그래프 표현 학습 연구,2023,"['딥 러닝', '그래프 표현 학습', '사과 품질 분류', '랜덤워크 위치 인코딩', 'Deep learning', 'Graph representation learning', 'Apple Quality Classification', 'Randomwalk positional encoding']","최근 농가의 사과 품질 선별 작업에서 인적자원의 한계를 극복하기 위해 합성곱 신경망(CNN) 기반 시스템이 개발되고 있다. 그러나 합성곱 신경망은 동일한 크기의 이미지만을 입력받기 때문에 샘플링 등의 전처리 과정이 요구될 수 있으며, 과도 샘플링의 경우 화질 저하, 블러링 등 원본 이미지의 정보손실 문제가 발생한다. 본 논문에서는 위 문제를 최소화하기 위하여, 원본 이미지의 패치 기반 그래프를 생성하고 그래프 트랜스포머 모델의 랜덤워크 기반 위치 인코딩 방법을 제안한다. 위 방법은 랜덤워크 알고리즘 기반 위치정보가 없는 패치들의 위치 임베딩 정보를 지속적으로 학습하고, 기존 그래프 트랜스포머의 자가 주의집중 기법을 통해 유익한 노드정보들을 집계함으로써 최적의 그래프 구조를 찾는다. 따라서 무작위 노드 순서의 새로운 그래프 구조와 이미지의 객체 위치에 따른 임의의 그래프 구조에서도 강건한 성질을 가지며, 좋은 성능을 보여준다. 5가지 사과 품질 데이터셋으로 실험하였을 때, 다른 GNN 모델보다 최소 1.3%에서 최대 4.7%의 학습 정확도가 높았으며, ResNet18 모델의 23.52M보다 약 15% 적은 3.59M의 파라미터 수를 보유하여 연산량 절감에 따른 빠른 추론 속도를 보이며 그 효과를 증명한다.","Recently, a convolutional neural network (CNN) based system is being developed to overcome the limitations of human resources in the apple quality classification of farmhouse. However, since convolutional neural networks receive only images of the same size, preprocessing such as sampling may be required, and in the case of oversampling, information loss of the original image such as image quality degradation and blurring occurs. In this paper, in order to minimize the above problem, to generate a image patch based graph of an original image and propose a random walk-based positional encoding method to apply the graph transformer model. The above method continuously learns the position embedding information of patches which don`t have a positional information based on the random walk algorithm, and finds the optimal graph structure by aggregating useful node information through the self-attention technique of graph transformer model. Therefore, it is robust and shows good performance even in a new graph structure of random node order and an arbitrary graph structure according to the location of an object in an image. As a result, when experimented with 5 apple quality datasets, the learning accuracy was higher than other GNN models by a minimum of 1.3% to a maximum of 4.7%, and the number of parameters was 3.59M, which was about 15% less than the 23.52M of the ResNet18 model. Therefore, it shows fast reasoning speed according to the reduction of the amount of computation and proves the effect."
양방향 어텐션 블록을 적용한 멀티모달 신용평가 모델,2023,"['신용평가', '멀티모달', '트렌스포머', '어텐션', '융합', 'credit rating', 'multimodal', 'transformer', 'attention', 'fusion']","현대 금융 시장은 글로벌화와 디지털 전환의 진화 속에서 변동성이 지속적으로 증가하는 동시에 신용 평가의 연구 및 실용적 중요성이 강조되고 있다. 특히 2019년에 시작된 코로나19 팬데믹은 금융 시장의 불안정성에 대해 각별한 주의와 관리가 필요함을 환기시켰으며, 그 결과 고도화된 신용 평가 방법론의 중요성이 다시금 부각되고 있는 실정이다. 이에 본 연구는 기존 신용평가 모델의 성능을 개선하기 위한 새로운 대안으로 멀티모달 특징을 효과적으로 반영하기 위해 양방향 어텐션 블록을 적용한 신용평가 모델을 제안한다. 제안 모델의 우수성은 SEC의 EDGAR 데이터베이스로부터 얻은 재무 정보와 MD&A 텍스트를 기반으로 검증할 계획이다.",다국어 초록 정보 없음
3D 치아 스캔 데이터를 위한 포인트 클라우드와 메쉬 기반의 분할 모델 성능 비교,2023,"['3D 분할 모델', '3D 스캔 데이터', '치아 표면 데이터 분할', '딥러닝', '3D segmentation models', '3D scan data', 'tooth surface data segmentation', 'deep learning']","최근 3D 구강 스캔 데이터를 이용하여 치아를 자동으로 분할 및 분류하거나 치아 형상을 예측하는 등의 시도가 활발히 진행되고 있다. 본 연구에서는 치아 표면 데이터에서 각 치아를 분할하는 데에 최근에 제안된 딥러닝 기반의 3D 분할 모델들을 적용하고 그 성능을 비교 평가하였다. 비교에 사용한 모델들은 데이터 형식에 따라 point cloud 기반의 모델과 mesh 기반의 모델로 구분된다. 또한, 예측된 결과물에 후처리 작업으로 graph cut 기법을 적용하여 각 모델의 분할 성능 향상을 비교 평가하였다. 실험 결과, point cloud 기반의 PointTransformer와 mesh 기반의 MeshSNet이 가장 우수한 분할 성능을 보였고, 특히 PointTransformer는 최적의 분할로 가장 빨리 수렴하였다. 결론적으로 치아 외관을 구성하는 포인트 또는 메쉬의 인접성 정보와 Transformer의 활용이 치아 분할 성능 향상에 중요한 영향을 주는 것으로 분석된다.","Recently, automatic segmentation, classification and shape reconstruction of teeth using 3D oral scan data have been actively attempted. In this study, recently proposed 3D segmentation models based on deep learning from tooth surface data were applied to segment each tooth and their performance was compared and evaluated. The models used in this study were divided into point cloud-based and mesh-based models according to the data format. The performance improvement of each model was also compared and evaluated by applying a graph cut algorithm as a segmentation post-process. The experimental results showed that PointTransformer as a point cloud-based model and MeshSNet as a mesh-based model had the best segmentation performance. PointTransformer also converged the fastest into optimal segmentation. We can conclude that the proximity information of the points or meshes in the surface data and the use of the transformer had an important effect on improving tooth segmentation performance."
연속 이미지 분석을 통한 상황 인지 인공지능 모델 연구 : 교통사고 과실 산정에 적용,2023,"['Deep learning', 'Video Recognition', 'Negligence Rate']","교통사고 발생 시 과실 비율 산정은 당사자들에게 많은 시간과 비용이 들어가는 문제이다. 특히 교통수단과 인프라가 발전함에 따라 교통사고는 복잡해져 전문 변호사 사이에서도 의견이 분분하다. 딥러닝 기반의 비디오 분석은 최근 여러 모델과 기법들이 제시되면서 점점 다양한 분야에서 사용되고 있다. 본 연구에서는 이러한 비디오 분석 모델들을 교통사고 과실 비율 산정에 적용하여 문제들을 해결하고자 한다. 최근 많이 연구 되고 있는 비디오 분석 모델 프레임 워크인 CNN 계열 모델과 Transformer 계열 모델들을 비교 분석하여 보다 적절한 네트워크 아키텍처가 무엇인지 연구해보고자 한다. 교통사고 과실 비율을 예측값으로 설정하여 회귀 방법론으로 문제를 정의하였고, 실제 판례가 존재하는 교통사고 영상을 기반으로 네트워크를 학습시킨다. 비디오 분석 분야의 인공지능 모델은 프레임 단위로 사고 순간의 영상을 분석할 수 있어 보다 효과적으로 과실 비율 산정에 도움을 줄 수 있을 것으로 예상된다.",다국어 초록 정보 없음
리그 오브 레전드에서 과정 데이터와 설정 데이터를 활용한 실력 분류 인공지능 모델 제작,2023,"['Game Skill Estimation', 'eSports', 'Game Matchmaking', 'Deep Learning', '게임 실력 추정', 'e-스포츠', '게임 매치메이킹', '딥 러닝']","게임에서 실력 분류는 인정, 목표, 매치메이킹에 중요하지만, 현재 방식은 결과만을 반영하여 게임 과정에서 나타나는 실력을 고려하지 못한다. 본 논문에서는 e스포츠 게임인 리그 오브 레전드의 과정과 설정 데이터를 활용하여 경기의 실력 등급을 분류하는 인공지능 모델을 제작한다. 이를 위해 7개 티어별로 총 700,000개의 경기 데이터를 수집하고, LSTM 모델과 트랜스포머 모델을 학습하여 분류 작업을 수행한다. 학습 결과, 오버샘플링을 적용한 LSTM 모델이 0.88의 정확도를 기록했다. 과정과 설정을 반영한 실력 분류 모델을 통해 실력 측정에 새로운 시각을 제공할 수 있기를 기대한다.","Classification of skill is important in games, but the current method does not consider skill in the game process. In this paper, create an AI model that classifies the skill level of the game by using the process data and setting data of League of Legends. To this end, 700,000 game data is collected, and LSTM and transformer models are trained to perform classification. As a result of learning, LSTM model with oversampling recorded an accuracy of 0.88. Hope to provide a new perspective on skill measurement through skill classification reflecting the process and settings of the proposed model."
모바일 환경을 고려한 딥러닝 기반 모델의 유방암 분류 비교 실험,2023,"['딥러닝', '유방암 분류', '모바일']","유방암은 전 세계적으로 여성들에게서 많이 발생하는 심각한 질병이다. 최근 딥러닝 발전과 더불어 유방암 진단의 보조적인 역학을 하는 Computer-aid detection/diagnosis (CAD) 시스템이 많이 발전했으나 최근 vision transformer (ViT) 나 MLP-Mixer 를 기반으로 하는 최신 모델의 유방암 분류 성능 비교 연구는 미비한 상황이다. 또한 Mobile 환경으로의 CAD 시스템의 이식에 대한 연구가 활발하게 진행되고 있어, 비교연구가 단순히 모델의 정확도에 국한되지 않고 자원이 제한된 모바일 환경으로의 전환이 용이하도록 효율성에 대해서도 같이 살펴보아야 필요성이 높아졌다. 그렇기 때문에 본 연구에서는 전통적인 CNN 모델뿐만 아니라 ViT 및 MLPMixer 기반의 모델들에 대한 정확도 및 효율성에 대한 비교 연구를 진행하였다. 5 개의 모델 중에서 ViT 기반 모델인 MobileViT-S 모델이 가장 높은 정확도인 0.7477 을 달성하였으며, 짧은 훈련 시간을 소모하여 유방암 분류 과제에서의 최선의 모델임을 알 수 있다.",다국어 초록 정보 없음
트랜스포머 기반 압타머-단백질 상호작용 예측 분류기와 유전알고리즘을 이용한 압타머 후보 서열 생성 시스템,2023,"['Drug Discovery', 'Aptamer-Protein Interaction', 'Machine Learning', 'Deep Learning', 'Transformer', 'Genetic Algorithm', '신약 개발', '압타머-단백질 상호작용', '머신 러닝', '딥 러닝', '트랜스포머', '유전알고리즘']","코로나19의 유행으로 백신, 치료제, 진단키트와 같은 신약 개발의 중요성이 강조되고 있고, 신속하고 효과가 뛰어난 신약을 개발하기 위한 인공지능 기술 적용도 확대되고 있다. 이러한 신약 개발의 한 가지 방법으로 차세대 바이오 물질인 ‘압타머’를 이용하는 방법이 관심을 받고 있다. 압타머는 3차원 구조를 가지는 단일 가닥 올리고 뉴클레오타이드로 표적 단백질에 특이적으로 결합하는 특징이 있다. 그리고 압타머는 기존 신약 개발에 활용되는 바이오 물질보다 안정성과 생산성이 높으므로 감염병 실험실 검사, 암 치료제 등 다양한 신약 개발 분야에서 활용되고 있다. 그러나 압타머 후보 서열을 발굴하기 위한 대표적인 실험인 SELEX는 표적 단백질에 결합하는 압타머 후보 물질을 찾는 데 많은 시간이 걸린다는 단점이 있다. 본 논문에서는 압타머 후보 서열 발굴에 많은 시간이 걸리는 SELEX 실험의 단점을 보완하기 위한 컴퓨터 시뮬레이션 기반 연구를 진행하였다. 논문에서는 압타머 후보 서열 발굴을 위한 트랜스포머(Transformer) 기반 압타머-단백질 상호작용 예측 분류 모델을 개발하였고, 이와 유전알고리즘을 이용한 압타머 후보 서열 생성 시스템을 제안한다. 설계한 시스템을 이용하여 뉴클레오타이드 서열을 생성할 수 있었고, 생성된 후보 서열들의 품질 측정을 위해 ZDOCK 분자 구조 도킹 시뮬레이션을 이용하였다. 그 결과, 생성된 후보 서열들이 실제 압타머들보다 도킹 점수가 비슷하거나 높은 것을 확인할 수 있었다.","Due to the COVID-19 Pandemic, the discovery of drugs, vaccines, and diagnosis kits has been one of major research topics. To improve the efficiency of the drug discovery, artificial intelligence has been actively applied in the field. Aptamers, one of next-generation biomaterials in the drug discovery, attracts a lot of attention in recent decades. Aptamers are single-strand oligonucleotides that comprise tertiary structures and bind to specific target proteins. Aptamers are considered safer and more stable than traditional biomaterials. Therefore, Aptamers are used in various new drug development fields, such as laboratory tests for infectious diseases and cancer treatments. However, SELEX, a representative experiment for aptamer discovery, face at challenges since it takes a lot of time to determine aptamer sequences that bind to a given target protein. In this study, we developed a computer-based method to reduce the cost for aptamer discovery. And we also developed a transformer-based aptamer-protein interaction prediction classification model for discovering aptamer candidate sequences, and propose a system for generating aptamer candidate sequences using genetic algorithms. Nucleotide sequences could be generated using the designed system, and a molecular structure docking simulation called ZDOCK was used to measure the quality of the generated candidate sequences. As a result, generated sequences were confirmed that the docking score was similar or higher than that of the actual aptamers."
트랜스포머 알고리즘을 활용한 탄소나노튜브와 플라이애시 혼입 시멘트 복합재료의 압저항 특성 분석,2023,[],"본 논문에서는 시멘트에 탄소나노튜브를 혼입하여 전기 전도성을 향상시킨 복합재료의 압저항 특성을 딥러닝 기반 트랜스포머 알고리즘을 적용하여 분석하였다. 훈련 데이터 확보를 위한 실험수행을 병행하였으며, 기존 연구문헌을 참조하여 배합설정, 시편제작, 화학조성 분석, 압저항 성능측정 실험을 수행하였다. 특히 본 연구에서는 탄소나노튜브 혼입 시편뿐 아니라 플라이애시를 바인더 대비 50% 대체한 시편에 대한 제작 및 성능평가를 함께 수행하여, 전도성 시멘트 복합재료의 압저항 특성 향상 가능성을 탐구하였다. 실험결과, 플라이애시 대체 바인더의 경우 보다 안정적인 압저항 특성결과가 관찰되었으며, 측정된 데이터의 80%를 이용하여 트랜스포머 모델을 훈련시키고 나머지 20%를 통해 검증하였다. 해석 결과는 실험적 측정과 대체로 부합하였으며, 평균 절대 오차 및 평균 제곱근 오차는 각각 0.069~0.074와 0.124~0.132을 나타내었다.","In this study, the piezoresistive properties of cementitious composites enhanced with carbon nanotubes for improved electrical conductivity were analyzed using a deep learning-based transformer algorithm. Experimental execution was performed in parallel for acquisition of training data. Previous studies on mixture design, specimen fabrication, chemical composition analysis, and piezoresistive performance testing are also reviewed in this paper. Notably, specimens in which fly ash substituted 50% of the binder material were fabricated and evaluated in this study, in addition to carbon nanotube-infused specimens, thereby exploring the potential enhancement of piezoresistive characteristics in conductive cementitious materials. The experimental results showed more stable piezoresistive responses in specimens with fly-ash substituted binder. The transformer model was trained using 80% of the gathered data, with the remaining 20% employed for validation. The analytical outcomes were generally consistent with empirical measurements, yielding an average absolute error and root mean square error between 0.069 to 0.074 and 0.124 to 0.132, respectively."
계층별 트랜스포머를 활용한 비선형 스타일 결합 기반 생성적 스타일 변환 기법,2023,"['Transformer', 'Style mixing', 'Stylization']","Stylization은 주어진 이미지를 원하는 특정 스타일로 변환하는 기술이다. 초기의 연구에서는 그래디언트를이용한 연구가 진행되었으나, 최근에는 고품질의 얼굴 이미지 생성이 가능한 생성 모델의 발전으로, 생성망을 활용한 stylization 기술이 발전되어왔다. 이 중 JoJoGAN은 GAN inversion을 통해 찾아낸 참조 스타일의 잠재 벡터와 임의의 노이즈의 가중합 통해 fine-tuning이 가능한 학습 과정을 소개했으며, 한 장의 참조 이미지만을 사용해 우수한 스타일 변환이 가능함을 보였다. 하지만 기존 가중합 기반 스타일 결합은 단순 선형 결합 형태로써 스타일 코드를 표현하는 매니폴드가 제한되는 문제점을 갖는다. 따라서 해당 잠재공간을 활용한 stylization 결과물은 미세한 표현을 효과적으로 반영하지 못함과 동시에 다양성에서 한계를 갖는다. 이러한 문제점을 해결하고자 본 논문에서는 두 가지 방법을 제안한다. 첫 번째로 다양성과 생성 품질을 높이기 위한 비선형적 스타일 결합으로써 스타일 잠재 공간을 폭 넓게 구성하는 방법이다. 이는 확장된 스타일 공간에서의 다양한 샘플링을 가능하게 함으로써 결과물의 품질 및 다양성을 향상시킨다. 두 번째로 스타일 결합에 있어서 공간적 장기 의존성을 파악해 의미 있는 스타일 특징의 조합을 만들어낼 수 있도록 계층별 Transformer 구조를 활용했다. 즉, coarse, middle, fine 계층에 대해 별도로 독립적인 신경망을 두어 스타일 코드를 인코딩한다. 제안하는 방법을 통해 전체적 외형은 원본의 외형을 따르도록 하고 미세영역에 대해 참조 스타일을 반영하도록 유도함으로써, 정량적/정성적/주관적 실험 결과를 통해 개선된 stylization 성능을 확인했다.","Stylization is the method of transforming a given image into a specific desired style. In early studies, studies using gradients were conducted, but recently, stylization using generative networks have been developed along with the development of a generative model capable of generating high-quality facial images. Among them, JoJoGAN introduced a learning process that enables fine-tuning through the weighted sum of the reference style latent vector and random noise found through GAN inversion and showed that excellent style transformis possible using only one reference image. However, the existing weighted sum-based style combination has a problem in that the manifold expressing the style code is limited as a simple linear combination form. Therefore, the result of stylization using the potential space does not reflect the fine expression effectively and at the same time has a limit in diversity. To solve this problem, this paper proposes two methods. First, it is a method to broadly compose a style latent space by combining non-linear stylization to increase the diversity and quality of creation. This allows for different sampling in an expanded style space, improving the quality and variety of the output. Second, we used a hierarchical Transformer structure to identify spatial long-term dependencies in style combinations to generate meaningful combinations of style features. In other words, style codes are encoded by separate independent neural networks for coarse, medium and fine layers. In the proposed method, improved stylization performance was confirmed through quantitative/qualitative/subjective experiments by inducing the overall appearance to follow the original appearance and to reflect the reference style for fine details."
번역 모델을 빠르게 학습하도록 유사 길이 문장들로 미니배치 구성,2023,"['신경 기계 번역', '정렬', '미니배치', '학습 속도', '데이터 로더', 'neural machine translation', 'sorting', 'mini-batch', 'training time', 'data loader']","Transformer 모델은 Neural Machine Translation과 같은 자연 언어 처리 작업에 혁명을 일으켰고, 아키텍처 연구에 많은 노력을 통해 효율성과 정확성을 높여왔다. 개선을 위한 잠재적인 영역 중 하나는 Transformer가 사용하지 않을 빈 토큰의 계산 횟수를 줄여 불필요한 계산 부담을 줄이는 것이다. 계산 부담을 줄이는 방법으로, 우리는 데이터 로더에서 mini-batch 를 만들기 전에, 길이에 따라 번역 문장 쌍을 정렬한 뒤 유사한 길이의 문장들로 mini-batch 를 구성함으로써 계산 능력의 낭비를 최소화하는 방법을 제안한다. 이때, 정렬의 양은 독립적이고 동일하게 분포 된 (i.i.d) 데이터 가정을 위반 할 수 있으므로 부분 정렬한다. 실험에서 영어-한국어 및 영어-루간다 언어 쌍에 적용하여 기계 번역을 수행했고, 번역 성능을 유지하면서 계산 시간에 이득이 있음을 확인했다. 제안된 방법은 모델 구조와 독립적이여서 다양한 길이를 갖는 데이터로부터 학습하는 경우에 쉽게 사용될 수 있다.",다국어 초록 정보 없음
우주 거대구조에서 트랜스포머를 활용한 우주론적 매개변수 추정,2023,"['우주론', '우주 거대구조', '딥러닝', 'Cosmology', 'Large scale structure', 'Deep neural networks']","우주 거대구조의 비선형 진화과정 속에는 표준모형 너머 새로운 우주론적 패러다임을 구축하기 위해 필요한 중요한 정보가 담겨있기 때문에, 이를 정밀하게 탐색하는 일은 현대 우주론에서 중요한 과제이다. 최근 딥러닝(deep learning)을 활용하여 직접 거대구조 이미지로부터 우주론적 매개변수를 효율적으로 추정하는 연구가 활발히 이루어지고 있다. 거대구조 시계열 데이터 속에는 비선형 효과와 함께 당시 우주를 이루는 에너지 밀도에 대한 정보가 들어가 있기 때문에 시계열 데이터의 상관관계를 효과적으로 학습 할 수 있는 딥러닝 기법을 활용하면 매개변수 추정의 정확도를 높힐 수 있다. 본 논문에서는 트랜스포머(Transformer) 기반 딥러닝 모델을 고안하고, 이를 바탕으로 물질 밀도 계수(Ωm) 및 물질 요동 진폭을 정량화한 변수(σ8)를 추정하는 방법을 소개한다.","The nonlinear evolution of large scale structures (LSSs) can disclose key cosmological information for understanding the physics beyond the standard model. In recent years, the use of deep neural networks in the direct extraction of cosmological information from LSS maps has gained increasing attention among research community. As the evolution of LSSs is governed by a growth factor that depends on contents of the universe combined with nonlinear eﬀects, if neural networks can capture correlations at various epochs, then precision measurements of cosmological parameters can be improved. In this paper, we perform N-body simulations and demonstrate that image-based transformer networks conﬁgured for time-series data can be enhanced for the accurate extraction of Ωm and σ8 parameters."
Vision-Language 모델에서의 Variational 오토인코더와 대조 학습을 이용한 멀티모달 정렬,2023,"['이미지 캡셔닝', '멀티모달', '임베딩 정렬', 'variational 오토인코더', '대조 학습', 'image captioning', 'multimodal', 'alignment', 'variational autoencoder', 'contrastive learning']",국문 초록 정보 없음,"Many previous vision-language model studies have been proposed to train image-text data with each encoder independently. Since these models process each modality independently on different encoders, a heterogeneity gap occurs because there is no association between image embedding and text embedding, and each has different embedding vector spaces. Thus, we propose a Transformer-VAE-CL model by adding variational autoencoder and contrastive learning structures to the existing Transformer-based pretrained model to solve this problem. We train the model using the proposed methodologies so that image embedding and text embedding have similar vector representations. The model training and evaluation use the MSCOCO captioning dataset, and performance is evaluated by BLEU, ROUGE-1, ROUGE-L, and METEOR. The Transformer-VAE-CL model using a variational autoencoder and contrastive learning proposed in this paper obtains 12.1 for BLEU, 46.1 for ROUGE-1, 44.7 for ROUGE-L and 39.9 for METEOR, higher performance than the existing Transformer-based model."
ProphetNet 모델을 활용한 시계열 데이터의 열화 패턴 기반 Health Index 연구,2023,"['Health Index', 'Degradation Patterns', 'Time Series Data', 'ProphetNet Model']",국문 초록 정보 없음,"The Fourth Industrial Revolution and sensor technology have led to increased utilization of sensor data. In our modern society, data complexity is rising, and the extraction of valuable information has become crucial with the rapid changes in information technology (IT). Recurrent neural networks (RNN) and long short-term memory (LSTM) models have shown remarkable perform- ance in natural language processing (NLP) and time series prediction. Consequently, there is a strong expectation that models excelling in NLP will also excel in time series prediction. However, current research on Transformer models for time series prediction remains limited. Traditional RNN and LSTM models have demonstrated superior performance compared to Transformers in big data analysis. Nevertheless, with continuous advancements in Transformer models, such as GPT-2 (Generative Pre-trained Transformer 2) and ProphetNet, they have gained attention in the field of time series prediction. This study aims to evaluate the classification performance and interval prediction of remaining useful life (RUL) using an advanced Transformer model. The performance of each model will be utilized to establish a health index (HI) for cutting blades, enabling real-time monitoring of machine health. The results are expected to provide valuable insights for machine monitoring, evaluation, and management, confirming the effectiveness of advanced Transformer models in time series analysis when applied in industrial settings."
대형 사전훈련 모델의 파인튜닝을 통한 강건한 한국어 음성인식 모델 구축,2023,"['deep learning', 'machine learning', 'speech recognition']",국문 초록 정보 없음,"Automatic speech recognition (ASR) has been revolutionized with deep learning-based approaches, among which self-supervised learning methods have proven to be particularly effective. In this study, we aim to enhance the performance of OpenAI’s Whisper model, a multilingual ASR system on the Korean language. Whisper was pretrained on a large corpus (around 680,000 hours) of web speech data and has demonstrated strong recognition performance for major languages.However, it faces challenges in recognizing languages such as Korean, which is not major language while training.We address this issue by fine-tuning the Whisper model with an additional dataset comprising about 1,000 hours of Korean speech. We also compare its performance against a Transformer model that was trained from scratch using the same dataset. Our results indicate that fine-tuning the Whisper model significantly improved its Korean speech recognition capabilities in terms of character error rate (CER). Specifically, the performance improved with increasing model size.However, the Whisper model’s performance on English deteriorated post fine-tuning, emphasizing the need for further research to develop robust multilingual models. Our study demonstrates the potential of utilizing a fine-tuned Whisper model for Korean ASR applications. Future work will focus on multilingual recognition and optimization for real-time inference."
K-means 클러스터링과 트랜스포머 기반의 교차 도메인 추천,2023,"['cross-domain recommendation', 'data sharing', 'K-means clustering', 'transformer network']","교차 도메인 추천은 다른 도메인에 있는 관련 사용자 정보 데이터와 아이템 데이터를 공유하는 방법입니다. 주로 사용자 중복이 많은 온라인 쇼핑몰이나 유튜브, 넷플릭스와 같은 멀티미디어 서비스 컨텐츠에서 사용됩니다. K-means 클러스터링을 통해 사용자 데이터와 평점을 기반으로 군집화를 실시하여 임베딩을 생성합니다. 이 결과를 트랜스포머 네트워크를 통해 학습한 후 사용자 만족도를 예측합니다. 그런 다음 트랜스포머 기반 추천 모델을 사용하여 사용자에게 적합한 아이템을 추천합니다. 이 연구를 통해 추천함으로써 더 적은 시간적 비용으로 초기 사용자 문제를 예측하고 사용자들의 만족도를 높일 수 있다는 결과를 실험을 통해 보여주었습니다.",다국어 초록 정보 없음
딥러닝을 활용한 세계태권도 남자부 선수들의 경기유형 분류: 트랜스포머 알고리즘 적용,2023,"['태권도', '경기유형', '군집분석', 'Transformer learning', 'Taekwondo', 'Match Types', 'cluster analysis', 'Transformer learning']","이 연구는 세계태권도선수권대회 남자부 경기를 대상으로 승⋅패집단에 따른 태권도 선수들 경기유형을 분류하여 비교분석 하는 목적으로 설계하였다. 이 연구의 목적을 위해 세계대회에 참여한 남자부 경기 754라운드를 분석경기로 선정하였으면, 경기내용 기록을 위해 22개 변인을 선정하였다. 22개의 변인의 특징을 찾아 군집분석 모델링을 통해 선수 유형을 확인하였으며, 데이터의 값들의 변화 감지에 좋은성능을 보이는 Transformer learning과 데이터의 복잡한 데이터들의 특징 요소들을 파악할 수 있는 차원축소 모델을 활용하였다. 그 결과 첫째 태권도 선수들 경기유형을 분류에 있어 모델 2 즉, 임베딩을 활용한 모델이 적합한 것으로 나타났으며, 경기유형은 승⋅패 집단별 4개로 구분되었다. 둘째, 승리집단의 군집에 대한 명명으로는 군집 1 : 득점관리_유지형, 군집 2 : 선제득점_체력형, 군집 3 : 선제득점_관리형, 군집 4 : 얼굴득점_역전형이며, 패배집단의 군집에 대한 명명은 군집 1 : 역습공격_체력형, 군집 2 : 회전공격_득점약세형, 군집 3 : 회전공격_체력형, 군집 4 : 회전공격_시도형이다.","This study first looked into This study was designed for the purpose of comparative analysis by classifying the match types of Taekwondo players according to winning and losing groups in the men’s matches of the World Taekwondo Championships.For the purpose of this study, 754 men’s sets participated in world competitions were selected as analysis matches, and 22 variables were selected for recording the contents of the matches .Characteristics of 22 variables were found and the player type was identified through cluster analysis modeling. Transformer learning which shows good performance in detecting changes in data values in deriving the characteristics of variables, and the dimensionality reduction model that can identify the characteristic elements of complex data of data was used.As a result, first, model 2, that is, a model using 임베딩, was found to be appropriate for classifying the types of taekwondo players matches, and the types of matches were divided into 4 groups by winning and losing groups.Second, the names of the clusters of the winning group are cluster 1: scoring management_maintenance type, cluster 2: preemptive scoring_physical force type, cluster 3: preemptive scoring_management type, cluster 4: face scoring_reverse type, and the names of clusters of the losing group are cluster 1: counterattack_ physical force type, cluster 2: spinning attack_weak score type, cluster 3: spinning attack_ physical force type, cluster 4: spinning attack_attempt type."
드론 촬영 이미지 데이터를 기반으로 한 도로 균열 탐지 딥러닝 모델 개발,2023,"['딥러닝', '드론', 'Deep Learning', 'Swin Transformer', 'Backbone', 'Loss Function', 'Drone']","드론은 국토조사, 수송, 해양, 환경, 방재, 문화재, 건설 등 다양한 분야에서 활용되고 있다. 또한 사물인터넷(Internet of Things), 인공지능(Artificial Intelligence) 등과 관련하여 4차 산업 혁명의 핵심기술을 검증하고 적용시킬 수 있는 기술로 떠오르고 있다. 본 연구에서는 드론을 활용하여 균열을 자동으로 탐지할 수 있는 딥러닝 모델을 개발하고자 한다. 딥러닝 학습을 위한 이미지 데이터는 Mavic3 드론을 이용하여 수집하였고 촬영고도는 20m, ×7배율로 촬영하였다. 촬영 시 약 2m/s의 속도로 전진하여 영상을 찍고, 프레임을 추출하는 식으로 데이터를 수집하였다. 이런식으로 수집한 데이터를 통해 딥러닝 학습을 진행하였다. 본 연구에서는 딥러닝 학습모델로 Backbone으로는 Swin Transformer, Architecture로 UperNet을 사용하였다. 약 800장의 라벨링 된 데이터를 Augmentation기법으로 데이터 양을 증가시키고 3차에 걸쳐 학습을 진행하였다. 1차와 2차 학습 시 Cross-Entropy loss function을 사용하였고 3차 학습 시 Tversky Loss Function을 사용하였다. 학습결과, 균열 탐지와 균열율을 계산할 수 있는 모델을 개발하였다. 또한, 드론의 위치 정보를 이용해 특정 도로의 한 차선 균열율을 계산할 수 있는 모델을 개발하였다. 향후 추가적인 연구를 통하여 균열탐지모델의 고도화를 사물인터넷(IoT)과의 융합으로 이루었을 때 소파보수(Patching)나 포트홀(Pothole)의 탐지가 가능할 것으로 보인다. 또한 드론의 실시간 탐지 업무수행으로 포장 유지 보수구간에 대한 탐지를 신속하게 확보할 수 있을것으로 기대된다.",다국어 초록 정보 없음
내부자 이상행위 탐지 모델 구현을 위한 탐색적 데이터 분석 (EDA),2023,[],본 논문은 사용자 이상행위 분석 모델을 위한 개념 증빙 (Proof of Concept) 구현에서 필수적으로 요구되는 학습용 데이터셋에 관한 것으로 트랜스포머 기반의 사전학습 모델을 구현하기 위해 공개 데이터셋을 학습 모델의 입력 데이터로 가공하기 위한 탐색적 데이터 분석(EDA) 및 결과에 대해 설명한다. 본 EDA 과정에서 공개 데이터셋으로 CERT r6.2버전의 데이터셋에 대해 모델 학습용으로 사용할 다양한 종류의 특징 값을 추출하고 그 결과를 제시하였다.,다국어 초록 정보 없음
딥러닝 객체 분할 모델 및 RGB-D 영상 기반 수확 전 토마토 생체중 예측 기술 개발,2023,"['토마토', '딥러닝', 'RGB-D', '객체분할', '생체중']","과실의 생장단계에서의 크기, 형상, 색상은 과실의 품질과 수확시기에 대한 정보를 담고 있다. 특히, 과실의 크기의 경우 과실의 생장 정도와 최종 생산량을 예측하는 데 있어 중요한 정보이다. 이러한 외부 표현 정보에 대한 모니터링을 위해, 영상을 기반으로 한 많은 연구들이 수행되었다. 특히 최근 영상기반의 연구들에서는, 합성곱신경망이나 트랜스포머 등 딥러닝 모델들이 이전의 특징추출 기반의 모델들 대비 높은 성능을 보인다. 본 연구에서는, 재배 중인 토마토의 RGB-D 영상을 촬영하여 크기를 추정하고 크기정보를 통하여 토마토의 생체중을 예측하고 그 성능을 평가하고자 하였다. 영상은 시설원예 토마토들에 대하여 촬영되었으며, 촬영된 토마토는 수확후 부피 및 무게를 측정하였다. 영상 내 토마토의 영역은 딥러닝 객체분할 모델로 인식되었으며, 인식된 영역의 크기 정보를 통해 토마토의 생체중이 예측되었다.",다국어 초록 정보 없음
CLS 토큰으로 초기화된 양방향 LSTM을 이용한 DPT 기반 단안 깊이 추정 모델의 성능 향상,2023,"['monocular depth estimation', 'vision transformer', 'LSTM', 'transfer learning', 'bi-directional long-short term memory']",국문 초록 정보 없음,"Monocular depth estimation (MDE) is a computer vision task that estimates the depth value for each pixel in a single monocular RGB image. Given its inherently ill-posed nature, recent studies have focused on data-driven learning-based methods, particularly deep learning techniques based on convolutional neural networks. Building on the success of vision transformers (ViTs), ViT-based models have gained widespread attention for MDE applications. To enhance the performance of MDE, we introduce an intermediate bi-directional long-short term memory (BiLSTM) module, a common component in natural language processing, to learn correlations between each token in a sequence both in the forward and backward directions. Given that transformers naturally encode sequential data, leveraging BiLSTMs to learn the correlations between each token in both directions is a reasonable approach. In addition, we enhance the weight initialization method for BiLSTMs by employing the CLS tokens of transformers, which was originally developed for natural language processing, to make it applicable for processing two-dimensional image data. Subsequently, we developed an MDE model by inserting a BiLSTM module initialized using our method between the encoder and decoder of a DPT-based architecture. Our experiments reveal notable results, with our model achieving 0.913 in δ_1 and 0.097 in AbsRel for the NYU Depth V2 dataset. These results demonstrate a significant improvement over the previous models."
CNN 기반 딥러닝 모델을 통한 폐암 컴퓨터 보조 진단 시스템 개발,2023,"['CADx', 'Cancer Diagnosis', 'Classification', 'Convolution Neural Network', 'Deep learning', 'Lung cancer', 'Malignant Tumor']",국문 초록 정보 없음,"Lung cancer ranked second in Korea domestic cancer incidence in 2020 and second in death rate. Lung cancer often has no early symptoms, so patients often miss the time of treatment. Accordingly, in Korea, lung cancer has been included in the national cancer screening since 2019. However, among misdiagnosis cases, lung cancer had the highest misdiagnosis rate, and the accuracy of screening may vary depending on the medical specialist's skill level and fatigue. Accordingly, this paper proposed a lung cancer CADx(Computer-Aided Diagnosis) system based on EfficientNetV2-L and ConvNeXt-B. EfficientNetV2 is a model that can have high classification performance with a small number of parameters using the Training-Aware NAS (Neural Architecture Search) method. ConvNeXt is a network that achieves higher performance than ViT(Vision Transformer) by combining the latest techniques with ResNet-50 as a base model. Medical imaging generally suffers from a data shortage problem. Therefore, we augmented the lung cancer dataset using AutoAugment using the ImageNet augmentation policy. Through this method, the sensitivity in classifying malignant(lung cancer) and normal improved from 0.8354 to 0.9638 in EfficientNetV2 and from 0.9796 to 0.9963 in ConvNeXt.AUC (Area Under the ROC Curve) also improved from 0.9967 to 0.9974 for EfficientNetV2 and from 0.9973 to 1.0000 for ConvNeXt. Additionally, noise that may generally occur in CT images was added and compared through Gaussian noise.EfficientNetV2's Sensitivity was 0.7417 in the original model and 0.8954 in the model to which AutoAugment was applied, representing a decrease of 9.37% and 6.84%, respectively. In contrast, ConvNeXt exhibited a Sensitivity of 0.9796 in the original model and 0.9963 in the model to which AutoAugment was applied, showing no decrease in performance. This led to the development of a CADx system that demonstrates excellent performance."
TECD : 용종 분할을 위한 트랜스포머와 합성곱 신경망 결합 구조,2023,"['인공지능', '의료 영상 분할', '용종 분할', '딥러닝', '트랜스포머']","인공지능의 기술의 발달로 인해 의료 분야에도 진단, 예방, 시각화 등 다양한 분야 인공지능이 활용되고 있다. 그러나 환자의 개인 정보 보호를 위해 공개된 데이터가 제한적이어서 인공지능 학습에 어려움이 있다. 본 연구에서는 소수의 데이터로도 안정적으로 학습할 수 있는 TECD (Transformer Encoder Convolutional neural network Decoder)를 제안한다. TECD는 SegFormer의 인코더와 Attention U-net의 디코더를 결합하여 사용한다. 이는 트랜스포머와 합성곱 신경망의 단점을 보완할 수 있도록 설계되었다. 트랜스포머는 이미지의 특징을 풍부하게 잘 추출할 수 있으나 많은 학습 데이터가 필요하며, 합성곱 신경망은 상대적으로 적은 데이터로도 학습할 수 있으나 이미지의 특징 추출에 한계가 있다. TECD는 이러한 두 구조의 장점만 효과적으로 결합했다. 본 연구에서는 용종 분할 데이터인 Kvasir-SEG 데이터셋을 사용하여 실험을 진행하였다. 실험 결과, TECD는 소수의 데이터를 가지고 학습할 때, 트랜스포머 기반의 SegFormer와 합성곱 신경망 기반의 Attention-U-net보다 더 우수한 성능을 보였다. 대장내시경 영상 내의 용종 분할 작업 외에 다양한 의료 영상 분야에 TECD가 적용될 수 있을 것이라 기대한다. 더불어 의료 분야와 같이 소수의 데이터만 공개되는 분야에서도 트랜스포머 기반의 모델을 활용할 수 있는 가능성을 제시한다.",다국어 초록 정보 없음
BERT를 활용한 전자정부표준프레임워크 코드 생성 모델 구축,2023,"['BERT', 'CodeBERT', 'KoBERT', 'e-government Framework', 'Spring MVC pattern', 'Source code generation']","전자정부프레임워크에 기반하여 다양한 프로젝트를 수행하면서 코드 작성에 많은 어려움이 있다. 이에 인공지능 기반의 딥 러닝(Deep Learning)을 활용하여 사람이 대화를 위해 사용하는 자연어를 컴퓨터가 이해할 수 있게 변환하는 다양한 사례가 증가하고 있다. OpenAI의 Codex 모델을 사용하여 Github가 보유한 소스 코드를 학습시켜 개발자가 통합 개발 환경(Integrated Development Environment, IDE)에서 GitHub Copilot Extension을 설치하고 주석이나 함수명을 입력하면 인공지능이 작성해야 할 소스 코드를 자동 완성하여 제시해 주어 개발자의 생산성을 높여주고 있다. 다만 인공지능이 학습하지 않은 코드에 대해서는 완벽한 코드를 제시하지 못하며 간혹 AI가 개발자의 의도를 잘못 이해하고 많은 부분을 수정해야 하는 코드를 제시하기도 한다. 그렇게 되면 오히려 생산성에 마이너스가 될 수도 있다. 본 논문에서는 Github의 Copilot이 Github repository를 이용하여 코드를 학습시켰듯이 전자정부표준프레임워크를 기반으로 한 프로젝트에서 사용하고 있는 소스 코드를 BERT(Bidirectional Encoder Representations from Transformers) 모델을 통해 학습시켜 소스 코드를 자동으로 생성하는 코드 생성 모델을 제안한다. 이를 통해 프로젝트의 생산성 향상 도구로 활용되어 코드 작성 시간을 줄여줄 뿐만 아니라, 개발 과정의 노하우를 모델에 축적하여 개발자의 의사소통 도구로 다양한 분야에 활용할 수 있을 것으로 사료된다.","There are many difficulties in writing code while performing various projects based on the e-government framework. Therefore, artificial intelligence(AI)-based software development tools, various cases of converting natural language used when talking to people into a programming language that computers can understand are increasing. By using OpenAI's Codex model to learn the source code owned by Github, developers install GitHub Copilot Extension in an integrated development environment(IDE) and enter annotations or function names, automatically completing and presenting the source code to be written by artificial intelligence to increase developer productivity. However, for codes that artificial intelligence has not learned, it does not present perfect codes, and sometimes AI misunderstands the developer's intentions and presents codes that need to be modified in large part. If that happens, it could rather negatively affect productivity. In this paper, we propose a code generation model that automatically generates the source code by learning the source code used in projects based on the e-government standard framework to the BERT model, as Github Copilot learned the code using the Github repository. Through this, it is believed that it can be used as a tool to improve the productivity of the project, reducing code writing time, and accumulating know-how in the development process in the model to be used in various application cases as a communication tool for developers."
멀티모달 딥러닝을 이용한 우울증 진단 모델,2023,"['우울증 탐지', '모달리티 퓨전', '멀티모달', '피처 추출', 'Depression Detection', 'Fusion', 'Multimodal', 'Feature expraction']","본 연구는 화자의 텍스트 신호와 음성 신호를 입력 값으로 하여 두 개의 모달리티를 융합하고, 우울증 여부를 탐지하는 모델을 제안한다. DAIC-WOZ 데이터셋을 바탕으로 CNN을 활용하여 음성 특징을, 트랜스포머를 활용하여 텍스트 특징을 추출한 후 텐서 퓨전 네트워크를 통해 두 개의 모달리티를 융합하였다. 또한 최종 레이어에서 LSTM을 사용하여 화자의 우울증 여부를 탐지하는 모델을 구축하였다. 본 연구는 일상 대화에서 환자 스스로 우울증 탐지를 가능하게 하여 정신 질환 진단에 대한 접근성을 높일 수 있다는 가능성을 제시한다. 본 연구에서 제안된 모델을 발전시켜 음성 대화 시스템을 연결한다면 주기적으로 병원을 방문할 수 없는 환자나 병원 방문에 대한 거부감이 있는 환자들이 보다 쉽게 자신의 상태를 점검하며 회복을 도모할 수 있을 것이다. 나아가 다양한 정신질환에 대한 멀티레이블 분류로 확장하여 간편한 자가 정신 질환 진단 도구로 활용될 수 있을 것이다.","This study proposes a model that fuses two modalities with the speakers text and voice signals as input values and detects depression. Based on the DAIC-WOZ dataset, voice features were extracted using CNN, text features were extracted using Transformers, and two modalities were fused through a tensor fusion network. We also build a model to detect whether the speaker is depressed or not using LSTM in the final layer. This study suggests the possibility of increasing access to mental illness diagnosis by enabling patients to detect depression on their own in daily conversations. If the model proposed in this study is developed and the voice conversation system is connected, it will be easier for patients who cannot visit the hospital periodically or who are reluctant to visit the hospital to check their condition and seek recovery. Furthermore, it can be expanded to multi-label classification for various mental diseases and used as a simple self-mental disease diagnosis tool."
RGB 영상 및 딥러닝 영상분할 모델을 이용한 사과 과실 표면결함 검출 기술 개발,2023,"['사과', '표면결함', '선별', '검출', '딥러닝', '영상분할', '트랜스포머']","사과 과실의 표면에는 열과, 멍, 상처, 병해 등 다양한 유형의 결함이 발생할 수 있으며, 이는 사과의 상품성을 저해하기 때문에 수확 후 유통과정에서 결함 유무에 대한 선별이 이루어지고 있다. 현재 대부분의 산지유통센터에서 사과 표면 결함에 대한 선별은 작업자들의 육안 선별에 의존하고 있으며, 작업의 정확성과 효율성 개선을 위해서는 영상 기반의 자동 선별 기술이 필요한 상황이다. 본 연구에서는 사과 과실에 대한 RGB 영상과 딥러닝 영상분할(semantic segmentation) 모델을 활용하여 사과 표면에 발생하는 대표적인 결함 유형인 열과, 멍, 상처, 병해에 해당하는 영역을 자동으로 검출하는 기술을 개발하였다. 이를 위하여 표면결함이 존재하는 사과 과실의 영상을 수집하여 결함 정보에 대한 데이터셋을 구축하였고, 트랜스포머(transformer) 기반의 최신 딥러닝 영상분할 모델을 적용하여 표면결함의 영역과 유형을 학습하였다. 이를 통하여 사과 과실의 표면결함 유무를 진단하고 결함 영역을 유형 별로 검출할 수 있는 알고리즘을 개발하였으며, 영역 검출 성능 및 분류 정확도를 평가하였다.",다국어 초록 정보 없음
효율적인 이미지 검색 시스템을 위한 자기 감독 딥해싱 모델의 비교 분석,2023,"['Deephashing', 'Image Retrieval', 'Variational Inference', 'Self-supervised Learning', 'Attention Mechanism', '딥해싱', '이미지 검색', '변분 추론', '자기 지도 학습', '어텐션 메커니즘']",국문 초록 정보 없음,"In hashing-based image retrieval, the hash code of a manipulated image is different from the original image, making it difficult to search for the same image. This paper proposes and evaluates a self-supervised deephashing model that generates perceptual hash codes from feature information such as texture, shape, and color of images. The comparison models are autoencoder-based variational inference models, but the encoder is designed with a fully connected layer, convolutional neural network, and transformer modules. The proposed model is a variational inference model that includes a SimAM module of extracting geometric patterns and positional relationships within images. The SimAM module can learn latent vectors highlighting objects or local regions through an energy function using the activation values of neurons and surrounding neurons. The proposed method is a representation learning model that can generate low-dimensional latent vectors from high-dimensional input images, and the latent vectors are binarized into distinguishable hash code. From the experimental results on public datasets such as CIFAR-10, ImageNet, and NUS-WIDE, the proposed model is superior to the comparative model and analyzed to have equivalent performance to the supervised learning-based deephashing model. The proposed model can be used in application systems that require low-dimensional representation of images, such as image search or copyright image determination."
비전 트랜스포머를 위한 열 탈락 정규화 기법,2023,[],국문 초록 정보 없음,"In this paper, we inject an inductive bias into the dropout operation by considering the attention operation characteristics of vision transformers. Based on this approach, we can address the issue of excessive regularization in conventional dropout and improve performance. Experimental evaluations on ImageNet32, CIFAR-100, and CIFAR-10 datasets demonstrate that column dropout outperforms conventional dropout methods. The results indicate that column dropout can effectively improve the performance of various vision transformers. Our research findings suggest that introducing column dropout as a regularization technique can be beneficial for improving model performance."
오류 유형에 따른 생성요약 모델의 본문-요약문 간 요약 성능평가 비교,2023,"['Natural Language Processing', 'Generative Text Summarization', 'Quality Estimation', 'Meta-Evaluation', '자연어처리', '텍스트 생성요약', '품질예측', '메타평가']",국문 초록 정보 없음,"Generative Text Summarization is one of the Natural Language Processing tasks. It generates a short abbreviated summary while preserving the content of the long text. ROUGE is a widely used lexical-overlap based metric for text summarization models in generative summarization benchmarks. Although it shows very high performance, the studies report that 30% of the generated summary and the text are still inconsistent. This paper proposes a methodology for evaluating the performance of the summary model without using the correct summary. AggreFACT is a human-annotated dataset that classifies the types of errors in neural text summarization models. Among all the test candidates, the two cases, generation summary, and when errors occurred throughout the summary showed the highest correlation results. We observed that the proposed evaluation score showed a high correlation with models finetuned with BART and PEGASUS, which is pretrained with a large-scale Transformer structure."
제어 흐름 기반 그래프 트랜스포머를 이용한 악성코드 공격의 기능적 특징 학습,2023,"['악성코드 탐지', '제어 흐름 그래프', '공격 경로 샘플링', '그래프 임베딩', '그래프 트랜스포머', 'malware detection', 'control flow graph', 'attack path sampling', 'graph embedding', 'graph transformer']",국문 초록 정보 없음,"To minimize false negatives in malware classification, it is important to capture local characteristics of a program, such as the control flow between operation blocks and memory-register addresses. However, existing methods that optimize the loss function of a classifier without considering the functional characteristics of malware have limitations in recall due to new attack paths and complex control flow graphs. In this paper, we propose a method that explicitly samples and embeds the control flow graphs to learn functional characteristics, such as API calls, rootkit DLL installation, and specific virtual memory access, and improve recall. To model the functional patterns of malware from the control flow graphs, we sample attack paths from the control flow of the malware and classify the types of malware using a graph embedding function based on the transformer. We evaluate the proposed method using a real-world malware benchmark dataset, Microsoft Challenge. By explicitly learning the control flow of the malware, we achieved a recall of 97.89% and significantly improved the accuracy (99.45%) compared to the latest and most advanced method's classification accuracy (97.89%)."
한국어 문서 요약 모델의 성능 향상을 위한 포스트 트레이닝 기법,2023,"['KoBART', '포스트 트레이닝', '요약', '사전 학습', '미세 조정', 'KoBART', 'post training', 'summarization', 'pre-training', 'fine-tuning']",국문 초록 정보 없음,"The document summarization task generates a short summary based on a long document.Recently, a method using a pre-trained model based on a transformer model showed high performance.However, as it was proved that fine-tuning does not train the model optimally due to the learning gap between pre-training and fine-tuning, post-training, which is additional training between pre-training and fine-tuning, was proposed. This paper proposed two post-training methods for Korean document summarization. One was Korean Spacing, which is for learning Korean structure, and the other was First Sentence Masking, which is for learning about document summarization. Experiments proved that the proposed post-training methods were effective as performance improved when the proposed post-training was used compared to when it was not."
학습데이터 크기 변화에 따른 트랜스포머 아키텍처의 여름철 실내온도와 열부하 예측성능 평가,2023,"['기계학습', '심층학습', '시계열 예측', '모델예측제어', '트랜스포머', '어텐션 메커니즘', '순환신경망', '장단기 메모리', '다층 퍼셉트론', 'Machine Learning', 'Deep Learning', 'Time-Series Forecasting', 'Model Predictive Control(MPC)', 'Transformer', 'Attention Mechanism', 'Recurrent Neural Network(RNN)', 'Long Short-Term Memory(LSTM)', 'Multilayer Perceptron(MLP)']",국문 초록 정보 없음,다국어 초록 정보 없음
장기 변동성을 반영하기 위한 확산 모델을 활용한 주가 지수 확률 분포 예측,2023,"['Deep learning', 'Time series forecasting', 'Stochastic differential equations', 'Financial time series', 'Stock market index', 'Probabilistic forecasting']",국문 초록 정보 없음,"Many of the existing time series forecasting studies in stock market were statistical or machine learning methods based on point prediction. However, typical point forecasting methods fail to consider the distribution of data, which may ignore extreme market situations. This can be vulnerable especially for the long-term forecasting, given the high volatility in the stock market. To resolve the issue, we propose a stock index probabilistic distribution prediction framework based on the Denoising Diffusion Model, called StockGrad. Specifically, StockGrad adopt a sparse transformer encoder as feature extractor to yield zero probability for time steps that are less relevant to the prediction time step. For probability distribution prediction, our model transforms the white noise into the distribution of interest through the Markov chain using the TimeGrad time series generation methodology. Our proposed methodology can help investors make decisions on investment or risk management. Our experiments show that proposed StockGrad framework outperforms the existing deep learning probability models by about 3.71%, 1.53% and 1.71% on S&P500, NIKKEI225, and KOSPI200 stock index data sets, respectively. Also, it is experimentally demonstrated that using the sparse transformer encoder as a feature extractor captures historical time points related to predictive bulbs well, improve the performance of the long-term prediction."
자기 지도 사전 학습 Swin UNETR 모델을 이용한 3차원 체성분 분할,2023,[],국문 초록 정보 없음,"As a biomarker, the distribution and volume of fat and muscle in Abdominal and Pelvic CT(APCT) are significant. Based on selfsupervised pre-trained Swin UNet Transformer (Swin UNETR) model, 3D body composition segmentation was performed on the entire abdomen and pelvic scan images. Swin UNETR, a Swin Transformer-based encoder and CNN-based decoder structure, can improve segmentation performance by using hierarchical information. Additionally, it was expected that utilizing a self-supervised, pre-trained model that learned human anatomical patterns would improve training efficiency. This study conducted segmentation using a total of 8 body composition labels while varying the amount of training data and verified its feasibility using pre-trained Swin UNETR model effective for the data and task."
전국 기상 관측 데이터를 활용한 전력수요 예측 모델 구현,2023,"['DLinear Algorithm', 'Energy Transition', 'Power Demand Forecasting', 'Transformer Algorithm', 'Weather Factors']","에너지 자원 분야에서 전기 에너지는 매우 특수한 성질을 띄는 자원 중 하나로, 생산과 동시에 소비가 이뤄져야 하는 특징을 갖고 있다. 이는 아직까지 전기에너지의 효과적인 저장방법이 존재하지 않기 때문으로, 전력 계통의 안정적인 운영을 위해서는 수요와 공급의 균형을 유지하는 것이 매우 중요하다. 특히, 불안정한 균형에 의한 과전압이나 과소전압은 운영안정성을 크게 훼손하게 되며, 최악의 경우 대규모 정전사태 등 심각한 사회 문제를 야기할 수 있다. 이러한 이유로 전력수요를 예측하는것은 해당 분야에서 가장 중요한 연구 주제 중 하나이다. 따라서, 본 연구에서는 최신의 알고리즘을 이용하여 전력수요 예측 모델을 설계하고, 안정적인 전력계통 운영을 위한 신뢰성있는 기반을 제공하고자 한다. 이를 통해 실시간 전력 수요 예측을 통한 안정적인 전력 계통 운영의 기틀을 마련하고, 나아가 향후 재생에너지발전 등 탄소 중립을 위한 에너지 전환 사업의 기반을 다지고자 한다.",다국어 초록 정보 없음
딥러닝 기반 넙치 질병 증상 분류 모델 성능 분석,2023,"['넙치 질병', '질병 증상 분류', '학습 데이터 검증', 'YOLOv8 모델', 'Swin 모델', 'Paralichthys Olivaceus Diseases', 'Disease Symptoms Classification', 'Learning Data Verification', 'YOLOv8 Model', 'Swin Model']",국문 초록 정보 없음,"Halibut farming in Korea accounts for more than half of the fishery farming industry. However, 25 to 30 percent of halibut fish deaths are caused by disease per year, which has a very bad effect on the economic feasibility of halibut farming. The accurate diagnosis of halibut disease symptoms in real time is very important for the economic growth of halibut fish farms. In this paper, we propose an independent learning data collection method suitable for a deep learning-based halibut disease symptom classification model, a learning data purification and verification technique that can eliminate labeled learning data set errors, and an equal learning data separation technique, and apply the proposed technique to compare and analyze the halibut disease classification performance for 33 categories of halibut disease symptoms using CNN-based YOLov8 model and Vision Transformer-based Swin model. The YOLOv8 model learned up to 100 Epoch, showing a performance of 0.899 mAP recognition rate, 3 minutes of learning time per 10 Epoch, and 15.4 GB of VRAM usage. The Swin model learned up to 50 Epoch, and the mAP was 0.91, the learning time was 162 minutes per 10 Epoch, and the VRAM usage was 21.3GB. When comparing the performance of the YOLOv8 model and the Swin model, the Swin model showed a good mAP performance recognition rate with less Epoch, but in terms of learning speed, the YOLOv8 model completed the learning with an overwhelmingly short learning time. As shown in the results of this study, if a system that can diagnose halibut disease symptoms in real time using the latest deep learning model is developed, the productivity of the floating halibut style is expected to increase significantly."
가려진 사람의 재식별을 위한 트랜스포머 기반 교차 어텐션과 특징 다양화,2023,"['Deep learning', 'Occluded Person Re-ID', 'Transformer', 'keypoint Heatmap', 'Cross-attention', 'Feature Diversity']",국문 초록 정보 없음,"Occluded person re-identification is a very difficult because the specific person is occluded by obstacles or other persons or by oneself. Major works adopt transformer-based approach show excellent performances, but they used a basic transformer only. In this paper, we suggest the various techniques to improve the transformer-based Re-ID method for the occluded person as follows. First, after extracting the heatmap and then deleting random body parts on the heat map, accurate keypoint information is obtained in data augmentation. Second, Cross-attention between the keypoint heatmap and the output of the transformer's middle layer is provided to focus more on the non-occluded person area. Third K-menas clustering is utilized to enhance the representation of local features, and the structure of the network is proposed to improve the diversity of the features. We evaluate mAP and Rank-1 performance on the Occluded-Duke and Market-1501 dataset and compare the proposed model with existing state-of-the-art techniques. Experimental results show that our method outperforms state-of-the-art methods."
노이즈 제거 보조 학습을 적용한 트랜스포머 기반의 3차원 개체 분할,2023,"['point cloud', '3d instance scene segmentation', 'transformer decoder', 'denoising', 'auxiliary learning']",국문 초록 정보 없음,"3D point cloud instance segmentation, as a task in comprehending 3D scenes, involves predicting both 3D masks and class labels for individual object instances within a given point cloud. The development of an efficient transformer-based model for this task requires addressing the following key issues: refining instance masks and positions, initializing instance queries, and incorporating auxiliary task learning. To overcome the limitations of existing models, our study proposes a novel transformer-based model, T3DIS. This model refines both the mask and position, along with the query content of each instance during instance query decoding, thereby enhancing the quality of the final instance features. To expedite the instance decoding process, the model initializes the initial instance queries using a finite set of representative points selected from the point cloud. Furthermore, our approach incorporates auxiliary denoising task learning to facilitate rapid training of the transformer decoder. Through experiments conducted on the ScanNet-V2 benchmark dataset, we demonstrated the superiority of the proposed model. The evaluation involves comparing different methods of instance query initialization, position refinement, and auxiliary query denoising."
트랜스포머 기반 효율적인 자연어 처리 방안 연구,2023,"['machine learning', 'natural language processing', 'transformer', 'artificial intelligence']",국문 초록 정보 없음,"The natural language processing models used in current artificial intelligence are huge, causing various difficulties in processing and analyzing data in real time. In order to solve these difficulties, we proposed a method to improve the efficiency of processing by using less memory and checked the performance of the proposed model. The technique applied in this paper to evaluate the performance of the proposed model is to divide the large corpus by adjusting the number of attention heads and embedding size of the BERT[1] model to be small, and the results are calculated by averaging the output values of each forward. In this process, a random offset was assigned to the sentences at every epoch to provide diversity in the input data. The model was then fine-tuned for classification. We found that the split processing model was about 12% less accurate than the unsplit model, but the number of parameters in the model was reduced by 56%."
TeGCN:씬파일러 신용평가를 위한 트랜스포머 임베딩 기반 그래프 신경망 구조 개발,2023,"['씬파일러', '채무 불이행 예측', '그래프 합성곱 신경망', '범주형 변수 임베딩', 'TeGCN', 'Thin Filer', 'Default Prediction', 'Graph Convolutional Network', 'Categorical Feature Embedding', 'TeGCN']",국문 초록 정보 없음,"As the number of thin filers in Korea surpasses 12 million, there is a growing interest in enhancing the accuracy of assessing their credit default risk to generate additional revenue. Specifically, researchers are actively pursuing the development of default prediction models using machine learning and deep learning algorithms, in contrast to traditional statistical default prediction methods, which struggle to capture nonlinearity. Among these efforts, Graph Neural Network (GNN) architecture is noteworthy for predicting default in situations with limited data on thin filers. This is due to their ability to incorporate network information between borrowers alongside conventional credit-related data. However, prior research employing graph neural networks has faced limitations in effectively handling diverse categorical variables present in credit information. In this study, we introduce the Transformer embedded Graph Convolutional Network (TeGCN), which aims to address these limitations and enable effective default prediction for thin filers. TeGCN combines the TabTransformer, capable of extracting contextual information from categorical variables, with the Graph Convolutional Network, which captures network information between borrowers. Our TeGCN model surpasses the baseline model’s performance across both the general borrower dataset and the thin filer dataset. Specially, our model performs outstanding results in thin filer default prediction. This study achieves high default prediction accuracy by a model structure tailored to characteristics of credit information containing numerous categorical variables, especially in the context of thin filers with limited data. Our study can contribute to resolving the financial exclusion issues faced by thin filers and facilitate additional revenue within the financial industry."
트랜스포머 기반 MUM-T 상황인식 기술: 에이전트 상태 예측,2023,"['MUM-T', 'Situation Awareness', 'Military AI', 'Robot Intelligence']",국문 초록 정보 없음,"With the advancement of robot intelligence, the concept of man and unmanned teaming (MUM-T) has garnered considerable attention in military research. In this paper, we present a transformer-based architecture for predicting the health status of agents, with the help of multi-head attention mechanism to effectively capture the dynamic interaction between friendly and enemy forces. To this end, we first introduce a framework for generating a dataset of battlefield situations. These situations are simulated on a virtual simulator, allowing for a wide range of scenarios without any restrictions on the number of agents, their missions, or their actions. Then, we define the crucial elements for identifying the battlefield, with a specific emphasis on agents’ status. The battlefield data is fed into the transformer architecture, with classification headers on top of the transformer encoding layers to categorize health status of agent. We conduct ablation tests to assess the significance of various factors in determining agents’ health status in battlefield scenarios. We conduct 3-Fold corss validation and the experimental results demonstrate that our model achieves a prediction accuracy of over 98%. In addition, the performance of our model are compared with that of other models such as convolutional neural network (CNN) and multi layer perceptron (MLP), and the results establish the superiority of our model."
사용자 사전과 형태소 토큰을 사용한 트랜스포머 기반 형태소 분석기,2023,"['딥러닝', '트랜스포머', '자연어 처리', '형태소 분석', 'Deep Learning', 'Transformer', 'Natural Language Processing', 'Morpheme Analysis']","형태소는 한국어에서 의미를 가진 최소단위이기 때문에, 한국어 언어모델의 성능을 높이기 위해서는 정확한 형태소 분석기의 개발이 필요하다. 기존의 형태소 분석기는 대부분 어절 단위 토큰을 입력 값으로 학습하여 형태소 분석 결과를 제시한다. 하지만 한국어의 어절은 어근에 조사나 접사가 부착된 형태이기 때문에 어근이 같은 어절이어도 조사나 접사로 인해 의미가 달라지는 성향이 있다. 따라서 어절 단위 토큰을 사용하여 형태소를 학습하면 조사나 접사에 대한 오분류가 발생할 수 있다. 본 논문에서는 형태소 단위의 토큰을 사용하여 한국어 문장에 내재된 의미를 파악하고, Transformer를 사용한 시퀀스 생성 방식의 형태소 분석기를 제안한다. 또한, 미등록 단어 문제를 해결하기 위해 학습 말뭉치 데이터를 기반으로 사용자 사전을 구축하였다. 실험 과정에서 각 형태소 분석기가 출력한 형태소와 품사 태그를 함께 정답 데이터와 비교하여 성능을 측정하였으며, 실험 결과 본 논문에서 제시한 형태소 분석기가 기존 형태소 분석기에 비해 성능이 높음을 증명하였다.","Since morphemes are the smallest unit of meaning in Korean, it is necessary to develop an accurate morphemes analyzer to improve the performance of the Korean language model. However, most existing analyzers present morpheme analysis results by learning word unit tokens as input values. However, since Korean words are consist of postpositions and affixes that are attached to the root, even if they have the same root, the meaning tends to change due to the postpositions or affixes. Therefore, learning morphemes using word unit tokens can lead to misclassification of postposition or affixes. In this paper, we use morpheme-level tokens to grasp the inherent meaning in Korean sentences and propose a morpheme analyzer based on a sequence generation method using Transformer. In addition, a user dictionary is constructed based on corpus data to solve the out-of-vocabulary problem. During the experiment, the morpheme and morpheme tags printed by each morpheme analyzer were compared with the correct answer data, and the experiment proved that the morpheme analyzer presented in this paper performed better than the existing morpheme analyzer."
임베디드 엣지 플랫폼에서의 경량 비전 트랜스포머 성능 평가,2023,"['Vision Transformer', 'On-device AI', 'Image Classification', 'Quantization', 'Hardware Accelerators', 'Performance Evaluation']",국문 초록 정보 없음,"Recently, on-device artificial intelligence (AI) solutions using mobile devices and embedded edge devices have emerged in various fields, such as computer vision, to address network traffic burdens, low-energy operations, and security problems. Although vision transformer deep learning models have outperformed conventional convolutional neural network (CNN) models in computer vision, they require more computations and parameters than CNN models. Thus, they are not directly applicable to embedded edge devices with limited hardware resources. Many researchers have proposed various model compression methods or lightweight architectures for vision transformers; however, there are only a few studies evaluating the effects of model compression techniques of vision transformers on performance. Regarding this problem, this paper presents a performance evaluation of vision transformers on embedded platforms. We investigated the behaviors of three vision transformers: DeiT, LeViT, and MobileViT. Each model performance was evaluated by accuracy and inference time on edge devices using the ImageNet dataset. We assessed the effects of the quantization method applied to the models on latency enhancement and accuracy degradation by profiling the proportion of response time occupied by major operations. In addition, we evaluated the performance of each model on GPU and EdgeTPU-based edge devices. In our experimental results, LeViT showed the best performance in CPU-based edge devices, and DeiT-small showed the highest performance improvement in GPU-based edge devices. In addition, only MobileViT models showed performance improvement on EdgeTPU. Summarizing the analysis results through profiling,  the degree of performance improvement of each vision transformer model was highly dependent on the proportion of parts that could be optimized in the target edge device. In summary, to apply vision transformers to on-device AI solutions, either proper operation composition and optimizations specific to target edge devices must be considered."
Development of CNN-Transformer Hybrid Model for Odor Analysis,2023,"['CNN-Transformer Hybrid Model', 'CNN+LSTM Hybrid Model', 'CNN model', 'LSTM Model', 'ELM Model', 'Odor.']",국문 초록 정보 없음,"The study identified the various causes of odor problems, the discomfort they cause, and the importance of the public health and environmental issues associated with them. To solve the odor problem, you must identify the cause and perform an accurate analysis. Therefore, we proposed a CNN-Transformer hybrid model (CTHM) that combines CNN and Transformer and evaluated its performance. It was evaluated using a dataset consisting of 120,000 odor samples, and experimental results showed that CTHM achieved an accuracy of 93.000%, a precision of 92.553%, a recall of 94.167%, an F1 score of 92.880%, and an RMSE of 0.276. Our results showed that CTHM was suitable for odor analysis and had excellent prediction performance. Utilization of this model is expected to help address odor problems and alleviate public health and environmental concerns."
대규모 언어 모델(LLM)의 사전 지식을 활용한  3차원 장면 그래프 생성,2023,"['Point Cloud', '3D Scene Graph Generation', 'Prior Knowledge', 'Large Language Model', 'Graph Neural Network']",국문 초록 정보 없음,"In this paper, we propose a novel 3D scene graph generation model, L3DSG, which can make use of rich prior knowledge obtained from large language model (LLM) by prompt engineering. The proposed model is built upon our previous 3D scene graph generation model, C3DSG, that adopts Point Transformer as 3D geometric feature extractor and uses  the NE-GAT graph neural network as context reasoner. The new proposed model addresses the inability of C3DSG to utilize prior knowledge on indoor physical environments. It focuses on issues of how to obtain prior knowledge from LLM and how to make use of it for predicting objects and their relations effectively. The proposed model is extended from C3DSG by adding several elaborate modules to prompt, encode, and fuse prior knowledge from LLM. Through various experiments using the benchmark dataset 3DSSG, we show the superiority of the proposed model."
패킷 페이로드 분석을 활용한 트랜스포머 기반 침입탐지시스템,2023,"['자연어 처리', '침입탐지시스템', '패킷 페이로드', '트랜스포머', 'UNSW-NB15', 'Natural Language Processing', 'IDS', 'Packet Payload', 'Transformer', 'UNSW-NB15']",국문 초록 정보 없음,"Intrusion detection systems that learn metadata of network packets have been proposed recently.However these approaches require time to analyze packets to generate metadata for model learning, and time to pre-process metadata before learning. In addition, models that have learned specific metadata cannot detect intrusion by using original packets flowing into the network as they are. To address the problem, this paper propose a natural language processing-based intrusion detection system that detects intrusions by learning the packet payload as a single sentence without an additional conversion process.To verify the performance of our approach, we utilized the UNSW-NB15 and Transformer models. First, the PCAP files of the dataset were labeled, and then two Transformer (BERT, DistilBERT) models were trained directly in the form of sentences to analyze the detection performance. The experimental results showed that the binary classification accuracy was 99.03% and 99.05%, respectively, which is similar or superior to the detection performance of the techniques proposed in previous studies. Multi-class classification showed better performance with 86.63% and 86.36%, respectively."
딥러닝을 활용한 크라우드 펀딩 성공 예측 모델 연구,2023,"['Crowdfunding', 'Deep Learning', 'Language Model', 'Predictive Model', 'Text Analysis', 'Multimodal', '크라우드 펀딩', '딥러닝', '언어모델', '예측모델', '텍스트 분석', '멀티모달']",국문 초록 정보 없음,"Crowdfunding platforms have grown as a means of initial funding for startups, and they are also being used for various purposes beyond funding, such as pre-selling products for market assessment and selling the works of creators. Existing research on crowdfunding has mainly used quantitative data such as video views, image counts, and duration as variables. Some studies have incorporated unstructured text variables, utilizing metrics like the number of parts of speech, sentence length, or topics extracted through topic analysis. However, these variables often lack the contextual meaning of the text or provide limited reflection.In this study, language models are employed to extend the use of text and incorporate contextual meaning. Two models, DNN prediction models and classification models, were employed for the research. For text variables, pretrained BERT models released in 2017 and Transformer Encoder models trained directly on text data were utilized. Unlike previous research, the dependent variable was set as the number of funding supporters, and DNN was constructed using both text and numeric data. Furthermore, data featuring the increase in supporters on a daily basis was also utilized.This study collected data from Wadiz, a crowdfunding site, spanning from January 2021 to January 2023, encompassing a total of 9,755 completed funding projects. The collected data includes project categories, funding names, descriptions, main text, funding duration, funding amount, number of supporters, achievement rate, option prices, option quantities, counts of main images and videos, counts of images and videos in the main text, scroll length, and daily funding amount. By employing text in DNN models, using data analyzed by BERT and Transformer Encoder alongside numeric data, a different structural form from traditional regression models was achieved, resulting in improved outcomes. This study presents a new approach for both platform users and operators to understand and predict crowdfunding success."
영상 복원을 위한 다차원 윈도우 기반 비전 트랜스포머 네트워크,2023,"['영상 복원', '맥락정보', '비전 트랜스포머', '다중차원 윈도우', '딥러닝', 'Image Inpainting', 'Contextual Information', 'Vision Transformer', 'Multiscale  Window', 'Deep Learning']",국문 초록 정보 없음,다국어 초록 정보 없음
A Research on the Application of Face Recognition Algorithm  Based on Convolutional Model and Transformer Model in  Community Monitoring System,2023,"['facial recognition', 'Vision Transformer', 'ultra-lightweight.', '얼굴식별', '비전 트랜스포머', '울트라 라이트웨이트']",국문 초록 정보 없음,"The promotion of intelligent security community construction has greatly enhanced the intelligence and safety of residential areas. In order to further establish a security-oriented community, this paper proposes the utilization of facial recognition based on community surveillance footage to identify suspicious individuals. To address the difficulties in capturing facial images caused by factors such as low pixel resolution and varying shooting angles in surveillance footage, the following optimization strategies are proposed in this paper : Firstly, a lightweight global search facial detection network is designed based on convolutional modules and Vision Transformer modules. The Vision Transformer module is introduced to enhance the global retrieval capability of the network. Secondly, the structure of the Vision Transformer module is optimized by adding pooling layers in the feature block extraction and segmentation stage to reduce the number of module parameters. The feature blocks are mapped and computed with the feature maps to improve the corresponding feature correlation. Thirdly, in the face alignment stage, an Anchor Free mechanism is adopted to generate elliptical face localization regions for more accurate fitting of faces and reducing interference from other background information in the final identity recognition stage. Finally, the similarity between faces is calculated using Euclidean space distance to determine corresponding personnel identities. Through relevant experiments and tests on the self-built facial identity dataset in this paper's residential surveillance system, the proposed facial detection network achieves an average improvement of 3.11% in detection accuracy compared to other detection networks, reaching 97.19%. In terms of facial identity recognition, the designed model achieves an average improvement of 3.43% with a recognition accuracy of 95.84."
MPC-TimesNet: 다변량 주기성 정보를 반영한 대기오염도 장기예측 모델,2023,"['Deep learning', 'Air pollution', 'Time-series', 'Long-term forecasting', 'Multivariate forecasts', 'Multiple Periodicity Consideration (MPC)', 'Transformer']",국문 초록 정보 없음,"Urban air pollution has a significant impact on environment, public health, social economic, and policy decision. In accordance with increasing global necessity of constant monitoring and accurate forecasting of air pollution among governments and institutions, research into applying transformer-based models to predict pollutants preceded. However, the models have been limited in their ability to thoroughly consider local information within periodicity, which is one of the key inherent patterns in pollution data. To address the limitation, this paper proposes MPC(Multiple Periodicity Consideration)-TimesNet model designed to handle intricate locality information within periodicity by considering not only low frequencies information, but also that of high frequencies. The proposed model added HFEM(High Frequency Extraction Module) to the previous TimesNet model in order to exquisitely account for high frequencies data. In addition, we lowered the size of representations as the self-information gets smaller to address noise issues. To evaluate our model, we collected data of five air pollutants (NO2, OZ, CO2, SO2, PM10) among three regions―Songpa-gu, Youngdeungpo-gu, Jung-gu―in Seoul. As a result of the experiment, the proposed model recorded 0.3615, 0.3277, 0.3893 in mean square error and 0.3764, 0.3864, 0.39 in mean absolute error, respectively. Our model outperforms twelve previous SOTA models including baseline in all regions. In conclusion, we verified that summating HFEM increased the effectiveness of the model in forecasting air pollution by comparing their performance to the baseline, TimesNet."
해충 이미지 분류를 위한 멀티스케일 교차 주의집중 비전 트랜스포머,2023,"['pest imgae classification', 'region-of-interests', 'cross attention', 'vision transformer']",국문 초록 정보 없음,"Pests damage crops, resulting in reductions in crop productivity and quality. Therefore, it is important to promptly and accurately identify pest types and perform appropriate control work in a timely manner. Existing pest identification requires a lot of time because it is performed by visual judgment of experts. Therefore, in this paper, in order to realize the unmanned automation of pest prediction, we propose a deep learning model for automatically discriminating pest types from pest images. In particular, in this study, we propose a multiscale vision transformer model with a built-in cross-attention module based on the region-of-interest(ROI). The proposed model is designed as a dual branch, and the attentional function is strengthened by exchanging class tokens and patch tokens between the pest branch and the ROI branch. Through the experimental results, it was confirmed that the proposed cross-attention module can improve the feature extraction ability and improve the accuracy of the final classification model by about 1.3%."
Improved Detection of Urolithiasis Using High-Resolution  Computed Tomography Images by a Vision Transformer Model,2023,"['Deep learning', 'Ureteral calculi', 'Urolithiasis', 'Machine learning', 'Artificial intelligence']",국문 초록 정보 없음,"Purpose: Urinary stones cause lateral abdominal pain and are a prevalent condition among younger age groups. The diagnosis typically involves assessing symptoms, conducting physical examinations, performing urine tests, and utilizing radiological imaging. Artificial intelligence models have demonstrated remarkable capabilities in detecting stones. However, due to insufficient datasets, the performance of these models has not reached a level suitable for practical application. Consequently, this study introduces a vision transformer (ViT)-based pipeline for detecting urinary stones, using computed tomography images with augmentation.Methods: The super-resolution convolutional neural network (SRCNN) model was employed to enhance the resolution of a given dataset, followed by data augmentation using CycleGAN. Subsequently, the ViT model facilitated the detection and classification of urinary tract stones. The model’s performance was evaluated using accuracy, precision, and recall as metrics.Results: The deep learning model based on ViT showed superior performance compared to other existing models. Furthermore, the performance increased with the size of the backbone model.Conclusions: The study proposes a way to utilize medical data to improve the diagnosis of urinary tract stones. SRCNN was used for data preprocessing to enhance resolution, while CycleGAN was utilized for data augmentation. The ViT model was utilized for stone detection, and its performance was validated through metrics such as accuracy, sensitivity, specificity, and the F1 score. It is anticipated that this research will aid in the early diagnosis and treatment of urinary tract stones, thereby improving the efficiency of medical personnel."
트랜스포머 로봇의 지형별 걸음새 생성 알고리즘 개발,2023,"['Transformer(트랜스포머)', 'Robot(로봇)', 'Walking(보행)', 'Control(제어)', 'MPC(모델 예측 제어)']",국문 초록 정보 없음,다국어 초록 정보 없음
Transformer-based Prediction Model for SHM System of the Seohae Grand Bridge,2023,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 기반 한국어 요약을 위한 메타모픽 테스트 케이스 생성,2023,"['Korean natural language processing', 'T5', 'metamorphic testing', 'test case generation', 'document summary performance metrics', '한국어 자연어 처리', 'T5', '메타모픽 테스팅', '테스트 케이스 생성', '문서요약 성능지표']",국문 초록 정보 없음,"Recently, AI-based software such as ChatGPT has become popular. Consequently, interest in quality assurance testing of software is increasing. This study proposes a test case using a metamorphic relationship for a transformer-based Korean summary for software testing. First, the test set using the Defense Daily is transformed using certain rules and then entered into the T5 model.After inputting the transformed test set, we checked whether the output result according to the input and the existing output result satisfy the metamorphic relationship. We then evaluated the performance of the model using the document summary performance metrics Rouge and Rdass. The experimental results showed that MR1, which transforms names or nouns, and MR5 and MR6, which transform nouns/verbs into synonyms, satisfy the metamorphic relationship with 82%. In addition, the summarization performance of the T5 improved by 13% compared to the models in the previous study.After that, We used Rouge-u, Rouge-su, and Rdass scores. These are the scores that were not covered in the previous studies. Through these scores, the types of performance scores that evaluate the Korean summaries were expanded."
Transformer-based Prediction Model for SHM System of the Seohae Grand Bridge,2023,"['Deep Learning', 'Transformer-based Prediction Model', 'Seohae Grand Bridge', 'Structural Health Monitoring (SHM) system']",국문 초록 정보 없음,다국어 초록 정보 없음
TFT 모델 및 진동 신호 분해 기반 모터 구동 설비의 상태 모니터링 및 이상 탐지,2023,"['predictive maintenance', 'equipment health monitoring', 'anomaly detection', 'temporal fusion transformer', 'vibration signal decomposition', '.']",국문 초록 정보 없음,"Predictive maintenance is a technology that determines the status of equipment through real-time analysis and takes necessary actions in advance, and it is essential to build a model that can accurately forecast the status of equipment. This paper proposes an equipment health monitoring and anomaly detection system for predictive maintenance of motor-driven equipment, which is a core facility in manufacturing plants. The proposed system collects real-time data from IoT sensors attached to the target equipment, stores it in time-series databases, and provides the detailed health information of the equipment in real time by using state prediction and analysis algorithms and a web-based visualization platform. In particular, based on the TFT model with excellent time series forecasting performance, PLC data for equipment control and vibration signal decomposition data are used for training to improve the prediction performance."
이족 트랜스포머 로봇의 외란 대응 자세 안정화 제어,2023,"['Posture Stabilization Control', 'Biped Robot', 'Disturbance Observer']",국문 초록 정보 없음,"This paper describes the posture stabilization control of a bipedal transformer robot being developed for military use. An inverted pendulum model with a rectangular that considers the robot’s inertia is proposed, and a posture stabilization moment that can maintain the body tilt angle is derived by applying disturbance observer and state feedback control. In addition, vertical force and posture stabilization moments that can maintain the body height and balance are derived through QP optimization to obtain the necessary torques and vertical force for each foot. The roll and pitch angles of the IMU sensor attached to the robot’s feet are reflected in the ankle joint to enable flexible adaptation to changes in ground inclination. Finally, the effectiveness of the proposed algorithm in posture stabilization is verified by comparing and analyzing the difference in body tilt angle due to disturbances and ground inclination changes with and without algorithm application, using Gazebo dynamic simulation and a down-scale test platform."
폐렴 이미지 분류를 위한 Swin 과 ResNet 모델 비교,2023,"['ResNet', 'Swin Transformer', 'deep learning', 'pneumonia detection', 'chest X-ray.']",국문 초록 정보 없음,"Pneumonia is a respiratory disease that causes infection in both the upper respiratory tract and the lungs. It is considered one of the leading causes of infection-related deaths in children. Chest X-ray images have proven helpful in diagnosing pneumonia. It is essential for early diagnosis of pneumonia to control the spread of the disease and save the patient. Therefore, there is a need for deep learning artificial intelligent systems to assist clinicians in early and better diagnosis. In this study, Residual Neural Network (ResNet) and Swin Transformer are used to classify pneumonia and healthy chest X-ray images from the Chest X-Ray Images dataset. Experimental results show that the ResNet achieved a maximum accuracy of 99.00% in detecting pneumonia after ten epochs. Whereas the Swin transformer achieved a maximum accuracy of 98.46% in detecting pneumonia after ten epochs."
옷을 착용한 3D 인체 모델 텍스쳐 맵의 초해상화,2023,"['Texture map image super-resolution', 'clothed human texture map', '3D clothed human texture map super-resolution']","본 논문은 옷을 착용한 3D 인체 메쉬의 텍스쳐 맵에 특화된 초해상화 방법을 제안한다. 메쉬의 segmentation map 과 normal map 정보를 활용하는 transformer 기반의 네트워크는 3 차원 메쉬의 텍스쳐 맵 중, 옷의 질감과 같은 세부적인 부분을 더욱 세밀하게 초해상화 할 수 있다. 저해상도 3 차원 인체 메쉬의 텍스쳐 맵을 초해상화 하는 실험 결과, 본 논문에서 제안한 방법이 기존 방법보다 더 정확하게 텍스쳐 맵을 초해상화 할 수 있었다. 제안하는 방법을 활용하여 이미지나 동영상으로부터 복원된 3 차원 인체메쉬의 저해상도 텍스쳐를 개선할 수 있으며, 이는 다양한 응용에 사용될 수 있다.",다국어 초록 정보 없음
Designing Signal Peptides Using the Transformer Model for the Enhanced Secretion of Proteins,2023,"['Signal Peptide', 'Secretory Protein', 'Transformer', 'Machine Learning', 'Generative Model']",국문 초록 정보 없음,다국어 초록 정보 없음
DNA to Protein: A Transformer Model for Custom Transcription Factor Design in Microbial Systems,2023,"['Transcription Factors', 'Deep learning', 'De novo design']",국문 초록 정보 없음,다국어 초록 정보 없음
GPT를 이용한 Git의 커밋메시지 분류모델 제안,2023,"['커밋 메시지(Commit Message)', 'BERT(Bidirectional Encoder Representations from Transformers)', 'GPT(Generative Pre-trained Transformer)']",국문 초록 정보 없음,다국어 초록 정보 없음
Missing Data Imputation Using Transformer Model for Distribution System State Estimation,2023,"['Distribution System State Estimation', 'Machine Learning', 'Deep Learning', 'Convolutional Neural Networks', 'Multi-Layer Perceptron', 'Transformer model']",국문 초록 정보 없음,다국어 초록 정보 없음
인캐빈 환경 적용 목적의 경량화된 단안 깊이 추정 모델,2023,"['Monocular Depth Estimation', 'Transformer', 'Lightweight Deep Learning Model']",국문 초록 정보 없음,다국어 초록 정보 없음
도로건설공사 프로젝트의 지연 예측 및 관리 모델,2023,"['도로건설공사 프로젝트 관리', '지연 유형 예측', '지연 일수 예측', 'Generative Pretrained Transformer 3', 'Extreme Gradient Boost', 'Categorical Boost', 'Risk Probability-Impact Matrix']",국문 초록 정보 없음,다국어 초록 정보 없음
C3DSG: 실내 환경 포인트 클라우드를 이용한 3차원 장면 그래프 생성 모델,2023,"['scene understanding', 'point cloud', '3D scene graph generation', 'graph neural network', '장면 이해', '포인트 클라우드', '3차원 장면 그래프 생성', '그래프 신경망']",국문 초록 정보 없음,"To design an effective deep neural network model to generate 3D scene graphs from point clouds, the following three challenging issues need to be resolved: 1) to decide how to extract effective geometric features from point clouds, 2) to determine what non-geometric features are used complementarily for recognizing 3D spatial relationships between two objects, and 3) to decide which spatial reasoning mechanism is used. To address these challenging issues, we proposed a novel deep neural network model for generating 3D scene graphs from point clouds of indoor environments. The proposed model uses both geometric features of 3D point cloud extracted using Point Transformer and various non-geometric features such as linguistic features and relative comparison features that can help predict the 3D spatial relationship between objects. In addition, the proposed model uses a new NE-GAT graph neural network module that can apply attention to both object nodes and edges connecting them to effectively derive spatial context between objects. Conducting a variety of experiments using 3DSSG benchmark dataset, effectiveness and superiority of the proposed mode were proven."
스타일 전이 기술의 사용성 향상을 위한 attention map 기반 모델 최적화,2023,"['스타일 전이', '셀프 어텐션', '피드포워드 네트워크', '영상 처리', '딥러닝', 'Style Transfer', 'Self Attention', 'Feed-forward network', 'Image Processing', 'Deep Learning']",국문 초록 정보 없음,"Style transfer is one of deep learning-based image processing techniques that has been actively researched recently. These research efforts have led to significant improvements in the quality of result images. Style transfer is a technology that takes a content image and a style image as inputs and generates a transformed result image by applying the characteristics of the style image to the content image. It is becoming increasingly important in exploiting the diversity of digital content. To improve the usability of style transfer technology, ensuring stable performance is crucial. Recently, in the field of natural language processing, the concept of Transformers has been actively utilized. Attention maps, which forms the basis of Transformers, is also being actively applied and researched in the development of style transfer techniques. In this paper, we analyze the representative techniques SANet and AdaAttN and propose a novel attention map-based structure which can generate improved style transfer results. The results demonstrate that the proposed technique effectively preserves the structure of the content image while applying the characteristics of the style image."
디지털 병리 대장암 분화도 예측을 위한 순서학습 기반 비전 트랜스포머 기술,2023,[],국문 초록 정보 없음,"We propose a deep learning based digital pathology method that can classify colorectal cancers from digitized whole slide images. The conventional digital pathology methods approach cancer grading as a categorical classification problem, where the goal is to classify them into appropriate classes. However, in the case of cancer cells, the higher the grade or differentiation of each class, the poorer the condition of the cancer is, making simple categorical classification insufficient to address this issue. Therefore, in this paper, we formulate cancer grading as both categorical and ordinal classification problems and conduct two cancer grading tasks simultaneously. To achieve this, we build a deep learning model based on vision transformer and order learning. The proposed method is evaluated using a colorectal tissue dataset. Experimental results show that our method is able to accurately classify cancer grades and outperforms other competing models."
영상 데이터 감정 분류를 위한 멀티 모달 기반의 ViT 모델,2023,"['영상 콘텐츠(Video content)', '감정분류(emotion classification)', 'VGG(Visual Geometry Group)', 'ViT(Vision Transformer)']",국문 초록 정보 없음,다국어 초록 정보 없음
마스크된 복원에서 질병 진단까지: 안저 영상을 위한 비전 트랜스포머 접근법,2023,[],국문 초록 정보 없음,"In this paper, we introduce a pre-training method leveraging the capabilities of the Vision Transformer (ViT) for disease diagnosis in conventional Fundus images. Recognizing the need for effective representation learning in medical images, our method combines the Vision Transformer with a Masked Autoencoder to generate meaningful and pertinent image augmentations. During pre-training, the Masked Autoencoder produces an altered version of the original image, which serves as a positive pair. The Vision Transformer then employs contrastive learning techniques with this image pair to refine its weight parameters. Our experiments demonstrate that this dual-model approach harnesses the strengths of both the ViT and the Masked Autoencoder, resulting in robust and clinically relevant feature embeddings. Preliminary results suggest significant improvements in diagnostic accuracy, underscoring the potential of our methodology in enhancing automated disease diagnosis in fundus imaging."
CLES-BERT: 에세이 점수 예측을 위한 대조학습 기반 BERT 모델,2023,"['automated essay scoring', 'BERT', 'multi-task loss function', 'contrastive learning', 'sampling']",국문 초록 정보 없음,"Creativity is an important ability in the 4th industrial revolution so writing is one of educational tools to improve creativity. However, student’s essays have been mainly evaluated subjectively in schools. To address this problem, Automated Essay Evaluation(AES) plays an important role in objective evaluation in addition to reducing the time and effort of instructors. This paper presents a novel AES model in which contrastive learning-based loss function is added to BERT. Furthermore, for contrastive learning, positive and negative samples are selected based on mean embedding vectors per essay score. The experimental results show that the proposed Contrastive Learning Essay Scoring-Bidirectional Encoder Representations from Transformers(CLES-BERT) improved average accuracy up to 3%, compared to main AES models, in Automated Student Assessment Prize(ASAP) data set."
히트맵 이미지를 활용한 딥러닝 기반의 EPL 경기유형 분석: 비전 트랜스포머 알고리즘 적용,2023,"['image analysis', 'k-means algorithm', 'cluster analysis', 'direct type', 'build-up type', '이미지분석', 'k-means 알고리즘', '군집분석', 'Direct 유형', 'Build-up 유형']",국문 초록 정보 없음,"The purpose of this study is to pre-process soccer heat map image data using a vision transformer model and to classify soccer game types through cluster analysis. To achieve the purpose of the study, 760 heat map images and 35 match records of 380 matches in the 2022-2023 season of the English Premier League were selected as research data. As for the analysis method, after embedding was performed using the DINO-ViT16 model among vision transformer models, principal component analysis was performed for dimension reduction.Afterwards, the k-means algorithm was applied to cluster the game types, and at this time, the silhouette index was used to select the optimal number of clusters. The results of this study are as follows. First, the game type of soccer was classified into two clusters, and as a result of comparing the game records of each cluster, it was found that the number of passes and ball occupancy of cluster 2 was higher than that of cluster 1. Cluster 1 is the direct type, and cluster 2 is the build-up type. Second, as a result of detailed cluster analysis on cluster 1, it was classified into two clusters. As a result of comparing game records and heat maps for each cluster, cluster 1-1 was a defense-oriented unified attack pattern type, and cluster 1-2 was a defense-oriented various attack pattern. Third, as a result of detailed cluster analysis on cluster 2, it was classified into 3 clusters, and as a result of comparing the match records and heat maps for each cluster, it is determined that cluster 2-1 was a type with various attack patterns through high ball occupancy in the central area, cluster 2-2 is a type that dominates the game with very high ball occupancy and passes, and cluster 2-3 is a type that has a pattern of attack focused on a space behind the defense through high ball occupancy or a specific area."
Video Vision Transformer를 이용한 벼의 수확량 예측과 Self-attention 시각화,2023,"['다중 스펙트럼 영상', '수확량 예측', 'Self-attention', 'Tubelet embedding', 'Video Vision Transformer', 'Multi-spectral images', 'Self-attention', 'Tubelet embedding', 'Video Vision Transformer', 'Yield prediction']","정부와 농민단체는 매년 쌀을 얼마나 생산할 수 있을지 예측하는 문제에 많은 관심을 기울이고 있다. 하지만 해마다 변하는 기상이변과 다양한 병충해와 같은 변동요인들로 인하여 벼의 수확량을 정확하게 예측하는 것을 어렵게 만든다. 본 연구에서는 무인항공기에 탑재된 다중 스펙트럼 센서를 통해 벼의 생육기간 동안 이미지를 여러 번 수집하고, 딥러닝 알고리즘을 이용하여벼 수확량을 예측하였다. 다중 스펙트럼 이미지는 일정 간격을 두고 여러 번 촬영된 일종의 영상 데이터로 볼 수 있으며, 딥러닝 알고리즘 중에서 트랜스포머 구조를 영상 컴퓨터 비전에 적용한 Video Vision Transformer(ViViT) 모델을 사용하여 벼 수확량을 예측하였다. ViViT 모델은입력 영상을 일정한 크기로 분할한 패치(patch)들을 생성하는데, 이 패치의 크기를 다르게 설정하여 모델을 학습한 결과 작은 패치 크기를 사용할수록 예측력이 좋아지는 것으로 나타났다. 또한이미지 처리 분야에서 사용되어온 CNN(Convolutional Neural Network) 구조에서 영상을 입력으로받는 3D CNN 모델과 예측 성능을 비교해본 결과 작은 패치 크기를 사용한 ViViT 모델의 성능이 더 우수한 것으로 나타났다. ViViT 모델의 학습된 가중치 행렬을 heat map으로 시각화한 결과 8월 중후반에 촬영된 이미지가 수확량 예측에 중요하게 나타나 벼를 수확하기 약 두 달 전에수확량 예측이 가능할 것으로 보인다.","The government and farmers' organizations are paying much attention to the problem of predicting how much rice can be produced each year. However, it is difficult to accurately predict the yield of rice due to variable factors such as extreme climate change and various pests and diseases that change every year. In this study, images were collected several times during the growing season of rice through a multi-spectral sensor mounted on an unmanned aerial vehicle, and rice yield was predicted using a deep learning algorithm. Multispectral images can be viewed as a kind of image data taken several times at regular intervals, and rice yield was predicted using the Video Vision Transformer (ViViT) model, which applies the Transformer structure to image computer vision among deep learning algorithms. The ViViT model generates patches by dividing the input image into a certain size, and as a result of learning the model by setting the size of these patches differently, it was found that the smaller the patch size, the better the predictive power. In addition, as a result of comparing prediction performance with a 3D CNN model that receives an image as an input in a CNN (Convolutional Neural Network) structure used in the image processing field, it was found that the ViViT model using a small patch size performed better. As a result of visualizing the weight matrix of the ViViT model as a heat map, images taken in mid- to late August appear to be important in yield prediction, making it possible to predict yield about two months before rice harvest."
Vision Transformer를 활용한 비디오 분류 성능 향상을위한 Fine-tuning 신경망,2023,"['Fine-tuning', 'Transfer Learning', 'Video Classification', 'Vision Transformer', 'Non-Local', 'Attention']","본 논문은 Vision Transformer를 기반으로 하는 Video Classification의 성능을 개선하는 방법으로 fine-tuning를 적용한 신경망을 제안한다. 최근 딥러닝 기반 실시간 비디오 영상 분석의 필요성이 대두되고 있다. Image Classification에 사용되는 기존CNN 모델의 특징상 연속된 Frame에 대한 연관성을 분석하기 어렵다는 단점이 있다. 이와 같은 문제를 Attention 메커니즘이 적용된 Vistion Transformer와 Non-local 신경망 모델을 비교 분석하여 최적의 모델을 찾아 해결하고자 한다. 또한, 전이 학습 방법으로 fine-tuning의 다양한 방법을 적용하여 최적의 fine-tuning 신경망 모델을 제안한다. 실험은 UCF101 데이터셋으로 모델을 학습시킨 후, UTA-RLDD 데이터셋에 전이 학습 방법을 적용하여 모델의 성능을 검증하였다.","This paper proposes a neural network applying fine-tuning as a way to improve the performance of VideoClassification based on Vision Transformer. Recently, the need for real-time video image analysis based on deeplearning has emerged. Due to the characteristics of the existing CNN model used in Image Classification, it isdifficult to analyze the association of consecutive frames. We want to find and solve the optimal model bycomparing and analyzing the Vision Transformer and Non-local neural network models with the Attention mechanism.In addition, we propose an optimal fine-tuning neural network model by applying various methods of fine-tuningas a transfer learning method. The experiment trained the model with the UCF101 dataset and then verified theperformance of the model by applying a transfer learning method to the UTA-RLDD dataset."
Bird's Eye View Semantic Segmentation based on Improved Transformer for Automatic Annotation,2023,"['Transformer', 'Semantic Segmentation', 'High-Definition maps', 'Automatic Annotation']",국문 초록 정보 없음,"High-definition (HD) maps can provide precise road information that enables an autonomous driving system to effectively navigate a vehicle. Recent research has focused on leveraging semantic segmentation to achieve automatic annotation of HD maps. However, the existing methods suffer from low recognition accuracy in automatic driving scenarios, leading to inefficient annotation processes. In this paper, we propose a novel semantic segmentation method for automatic HD map annotation. Our approach introduces a new encoder, known as the convolutional transformer hybrid encoder, to enhance the model's feature extraction capabilities. Additionally, we propose a multi-level fusion module that enables the model to aggregate different levels of detail and semantic information. Furthermore, we present a novel decoupled boundary joint decoder to improve the model's ability to handle the boundary between categories. To evaluate our method, we conducted experiments using the Bird's Eye View point cloud images dataset and Cityscapes dataset. Comparative analysis against state-of-the-art methods demonstrates that our model achieves the highest performance. Specifically, our model achieves an mIoU of 56.26%, surpassing the results of SegFormer with an mIoU of 1.47%. This innovative promises to significantly enhance the efficiency of HD map automatic annotation."
Swin Transformer와 Sentinel-1 영상을 이용한 우리나라 저수지의 수체 탐지,2023,"['Agricultural reservoir', 'Waterbody detection', 'Sentinel-1', 'Swin Transformer', '농업용 저수지', '수체 탐지']",국문 초록 정보 없음,"In this study, we propose a method to monitor the surface area of agricultural reservoirs in South Korea using Sentinel-1 synthetic aperture radar images and the deep learning model, Swin Transformer. Utilizing the Google Earth Engine platform, datasets from 2017 to 2021 were constructed for seven agricultural reservoirs, categorized into 700 K-ton, 900 K-ton, and 1.5 M-ton capacities. For four of the reservoirs, a total of 1,283 images were used for model training through shuffling and 5-fold cross-validation techniques. Upon evaluation, the Swin Transformer Large model, configured with a window size of 12, demonstrated superior semantic segmentation performance, showing an average accuracy of 99.54% and a mean intersection over union (mIoU) of 95.15% for all folds. When the best-performing model was applied to the datasets of the remaining three reservoirs for validation, it achieved an accuracy of over 99% and mIoU of over 94% for all reservoirs. These results indicate that the Swin Transformer model can effectively monitor the surface area of agricultural reservoirs in South Korea."
DTD Transformer-MAF: 인접 시점의 다양한 맥락 정보를 강조한 우리나라 주요 수출 8개국의 장기 원화 환율 분포 추정,2023,"['Auto regressive model', 'Deep learning', 'MAF (Masked Autoregressive Flow)', 'Multivariate exchange rate forecasts', 'Normalizing flows', 'Transformer']",국문 초록 정보 없음,"The exchange rate fluctuates elastically depending on the domestic and international conditions and has a great impact on the domestic economy as well as the export and import of each country. Recently, the deterioration of Vietnam's economic situation, including the US-China conflict, has caused a major downturn in Korean exporters along with the depreciation of the Korean Won. Therefore, this study aims to help manage their exchange rate risks by predicting the trends of the exchange rates of 8 major exporting countries based on Transformer-MAF, which has shown the highest performance in multivariate exchange rate prediction studies. However, the Transformer-MAF only conveys overall contextual information to the model through an single linear embedding module, and fails to emphasize the contextual information important for predicting the fluctuation trend of each country. Therefore, we proposed a Dilated Temporal Dependency Embedding module to the Transformer-MAF to enable effective multivariate long-term prediction based on various dependency relationships at adjacent time points. The proposed embedding module consists of a high-dimensional linear projection layer that can extract comprehensive contextual information of each country, and successive randomized masking and dilated conv blocks that deliver the various contextual information from different neighboring time points in each forward propagation. Experimental results show that the proposed model has the smallest standard deviation and outperforms all comparison models with a 0.75987 CRPSsum (±0.00202), provinging that effective multivariate exchange rate forecasting is possible with only a small increase in computation."
LLM(Large Language Model) 속성과 성능 연관성 연구,2023,"['거대언어 모델', '트랜스포머', '셀프 어텐션', '전이학습', 'large language model', 'GPT', 'LLaMA', 'transformer', 'Self-attention', 'Transfer learning']",국문 초록 정보 없음,"OpenAI's ChatGPT showed remarkable improvements over existing technologies, and active research and development and utilization of generative AI began. Advances in computing power have led to the use of large-scale artificial intelligence language models (LLMs). This study presents methods to evaluate performance when building an LLM and factors to be considered throughout the entire process of building an LLM to increase performance. It includes the entire process of building a model by learning from raw data and fine-tuning the created backbone model. We present how to improve performance from considering the quantity and quality of learning data, model and parameter size, learning frequency, and fine tuning. When it comes to the English dataset, there have been achievements in improving performance while reducing the model size, while in the case of Korean, it is essential to build and disseminate an open source model that is small but has excellent performance and can be used without restrictions for research and development and services."
Temporal Fusion Transformer를 이용한 대형마트 판매량의 다단계 시계열 수요예측,2023,"['다단계 시계열 예측', '다계층 예측', '대형마트 판매량 예측', '시간적 융합 트랜스포머(TFT)', '부분 데이터풀링', '모델 평균화', 'Multi-Step Time Series Forecasting', 'Multi-level Forecasting', 'Sales Forecasting for Hypermarkets', 'Temporal Fusion Transformers (TFT)', 'Partial Data Pooling', 'Model Averaging']","수요예측은 모든 산업에서 사업 기획 및 운영 계획의 중요한 기초 자료로 사용된다. 본 논문에서는 수요예측 경진대회인 M5 Competition 데이터를 대상으로 Temporal Fusion Transformer(TFT) 모형을 적용하였고, 이 대회에서 우승한 DRFAM 기법과 정확도를 비교하였다. M5 Competition의 Walmart 데이터셋 중 CA_1 매장의 판매량 데이터를 대상으로 성능을 평가하였으며, 매장(store) 수준과 카테고리(category) 수준의 데이터풀(data pool)로 각각 TFT 모형을 학습한 후 예측값을 산술평균하는 방식을 사용하였다. 그 결과, 세 가지 수준의 데이터풀에 대해 직접적 예측모형(direct forecasting)과 재귀적 예측모형(recursive forecasting)으로 총 6개의 LightGBM 모형을 학습하여 산술평균으로 예측하는 DRFAM 기법보다 평균적으로 개선된 예측 정확도를 달성하였다. 이를 통해 TFT 모형이 자기-어텐션 구조를 사용하여 시계열에서 변수와 판매량 간의 관계를 충분히 학습하였음을 알 수 있었다. DRFAM 기법의 직접적 예측모형과 재귀적 예측모형이 28일 간의 예측을 위하여 28회 반복호출을 해야 하지만, TFT 모형은 다중 출력 구조이기 때문에 한번 모형 호출로 28개의 시계열 예측이 가능하다. 본 논문에서 제안한 TFT 기반의 예측모형은 보다 빠르고 정확한 시계열 예측을 제공하여 다양한 분야에 확대 적용할 수 있을 것으로 기대한다.","Demand forecast is used as basic data for business and operation planning in all industries. In this paper, the Temporal Fusion Transformer (TFT) architecture was applied to the data of the M5 Competition, a famous forecasting competition, and the accuracy of the TFT-based forecasting method was compared with that of the DRFAM method, that had won the competition. The performance was evaluated for the sales data of CA_1 store in the Walmart dataset of the M5 Competition. The TFT models were trained with two data pools at the store level and category level, respectively, and the final forecast was calculated by arithmetically averaging the prediction results of the two models. As a result, the TFT-based method obtained better forecasts than the DRFAM method, which trained six LightGBM models with direct forecasting and recursive forecasting for three levels of data pools and predicted with the arithmetic average of the six trained models. It was found that the TFT-based method had sufficiently learned the relationship between variables and sales volumes in the time-series using the self-attention structure of TFT. While the direct and recursive forecasting models of the DRFAM method require 28 repeated calls for 28 days of forecasting, the TFT-based method can obtain 28 time-series forecasts with a single model call because of its multi-output structure. The proposed TFT-based forecasting method is expected to be applicable to various fields by providing faster and more accurate time-series forecasts."
설비의 예지 보전을 위한 Transformer 기반의 전력 품질 예측 및 이상 상황 탐지,2023,"['predictive maintenance', 'transformer', 'long-term time-series forecasting', 'anomaly detection', '.']",국문 초록 정보 없음,"With the recent introduction of smart factories in earnest, the importance of predictive maintenance, which continuously monitors the state of health of machinery and takes necessary measures in advance, has been emphasized. The power quality is one of the important factors that affect the health status of facilities. If the quality of power is poor, many unnecessary costs are incurred. Therefore, it is necessary to predict this in advance and reduce the cost. In this paper, we propose a method that predicts the quality-related index value and detects the abnormal situation with the transformer-based long-term time series forecasting model. Here, we utilized DILATE loss function instead of MSE, due to the characteristics of the power quality time series data, the non-stationary, and the rapid change in value. As a result, the performance of both forecasting and detection is improved."
Transformer 기반 인물 재식별을 위한 상대적 위치 임베딩 활용방법,2023,"['Person re-identification', 'Transformer', 'relative positional embedding']",국문 초록 정보 없음,"In this letter, we propose a training scheme for a transformer-based person re-identification model using relative positional embeddings. To overcome the limitations of existing methods that rely on the visual information of an image, we define the topological and positional characteristics of a person's body structure through relative positional embeddings and uses them as an additional cue. In a set of experiment conducted for five popular person ReID benchmark datasets, the proposed scheme brings promising improvement."
Vision Transformer를 이용한 UAV 영상의 벼 도복 영역 진단,2023,"['벼 도복', '무인 항공기', '딥러닝', '시멘틱 세그멘테이션', 'rice lodging', 'unmanned aerial vehicle', 'deep learning', 'semantic segmentation', 'vision transformer']","쌀 수확량 감소에 크게 영향을 주는 것은 집중호우나 태풍에 의한 도복 피해이다. 도복 피해 면적 산정 방법은 직접 피해 지역을 방문하는 현장 조사를 기반으로 육안 검사 및 판단하여 객관적인 결과 획득이 어렵고 많은 시간과 비용이 요구된다. 본 논문에서는 무인 항공기로 촬영된 RGB 영상을 Vision Transformer 기반 Segformer을 활용한 벼 도복 영역 추정 및 진단을 제안한다. 제안된 방법은 도복, 정상, 그리고 배경 영역을 추정하고 종자관리요강 내 벼 포장 검사를 통해 도복률을 진단한다. 진단된 결과를 통해 벼 도복 피해 분포를 관찰할 수 있게 하며, 정부 보급종 포장 검사에 활용할 수 있다. 본 연구의 벼 도복 영역 추정 성능은 평균 정확도 98.33%와 mIoU 96.79%의 성능을 나타내었다.","The main factor affecting the decline in rice yield is damage caused by localized heavy rains or typhoons. The method of analyzing the rice lodging area is difficult to obtain objective results based on visual inspection and judgment based on field surveys visiting the affected area. it requires a lot of time and money. In this paper, we propose the method of estimation and diagnosis for rice lodging areas using a Vision Transformer-based Segformer for RGB images, which are captured by unmanned aerial vehicles. The proposed method estimates the lodging, normal, and background area using the Segformer model, and the lodging rate is diagnosed through the rice field inspection criteria in the seed industry Act. The diagnosis result can be used to find the distribution of the rice lodging areas, to show the trend of lodging, and to use the quality management of certified seed in government. The proposed method of rice lodging area estimation shows 98.33% of mean accuracy and 96.79% of mIoU."
Vision Transformer를 활용한 단일 영상 카메라 캘리브레이션,2023,"['Camera Calibration', 'Computer Vision', 'Deep Learning', '.']",국문 초록 정보 없음,"Camera calibration is the process of determining both the intrinsic parameters, such as focal length, principal point, and distortion coefficients, as well as the extrinsic parameters, such as the position and orientation of the camera. Accurately estimating the internal parameters is a crucial task in the field of computer vision, but traditional methods have limitations in terms of complexity and constraints. To overcome these limitations, research using deep learning for camera calibration with a single image has been proposed, but it suffers from reduced accuracy. In this paper, we propose a model that utilizes Vision Transformers by adding tokens to the features obtained using EfficientNetV2 to estimate the camera's internal parameters. We compare our approach with previous single-image calibration studies and confirm that our model achieves higher accuracy."
고해상도 드론 데이터에 이미지 슬라이싱과 Transformer 기법을 적용한 불법작물 객체 탐지 시스템 설계 및 구현,2023,"['객체 탐지', '이미지 슬라이싱', '인공지능', '딥러닝', '드론', 'object detection', 'image slicing', 'artificial intelligence', 'deep-learning', 'drone']",국문 초록 정보 없음,"In this paper, we studied how to apply slicing techniques to 4K and 8K images using high-resolution drone data, and used them to detect and de-identify personal information objects (car, people), and designed and implemented a system that detects and visualizes illegal crop objects in de-identified images.For privacy object detection and de-identification, single-stage techniques such as Yolov5 and Gaussian Blurring were applied, SwinTransformer, Soft-Teacher, and Fast-RCNN techniques were applied to detect illegal crop objects, and SAHI open source framework was used as image slicing techniques. The illegal crop object detection model used an ensemble Soft-Teacher model using SwinTransformer as a Backbone network and Fast-RCNN as a detector. Experiments by applying image slicing techniques to this model showed that the mAP was 0.663, which is improved from 0.456, which is the mAP of the model without applying the conventional image slicing techniques."
음악 작곡 AI의 확장성 문제 : LSTM과 Transformer의 Sequence 길이 제한에 따른 작품 구조 완결성의 변화,2023,"['음악 작곡 AI', 'LSTM과 Transformer', 'Sequence', 'AI in Music Composition', 'LSTM and Transformer', 'Sequence']","본 연구는 인공 신경망과 음악 창작의 융합에 대해 새로운 시각을 제시한다. 주요 연구 대상은 Long Short-Term Memory (LSTM)과 Transformer, 두 가지 주요한 인공 신경망 구조로, 이들이 시 퀀스 데이터 처리와 음악 작곡 AI의 확장성에 어떠한 영향을 미치는지에 대해 심도 있게 분석하 였다. 본 연구의 초점은 각 모델의 시퀀스 길이 제한이 음악 작품의 구조 완결성에 미치는 영향에 두어 있다. 이를 위해, 다양 한 시퀀스 길이를 가진 음악 데이터를 LSTM과 Transformer 모델에 학습시킨 후 각 모델의 학습 손실 추이와 학습 데이터에 대한 재현능력을 평가하였다. 그리고 이 러한 기술적 분석 결과를 바탕으로 각 모델이 생성한 음악 작품의 구조적 완결성을 Quantitative Evaluation방식으로 평가하였다. 이 과정에서 고려된 요소들은 화성 구 조, 박자, 멜로 디 라인의 유기성 등이다. 본 연구의 결과로, Transformer는 LSTM에 비해 긴 시퀀스 처리에 더욱 뛰어난 성능을 보였다. 이는 Transformer의 self-attention 메커니즘이 LSTM의 vanishing gradient 문제를 해결하며 더 긴 음 악 시퀀스를 효과적으로 학습하고 재현할 수 있게끔 만든다는 점에서 찾을 수 있었 다. 또한, 이러한 기능성 차이가 음악 작품의 구조 완결성에 영향을 미친다는 것도 확인하였다. 이를 통해 본 연구는 AI 기술을 통한 음악 작곡의 한계와 가능성에 대해 심층적으로 탐색하고 이해하는 데에 크게 기여하였다. 인공지능이 창작 분야에 어떤 방식으로 활용되고, 그 과정에서 어떤 한계와 가능성을 가지는지에 대한 이해는 기술 발전을 음악 창작이라는 인간의 영역으로 확장하는 데 중요한 역할을 수행한다.","This study presents a new perspective on the convergence of artificial neural networks and music creation. The main subjects of the study are two major artificial neural network structures, Long Short-Term Memory (LSTM) and Transformer, which are analyzed in depth how they affect sequence data processing and the scalability of music composition AI. The focus of this study is on the effect of the sequence length limit of each model on the structural completeness of the musical work. To this end, music data with various sequence lengths were learned in the LSTM and Transformer models, and then the learning loss trend and reproduction ability of each model were evaluated. In addition, based on the results of this technical analysis, the structural completeness of the musical works generated by each model was evaluated using the Quantitative Evaluation method. Factors considered in this process include harmony structure, beat, and organicity of the melody line. As a result of this study, Transformer showed better performance in long sequence processing than LSTM. This was found in that Transformer's self-attention mechanism solves LSTM's vanishing gradient problem and allows it to effectively learn and reproduce longer musical sequences. It was also confirmed that these differences in functionality affect the structural completeness of music works. Through this, this study greatly contributed to in-depth exploration and understanding of the limitations and possibilities of music composition through AI technology. Understanding how artificial intelligence is used in the field of creation and what limitations and possibilities it has in the process plays an important role in expanding technological development into the human realm of music creation."
"Power conversion system integrating OBC and LDC using tapped transformers for weight, volume, and cost reductions in electric vehicles",2023,"['Cost model analysis', 'Electric vehicles charger', 'Low voltage dc-to-dc converter', 'On-board charger']",국문 초록 정보 없음,"Electric vehicle (EV) on-board chargers (OBC) use high-frequency transformers for isolation between the grid power and the propulsion battery. Additionally, low voltage dc-to-dc converters (LDC) require high-frequency transformers for high step-down and electrical isolation. Although a high-frequency transformer is used for isolation, it is desirable to minimize the use of magnetic materials because the isolation breakdown between the windings of the transformer occurs due to vibration. It also increases the weight and volume, and reduces the mileage of vehicles. Therefore, this paper proposes an OBC–LDCintegrated system without additional windings by dividing the transformer windings used for the OBC. Depending on the connection state of the selective switch in the proposed system, it is divided into a propulsion battery charging mode and an auxiliary battery charging mode. At the same time, the selective switch converts the required transformer turns ratio in the changed operation mode. The proposed OBC–LDC integrated system is verified through prototype experiments. In addition, the volume and weight of a conventional vehicle charger and the proposed integrated charger are compared, and the economic benefits of the proposed charger are derived through cost model analysis (CMA)."
Comparative Analysis of Swin Transformer and Residual Neural Network for Pneumonia Classification,2023,"['Swin Transformer', 'ResNet', '딥러닝 모델', '폐렴 감지', '흉부 엑스레이', 'Swin Transformer', 'ResNet', 'Deep learning models', 'Pneumonia detection', 'Chest X-ray']",국문 초록 정보 없음,"PPneumonia is a respiratory infectious disease that causes fluids to fill the lungs. It is considered one of the leading causes of infection-related deaths in children and seniors worldwide. Clinicians usually use chest X-ray images to diagnose pneumonia. However, pneumonia is prone to be misdiagnosed because it overlaps with cold and flu, causing severe and critical medical complications. Consequently, alternative supportive diagnostic methods are needed to minimize human errors and assist clinicians. Several attempts have used artificial intelligence systems, mainly in deep learning methods, to assist clinicians in early pneumonia diagnoses. However, further studies are required to consolidate the use of deep learning as an assistant tool to diagnose pneumonia accurately. In this study, we examine the Swin Transformer and the Residual Neural Network’s performance in classifying pneumonia and healthy chest X-ray images using the Guangzhou Women and Children’s Medical Center dataset and the COVID19, Pneumonia and Normal Chest X-ray Posteroanterior dataset. The experiment results demonstrate that the Swin Transformer achieves an accuracy of 98.9% in the Chest X-ray images dataset and 92.35% in the COVID19, Pneumonia and Normal Chest X-ray Posteroanterior dataset, while the Residual Neural Network achieves an accuracy of 97.9% and 88.8% respectively in classifying pneumonia. These results indicate that the Swin Transformer outperforms the Residual Neural Network as a tool for assisting clinicians in diagnosing pneumonia. Thus, the Swin Transformer may help in early decision-making, leading to treatment initiation and improving patient's health."
High‑frequency planar transformer optimal design and analysis,2023,"['DC–DC converter', 'Matrix transformer', 'Loss model', 'Winding structure optimization']",국문 초록 정보 없음,"Due to the rapid development of modern industry, there is an increasing demand for isolated switching power supplies (SMPS) with a high efficiency and a high power density. Magnetic components, which occupy most of the converter volume, have been a key bottleneck in achieving high power density. This paper presents an optimal design method for high power density planar transformers. Applying the principle of flux cancellation, a matrix transformer that originally needed an independent magnetic core was integrated into a single magnetic core to reduce the volume of the core. Meanwhile, a loss model of the transformer is established, and an optimized design method of the winding structure is given according to that model to further reduce the winding loss of the transformer. Finally, an LLC converter with a resonant frequency of 1 MHz is designed to verify the practical performance of the proposed transformer."
Integration of High Leakage Inductance Transformers Utilizing Genetically Optimized Curved Foil Windings,2023,"['3d-printing', 'transformer', 'magnetic shunt', 'shaped foil windings']",국문 초록 정보 없음,"In this paper, a transformer design with a magnetic shunt is proposed to achieve an efficient transformer with a precisely defined stray inductance. Highly compact transformer with a precise stray inductance are needed in highly compact dual-active-bridge and resonant converter topologies. To avoid additional eddy currents induced by the inserted shunt, 3D printed bobbins are utilized to shape the stray flux and consequently omit additional eddy currents in the windings. The transformer is simulated with a FEM-Tool and additional loss calculations. The simulation results are used as input parameters for a genetic optimizer to find suitable core and copper geometries. A design is selected from the optimization and a prototype of the transformer is realized. Finally, measurement results are presented to verify the simulations. In comparison to a non-optimized design, the optimized copper windings can reduce the AC-short-circuit resistance by 5.92 %."
A Comparative Study of LSTM and Transformer Models in Music Melody Generation,2023,"['AI Composition', 'Music Generation', 'Deep Learning', 'Transformer', 'AI 作曲', '音乐生成', '深度学习']",국문 초록 정보 없음,【背景】近年来，利用深度学习模型生成音乐已经发展为AI 音乐的主流方向，但音乐生成任务的主流模型仍然存在着一些问题，其中最大的问题就是不能有效地模拟音乐结构，使计算机创作出符合音乐结构的乐曲。【目的】为此，我们需要探究哪些模型能够很好地模拟音乐的结构，创造更加人性化的音乐。【方法】我们通过对比实验，对比LSTM 与Transformer 模型所生成音乐的优点与缺点，并在此基础上提出改进方案。【结果】实验结果证明，LSTM 在较短的序列上模拟音乐结构的表现优于Transformer，但其无法处理过长的序列；而Transformer 在处理较长序列的表现优于LSTM，并通过改进后能在较长的序列上有效地模拟音乐结构，创作出符合人类音乐听觉的乐曲。【结论】因此我们认为，Transformer 模型更加适合AI 音乐作曲任务，并在未来通过改进其注意力机制来提高音乐结构的识别能力是音乐生成的主流方向。
Small-Signal Modeling and Control of three-phase Hybrid Transformer Considering Practical Impedances,2023,"['Small-signal modeling', 'practical impedances', 'three-phase hybrid transformer']",국문 초록 정보 없음,"This paper presents the small-signal models of a three-phase transformer. The derived small-signal models reflect the circuit’s practical impedances caused by the distribution and the low frequency magnetic transformers which are parts of the hybrid transformer. To simplify the model derivations, the impedances are incorporated in the LC filters which are attached to the input and the output of the back-to-back power converter. The state equations of the hybrid transformer are established, and the small-signal models are derived. Furthermore, the simplified models are suggested and compared with the originally derived small-signal models. Using the derived simplified models, the secondary constant voltage controller of the power stage is designed. The simulation and experimental results verify the effectiveness of the derived small-signal models and the controller design method."
Model-Based Dynamic Control of Two Degrees-Of-Freedom Modulation for Dual Active Half-Bridge Converter,2023,"['Dual active half-bridge converter (DAHB)', 'minimum rms current', 'modulation scheme', 'model-based control']",국문 초록 정보 없음,"This paper proposes a model-based dynamic control for dual active half-bridge converters to satisfy both a loss-minimizing operation and a high-dynamic response. First, the loss-minimizing operation is analyzed based on a two degrees-of-freedom (DOF) modulation scheme, where two variables, a phase shift and a duty ratio, are determined to minimize a transformer rms current. The 2-DOF modulation could be changed to the 1-DOF modulation according to operating conditions, where a smooth transition between the modulation schemes is required. Moreover, the output voltage control should be designed to provide fast dynamic performance under source and load variations. Therefore, the voltage controller is unified with the modulation schemes where a model-based control is adopted to lessen the burden of an error compensation part. A model-based calculation part improves the dynamics of the 2-DOF modulation with a load current feedforward. Lastly, the effectiveness of the proposed method is verified by the simulation and experimental results."
Transformer Design Considering Fringing Effect for High Frequency Application,2023,"['dc-dc converter', 'LLC resonant converter', 'high frequency transformer (HFT)', 'shell type transformer', 'magnetic equivalent model']",국문 초록 정보 없음,"This paper describes the research conducted on the design method of HFT (High Frequency Transformer) in high frequency application. The proposed design method accurately predicts inductance based on magnetic equivalent model considering MPL (Magnetic Path Length) and Fringing effect. Here, the multi-objective optimization technique is applied to optimize the HFT design. The effectiveness of the proposed method is verified through an experiment of a 500W LLC resonant converter."
SPPT: Siamese Pyramid Pooling Transformer for Visual Object Tracking,2023,"['Visual Transformer Tracking', 'Pyramid Pooling Attention', 'Feature Extraction and Correlation', 'Enhanced Correlation Block']",국문 초록 정보 없음,"Recently, visual transformer-based tracking has achieved significant success owing to its effective attention modeling strategies and global context feature extraction. However, most transformer trackers are based on the canonical Siamese and correlation-based tracking paradigm, which comprises three stages: feature extraction, feature fusion, and similarity function learning. This paradigm is speculated to weaken the cross-correlation between the template and search features while increasing the computational cost of the tracking model. Hence, we propose a Siamese pyramid pooling transformer (SPPT) to implement a one-stream end-to-end visual object tracking framework with two newly proposed modules: an iterative pooling attention-based feature extraction and correlation (P-FEC) module and an iterative enhanced correlation block (ECB). The P-FEC module can simultaneously perform feature extraction and correlation, whereas the ECB can enhance feature integration and target-aware feature embedding learning. The SPPT has a much shorter attention sequence length, fewer parameters, and fewer floating-point operations per second (FLOPs) than existing transformer-based trackers. Extensive experiments on the LaSOT, TrackingNet, and GOT-10k benchmarks demonstrate that our proposed SPPT tracker achieves state-of-the-art tracking performance in terms of precision and success scores, as compared with most convolutional neural network-based and transformer-based trackers."
Uncover This Tech Term: Foundation Model,2023,"['Foundation model', 'Artificial intelligence', 'Transformer', 'Large language model', 'ChatGPT', 'Representation', 'Few shot', 'Zero shot']",국문 초록 정보 없음,다국어 초록 정보 없음
Deep-Learning-Based Precipitation Nowcasting Using Memory-Efficient Bidirectional Transformers,2023,"['Deep learning model', 'Generative model', 'Transformers', 'Radar dataset', 'Precipitation nowcasting']",국문 초록 정보 없음,다국어 초록 정보 없음
A Knotted-Ribbon Model for Estimation of Anisotropic Thermal Conductivities of Windings with Rectangular Wires,2023,"['Equivalent thermal conductivity', 'rectangular wires', 'thermal modeling', 'windings']",국문 초록 정보 없음,"Overheating of the windings is one of the major concerns regarding the reliability of motors, generators and transformers. However, thermal modeling of windings is challenging due to the fine and heterogeneous structures. Without adequately addressing the microscopic heat flux field in windings, the commonly used models, even those with conductor insulation accounted for, may give divergent estimations for the equivalent thermal conductivity of windings. Considering the negligible thermal resistance of conductors, a knotted-ribbon model based on the perfect-conductor assumption is proposed for windings formed with rectangular wires, which enables us to derive a closed-form solution to the heat flux field in the impregnation between rectangular wires. From this heat flux solution, the effective thermal resistance of the impregnation, which constitutes the vast majority of that of the whole winding, can then be calculated, based on which a homogenization model is obtained for the anisotropic thermal conductivity of the winding."
High-bandwidth Control Structure for Solid-State-Transformers with EtherCAT Protocol,2023,"['solid state transformer', 'EtherCAT', 'high-voltage isolation', 'controller']",국문 초록 정보 없음,"This paper proposes a high-bandwidth control structure for solid-state-transformers (SSTs). To fabricate a medium-voltage (MV, 25 kVrms) SST, a 42-level AC/DC rectifier and forty-two dual-active-bridge (DAB) converters are required. However, it is challenging to simultaneously control these converters due to synchronization, propagation delay, and complexity. In this paper, an optimized structure for MV high-level SST controller is proposed. An industrial EtherCAT master PC and state-of-the-art micro-control-units (MCUs) TMS320F28388D are employed to fabricate the control environment. Using the serial-peripheral-interface (SPI) and EtherCAT protocols, a 10 kHz bandwidth data communication between controllers was achieved within a 100 𝝁s delay. Based on the fast communication environment, a partially decentralized voltage balance control scheme, which reduces communication data size, is presented. The proposed controller is evaluated with a PLECS simulation model of the 42-level SST system."
Deep-Learning-Based Precipitation Nowcasting Using Memory-Efficient Bidirectional Transformers,2023,"['Deep learning model', 'Generative model', 'Transformers', 'Radar dataset', 'Precipitation nowcasting']",국문 초록 정보 없음,다국어 초록 정보 없음
BERT-Based Logits Ensemble Model for Gender Bias and Hate Speech Detection,2023,"['BERT Embedding Model', 'Gender Bias', 'Hate Speech Detection', 'Logistics Ensemble']",국문 초록 정보 없음,"Malicious hate speech and gender bias comments are common in online communities, causing social problemsin our society. Gender bias and hate speech detection has been investigated. However, it is difficult becausethere are diverse ways to express them in words. To solve this problem, we attempted to detect malicious commentsin a Korean hate speech dataset constructed in 2020. We explored bidirectional encoder representationsfrom transformers (BERT)-based deep learning models utilizing hyperparameter tuning, data sampling, andlogits ensembles with a label distribution. We evaluated our model in Kaggle competitions for gender bias,general bias, and hate speech detection. For gender bias detection, an F1-score of 0.7711 was achieved usingan ensemble of the Soongsil-BERT and KcELECTRA models. The general bias task included the gender biastask, and the ensemble model achieved the best F1-score of 0.7166."
ToMato: Token Merging을 이용한 Vision Transformer 가속화,2023,[],국문 초록 정보 없음,"ViT(Vision Transformer) shows outstanding performance in various vision tasks by splitting images into patches and passing them through transformer blocks. However, the large model size and computational cost of ViT result in high inference latency and hindered acceleration. To accelerate ViT efficiently, we introduce ToMato(Token Merging at Once), a simple framework that recursively merges tokens by comparing similarity to adjacent tokens at the first transformer block. Applying the ToMato to DeiTbase model, we find that this reduces latency by 22.19% while maintaining high Top-1 accuracy of 80.14%. Our codes are available at https://github.com/Transformer04/ToMato."
RNN-Based Main Transformer OLTC Control for SMR Integration into a High Renewable Energy Penetrated Grid,2023,"['Voltage stability', 'Small modular reactor', 'Renewable energy', 'Recurrent neural networks', 'On-load tap-changer']",국문 초록 정보 없음,"Voltage control is desirable for voltage stability of an electrical power system. Isolated high renewable energy grids have become a common phenomenon in the wake of the global shift to clean renewable energy sources (RES) fronted to achieve carbon neutrality. The most dominant RES are solar photovoltaic and wind energy which rely on switched power-electronic devices to cope with grid reactive power requirements. These devices often fall out of step with other synchronous generators due to the mismatch in synchronizing speed whenever a system disturbance occurs. Previous studies propose that voltage and frequency instability can be mitigated by the integration of a small modular reactor (SMR) in the RES grid. SMRs have load-following capability, reactive power compensation and supplementary voltage regulation capability; which is achieved using the main transformer’s on-load tap changer (OLTC). This paper proposes application of advanced machine learning-based recurrent neural networks (RNN) for the SMR main transformer OLTC control by leveraging on multiple parameters to predict the appropriate percentage tap values. The RNN model is built and tested using simulation data obtained from load flow analysis using the electrical transient and analysis program."
ProbAttnGuard: Reinforcing Transformers with Robust Stochastic Attention,2023,[],국문 초록 정보 없음,"Recent advances in Transformer-based models have shown remarkable performance in text classification tasks. However, these models can be vulnerable to out-of-distribution (OOD) and adversarial attack scenarios. In this paper, we propose a robust Transformer model that leverages stochastic attention to improve its resilience against such challenges. By sampling attention weights from probability distributions during training, our model achieves better test accuracy on both in-distribution and OOD datasets, as well as adversarial attack datasets, compared to deterministic vanilla models. We conduct experiments using various text classification tasks, including sentiment analysis and news article classification. Our results demonstrate that the proposed model exhibits improved performance and reduced performance gap between in-distribution and OOD or adversarial attack datasets. In future work, we aim to investigate the effect of contextual priors and develop more advanced models for enhanced robustness."
Improving Global and Local Feature Extraction with Swin Transformer on Monocular Depth Estimation,2023,"['GLPN', 'Swin Transformer', 'monocular depth estimation']",국문 초록 정보 없음,"Global-Local Path Network is a monocular depth estimation network. It presents a new method for integrating global features from an encoder and local features from a decoder through a Selective Feature Fusion module. In this paper, we propose that replacing the SegFormer encoder with the Swin Transformer leads to an improved GLPN, called Swin Transformer-Global-Local-Path-Network. We train the network with modified NYU Depth V2 datasets. Therefore, with the 0.034 RMSE, 0.075 AbsRel, 0.033 log10, 0.951 Delta 1, 0.994 Delta 2, 0.999 Delta 3, our network using a tiny version of Swin Transformer outperforms the previous GLPN model."
The DC terminal dynamic model of resonant converter,2023,"['resonant converter', 'DC terminal dynamic model', 'solid state transformers', 'DC-DC converters']",국문 초록 정보 없음,"Resonant DC-DC converters are widely used as power electronic transformers, and their operating characteristics have received much attention in order to better study their dynamic processes. This paper analyzes the steady-state and transient-state processes of resonant converters, presents a DC terminal dynamic model of resonant DC-DC converters. Then, using a clarify and rigorous way derives it, which compensates for the shortage of past studies. Finally, verifying the derivation of the paper by simulation."
Challenges in Implementing Vision Transformer as a Detection Transformer,2023,"['Object detection', 'Detection transformer', 'Computer Vision']",국문 초록 정보 없음,"In recent object detection research, there has been a growing focus on Detection Transformers predicting bounding boxes directly. However, Detection Transformers face challenges such as slow convergence and difficulty in detecting small objects. We attribute these issues to the insufficient feature extraction capability of the backbone. Therefore, we employ the high-performing backbone, the Pyramid Pooling Transformer to detection Transformer. However, we observe a problem where, despite rapid initial convergence, the model fails to converge effectively after a certain point in training. We discuss the underlying causes of this issue in this study."
McSimA+ 시뮬레이터를 사용한 Vision Transformer 추론 과정의 레이어 별 Memory Bottleneck 분석,2023,[],국문 초록 정보 없음,"As deep learning models continue to grow in scale, the number of parameters in these models has increased, causing a significant memory bottleneck in conventional von Neumann architecture-based systems. To address this issue, a new memory technology such as Processing-In-Memory (PIM) is being developed, and its importance is also steadily being emphasized. However, since PIM designs additional logic to the existing memory structure, an in-depth analysis of the workload suitable for PIM is required in advance to prevent unnecessary overhead in the design process. In this paper, in order to verify the suitability of the recently popular Vision in Transformer (ViT) model for PIM, we build a deep learning model analysis environment using McSimA+ simulator and analyze the memory bottleneck of the ViT inference workload by layer. The analysis results show that the ViT is a very memory-intensive workload because Last-to-First Miss Ratio (LFMR) and Last Level Cache Miss Per Kilo Instruction (LLC MPKI) of the ViT, which are composed of embedding, multi-head self attention, and multi-layer perceptron layers, are 88.64 and 45.31, respectively, on average. As a result, the ViT is an appropriate workload to achieve significant system acceleration and power savings through PIM systems, unlike computationally intensive convolution neural networks (CNNs)."
Dynamic Tracking Aggregation with Transformers for RGB-T Tracking,2023,"['Cross-modal Fusion', 'Dynamic Tracking Aggregation', 'RGB-T Tracking', 'Transformers']",국문 초록 정보 없음,"RGB-thermal (RGB-T) tracking using unmanned aerial vehicles (UAVs) involves challenges with regards tothe similarity of objects, occlusion, fast motion, and motion blur, among other issues. In this study, we proposedynamic tracking aggregation (DTA) as a unified framework to perform object detection and data association.The proposed approach obtains fused features based a transformer model and an L1-norm strategy. To link thecurrent frame with recent information, a dynamically updated embedding called dynamic tracking identification(DTID) is used to model the iterative tracking process. For object association, we designed a long short-termtracking aggregation module for dynamic feature propagation to match spatial and temporal embeddings. DTAachieved a highly competitive performance in an experimental evaluation on public benchmark datasets."
Chained Mean-Teacher를 이용한 Vision Transformer 객체 검출 연구,2023,[],국문 초록 정보 없음,"Recent advancements in deep-learning-based training methods have focused on enhancing object detection performance. These advancements include techniques aimed at accelerating learning and improving the efficiency of the training process, along with efforts to enhance the network model itself. Vsion Transfomers like Swin Transformer and Vit-Adapter have gained significant attention. Recently, we have reported performance improvements by simplifying the Soft Teacher technique and employing successively multiple mean teacher models with multiple pipelines with the splitted dataset as inputs. In this study, we aim to validate the enhancement of object detection performance using the various Vision Transformer network models using the Chained Mean Teacher (CMT) technique."
다양한 뉴스데이터를 이용한 자연어 처리모델 성능 비교,2023,"['performance comparison', 'NLP', 'fake news classification', 'dataset analysis']",국문 초록 정보 없음,"Natural Language Processing is one of the fields that attracts a lot of attention in deep learning, and with the introduction of transformer-based GPT[1] and BERT[2], it is showing tremendous performance improvement. In this paper, we compared and analyzed the performance of word embedding, neural network, and pre-trained language model, dependent othe model and data type of the news. ISOT, Kaggle, and Politifact datasets were used for fake news dataset as a result, BERT showed best performance in this study, however in Politifact Dataset, it showed relatively poor performance. We analyzed the structure of dataset and from the model perspectives to find out the reason why the performance differences were occurred."
A Feature Enhanced Transformer for Skin Lesion Segmentation,2023,"['skin lesion segmentation', 'transformer', 'attention mechanism']",국문 초록 정보 없음,"Automatic skin lesion segmentation in terms of skin lesion analysis is very important. However, it is still a challenging task due to the irregular shapes of the skin lesion. Traditional CNN-based methods usually cannot achieve a satisfactory segmentation performance. We present a novel network with a feature enhanced Transformer for skin lesion segmentation. Unlike earlier CNN-based U-net models, our model utilizes Transformer blocks to capture global and local features, improving the performance of medical image segmentation. By incorporating feature enhancement module at every skip connection layer, we substantially enhance feature fusion capabilities and improve the efficiency of the encoderdecoder structure. In the FEM, a squeeze and excitation attention module is introduced to enhance important feature and suppress unnecessary information. The experimental results show that our proposed model demonstrated the effectiveness on PH2 dataset."
Design and Simulation Analysis of Intercell Transformer based on Five-level T-type Inverter,2023,"['Five-level T-type Inverter', 'Intercell Transformer', 'Coupled Inductor', 'Maxwell']",국문 초록 정보 없음,"To improve the power density of the five-level T-type inverter, a three-phase intercell transformer (ICT) is proposed. The three-phase ICT is the integration of three separate coupled inductors of the proposed inverter on a single core. The ICT created by the coupled inductor often does not alter the output current but does lessen the current ripple in the windings. It also reduces losses, and with the smaller size, the power factor of the ICTs can be much higher than that of the inductor. The paper analyzes the structure of the six coil core, and an equivalent model of the magnetic circuit is established. The method was optimally analyzed by coupled simulation of Maxwell and Simplorer. Furthermore, this result is further verified in the PSIM simulation."
Developing prompts from large language model for extracting clinical information from pathology and ultrasound reports in breast cancer,2023,"['Automatic data processing', 'Artificial intelligence', 'Natural language processing', 'Breast cancer', 'Clinical reports']",국문 초록 정보 없음,"Purpose: We aimed to evaluate the time and cost of developing prompts using large language model (LLM), tailored to extract clinical factors in breast cancer patients and their accuracy.Materials and Methods: We collected data from reports of surgical pathology and ultrasound from breast cancer patients who underwent radiotherapy from 2020 to 2022. We extracted the information using the Generative Pre-trained Transformer (GPT) for Sheets and Docs extension plugin and termed this the “LLM” method. The time and cost of developing the prompts with LLM methods were assessed and compared with those spent on collecting information with “full manual” and “LLM-assisted manual” methods. To assess accuracy, 340 patients were randomly selected, and the extracted information by LLM method were compared with those collected by “full manual” method.Results: Data from 2,931 patients were collected. We developed 12 prompts for Extract function and 12 for Format function to extract and standardize the information. The overall accuracy was 87.7%. For lymphovascular invasion, it was 98.2%. Developing and processing the prompts took 3.5 hours and 15 minutes, respectively. Utilizing the ChatGPT application programming interface cost US $65.8 and when factoring in the estimated wage, the total cost was US $95.4. In an estimated comparison, “LLM-assisted manual” and “LLM” methods were time- and cost-efficient compared to the “full manual” method.Conclusion: Developing and facilitating prompts for LLM to derive clinical factors was efficient to extract crucial information from huge medical records. This study demonstrated the potential of the application of natural language processing using LLM model in breast cancer patients. Prompts from the current study can be re-used for other research to collect clinical information."
Generating Radiology Reports via Multi-feature Optimization Transformer,2023,"['Attention mechanism', 'Feature fusion', 'Radiology report', 'Transformer', 'Image-text generation']",국문 초록 정보 없음,"As an important research direction of the application of computer science in the medical field, the automatic generation technology of radiology report has attracted wide attention in the academic community. Because the proportion of normal regions in radiology images is much larger than that of abnormal regions, words describing diseases are often masked by other words, resulting in significant feature loss during the calculation process, which affects the quality of generated reports. In addition, the huge difference between visual features and semantic features causes traditional multi-modal fusion method to fail to generate long narrative structures consisting of multiple sentences, which are required for medical reports. To address these challenges, we propose a multi-feature optimization Transformer (MFOT) for generating radiology reports. In detail, a multi-dimensional mapping attention (MDMA) module is designed to encode the visual grid features from different dimensions to reduce the loss of primary features in the encoding process; a feature pre-fusion (FP) module is constructed to enhance the interaction ability between multi-modal features, so as to generate a reasonably structured radiology report; a detail enhanced attention (DEA) module is proposed to enhance the extraction and utilization of key features and reduce the loss of key features. In conclusion, we evaluate the performance of our proposed model against prevailing mainstream models by utilizing widely-recognized radiology report datasets, namely IU X-Ray and MIMIC-CXR. The experimental outcomes demonstrate that our model achieves SOTA performance on both datasets, compared with the base model, the average improvement of six key indicators is 19.9% and 18.0% respectively. These findings substantiate the efficacy of our model in the domain of automated radiology report generation."
Robust Sentiment Classification of Metaverse Services Using a Pre-trained Language Model with Soft Voting,2023,"['BERT', 'metaverse', 'natural language processing', 'pre-trained language model', 'ubiquitous computing']",국문 초록 정보 없음,"Metaverse services generate text data, data of ubiquitous computing, in real-time to analyze user emotions. Analysis of user emotions is an important task in metaverse services. This study aims to classify user sentiments using deep learning and pre-trained language models based on the transformer structure. Previous studies collected data from a single platform, whereas the current study incorporated the review data as “Metaverse” keyword from the YouTube and Google Play Store platforms for general utilization. As a result, the Bidirectional Encoder Representations from Transformers (BERT) and Robustly optimized BERT approach (RoBERTa) models using the soft voting mechanism achieved a highest accuracy of 88.57%. In addition, the area under the curve (AUC) score of the ensemble model comprising RoBERTa, BERT, and A Lite BERT (ALBERT) was 0.9458. The results demonstrate that the ensemble combined with the RoBERTa model exhibits good performance. Therefore, the RoBERTa model can be applied on platforms that provide metaverse services. The findings contribute to the advancement of natural language processing techniques in metaverse services, which are increasingly important in digital platforms and virtual environments. Overall, this study provides empirical evidence that sentiment analysis using deep learning and pre-trained language models is a promising approach to improving user experiences in metaverse services."
토픽모델링을 활용한 헬스리터러시 관련 국제 연구동향 분석,2023,"['health literacy', 'research trend', 'topic modeling', 'global research']",국문 초록 정보 없음,"Objectives: This study aimed to identify global trends and themes in health literacy research through topic modeling of the comprehensive literature, thereby suggesting future directions for health literacy research. Methods: Research papers on health literacy published from 2012 to 2021 were analyzed using a text mining technique called Bidirectional Encoder Representations from Transformers was applied for topic modeling. Results: Of the 12,842 retrieved papers on health literacy, the highest proportion was based on the themes of ‘mental health’ (1,161 papers, 9.04%) and ‘cancer prevention and screening’ (1,146 papers, 8.92%). These were followed by the themes of ‘digital healthcare’ (952 papers, 7.41%), ‘health information education’ (826 papers, 6.43%), and ‘older people’ (801 papers, 6.24%). The number of research papers on health literacy has increased constantly across all topics. Particularly, the topics of ‘infection prevention’ and ‘digital healthcare’ have grown rapidly since 2019. Conclusion: This study showed that health literacy research has expanded from healthcare at the individual level to disease prevention and health promotion at the population level throughout the life course. Future studies should develop a health literacy scale for national monitoring indices as well as optimal communication strategies for disease prevention and health promotion within low health literacy groups."
Transformer-Based Language Models as Psycholinguistic Subjects: Focusing on Understanding Metaphor,2023,"['metaphor', 'transformer language models', 'chatGPT', 'surprisal value', 'sensicality']",국문 초록 정보 없음,"Metaphor is a fundamental aspect of human language and cognition, playing a crucial role in communication, comprehension, and creative expression. In light of the recent advancements demonstrated by prominent language models, a pivotal question arises: Can these expansive language models effectively discern metaphorical knowledge? The primary objective involves comparing the surprisal values estimated from neural network language models like autoregressive and bidirectional language models to the reaction times of human when exposed to both metaphorical and literal sentences. Our secondary objective involves assessing the AI's comprehension of metaphors by utilizing the sensicality ratings generated by sophisticated ChatGPT. To achieve this, we used psycholinguistic methods, and adopted the experimental materials from Lai, Currana, and Menna (2009). We found the surprisal values estimated from the autoregressive language model demonstrate metaphor processing that closely resembles that of native speakers. Furthermore, ChatGPT's processing of conventional metaphorical sentences closely resembles its approach to literal sentences, mirroring the convergence observed in native speakers' ERP response to conventional metaphorical sentences and their alignment with that of literal sentences."
Vision-based Structural Displacement Measurement using Transforming Model Prediction,2023,"['Structural Displacement Measurement', 'Transformer', 'Deep learning']",국문 초록 정보 없음,다국어 초록 정보 없음
Exploring the feasibility of fine-tuning large-scale speech recognition models for domain-specific applications: A case study on Whisper model and KsponSpeech dataset,2023,"['automatic speech recognition', 'deep learning', 'Transformers', 'Whisper']",국문 초록 정보 없음,"This study investigates the fine-tuning of large-scale Automatic Speech Recognition (ASR) models, specifically OpenAI’s Whisper model, for domain-specific applications using the KsponSpeech dataset. The primary research questions address the effectiveness of targeted lexical item emphasis during fine-tuning, its impact on domain-specific performance, and whether the fine-tuned model can maintain generalization capabilities across different languages and environments.Experiments were conducted using two fine-tuning datasets: Set A, a small subset emphasizing specific lexical items, and Set B, consisting of the entire KsponSpeech dataset. Results showed that fine-tuning with targeted lexical items increased recognition accuracy and improved domain-specific performance, with generalization capabilities maintained when fine-tuned with a smaller dataset. For noisier environments, a trade-off between specificity and generalization capabilities was observed. This study highlights the potential of fine-tuning using minimal domain-specific data to achieve satisfactory results, emphasizing the importance of balancing specialization and generalization for ASR models. Future research could explore different fine-tuning strategies and novel technologies such as prompting to further enhance large-scale ASR models’ domain-specific performance."
인공신경망을 이용한 자연어처리 방법에 관한 연구,2023,"['인공신경망', '순환신경망', '어텐션(attention) 메커니즘', '맥락벡터(context vectors)', '은닉상태(hidden state)']","최근에 챗 GPT에 대한 관심이 커지고 있다. 챗 GPT는 자연어를 해석하고, 생성할 수 있는 인공지능 모델이다. 챗 GPT는 방대한 양의 텍스트 데이터를 학습하여 사용자의 질문에 상세하게 답을 할 수 있는 인공지능 시스템이다. 자연어 처리를 위해 도입된 최초의 인공신경망 모델은 순환신경망(recurrent neural network, rnn)이다. 순환신경망 중에서 LSTM 모델을 이용하여 언어 번역기를 구현한 것이seq2seq 모델이다. 그러나 문장의 길이가 일정하지 않을 경우 순환신경망 모델에 입력으로 넣기 위해서는 길이를 일정하게 맞추는 전처리 작업이 필요하다. 또한, 문장 내의 단어들간의 의미론적인 관계를 고려하지 않은 모델이므로 많은 문제점을 포함하고 있다. 이러한 문제를 해결하기 위해 seq2seq모델에 어텐션(attention) 개념을 추가한 모델이 좋은 성과를 얻게 되었다. 어텐션(attention) 메커니즘을 도입한 seq2seq 모델도 학습시간이 너무 길어지는 순환신경망의 문제점을 피할 수는 없었다. Attention을 추가한 seq2seq 모델에서 순환신경망을 제거하고 attention만을 이용하여 구현한 언어 번역기 모델이 transformer이다. Transformer는 언어 번역기 모델이므로 인코더와 디코더 구조로 이루어져 있다. 인코더는 번역전 언어를 처리하는 부분이고, 디코더는 번역 후 언어를 처리하는 부분이다. Transformer에서 디코더만을 이용하여 구현한 언어생생 모델이 GPT이다. 본 논문에서는 자연어 처리 인공신경망 모델의 발전과정과 각 모델들의 문제점들을 리뷰하고, 필요한 개선점들을 제시하고자 한다. 특히 챗 GPT 모델의 문제점들을 살펴보고, 이러한 문제점들을 보완할 수 있는 방법을 제시한다. 챗 GPT는 언어, 인지, 인간의 이해와 관련된 인문과학과 사회과학 분야의 연구 및 응용에는 적합하지 않다. 그러나, 챗 GPT를 포함하여 자연어 처리에 대한 연구는 매우 중요하고, 경영학의 다양한 분야에 응용할 수 있다. 특히 빅데이터 등 고객의 데이터를 분석하여 고개 관련 분야의 기업전략에 활용하는 것은 중요하다.",다국어 초록 정보 없음
언어학적 특징을 고려한 자연어 처리 기반 한국어 요약 시스템,2023,"['Pre-learning language model', 'Abstractive summary', 'Linguistic features', 'Extractive summary', 'Text compression and summary', 'KoBART']","인터넷을 통해 대규모 데이터가 유통되면서 인터넷 이용자들은 자신에게 필요한 데이터를 찾기 어려워졌다. 또한, 데이터의 형태가 텍스트인 경우 텍스트를 압축 및 요약하는 작업이 요구되고 있는 실정이다. 본 논문에서는 한국어 텍스트에 대해서 학습한 Transformer Encoder-Decoder 기반의 KoBART(Korean Bidirectional and Auto-Regressive Transformers) 모델을 활용하여 한국어로 구성된 텍스트를 압축 및 요약하기에 효율적인 시스템을 구축하였다. 본 시스템은 추출요약을 수행하는 전처리기와 생성요약을 수행하는 KoBART 모델로 구성하였다. 전처리기는 한국어의 언어학적 특징을 고려하여 특정 문구가 출현하였을 경우 해당 문장을 중심으로 추출요약을 수행하고 KoBART 모델은 전처리기가 처리하지 않은 텍스트들에 대해서 생성요약을 수행한다. 제안하는 시스템은 한국어로 구성된 텍스트를 압축 및 요약하기 위하여 한국어의 언어학적 특징을 고려한 전처리기와 사전학습 언어모델인 KoBART 모델을 활용하였으며 일반적인 추출요약 모델과 생성요약 모델에 비해 우수한 성능을 보였다. 이는 특정 국가의 언어가 가지는 특징을 보다 상세하게 분석하고 활용하는 방안이 우수한 성능의 사전학습 언어모델만을 활용하는 것보다 좋은 결과를 기대할 수 있다는 점을 시사한다. 본 논문이 언어학적 특징과 우수한 성능의 사전학습 언어모델의 시너지를 전파하는데 선도하는 연구가 될 수 있을 것으로 기대된다.","As large-scale data is distributed through the Internet, it has become difficult for Internet users to find the data they need. In addition, when the form of data is text, it is required to compress and summarize the text. In this paper, we construct an efficient system for compressing and summarizing text composed of Korean using the Transformer Encoder-Decoder-based KoBART (Korean Bidirectional and Auto-Regulatory Transformers) model. This system consisted of a preprocessor that performs an extraction summary and a KoBART model that performs a generation summary. The preprocessor performs an extraction summary based on the sentence when a specific phrase appears considering the linguistic characteristics of the Korean language, and the KoBART model performs a generation summary on texts not processed by the preprocessor. The proposed system used the preprocessor considering the linguistic features of Korean and the KoBART model, which is a pre-learning language model, to compress and summarize text composed of Korean, and showed superior performance compared to the general extraction summary model and generation summary model. This suggests that a method of analyzing and utilizing the characteristics of a specific country's language in more detail can expect better results than using only a pre-learning language model with excellent performance. It is expected that this paper will be a leading study in spreading the synergy of the pre-learning language model with linguistic features and excellent performance."
목적 지향 대화 시스템을 위한 어댑터 기반 학습 방법,2023,"['자연어 처리', '목적 지향 대화 시스템', '어댑터', '미세 조정', '학습 효율성.', 'Natural Language Processing', 'Task-Oriented Dialogue System', 'Adapter', 'Fine-Tuning', 'Training Efficiency']","최근에는 목적 지향 대화 시스템의 성능 향상을 위해 Hyper-Scale의 Transformer기반의 사전 학습된 언어 모델을 사용하고 있다. 하지만 사전 학습된 언어 모델의 크기가 증가함에 따라 Fine-tuning을 진행할 때 문제점이 발생한다. 언어 모델의 Fine-tuning 과정은 전체 파라미터를 학습하기 때문에 크기가 증가하면 학습 시간이 오래 소요된다. 또한 충분한 저장 공간이 필요하다. 본 연구는 이러한 문제점을 해결하기 위해 Transformer 기반의 언어 모델에 해당하는 파라미터는 학습하지 않고 이후 Adapter, LoRA의 구조를 결합해 대화 지식을 효율적으로 학습하는 방법을 제안한다. 제안 모델의 성능 평가는 목적 지향 대화 시스템에서 주로 쓰이는 벤치마크 데이터 셋인 Multi-WOZ 2.0 데이터를 사용했다. 실험결과 기존 모델의 Fine-tuning에 비해 8%의 파라미터로 학습을 진행했음에도 불구하고 제안모델은 기존 모델과 2% 오차범위 내의 비슷한 성능을 보였다. 따라서 학습시간과 저장공간의 효율성이 비약적으로 향상되었음을 증명하였다.","Recently, pretrained hyperscale language models based on Transformer architectures have been used to improve the performance of task oriented dialogue systems. However, as the size of pretrained language models increases, problems typically arise with fine-tuning. Because fine tuning processes for language models require learning the entire parameter set, training takes longer as the size of a model increases, and storage space requirements also increase accordingly. To solve these problems, in this study, we propose a method to reduce the number of parameters of a language model and allow it to learn more efficiently. The proposed approach enables more efficient learning of conversational knowledge by combining an adapter and low-rank adaptation (LoRA) without learning parameters corresponding to Transformer-based language models. We evaluated the performance of an implementation of the proposed approach using the MultiWOZ 2.0 benchmark dataset, which is commonly used to evaluate the performance of task-oriented dialogue systems. The results show that despite learning with 8% fewer parameters compared to fine-tuning an existing model, the proposed method showed similar performance within the 2% error range. These results demonstrate that the efficiency of the proposed model improved dramatically in terms of learning time and storage space requirements."
얼굴 특징점을 활용한 영상 편집점 탐지,2023,"['컴퓨터 비전', '딥러닝', 'Swin Transformer', '얼굴 특징점 검출', '영상 편집', 'Computer Vision', 'Deep Learning', 'Swin Transformer', 'Facial Keypoints Detection', 'Video Editing']","최근 미디어 분야에도 인공지능(AI)을 적용한 다양한 서비스가 등장하고 있는 추세이다. 하지만 편집점을 찾아 영상을 이어 붙이는 영상 편집은, 대부분 수동적 방식으로 진행되어 시간과 인적 자원의 소요가 많이 발생하고 있다. 이에 본 연구 에서는 Video Swin Transformer를 활용하여, 발화 여부에 따른 영상의 편집점을 탐지할 수 있는 방법론을 제안한다. 이를 위해, 제안 구조는 먼저 Face Alignment를 통해 얼굴 특징점을 검출한다. 이와 같은 과정을 통해 입력 영상 데이터로부터 발화 여부에 따른 얼굴의 시･공간적인 변화를 모델에 반영한다. 그리고, 본 연구에서 제안하는 Video Swin Transformer 기반 모델을 통해 영상 속 사람의 행동을 분류한다. 구체적으로 비디오 데이터로부터 Video Swin Transformer를 통해 생성되는 Feature Map과 Face Alignment를 통해 검출된 얼굴 특징점을 합친 후 Convolution을 거쳐 발화 여부를 탐지하게 된다. 실험 결과, 본 논문에서 제안한 얼굴 특징점을 활용한 영상 편집점 탐지 모델을 사용했을 경우 분류 성능을 89.17% 기록하여, 얼굴 특징점을 사용하지 않았을 때의 성능 87.46% 대비 성능을 향상시키는 것을 확인할 수 있었다.","Recently, various services using artificial intelligence(AI) are emerging in the media field as well However, most of the video editing, which involves finding an editing point and attaching the video, is carried out in a passive manner, requiring a lot of time and human resources. Therefore, this study proposes a methodology that can detect the edit points of video according to whether person in video are spoken by using Video Swin Transformer. First, facial keypoints are detected through face alignment. To this end, the proposed structure first detects facial keypoints through face alignment. Through this process, the temporal and spatial changes of the face are reflected from the input video data. And, through the Video Swin Transformer-based model proposed in this study, the behavior of the person in the video is classified. Specifically, after combining the feature map generated through Video Swin Transformer from video data and the facial keypoints detected through Face Alignment, utterance is classified through convolution layers. In conclusion, the performance of the image editing point detection model using facial keypoints proposed in this paper improved from 87.46% to 89.17% compared to the model without facial keypoints."
다국어 사용자 후기에 대한 속성기반 감성분석 연구,2023,"['BERT', 'multilingual BERT', 'natural language process', 'transformer  encoder', 'XLM-RoBERTa', 'BERT', '다국어 BERT', '자연어 처리', '트랜스포머 인코더', 'XLM-RoBERTa']","전자상거래 시장의 성장과 더불어 소비자들은 상품 및 서비스 구매 시 다른 사용자가 작성한 후기 정보에 기반하여 구매 의사를 결정하게 되며 이러한 후기를 효과적으로 분석하기 위한 연구가 활발히 이루어지고 있다.특히, 사용자 후기에 대해 단순 긍/부정으로 감성분석하는 것이 아니라 다면적으로 분석하는 속성기반 감성분석 방법이 주목받고 있다.속성기반 감성분석을 위한 다양한 방법론 중 최신 자연어 처리 기술인 트랜스포머 계열 모델을 활용한 분석 방법이 있다.본 논문에서는 최신 자연어 처리 기술 모델에 두 가지 실제 데이터를 활용하여 다국어 사용자 후기에 대한 속성기반 감성분석을 진행하였다.공개된 데이터 셋인 SemEval 2016의 Restaurant 데이터와 실제 화장품 도메인에서 작성된 다국어 사용자 후기 데이터를 활용하여 속성기반 감성분석을 위한 트랜스포머 계열 모델의 성능을 비교하였고 성능 향상을 위한 다양한 방법론도 적용하였다.다국어 데이터를 활용한 모델을 통해 언어별로 별도의 모델을 구축하지 않고 한가지 모델로 다국어를 분석할 수 있다는 점에서 효용 가치가 클 것으로 예상된다.","With the growth of the e-commerce market, consumers increasingly rely on user reviews to make purchasing decisions.Consequently, researchers are actively conducting studies to effectively analyze these reviews.Among the various methods of sentiment analysis, the aspect-based sentiment analysis approach, which examines user reviews from multiple angles rather than solely relying on simple positive or negative sentiments, is gaining widespread attention.Among the various methodologies for aspect-based sentiment analysis, there is an analysis method using a transformer-based model, which is the latest natural language processing technology.In this paper, we conduct an aspect-based sentiment analysis on multilingual user reviews using two real datasets from the latest natural language processing technology model.Specifically, we use restaurant data from the SemEval 2016 public dataset and multilingual user review data from the cosmetic domain.We compare the performance of transformer-based models for aspect-based sentiment analysis and apply various methodologies to improve their performance.Models using multilingual data are  expected to be highly useful in that they can analyze multiple languages in one model without building separate models for each language."
다양한 도메인 변화에 강건한 한국어 표 기계독해,2023,"['기계독해', '표 질의응답', '합성 데이터 생성', '도메인 적응', '도메인 일반화', 'machine reading comprehension', 'table question answering', 'domain adaptation', 'domain generalization']","표 데이터는 일반적인 텍스트 데이터와 다르게 구조적인 특장점으로 정보를 압축해 표현할 수 있다. 이는 표가 다양한 도메인에서 활용되는 것으로 이어지며, 기계독해 영역에서의 표 기계독해 능력이 차지하는 비중은 점점 커지고 있다. 하지만 도메인마다 표의 구조와 요구되는 지식이 달라 언어 모델을 단일 도메인으로 학습했을 때 다른 도메인에서의 모델의 평가 성능이 하락해 일반화 성능이 낮게 나타날 가능성이 크다. 이를 극복하기 위해서는 다양한 도메인의 데이터셋 구축이 우선이 되어야 하며, 단순 사전학습한 모델이 아닌 다양한 기법을 적용하는 것이 중요하다. 본 연구에서는 도메인 일반화 성능을 높이기 위해 도메인 간 불변하는 언어적 특성(Invariant-feature)을 학습하는 언어 모델을 설계한다. 각 도메인별 평가 데이터셋에서의 성능을 높이기 위해서 적대적 학습을 이용하는 방법과 표 데이터에 특화된 임베딩 레이어와 트랜스포머 레이어를 추가하는 모델의 구조를 변형하는 방법을 적용하였다. 적대적 학습을 적용했을 때는 표와 관련된 특화된 임베딩을 추가하지 않는 구조의 모델에서 성능이 향상되는 것을 확인했으며, 표에 특화된 트랜스포머 레이어를 추가하고 추가된 레이어가 표에 특화된 임베딩을 추가로 입력받도록 했을 때, 모든 도메인의 데이터에서 가장 향상된 성능을 보였다.",다국어 초록 정보 없음
대화형 AI 기반 그림동화 재창작 서비스,2023,"['대화형 인공지능 모델', 'GPT', '생성적 콘텐츠', '그림동화', '창작 인공지능', 'Conversational AI', 'ChatGPT', 'Generative Contents', 'Illustrated Fairy Tales', 'Creative AI']","본 연구에서는 아이들의 창의력 교육을 위한 동화 재창작 AI 서비스를 제시한다. 이야기의 제한된 선택지를 제공하는 기존 서비스들과 달리 제안하는 이야기 재창작 서비스는 사용자가 동화를 읽은 후 대화형 AI 모델인 GPT(생성적 사전학습 트랜스포머)가 스토리 및 사용자 응답 맥락에 따라 생성하는 일련의 질문에 응답을 주고받게 하며, 대화 종료 후엔 챗GPT가 요약한 동화 텍스트와 더불어 키워드 기반 자동 추천 이미지 요소를 참조하여 그림을 그릴 수 있도록 한다. 생성된 글과 그림은 재창작된 동화로 저장된다. 그림 그리기 단계에서는 키워드들에 대응되는 아이들의 스케치를 학습한 퀵드로우(QuickDraw) 데이터셋 API가 이야기와 매칭되는 드로잉을 제시한다. 서비스 구현 결과, 대화형 AI 모델은 질문 생성에 우수한 성능을 보였으며, 스케치 샘플 제공 기능은 사용자의 흥미 유발에 도움이 되었다. 초등학교 저학년생 대상 파일럿 테스트와 학부모 인터뷰를 통해 서비스의 개선점을 확인하였고, 독서 후 활동 교육에 활용될 수 있음을 확인하였다.","We propose an artificial intelligence (AI) service to assist children's creativity education through the reconstruction of fairy tales. In contrast to existing services that offer limited choices for story reconstruction, our proposed service allows the user to engage in interactive dialogue with a conversational AI using a Generative Pre-trained Transformer (GPT) model. After reading the fairy tale, the user responds to a series of questions generated by ChatGPT based on the story and user input. Upon completing the conversation, the generated fairy tale text, along with keyword-based automatically recommended image elements, help the user to draw accompanying illustrations. The combined text and drawings can be saved for future reference. QuickDraw dataset, a large-scale collection of sketches created by human users, is employed to suggest drawings relevant to story keywords. The implementation results demonstrate the good performance of the conversational AI in story question generation, while the provision of sketch samples contributes to user engagement. A pilot test with elementary school students and interviews with parents confirmed the service's potential for educational use in post-reading activities."
텍스트 요약 품질 향상을 위한 의미적 사전학습 방법론,2023,"['딥러닝', '추상 요약', '트랜스포머', '사전학습 언어 모델', 'Deep Learning', 'Abstract Summary', 'Transformer', 'Pre-trained Language Model', 'GSG']","최근 사용자에게 의미있는 정보만을 자동으로 간추리는 텍스트 자동 요약이 꾸준히 연구되고 있으며, 특히 인공신경망 모델인 트랜스포머를 활용한 텍스트 요약 연구가 주로 수행되고 있다. 다양한 연구 중 특히 문장 단위 마스킹을 통해 모델을 학습시키는 GSG 방식이 가장 주목을 받고 있지만, 전통적인 GSG는 문장의 의미가 아닌 토큰의 중복 정도에 기반을 두어 마스킹 대상 문장을 선정한다는 한계를 갖는다. 따라서 본 연구에서는 텍스트 요약의 품질을 향상시키기 위해, 문장의 의미를 고려하여 GSG의 마스킹 대상 문장을 선정하는 SbGSG(Semantic-based GSG) 방법론을 제안한다. 뉴스기사 370,000건과 요약문 및 레포트 21,600건을 사용하여 실험을 수행한 결과, ROUGE와 BERT Score 측면에서 제안 방법론인 SbGSG가 전통적인 GSG에 비해 우수한 성능을 보임을 확인하였다.","Recently, automatic text summarization, which automatically summarizes only meaningful information for users, is being studied steadily. Especially, research on text summarization using Transformer, an artificial neural network model, has been mainly conducted. Among various studies, the GSG method, which trains a model through sentence-by-sentence masking, has received the most attention. However, the traditional GSG has limitations in selecting a sentence to be masked based on the degree of overlap of tokens, not the meaning of a sentence. Therefore, in this study, in order to improve the quality of text summarization, we propose SbGSG (Semantic-based GSG) methodology that selects sentences to be masked by GSG considering the meaning of sentences. As a result of conducting an experiment using 370,000 news articles and 21,600 summaries and reports, it was confirmed that the proposed methodology, SbGSG, showed superior performance compared to the traditional GSG in terms of ROUGE and BERT Score."
PGB: BERT 프루닝을 위한 순서 변경 규칙 및 그룹화,2023,"['BERT compression', 'task-specific pruning', 'structured pruning', 'head pruning', 'BERT 압축', '과제별 프루닝', '구조화된 프루닝', '헤드 프루닝']","최근 사전 학습된 트랜스포머 계열의 모델은 자연어 처리, 이미지 인식 등 다양한 인공지능 분야에서 활발히 사용되고 있다. 그러나 해당 모델들은 수십억 개의 파라미터를 가지고 있어 추론 시에 상당한 연산량을 필요로 하며 자원이 제한된 환경에서 사용하기에는 많은 제약이 따른다. 이러한 문제들을 해결하기 위해 본 논문은 트랜스포머 모델에 대한 그룹화 기반의 새로운 구조화된 프루닝 방법인 PGB(Permutation Grouped BERT pruning)를 제안한다. 제안된 방법은 자원 제약 조건에 따라 최적의 어텐션 순서를 변경하는 방법을 찾고, 모델의 정보 손실을 최소화하기 위해 헤드의 중요도를 기반으로 불필요한 헤드에 대해 프루닝한다. 다양한 비교 실험을 통해 사전 학습된 BERT 모델에 대한 기존의 구조화된 프루닝 방법보다 본 논문에서 제안한 방법이 추론 속도 및 정확도 손실 측면에서 더 우수한 성능을 보임을 확인한다.",다국어 초록 정보 없음
컴퓨터언어학 분야 한국어 구문 연구의 현황,2023,"['Computational Linguistics', 'dependency parsing', 'language model', 'Transformer', 'corpus', '컴퓨터언어학', '의존 구문 분석', '언어 모델', '트랜스포머', '말뭉치']","본 논의에서는 컴퓨터언어학 분야에서 고려되는 ‘구문’의 정의를 살펴보고 이들이 연구에서 다루어지는 양상을 확인하였다. 컴퓨터언어학 및 자연언어처리 분야의 연구에서의 구문은 다른 언어학 분야와 크게 다른 정의를 내리고 있지는 않으나, 특히 문장을 이루는 통사적, 의미적 구조 정보와 깊은 관련이 있다고 볼 수 있다. 문장 내의 단어 혹은 다른 단위의 구성 성분들이 서로 만나 이루는 관계를 학습한 언어 모델이 그 지식을 활용해 다양한 언어 데이터를 처리하는 것이다. 본고에서는 ‘구문 분석 말뭉치’라는 이름으로 제공되어 한국어 컴퓨터언어학 연구에서 주로 분석 및 활용되는 데이터에 대해 설명하고, 이와 관련하여 이루어지고 있는 한국어 구문 분석 연구 및 배포된 라이브러리를 소개하였다. 또한 이러한 데이터가 포함하는 구문 정보를 학습하고 활용하는 BERT, GPT 등 언어 모델의 원리인 어텐션 메커니즘이 곧 문장 혹은 문서 내 구성 성분 사이의 관계성에 기반한 것임을 살펴보았다.",다국어 초록 정보 없음
자연어 및 시계열 데이터 처리를 지원하는 C++ 기반 오픈소스 딥러닝 프레임워크 WICWIU.v3,2023,"['딥러닝', '오픈소스', '프레임워크', '자연어 처리', '순환 신경망', '트랜스포머', 'deep learning', 'open source', 'deep learning framework', 'natural language processing', 'recurrent neural network', 'transformer']","WICWIU(위큐)는 국내 대학에서 최초로 공개한 오픈소스 딥러닝 프레임워크이다. 본 연구에서는 자연어 및 시계열 데이터 처리 기능들이 추가된 WICWIU.v3를 개발하였다. WICWIU는 C++ 환경을 위해 설계되었고, GPU 기반 병렬처리를 지원하며, 가독성과 확장성이 우수해 사용자가 직접 새로운 기능을 추가하기에 용이하다. CNN(Convolutional Neural Networks), GAN(Generative Adversarial Networks) 등 영상처리 모델에 중점을 둔 WICWIU.v1과 v2에 비해 WICWIU.v3에는 LSTM(Long Short-Term Memory Networks)과 GRU(Gated Recurrent Units)를 포함한 순환 신경망(RNN), 어텐션 모듈, 트랜스포머(Transformer) 등 자연어 및 시계열 데이터 처리를 위한 클래스와 함수들이 추가되었다. WICWIU.v3를 이용해 기계번역 및 텍스트 합성 모델을 구현함으로써 새로 추가된 자연어 및 시계열 처리 기능들이 정상적으로 동작함을 확인하였다.","WICWIU is the first open-source deep learning framework developed by Korean university. In this work, we developed WICWIU.v3 that includes features for natural language and time-series data processing. WICWIU was designed for C++ environment, and supports GPU-based parallel processing, and has excellent readability and extensibility, allowing users to easily add new features. In addition to WICWIU.v1 and v2 that focus on image processing models, such as convolutional neural networks (CNN) and general adversarial networks (GAN), WICWIU.v3 provides classes and functions for natural language and time-series data processing, such as recurrent neural networks (RNN), including LSTM (Long Short-Term Memory Networks) and GRU (Gated Recurrent Units), attention modules, and Transformers. We validated the newly added functions for natural language and time-series data by implementing a machine translator and a text generator with WICWIU.v3."
지도학습 기반의 부정·불량 식품 기사 자동 분류에 관한 연구,2023,"['인공 신경망 (Artificial Neural Network)', 'BERT(Bidirectional Encoder Representation from Transformers)', '지도 학습 (Supervised Learning)', '자연어 처리(Natural Language Process)']","최근 식품 공급망의 다양화와 생산, 유통, 소비 방식의 변화로 인해 부정·불량 식품이 증가하고 있어 이에 대한 새로운 대응책이 필요하다. 현재까지는 사람이 직접 부정·불량 식품 관련 기사를 모니터링하고 분석했으나, 처리해야 할 정보의 양이 많아지면서 기사 분석을 위한 비용이 크게 증가하고 있다. 이러한 문제를 해결하기 위해 본 연구에서는 지도학습 기반 모델을 사용한 부정·불량 식품 자동 분류 시스템을 제안한다. 해당 시스템에는 여러 BERT[1] 모델의 앙상블(Ensemble)을 적용하여 과적합(Overfitting)과 편향성(Bias)을 방지하였으며 동시에 분류(Classification) 성능을 향상시켰다. 모델의 분류학습과 성능 평가에는 사전에 수집된 1250개의 기사 데이터를 사용하여 실험을 수행했다. 실험 결과 자연어 처리 분야에서 여러 모델의 앙상블 기법은 단일 모델 대비 적은 데이터로 더 높은 분류 성능을 보이는 것을 확인했다.",다국어 초록 정보 없음
구글 신경망 번역 얼마나 진화했나?  초창기와 현재의 번역 품질을 중심으로,2023,"['traduction automatique', 'Google Neural Machine Translation', '(GNMT)', 'modèle RNN Seq2Seq', 'Transformer', '기계 번역', '구글 신경망 번역', 'RNN Seq2Seq 모델', '트랜스포머']","본 연구는 2016년 말 일반인들에게 소개된 구글 신경망 번역과 현재의 구글 신경망 번역의 불한 번역 결과물 품질 비교·분석을 통해 구글신경망 번역의 발전 현황을 이해해보고자 한다. 이를 위해 르몽드에서선별한 신문 기사의 문장 110개를 RNN Sep2Sep 구글 번역(2017년) 그리고 트랜스포머 인코더와 RNN 디코더를 결합한 하이브리드 모델의구글 번역(2022년)으로 처리한 다음 단어, 구문, 문장, 텍스트, 편집을중심으로 번역 품질 평가를 실시하였다. 그 결과, 번역 단위가 문장으로 설정되어 있는 구글 신경망 번역의 성능이 상당히 향상되었음을 확인할 수 있었다. 이는 문장을 구성하는 어휘들 간의 연결 관계를 파악하고 재현하는 능력이 향상된 것으로 번역 결과물의 정확도와 가독성의 개선으로 이어지고 있다. 하지만, 여전히 내용 오류, 부자연스러운표현이 빈번하게 발견되며 문장의 연결이 부자연스러워 번역문이 텍스트로서 제대로 기능하지 못하고 있음을 확인할 수 있었다.",다국어 초록 정보 없음
딥러닝기반 건축폐기물 이미지 분류 시스템 비교,2023,"['Automatic Classifier', 'Convolutional Neural Network(CNN)', 'Data Augmentation', 'Deep Learning', 'VGG-16', 'Vision Transformer(ViT)']","본 연구는 건축시 발생되는 폐기물의 자동분류를 위해 딥러닝 알고리즘을 활용해 건출 폐기물 데이터를 각각목재 폐기물, 플라스틱 폐기물, 콘크리트 폐기물로 분류하는 두 모델들을 통해서 성능 비교를 한다. 건축 폐기물의 분류를 위해 사용된 딥러닝 알고리즘은 합성곱 신경망 이미지 분류 알고리즘 VGG-16과 NLP를 기반으로 이미지를 시퀀스화 시킨ViT, Vision Transformer 모델을 사용했다. 건축 폐기물 데이터 수집을 위해 이미지 데이터를 전 세계 검색엔진에서 크롤링 하였고, 육안으로도 명확히 구분하기 어렵거나, 중복되는 등 실험에 방해되는 이미지는 전부 제외하여각 분류당 1천장씩 총 3천장의 이미지를 확보했다. 또한, 데이터 학습시에 모델의 정확도 향상에 도움을 주기 위해 데이터 확대 작업을 진행해 총 3만장의 이미지로 실험을 진행 하였다. 수집된 이미 데이터가 정형화 되어있지 않은 데이터임에도 불구하고 실험 결과는 정확도가 VGG-16는 91.5%, ViT 는 92.7%의 결과가 나타났다. 이는 실제 건축폐기물데이터 관리 작업에 실전 활용 가능성을 제시한 것으로 보인다. 본 연구를 바탕으로 추후에 객체 탐지 기법이나 의미론적 분할 기법까지 활용한다면, 하나의 이미지 안에서도 여러 세밀한 분류가 가능해 더욱 완벽한 분류가 가능할 것이다.",다국어 초록 정보 없음
홀로렌즈2 기반 손동작 인식 연구를 위한 손 영상 분할,2023,[],"최근 산업분야에서 활용성이 높은 홀로렌즈2 기기는 다양한 상황에 적용될 수 있는 손동작 인식 연구에 활용된다. 하지만 홀로렌즈2 원본 데이터는 배경을 포함한 원형의 이미지이므로 손동작 인식 모델 학습시 모델 성능을 저하시킨다. 본 논문에서는 이와 같은 홀로렌즈2 원본 데이터의 문제점을 해결하기 위해, 영상분할 기법을 제안하고 본 전처리 기법에 따른 손동작 인식 성능을 확인한다. 제안하는 영상분할 기법은 다음과 같다. 원본 데이터의 거리 정보를 이용하여 배경을 제거한 뒤, 픽셀 값의 변화량을 기준으로 물체들의 경계를 나누고 군집화 하여 가장 큰 면적을 가지는 손 형태를 분리한다. 영상분할이 인식 성능을 향상시키는지 검증하기 위해 CNN 기반 손동작 인식 모델인 C3D와 CNN+transformer를 사용한다. 본 기법을 사용했을 때, 각 모델에서 17.4%, 0.2% 성능 향상을 보인다.",다국어 초록 정보 없음
인공지능(AI)의 윤리적 지위: 인간과 비인간 사이에서 어울리기,2023,"['artificial intelligence', 'ethics', 'AI ethical principles', 'labor', 'co-creation', 'participation', 'social learning', '인공지능', '윤리', 'AI 윤리 원칙', '노동', '공동-생산', '참여', '사회적 학습']","이 연구에서는 인공지능(AI)이 어떻게 ‘윤리적으로 정당한 자신의 지위(ethical niche)’를 얻을 것인가에 관해 논한다. 이 논의를 풀어가기 위해서 우선 최근 인공지능(AI)을 대표하는 GPT 기반의 언어모델의 특질을 살펴본다. 이를 통해 인공지능(AI)의 성장이 인간(사회)의 참여와 깊이 연루되어 있음을 확인할 수 있다. 다음으로 인공지능(AI)의 윤리 모델에 대한 사전적 검토를 수행한다. 그동안 여러 AI 윤리 모델이 등장했지만, 대체로 새로운 기술의 사회적 수용 문제를 다루기보다는 그 충격을 줄이는 데 집중하고 있다. AI 윤리 문제는 산업사회 이후 지금까지의 인간과 기계의 관계를 전반적으로 재고할 때 현실적으로 작동하는 윤리 모델에 접근할 수 있다. 특히 노동에 대한 충분한 이해에서 출발하여 인공지능에 관한 기술사회적 분석을 종합할 필요가 있다. 마지막으로 인간과 기계의 관계를 재설정한다는 취지에 부합하는 ‘인공지능(AI) 윤리 최소 원칙’ 4가지를 제안한다. 이 원칙에 조응하는 예시도 찾아볼 것이다. 최종적으로 인공지능(AI)의 윤리적 목표가 인간의 윤리를 성취하는 문제와 긴밀히 연결되어 있음도 드러날 것이다.","This This article discusses how artificial intelligence (AI) can get “ its own ethically legitimate ethical niche.” First, we examine the characteristics of generative pre-trained transformer (GPT)-based language models representing AI. This exploration demonstrates that human input is crucial to AI’s advancement. Next, we conduct a preliminary review of AI’s ethical model. Several AI ethics models have emerged, but they mainly focus on reducing the impact of new technologies rather than dealing with social acceptance issues. We can approach AI ethics issues more realistically by reconsidering contemporary society’s relationship between humans and machines. Thus, it is necessary to start with a sufficient understanding of labor and synthesize different techno-sociological viewpoints on AI. Lastly, the article proposes four basic principles of AI ethics consistent with re-establishing the relationship between humans and machines and presents examples in response to this principle. The article concludes that closely aligning the ethical goals of AI with the pursuit of human ethics is essential."
광선 간 코사인 유사도를 활용한 퓨샷 신경망 방사장(Neural Radiance Fields),2023,[],"Neural Radiance Fields(NeRF)는 새로운 시점 합성(Novel View Synthesis) 분야에서 놀라운 성과를 보여주었다. 다만 각 물체 마다 개별 모델을 학습시켜야 하며 학습 과정에 많은 이미지가 필요하다는 점에서 과적합 모델이라는 제한이 있다. 이를 해결하기 위해 PixelNeRF, GBT 등 적은 수의 이미지로도 다양한 물체에 대해 작동하는 퓨샷 NeRF 모델이 제안되었다. 그 중, GBT 는 광선 간의 거리 관계를 트랜스포머의 편향 점수로 부과하여 쿼리 이미지 광선과 입력 이미지의 광선의 상대적인 관계를 학습한다. 본 연구는 GBT 를 베이스라인으로 하여 광선 간의 방향 정보를 추가 편향으로 부과할 것을 제안한다. 방향 정보를 부과하는 방법으로는 코사인 유사도를 사용하였으며, 제안된 방법을 통해 CO3D 데이터셋에서 퓨샷 새로운 시점 합성을 수행한 결과 평균적으로 LPIPS 성능이 향상되는 것을 확인하였다.",다국어 초록 정보 없음
Instance segmentation 기반 사과 객체 비대 분석기술 개발,2023,"['사과', 'RGB-D', '딥러닝', '객체분할', '비대 예측']","사과는 전세계적으로 가장 많이 소비되는 청과류 중 하나로, 맛을 비롯하여 콜레스테롤 수치를 낮춰주고 혈압을 조절해주는 등 다양한 건강적 이점을 갖는 과실이다. 최근 주목받고 있는 스마트팜 분야에서는, 원예작물 과실의 성장 상태에 대한 모니터링과 수확을 위한 영상 인식 기술에 대한 연구가 활발히 진행되고 있다. 이러한 기술은 농가의 과실 생산량 예측뿐만 아니라 수확의 자동화를 위해서도 필수적이다.최근 영상 기반의 연구들에서는, 딥러닝 기반의 모델들이 활발히 사용되고 있다. 특히 객체의 영역을 찾아내는 객체 분할 영역 또한 합성곱 신경망이나 트랜스포머 등의 딥러닝 알고리즘이 기존 방식들 대비 높은 성능을 보인다. 본 연구에서는 사과(홍로) 과실의 객체 분할 모델을 개발하기 위하여 노지 사과밭에서 RGB-D 영상을 촬영하여 데이터를 수집하였다. 획득한 데이터는 객체 영역이 폴리곤 형태로 라벨링 되었으며, 학습 및 평가를 거쳐 여러 객체 분할 모델들의 성능이 비교 평가되었다. 또한, 객체분할 모델 결과로 획득한 mask 정보 및 영상 촬영 시 획득한 depth 정보를 활용하여, 검출된 사과의 직경(횡경 및 종경), 면적, 부피 및 과중에 대한 값을 추산하였으며 실제 값들과 비교하여 알고리즘에 대한 오차를 분석하였다.",다국어 초록 정보 없음
인과추론을 이용한 공정한 분류기,2023,[],"Transformer 기반 자연어처리 언어모델의 성능이 빠르게 증가함에 따라 공정성에 대한 문제가 제기되었다. 예를 들어 혐오 발언 탐지 모델에서는 인종, 성별 같은 identity term의 영향을 강하게 받아 심각한 편향을 유발한다. 따라서 본 논문에서는 여러 민감한 속성에 대한 Word Confounder dictionary을 만들고 인과적 개입을 통해 identity term에 과적합하는 것을 방지하는 새로운 인과 추론 기반텍스트 분류 프레임 워크(CIT)를 제안한다. 우리는 CIT를 통해 bert를 fine-tuning하고 결과 모델이 benchmark corpora에 대해서 bias metrics한 결과가 다른 주류 접근법과 일치하거나 초과함을 보여준다.",다국어 초록 정보 없음
뉴스의 감성 분석을 사용한 주가 예측 방법론,2023,"['stock price prediction', 'sentiment analysis', 'technical data', 'sequence-to-sequence model', 'transformer']",딥러닝 모델을 사용하여 주식 가격을 예측하려는연구들은 꾸준히 진행되고 있다. 주식 예측 딥러닝모델들은 기본적으로 시계열 특성을 학습하고 이를기반으로 미래값을 예측하는 구조를 가진다. 하지만주식 가격은 외부 요인에 많은 영향을 받기 때문에기술적인 데이터를 학습한 시계열 예측 모델로 주가를 예측하면 정확도가 떨어진다. 본 논문에서는 주가데이터와 경제 뉴스 텍스트에서 추출한 감성 표현을사용해 주식 가격을 예측하는 방법론을 제안한다. 주가 데이터와 감성 표현을 모델의 입력에 동시에 사용하는 것이 아닌 주가 데이터의 중간 표현과 감성 표현을 결합하여 사용하는 방식을 제안한다.,다국어 초록 정보 없음
"허구, 미메시스, 그리고 문학",2023,"['허구', '미메시스', '텍스트 세계', '인지과학', '문학연구', 'fiction', 'mimesis', 'monde du texte', 'science cognitive', 'étude littéraire']","사전적으로는 “현실에서는 완전한 모델을 갖지 않는 상상력의 산물” 또는 “현실을 은폐하거나 꾸밀 목적으로 의식적 또는 무의식적으로 구성된 상상적 구조물”로 정의되는 허구는 흔히 물리적으로 존재하거나 증명할 수 있는 사실이 아닌 ‘지어낸’ 것, 그래서 진실과는 거리가 먼 거짓이나 모방, 가장, 흉내, 놈이, 환상 등의 부정적인 의미를 갖는다. 그렇다면 허구와 현실과의 관계는 어떻게 설정할 수 있으며, 왜 우리는 허구를 필요로 하는가? 허구 세계는 우리가 몸담고 있는 현실 세계와는 다르지만 어쨌든 하나의 세계임에는 분명하다. 하지만 그 세계가 과연 어떤 방식으로 존재하며 어떻게 현실에 영향을 미치는지에 대해서는 쉽게 말하기 어렵다. 이처럼 허구는 현실과 관련하여 사실 또는 실재 개념을 불러오고 이는 모방 또는 재현으로서의 미메시스라는, 문학과 예술 그리고 현실이 맺는 관계에 대한 성찰로 우리를 이끌어간다. 이러한 관점에서 우리논 사실과 허구의 경계와 관련된 역사와 허구의 교차, 텍스트 세계의 존재론적 위상 등의 문제를 생각해보고, 인지과학적 관점에서는 이 문제에 어떻게 접근하고 있는지 최근의 연구 경향을 소개함으로써 학제간 연구의 지형도를 그려볼 것이다. 허구에 대한 우리의 연구는 그와 연계된 개념들, 즉 가상현실과 증강현실, 메타버스 등의 개념을 탈-신비화하고 그 유효성과 한계를 성찰하는 데 기여할 수 있을 것이며, 무엇보다 허구 이야기로서의 문학이 갖는 보편성과 근원성을 보여줄 수 있을 것이다.","Définie comme “produit de l'imagination qui n'a pas de modèle complet dans la réalité”, la fiction est souvent considérée comme mensonge, dissimulation, imitation ou feintise ludique qui se trouve aux antipodes du réel et de la vérité. Il n'en est pas moins vrai qu'elle nous affecte et influence si profondément, d'une manière ou d'autre, que nous nous perdons parfois à la frontière de la fiction et le réel. Quel lien pourrait-on établir entre la fiction et le réel? quels sont le statut ontologique et la fonction de la fiction par rapport au réel? En partant d'une approche centrée sur la fiction littéraire, nous nous interrogeons sur la possibilité d'élargir notre problématique, non seulement dans une domaine littéraire, mais aussi philosophique et cognitive, pour dessiner une géographie des recherches actuelles de la fiction dans une perspective interdisciplinaire. Ces reflexions nous permettront d'évaluer et d'attester, à la croisée du réel et de la fiction, la puissance de la littérature, qui pourrait nous former et transformer."
BERT의 문맥 정보에 미치는 특이 차원의 영향,2023,"['자연어 처리', 'BERT', '아니소트로피', '클러스터링', '특이 차원', 'natural language processing', 'BERT', 'anisotropy', 'clustering', 'outlier dimensions']","최근 자연어 처리에서 트랜스포머 계열의 네트워크가 뛰어난 성능을 발휘하고 있다. 그에 따라 트랜스포머의 구조를 일부 변형하여 만든 언어 모델인 BERT 또한 많은 자연어 처리 태스크에서 사용되고, 좋은 결과를 보여주고 있다. BERT는 셀프 어텐션을 통해 문맥 정보를 담아서 임베딩 벡터를 인코딩하는데, 이 과정에서 관련 있는 단어의 임베딩 벡터들이 유사해진다. 이는 단어들 사이의 코사인 유사도를 통해 알 수 있는데, BERT의 초기 계층에서는 문맥 정보가 담기지 않아 단어 사이의 코사인 유사도가 낮지만, 뒤쪽 계층으로 갈수록 셀프 어텐션을 통해 문맥 정보가 추가되면서 단어 사이의 코사인 유사도가 높게 나오는 것을 알 수 있다. 하지만 높은 코사인 유사도 값을 가지게 되는 것에 있어서 임베딩 벡터의 특정 차원이 지배적인 역할을 한다는 연구가 존재한다. 본 연구에서는 그러한 차원이 문맥 정보를 포함하는 것에 어떤 영향을 미치는지를 확인하기 위해 그러한 차원을 지우기 전과 후의 단어 임베딩 벡터를 클러스터링한 후 두 결과를 비교하여 확인하였다.",다국어 초록 정보 없음
BERT 기반 리뷰 카테고리 분류 시스템,2023,"['BERT', 'KR-SBERT', '카테고리', '리뷰', '텍스트 마이닝']","현재 소비자들은 리뷰를 통해 기업에 대한 이미지를 확립한다. 이에 따라 리뷰 데이터를 분석하여 대중 의 평가를 알아내는 것이 중요하다. 원하는 정보를 가독성 높게 얻기 위해서는 명확한 카테고리를 바탕 으로 리뷰들을 분류하는 것이 중요하다. 그러나 기존의 한국어 문장 임베딩 알고리즘인 KR-SBERT는 유사도를 기반으로 한 카테고리 분류에서 정확도가 한계를 보인다. 이러한 한계를 극복하기 위해 본 논 문에서는 KR-SBERT에 지도 학습 분류기 모델을 추가하여 리뷰 카테고리 분류 성능을 향상시켰다. 실 험 결과, 이를 통해 분류 성능이 57% 향상되었다.",다국어 초록 정보 없음
텍스트 인식률 개선을 위한 한글 텍스트 이미지 초해상화,2023,"['Scene Text Image Super-Resolution', 'Korean Text', 'Transformer', '.']","카메라로 촬영한 야외 일반 영상에서 텍스트 이미지를 찾아내고 그 내용을 인식하는 기술은 로봇 비전, 시각 보조 등의 기반으로 활용될 수 있는 매우 중요한 기술이다. 하지만 텍스트 이미지가 저해상도인 경우에는 텍스트 이미지에 포함된 노이즈나 블러 등의 열화가 더 두드러지기 때문에 텍스트 내용 인식 성능의 하락이 발생하게 된다. 본 논문에서는 일반 영상에서의 저해상도 한글 텍스트에 대한 이미지 초해상화를 통해서 텍스트 인식 정확도를 개선하였다. 트랜스포머에 기반한 모델로 한글 텍스트 이미지 초해상화를 수행하였으며, 직접 구축한 고해상도-저해상도 한글 텍스트 이미지 데이터셋에 대하여 제안한 초해상화 방법을 적용했을 때 텍스트 인식 성능이 개선되는 것을 확인하였다.",다국어 초록 정보 없음
한국어 데이터를 활용한 data augmentation,2023,"['데이터 증강', '딥러닝', 'Data Augmentation', 'Deep Learning', 'transformer', 'BERT', 'GPT']","오늘날 자연어 처리 분야의 발달에 따라 text 데이터 증강의 중요성이 강조되고 있다. 왜냐하면 모델의 정확도를 향상시키고 싶을 때, 자원이 한정된 현실에서 데이터 증강은 필수적이기 때문이다. 그러나 text 데이터 증강은 단어의 위치를 바꾸고 단어를 제거하는 등의 데이터를 변형하는 과정에서 원래의 뜻과 달라질 위험이 있다. 또한, 구어체로 이루어진 데이터의 경우, 문법에 맞지 않는 데이터가 많기 때문에 학습이 끝난 후 모델이 표준어가 아닌 문장을 입력으로 받았을 때 문장을 이해하지 못하는 경우가 있다. 따라서 본 논문에서는 이러한 문제점들을 해결하기 위해, 문장 데이터의 라벨을 보존하며 데이터를 증강하고, 문법에 맞지 않는 문장을 원본 데이터에 추가하는 증강 방법을 제안한다.","With the development of natural language processing today, the importance of text data augmentation is being emphasized. This is because when you want to improve the accuracy of the model, data augmentation is essential in a reality with limited resources. However, text data augmentation risks being different from the original meaning in the process of transforming data, such as changing the position of words and removing words. In addition, in the case of colloquial data, there are many non-grammatical data, so after learning, the model may not understand the sentence when it receives a sentence that is not a standard language as an input. Therefore, in order to solve these problems, this paper proposes an augmented method that preserves the label of sentence data, enhances data, and adds non-grammatical sentences to the original data."
wav2vec2.0을 활용한 한국어 음성 감정 분류를 위한 데이터 샘플링 전략,2023,[],"음성 기반의 감정 분석은 인간의 감정을 정확하게 파악하는 데 중요한 연구 분야로 자리잡고 있다. 최근에는 wav2vec2.0과 같은 트랜스포머 기반의 모델이 음성 인식 분야에서 뛰어난 성능을 보이며 주목받고 있다. 본 연구에서는 wav2vec2.0 모델을 활용하여 한국어 감성 발화 데이터에 대한 감정 분류를 위한 데이터 샘플링 전략을 제안한다. 실험을 통해 한국어 음성 감성분석을 위해 학습 데이터를 활용할 때 감정별로 샘플링하여 데이터의 개수를 유사하게 하는 것이 성능 향상에 도움이 되며, 긴 음성 데이터부터 이용하는 것이 성능 향상에 도움이 됨을 보인다.",다국어 초록 정보 없음
Play with AI : GPT-3를 이용한 희곡 쓰기 및 가상 인간과의 연기 연구 - XR 콘텐츠 ‘Blue Space’ 제작 사례를 중심으로,2023,"['인공지능(Artificial Intelligent)', 'GPT-3', '희곡창작', '가상 인간(Virtual Human)', '자연어처리(Natural Language Processing)', '연극(Theatre)', '희곡(Play)', 'XR(Extended Reality)']","Play with AI 프로젝트의〈Blue Space〉는 공연 제작전반을 인공지능과의 공동 창작으로 만들어보고자 하는 아이디어에서 출발하여, 죽음을 이해하고자 인간을 살해한 AI의 이야기를 다룬 XR 콘텐츠이다. 트랜스포머 기반 언어 모델인 GPT-3로 3막의 짧은 희곡을 생성하고, 얼굴 인식 및 Text-to-Speech(TTS) 기술을 활용하여 3D 가상 인간에게 감정과 의도를 불어넣어 인간 배우가 같이 연기하도록 설계하였다. 본 사례를 통해 인공지능으로 완결성 있는 희곡 구조를 생성할 수 있다는 것을 확인한 바, 인공지능과 확장현실 기술을 활용한 공연 콘텐츠 제작의 가능성을 보여주는 사례라 할 수 있다.",다국어 초록 정보 없음
뉴로-심볼릭 구조 기반 온톨로지 생성기 제안,2023,[],"본 논문은 뉴로-심볼릭 구조를 바탕으로 일반 텍스트로부터 온톨로지 생성이 가능한 심층 신경망 기반 온톨로지 추출기를 제안한다. 온톨로지 추출 단계를 (i) 온톨로지 학습 및 (ii) 온톨로지 생성의 2 단계로 상정, (i) 일반 텍스트로부터 문장 구조 및 논리적 관계를 학습하는 트랜스포머 기반심층 생성 신경망 출력을 이용하여 (ii) 계층적으로 결합한 심볼릭 추론기로 온톨로지를 생성하는 뉴로-심볼릭 구조 온톨로지 추출기를 구현하였다. 1800 개 훈련 집합으로 학습 후 200 개 테스트 집합으로 평가한 결과, 정확도 91.9%, Precision 100%, Recall 99.1%로 비교 모델 OpenIE 의 성능에 비해서 각각 83.8%, 1.8%, 3.5% 개선된 것을 확인하였다. 정성적 품질에 있어서, 복잡한 문장 (예: 관계대 명사, 접속사, 중첩 구조)에서도 비교 모델에 비해 더 정밀한 온톨로지 생성 결과를 보였다.",다국어 초록 정보 없음
텍스트 기반 생성형 인공지능의 이해와 과학교육에서의 활용에 대한 논의,2023,"['generative artificial intelligence', 'conversational artificial intelligence', 'natural language processing', 'adaptive learning', 'connectivism', '생성형 인공지능', '대화형 인공지능', '자연어 처리', '적응 학습', '연결주의']","본 연구는 최근 주목받고 있는 텍스트 기반 생성형 인공지능에대해 관심과 활용이 증가함에 따라 과학교육적 측면에서의 활용을위해 생성형 인공지능의 주요 개념과 원리를 설명하고, 이를 효과적으로 활용할 수 있는 방안과 그 한계를 지적하며 이를 토대로 과학교육의 실행과 연구의 측면에서 시사점을 제공하는 것을 목적으로 한다.최근 들어 증가하고 있는 생성형 인공지능은 대체로 인코더와 디코더로 이뤄진 트랜스포머 모델을 기반으로 하고 있으며, 인간의 피드백을 활용한 강화학습과 보상 모델에 대한 최적화, 문맥에 대한 이해등을 통해 놀라운 발전을 이루고 있다. 특히, 다양한 사용자의 질문이나 의도를 이해하는 능력과 이를 바탕으로 한 글쓰기, 요약, 제시어추출, 평가와 피드백 등 다양한 기능을 수행할 수 있다. 또한 교수자가제시하는 예를 토대로 주어진 응답을 평가하거나 질문과 적절한 답변을 생성하는 등 학습자에 대한 진단과 실질적 교육내용의 구성 등많은 유용성을 가지고 있다. 그러나 생성형 인공지능이 가지고 있는한계로 인해 정확한 사실이나 지식에 대한 잘못된 전달, 과도한 확신으로 인한 편향, 사용자의 태도나 감정 등에 미칠 영향의 불확실성등에 대한 문제 등에 대해 해가 없는지 검토가 필요하다. 특히, 생성형인공지능이 제공하는 응답은 많은 사람들의 응답 데이터를 기반으로한 확률적 접근이므로 매우 거리가 멀거나 새로운 관점을 제시하는통찰적 사고나 혁신적 사고를 제한할 우려도 있다. 이에 따라 본 연구는 과학교수학습을 위해 인공지능의 긍정적 활용을 위한 여러 실천적제언을 제시하였다.","This study aims to explain the key concepts and principles of text-based generative artificial intelligence (AI) that has been receiving increasing interest and utilization, focusing on its application in science education. It also highlights the potential and limitations of utilizing generative AI in science education, providing insights for its implementation and research aspects. Recent advancements in generative AI, predominantly based on transformer models consisting of encoders and decoders, have shown remarkable progress through optimization of reinforcement learning and reward models using human feedback, as well as understanding context. Particularly, it can perform various functions such as writing, summarizing, keyword extraction, evaluation, and feedback based on the ability to understand various user questions and intents. It also offers practical utility in diagnosing learners and structuring educational content based on provided examples by educators. However, it is necessary to examine the concerns regarding the limitations of generative AI, including the potential for conveying inaccurate facts or knowledge, bias resulting from overconfidence, and uncertainties regarding its impact on user attitudes or emotions.Moreover, the responses provided by generative AI are probabilistic based on response data from many individuals, which raises concerns about limiting insightful and innovative thinking that may offer different perspectives or ideas. In light of these considerations, this study provides practical suggestions for the positive utilization of AI in science education."
사회 안전망 확충을 위한 모바일 360 영상에서의 딥러닝 기반 이벤트 인식 기술,2023,"['360 video', 'Sound classification', 'Audio Saliency detection', 'Event Visualization']","최근 공공장소에서의 위험 상황 발생에 의해 사회적으로 국민의 불안감이 심화되면서 범죄 및 사고 예방에 대한 경각심이 높아지고 있다. 그러나 현재 시행되고 있는 사회 안전 관련 방안들의 경우 효과가 불분명하거나 높은 운용 비용으로 지속가능성이 낮다는 단점이 있다. 본 논문에서는 보다 견고한 사회 안전망 확충을 위하여 모바일 360 영상에서의 딥러닝 기반 이벤트 인식 기술을 제안한다. 먼저, 수집한 360 영상에서 위험 상황을 인식하기 위하여 영상에서 추출한 음원 데이터를 바탕으로 트랜스포머 기반 사운드 분류 모델을 학습하고 위험 소리 분류에 대한 정량평가를 진행하였다. 또한, 사운드 기반 관심 영역 탐지기법을 적용하여 360 영상에서 위험 영역에 대한 정보를 실시간으로 시각화할 수 있도록 하였다. 본 연구 결과를 통해 사건/사고 발생 지역을 신속하게 인식 가능하여 사회 안전망 확충에 기여할 수 있을 것이라 기대한다.",다국어 초록 정보 없음
공사일지의 텍스트 마이닝을 통한 우천 공기지연 리스크 정량화,2023,"['Construction log', 'Unstructured data', 'Text mining', 'Schedule delay risk', 'Quantification', '공사일지', '비정형데이터', '텍스트마이닝', '공기지연 리스크', '정량화']","건설공사에서의 공기지연은 공사금액 증가, 발주처 클레임, 무리한 공기단축에 따른 건설공사의 질 하락 등 건설프로젝트에 악영향을 끼치는 주요 리스크 요인이다. 기존 연구에서는 공기지연 리스크의 중요도 및 우선순위를 파악하고 중요도에 따라 공정을 관리하였으나, 공기지연 리스크의 심도는 데이터 수집의 한계 등으로 정량화 연구가 미흡하다. 따라서 본 연구에서는 BERT (Bidirectional Encoder Representations fromTransformers) 언어 모델을 활용하여 비정형데이터로 저장된 공사일지의 작업내용을 분석 가능한 WBS (Work Breakdown Structure) 기반의 정형데이터로 변환하고 리스크 분류 및 도출 체계, 공정계획에 사용가능한 리스크 발생확률, 리스크 확률분포(심도)의 정량화 방안을 제시하였다. 제안된 프로세스를 고속도로공사 8개 공구에 적용하여, 39개 세부 공중 중 8개의 세부 공종에서 75건의 우천 공기지연 리스크를 도출하였다. K-S 검정을 통해 4개 공종에서 유의미한 확률분포를 도출하였으며 위험도를 비교하였다. 향후 본 연구에서 제시된 프로세스는 시공단계에서 발생하는 다양한 공기지연 요인의 도출 및 심도 정량화에 적용될 수 있을 것으로 기대된다.","Schedule delays present a major risk factor, as they can adversely affect construction projects, such as through increasing constructioncosts, claims from a client, and/or a decrease in construction quality due to trims to stages to catch up on lost time. Risk managementhas been conducted according to the importance and priority of schedule delay risk, but quantification of risk on the depth of scheduledelay tends to be inadequate due to limitations in data collection. Therefore, this research used the BERT (Bidirectional EncoderRepresentations from Transformers) language model to convert the contents of aconstruction log, which comprised unstructured data,into WBS (Work Breakdown Structure)-based structured data, and to form a model of classification and quantification of risk. Aprocess was applied to eight highway construction sites, and 75 cases of rain schedule delay risk were obtained from 8 out of 39 detailedwork kinds. Through a K-S test, a significant probability distribution was derived for fourkinds of work, and the risk impact wascompared. The process presented in this study can be used to derive various schedule delay risks in construction projects and to quantifytheir depth."
챗지피티(ChatGPT)의 등장과 국어교육의 대응,2023,"['챗지피티(ChatGPT)', '생성형 인공지능', '대화형 인공지능', '쓰기 교육의 본질 재정립', '쓰기 윤리', '거짓 정보 확산', '인공지능 윤리원칙', 'ChatGPT', 'generative AI', 'interactive AI', 'redefining the nature of writing education', 'writing ethics', 'spreading fake information', 'AI ethical principles']","이 연구의 목적은 생성형·대화형 인공지능 챗지피티(ChatGPT)의 등장으로 예견되는 국어교육 생태계의 변화상을 예측해 보고, 이러한 변화에 발 빠르게 대처하기 위해 국어교육에서 어떻게 대응해야 하는지 탐구하는 것이다. 이를 위해 챗지피티의 개발과 발전 과정, 주요 특징과 기능 등을 전반적으로 살펴보았다. 아울러 챗지피티의 기능 고도화에 따른 국어교육의 변화를 전망하고, 이에 대응하기 위해 국어교육이 나아가야 할 길을 탐구하였다.생성형·대화형 인공지능으로서 챗지피티의 주요 특징과 기능은 다음과 같다. 우선 생성형 인공지능으로서, 대규모 언어 모델과 생성형 사전 학습 변환기를 토대로 인터넷상 수많은 자료를 검색하고 학습하여 결과물을 산출한다. 다음으로 대화형 인공지능으로서, 일상적인 대화 형태로 의사소통이 가능하며, 이전 대화 내용을 기억했다가 후속 대화에 반영한다. 챗봇 방식으로 작동하며 ‘문장’ 형태로 질문하면 결과물을 텍스트 등 다양한 방식으로 출력해 준다.챗지피티 기능의 고도화가 국어교육에 미칠 영향과 이에 대한 대응 방안을 살펴보면 다음과 같다. 국어교육에 미칠 영향으로는 ①쓰기 교육 전반에 관한 도전, ②쓰기 윤리 문제의 심화, ③거짓 정보의 확산 등이 문제가 될 수 있다. 이에 대응 방안으로는 ①쓰기 교육의 본질 재정립, ②(가칭) 인공지능 활용 (글)쓰기의 윤리 원칙 마련 및 쓰기 윤리 교육 강화, ③ 비판적 읽기 지도를 통한 비판적 문식성 교육 강화가 필요하다.챗지피티의 등장과 나날이 고도화되는 기술의 발전이 국어교육을 향한 중대한 도전으로 느껴질 수도 있다. 하지만 그것의 부작용이나 단점에 주목하기 보다는 긍정적 활용과 도구적 활용의 이점에 주목할 필요가 있다. 따라서 기계와 인간의 대결이 아닌 상생과 공존의 관점과 도구적인 관점에서 이를 어떻게 활용할지에 관한 탐구가 훨씬 생산적일 것이며, 발전적일 것이다. 사람의 성장을 지원하는 인공지능 활용 연구가 지속해서 이어지기를 기대한다.","The purpose of this study is to predict changes in the Korean language education ecosystem that are expected with the advent of chatGPT, a generative and interactive AI, and to explore how Korean language education should respond to these changes quickly. To this end, the overall development and development process, main features and functions of ChatGPT were examined. In addition, the change of Korean language education according to the advancement of ChatGPT's function was predicted, and the way Korean language education should go was explored in order to respond to it.As a generative and conversational artificial intelligence, the main features and functions of ChatGPT are as follows. First of all, as generative artificial intelligence, it searches and learns numerous materials on the Internet based on large language models and generative pre-trained transformer to produce results. Next, as a conversational artificial intelligence, it is possible to communicate in the form of daily conversation, remembers the contents of previous conversations, and reflects them in subsequent conversations. It works like a chatbot, and when you ask a question in the form of a ‘sentence’, the result is output in various ways, such as text.The following is a look at the impact that the advancement of the ChatGPT function will have on Korean language education and the countermeasures against it. First, the impact on Korean language education can be problems such as ① the challenge of writing education in general, ② the deepening of the ethical problem of writing, and ③ the spread of fake information. To respond to this, it is necessary to ① redefine the nature of writing education, ② (tentative name) strengthen writing ethics education by preparing ethical principles for writing using artificial intelligence, and ③ expand critical literacy education through critical reading instruction.The emergence of ChatGPT and the development of technology that is advancing day by day may be felt as a significant challenge toward Korean language education. However, rather than paying attention to its side effects or shortcomings, it is necessary to pay attention to the advantages of positive and instrumental use. Therefore, the exploration of how to utilize it from the point of view of win-win and coexistence and instrumental point of view, rather than the confrontation between machines and humans, will be much more productive and progressive. I hope that research on the use of artificial intelligence to support human growth will continue."
자기등가회로법을 이용한 변압기 누설 자속 저감 설계,2023,"['Equivalent magnetic circuit(자기등가회로)', 'Leakage flux(누설 자속)', 'Leakage inductance(누설 인덕턴스)', 'Magnetizing inductance(자화 인덕턴스)', 'Power converter(전력 변환 기기)', 'Transformer(변압기)']",국문 초록 정보 없음,"In a transformer design, leakage flux must be considered to achieve high efficiency. In a conventional, it has been calculated by using the 3-dimensional finite element analysis, which can allow accurate prediction. However, it requires too much computational time. As its alternative, the equivalent magnetic circuit (EMC) was adopted, which can dramatically reduce the computational time while maintaining a reliability in the transformer design. In this paper, the EMC of reference transformer model was constructed and its experimental validation was carried out. Through the EMC, the leakage inductance according to design variables was analyzed. Then, the improved design was performed to reduce leakage flux of the reference model. Finally, the designed transformer was compared to the reference model."
인공지능 기반의 초등학생용 상담 챗봇 개발 및 효과성 검증,2023,"['Counseling Chatbot', 'Anxiety', 'AI', 'Transformer', 'Deep-learning', '상담 챗봇', '불안', '인공지능', '트랜스포머', '딥러닝']","인공지능 기술의 발전으로 챗봇은 단순한 테스크 업무에서 심리적인 측면을 다룰 수 있을 정도로 빠르게 발전하였다. 자연스러운 상호작용이 가능해진 상담 챗봇들은 전통적인 상담방식이 갖는 시간적·공간적·비용적 한계점들을 보완할 수 있다. 본 연구에서는 불안으로 인한 심리적 어려움을 겪는 초등학생을 대상으로 활용할 수 있는 상담 챗봇을 개발하고 그 효과성을 검증하고자 하였다. 상담 대화를 수집하여 상담 대화 데이터셋을 구성하였고, 이를 토대로 트랜스포머 모델 기반의 상담 챗봇을 개발하였다. 개발된 상담 챗봇을 초등학생들에게 적용한 결과, 학생들의 불안 수준에 유의한 차이가 나타났다. 향후 이 연구의 결과를 통해 개발된 상담 챗봇의 대화 모델 성능 개선을 통해 학생들의 심리정서적인 측면에서 긍정적인 도움을 제공할 수 있을 것으로 기대한다.","With the development of artificial intelligence technology, chatbots have developed rapidly enough to handle psychological and emotional aspects in simple tasks. Counseling chatbots that enable natural interaction overcome the temporal, spatial, and cost limitations of traditional counseling methods to enable effective treatment for clients. In this study, we tried to develop a chatbot that can be used for children suffering from depression in elementary school and verify its effectiveness. A chatbot was developed as a transformer model by collecting counseling cases through a literature survey and constructing a dataset. As a result of applying the developed chatbot to elementary school students for four weeks, it was found that the anxiety level of students decreased to a significant extent. Through the results of this study, chatbots developed based on counseling data can help students' psychological and emotional aspects, and it is expected to show greater effects through improved performance of future models."
MobileViT와 전이학습을 활용한 사람 자세 추정 알고리즘의 경량화,2023,"['딥러닝', '컴퓨터 비전', '키포인트 탐지', '사람 자세 추정', '비전 트랜스포머', 'Deep Learning', 'Computer Vision', 'Keypoint Detection', 'Human Pose Estimation', 'Vision Transformer']",국문 초록 정보 없음,"In this paper, we propose a model that can perform human pose estimation through a MobileViT-based model with fewer parameters and faster estimation. The based model demonstrates lightweight performance through a structure that combines features of convolutional neural networks with features of Vision Transformer. Transformer, which is a major mechanism in this study, has become more influential as its based models perform better than convolutional neural network-based models in the field of computer vision. Similarly, in the field of human pose estimation, Vision Transformer-based ViTPose maintains the best performance in all human pose estimation benchmarks such as COCO, OCHuman, and MPII. However, because Vision Transformer has a heavy model structure with a large number of parameters and requires a relatively large amount of computation, it costs users a lot to train the model. Accordingly, the based model overcame the insufficient Inductive Bias calculation problem, which requires a large amount of computation by Vision Transformer, with Local Representation through a convolutional neural network structure. Finally, the proposed model obtained a mean average precision of 0.694 on the MS COCO benchmark with 3.28 GFLOPs and 9.72 million parameters, which are 1/5 and 1/9 the number compared to ViTPose, respectively."
문서 단위 기계 번역 성능 향상을 위한 데이터 증강 기법,2023,"['기계 번역', '인공신경망 번역', '문서 단위 기계 번역', 'G-transformer', '데이터 증강', 'machine translation', 'Neural Machine Translation(NMT)', 'document-level machine translation', 'G-transformer', 'data augmentation']","최근 전체 문서의 문맥을 파악해 자연스러운 번역을 하기 위한 문서 단위 기계 번역 연구가 활발히 이루어지고 있다. 본 논문에서는 문서 전체의 문맥을 고려할 수 있는 G-Transformer를 한국어-영어 문서 단위 기계 번역에 적용하고, 문서 단위 병렬 코퍼스의 부족 문제를 해결하기 위해 문서 단위 기계 번역을 위한 데이터 증강 기법을 제안한다. 실험 결과, 본 논문에서 제안한 데이터 증강 기법들을 이용하여 문서 단위 기계 번역의 성능을 기본 모델(Transformer with Sentence Level Data)에 비해 S-BLEU 4.42 향상시킬 수 있었다.","In recent years, research on machine translation of each document has been actively conducted to understand the context of the entire document and perform natural translation. In this paper, we apply G-Transformer, which can consider the context of the entire document, to Korean-English document unit machine translation, and propose a data augmentation technique for document unit machine translation to solve the problem of lack of parallel corpus per document. As a result of the experiment, S-BLEU 4.37 is able to improve the performance of document unit machine translation compared to the basic model (Transformer with Sentence Level Data) by using the data augmentation technique proposed in this paper."
어텐션 메커니즘을 활용한 철도차량 공기압축기의 이상탐지,2023,"['air compressor', 'condition monitoring', 'anomaly detection', 'LSTM', 'attention mechanism', 'transformer']",국문 초록 정보 없음,"Condition-based diagnostic analysis of air compressors of railway vehicles is essential for passenger safety and maintenance cost reduction. When conventional long short-term memory (LSTM) autoencoders are used for detecting anomalies in time series data, the input data cannot be reconstructed appropriately when the sequence length is large. Recently, the attention mechanism was proposed to solve this problem encountered in LSTM autoencoders. In the current study, the anomaly detection performance of railway vehicle air compressors was improved using the attention mechanism. The data reconstruction performance of the LSTM autoencoder model was compared with that of the LSTM autoencoder model with the attention mechanism and the transformer model comprising only the attention mechanism by using the sensor data of an air compressor of the Airport Railroad Express train in Seoul, Korea. For artificially generated abnormal data, three anomaly scenarios were successfully detected using the transformer model, which exhibited the best data reconstruction performance among the three models. The results confirmed that transformer models with attention mechanisms can detect anomalies in the air compressors of railway vehicles in a timely manner."
Comparative Evaluation Study of Deep Learning Models for Enhanced Battery SOC Prediction,2023,"['Battery', 'SOC Prediction', 'Evaluation', 'Artificial Intelligence']",국문 초록 정보 없음,"This study emphasizes the necessity of artificial intelligence for rapid and accurate battery state-of-charge (SOC) prediction, a critical parameter in battery condition prediction. We compared and evaluated time series models previously used for SOC prediction, namely LSTM, GRU, and Transformer. In addition to model comparison, we experimented with data preprocessing techniques suitable for battery SOC prediction. The study utilized NASA's aging dataset comprising different cells under various experimental conditions. A Sliding Window technique was employed to multiply data and evaluate model performance. The results showed that the GRU model most effectively predicted battery SOC without data multiplication. However, after applying the Sliding Window technique to generate more learning data, the Transformer model outperformed others with an average RMSE of 0.032 and MAE of 0.006 across all batteries. This research paves the way for advancements in AI technology based on Transformer models for improved analysis of battery conditions, which can benefit manufacturing and recycling processes."
Deep Learning-Enabled Detection of Pneumoperitoneum in Supine and Erect Abdominal Radiography: Modeling Using Transfer Learning and Semi-Supervised Learning,2023,"['Abdominal radiography', 'Deep learning', 'Artificial intelligence', 'Pneumoperitoneum', 'Transfer learning', 'Pretraining', 'Knowledge distillation']",국문 초록 정보 없음,"Objective: Detection of pneumoperitoneum using abdominal radiography, particularly in the supine position, is often challenging. This study aimed to develop and externally validate a deep learning model for the detection of pneumoperitoneum using supine and erect abdominal radiography.Materials and Methods: A model that can utilize “pneumoperitoneum” and “non-pneumoperitoneum” classes was developed through knowledge distillation. To train the proposed model with limited training data and weak labels, it was trained using a recently proposed semi-supervised learning method called distillation for self-supervised and self-train learning (DISTL), which leverages the Vision Transformer. The proposed model was first pre-trained with chest radiographs to utilize common knowledge between modalities, fine-tuned, and self-trained on labeled and unlabeled abdominal radiographs. The proposed model was trained using data from supine and erect abdominal radiographs. In total, 191212 chest radiographs (CheXpert data) were used for pre-training, and 5518 labeled and 16671 unlabeled abdominal radiographs were used for fine-tuning and self-supervised learning, respectively. The proposed model was internally validated on 389 abdominal radiographs and externally validated on 475 and 798 abdominal radiographs from the two institutions. We evaluated the performance in diagnosing pneumoperitoneum using the area under the receiver operating characteristic curve (AUC) and compared it with that of radiologists.Results: In the internal validation, the proposed model had an AUC, sensitivity, and specificity of 0.881, 85.4%, and 73.3% and 0.968, 91.1, and 95.0 for supine and erect positions, respectively. In the external validation at the two institutions, the AUCs were 0.835 and 0.852 for the supine position and 0.909 and 0.944 for the erect position. In the reader study, the readers’ performances improved with the assistance of the proposed model.Conclusion: The proposed model trained with the DISTL method can accurately detect pneumoperitoneum on abdominal radiography in both the supine and erect positions."
다중 어댑터를 이용한 교차 언어 및 스타일 기반의 제목 생성,2023,"['Title Generation', 'Cross-lingual', 'Cross-style', 'Multiple Adapter', '제목 생성', '교차 언어', '교차 스타일', '멀티 어댑터']",국문 초록 정보 없음,"The title of a document is the brief summarization of the document. Readers can easily understand a document if we provide them with its title in their preferred styles and the languages. In this research, we propose a cross-lingual and style-based title generation model using multiple adapters. To train the model, we need a parallel corpus in several languages with different styles. It is quite difficult to construct this kind of parallel corpus; however, a monolingual title generation corpus of the same style can be built easily. Therefore, we apply a zero-shot strategy to generate a title in a different language and with a different style for an input document. A baseline model is Transformer consisting of an encoder and a decoder, pre-trained by several languages. The model is then equipped with multiple adapters for translation, languages, and styles. After the model learns a translation task from parallel corpus, it learns a title generation task from monolingual title generation corpus. When training the model with a task, we only activate an adapter that corresponds to the task. When generating a cross-lingual and style-based title, we only activate adapters that correspond to a target language and a target style. An experimental result shows that our proposed model is only as good as a pipeline model that first translates into a target language and then generates a title. There have been significant changes in natural language generation due to the emergence of large-scale language models. However, research to improve the performance of natural language generation using limited resources and limited data needs to continue. In this regard, this study seeks to explore the significance of such research."
Research on Developing a Conversational AI Callbot Solution for Medical Counselling,2023,"['Shared bikes', 'Demand forecasts', 'Linear regression', 'Machine learning', 'AI']",국문 초록 정보 없음,"In this study, we explored the potential of integrating interactive AI callbot technology into the medical consultation domain as part of a broader service development initiative. Aimed at enhancing patient satisfaction, the AI callbot was designed to efficiently address queries from hospitals' primary users, especially the elderly and those using phone services. By incorporating an AI-driven callbot into the hospital's customer service center, routine tasks such as appointment modifications and cancellations were efficiently managed by the AI Callbot Agent. On the other hand, tasks requiring more detailed attention or specialization were addressed by Human Agents, ensuring a balanced and collaborative approach. The deep learning model for voice recognition for this study was based on the Transformer model and fine-tuned to fit the medical field using a pre-trained model. Existing recording files were converted into learning data to perform SSL(self-supervised learning) Model was implemented. The ANN (Artificial neural network) neural network model was used to analyze voice signals and interpret them as text, and after actual application, the intent was enriched through reinforcement learning to continuously improve accuracy. In the case of TTS(Text To Speech), the Transformer model was applied to Text Analysis, Acoustic model, and Vocoder, and Google's Natural Language API was applied to recognize intent. As the research progresses, there are challenges to solve, such as interconnection issues between various EMR providers, problems with doctor's time slots, problems with two or more hospital appointments, and problems with patient use. However, there are specialized problems that are easy to make reservations. Implementation of the callbot service in hospitals appears to be applicable immediately."
The Study on Weibull Failure Models for Oil Gap–Paper Insulation Under Pulsating DC Voltage with Temperature,2023,"['Converter transformer', 'Oil gap–paper insulation', 'Accelerated electrical–thermal aging experiment', 'Pulsating DC voltage', 'Triple-variable Weibull failure model']",국문 초록 정보 없음,"This paper presents Weibull failure models by experimental work and the step-up accelerated electrical–thermal test of oil-impregnated paper with 0.2 mm thickness is carried out under pulsating DC voltage at different temperatures. The breakdown voltage and failure time of the oil gap–paper insulation specimens at the same voltage duration is obtained by accelerated electrical–thermal aging test. The Weibull failure models of three-variable oil gap–paper insulation modified by Fallou and Ramu are established to analyze the failure data of oil gap–paper under different electric field intensities and temperatures. The statistical test method was used to evaluate the applicability and accuracy of two modified triple-variables Weibull failure models. The comprehensive test index of the Fallou-modified WFD is 1.6851, and the comprehensive test index of the Ramu-modified WFD is 1.8124. The Fallou-modified results were more accurate than Ramu-modified results in estimating the statistical distribution of failure data. Results show that the proposed triple-variable Weibull failure model could fully reflect the insulation state of oil gap–paper under the action of electric and thermal fields."
Is ChatGPT a “Fire of Prometheus” for Non-Native English-Speaking Researchers in Academic Writing?,2023,"['Large language model', 'ChatGPT', 'Generative pretrained transformer', 'Artificial intelligence', 'Academic writing', 'Publication', 'Editing']",국문 초록 정보 없음,"Large language models (LLMs) such as ChatGPT have garnered considerable interest for their potential to aid non-native English-speaking researchers. These models can function as personal, round-the-clock English tutors, akin to how Prometheus in Greek mythology bestowed fire upon humans for their advancement. LLMs can be particularly helpful for non-native researchers in writing the Introduction and Discussion sections of manuscripts, where they often encounter challenges. However, using LLMs to generate text for research manuscripts entails concerns such as hallucination, plagiarism, and privacy issues; to mitigate these risks, authors should verify the accuracy of generated content, employ text similarity detectors, and avoid inputting sensitive information into their prompts. Consequently, it may be more prudent to utilize LLMs for editing and refining text rather than generating large portions of text. Journal policies concerning the use of LLMs vary, but transparency in disclosing artificial intelligence tool usage is emphasized. This paper aims to summarize how LLMs can lower the barrier to academic writing in English, enabling researchers to concentrate on domain-specific research, provided they are used responsibly and cautiously."
Adaptive Virtual Synchronous Machine Control for Asynchronous Grid Connections,2023,"['Solid-state transformer', 'virtual synchronous machine', 'frequency-based power control', 'back-to-back converter']",국문 초록 정보 없음,"Integrating large amounts of renewable energy resources into the modern power system increases the need for fast, flexible, and robust power distribution. To offset the loss of large centralized synchronous generators, the employment of solid-state transformers can achieve power flow control and enable LV distribution grids to support the MV grid. In this paper, a frequency-based active power controller for asynchronous grid connections, established by a back-to-back converter, is used and enhanced with an adaptive power set point calculation. The controller provides fast frequency support to the MV grid while reducing the impact on the LV distribution grid. The control concept is validated by combining a Controller Hardware-in-the-Loop setup with a mathematical power system model, highlighting the impact and capability of the system to support the frequency in the MV grid."
An Online Efficiency Optimization Strategy Based on Variable-Frequency Phase-Shift Modulation for Dual-Active-Bridge Converters,2023,"['Dual-active-bridge converter', 'online efficiency optimization', 'power router', 'variable-frequency phase-shift modulation']",국문 초록 정보 없음,"The DC transformer (DCT) is a key equipment of the power router. To improve the DCT efficiency, this paper proposes a variable-frequency phase-shift modulation (VFPSM) strategy for the dual-active-bridge (DAB) converter which operates at the fixed voltage-transfer-ratio. The optimal switching frequency can be calculated by the VFPSM strategy online through the proposed loss model to ensure that the total of switching loss, conduction loss, and core loss is the minimum under different output powers. Besides, the frequency control and phase-shift control are decoupled by using the proposed frequency compensation factor, which would not affect the characteristics of the original DAB control method. Experimental results show that compared with conventional fix switching frequency control, the proposed VFSPM strategy can improve efficiency over a wide power range, and efficiency can be improved by up to 7.4%."
THE INFLUENCE OF SPECIFIC EMOTIONS IN EWOM ON DONATION DECISIONS: DIFFERENCES BETWEEN MARKETERS AND USERS BY DONATION MOTIVATION,2023,"['Empathy-Helping', 'Specific Emotions', 'Transformer-Transfer Learning', 'Tweets']",국문 초록 정보 없음,"This research investigates the impact of specific emotions in electronic word-of-mouth (eWOM) on the monthly donations received by a non-profit organization (NPO). We employ the empathy-helping (empathy-altruism) hypothesis as a theoretical foundation, proposing that donation motivations should inform eWOM fundraising appeals. To do so, we analyzed 71,462 tweets about a charity from 23,430 users, categorizing them as either marketer-generated content (MGC) or user-generated content (UGC). To automatically detect six distinct emotions in the text, we utilized a transformer-transfer learning approach for emotion detection. This model was trained in a sequential manner, starting with self-reported emotions in over 3.6 million tweets and progressing to socially agreed-upon emotion datasets to mimic social-emotional development stages. Our findings revealed that emotions prompting empathy (such as sadness in MGC) and positive empathy (like joy in UGC) positively influence donation amounts in line with the empathy-helping hypothesis. We offer insights on how social media marketers can leverage these results to create and manage tweets that boost donations. This study contributes to marketing research and practice in three ways: (1) by being the first, to our knowledge, to examine the effect of specific emotions in eWOM on donation decisions, (2) by introducing a novel machine learning model capable of detecting emotions in large-scale eWOM, and (3) by providing actionable recommendations for NPOs to increase donations via emotionally driven social media messaging. As a result, marketing managers can more effectively use social media platforms to foster emotional connections between NPOs and donors."
단상 멀티 레벨 인버터의 출력 전력 전향 보상,2023,"['Multilevel Inverter', 'Solid-State Transformer', 'Active Front-End Converter', 'Dual-Active Bridge Converter', 'Feedforward Compensation Control']",국문 초록 정보 없음,This study proposes a feedforward compensation method based on output power estimation for 2-stage single-phase multilevel inverter-based solid-state transformer. The proposed method consists of the output power estimation by phase reference of DAB (Dual Active Bridge) converter and feedforward the d-axis current reference of AFE (Active Front-End) converter. Feedback controller for AFE and DAB converters is designed based on small signal model of each converter for verifying the proposed method. Simulations and experimental results are provided to confirm the effectiveness of the proposed method.
Compact Magnetron Power Supply for Industrial Heating Applications,2023,"['Modulators', 'High Voltage', 'Transformer Design', 'RF Power']",국문 초록 정보 없음,"This paper proposes a 100 kHz Silicon, Carbide based, 8.1 kW magnetron power supply for industrial heating applications. Operation at this high frequency along with transformer optimization results in a smaller physical footprint, which is advantageous in some heating applications, such as those undertaken offshore. Simulation work using an electrical model for the Magnetron is used to demonstrate the effectiveness of the control methodology presented and the experimental verification is ongoing. This power supply can also be scaled up for use in magnetrons over 100 kW."
Investigating syntactic transfer in second language learning of neural language models,2023,"['second language acquisition language model', 'interference effect', 'negative transfer', 'positive transfer', 'cross-lingual language model']",국문 초록 정보 없음,"Second language acquisition (SLA) research has extensively delved into cross-linguistic transfer, examining the impact of the linguistic structure of a native language on the acquisition of a second language. Such transfer effect can be either positive or negative, impeding the acquisition. In this paper, we employ transfer learning as a methodology for analyzing the encoding of grammatical structure in neural language models. This approach, transfer learning, involves pre-training the neural language model which is the Transformer-based language model, BabyRoberta, on Korean as the first language (L1). Afterward, we fine-tune the model with English as a second language (L2). Our task includes using the BLiMP test suite (Warstadt, 2020), broadly known as a benchmark for measuring the syntactic ability of neural language models. This allows us to provide insights into how neural language models represent abstract syntactic structures of English, incorporating the structural inductive biases acquired from Korean."
A Novel Two-Stage Training Method for Unbiased Scene Graph Generation via Distribution Alignment,2023,"['Scene Graph Generation', 'Transformer-based Architecture', 'Distribution Alignment', 'Model-independent', 'Visual Genome Dataset.']",국문 초록 정보 없음,"Scene graphs serve as semantic abstractions of images and play a crucial role in enhancing visual comprehension and reasoning. However, the performance of Scene Graph Generation is often compromised when working with biased data in real-world situations. While many existing systems focus on a single stage of learning for both feature extraction and classification, some employ Class-Balancing strategies, such as Re-weighting, Data Resampling, and Transfer Learning from head to tail. In this paper, we propose a novel approach that decouples the feature extraction and classification phases of the scene graph generation process. For feature extraction, we leverage a transformer-based architecture and design an adaptive calibration function specifically for predicate classification. This function enables us to dynamically adjust the classification scores for each predicate category. Additionally, we introduce a Distribution Alignment technique that effectively balances the class distribution after the feature extraction phase reaches a stable state, thereby facilitating the retraining of the classification head. Importantly, our Distribution Alignment strategy is model-independent and does not require additional supervision, making it applicable to a wide range of SGG models. Using the scene graph diagnostic toolkit on Visual Genome and several popular models, we achieved significant improvements over the previous state-of-the-art methods with our model. Compared to the TDE model, our model improved mR@100 by 70.5% for PredCls, by 84.0% for SGCls, and by 97.6% for SGDet tasks."
영화확산에 대한 온라인구전의 차원 및 영향력 분석,2023,"['Movie', 'eWOM', 'Sentiment', 'BERT Model', 'Panel Error Correction Model']",국문 초록 정보 없음,"In this study, using Korean movie box office data and online review data, we explore the 4 dimensions of eWOM(electronic(or online) word-of-mouth) (volume, valence, variance, and sentiment), and their effects on the number of audiences according to the analysis method. The volume, valence, and variance of eWOM were measured by the number of reviews, average rating, and rating variance, respectively, while the sentiment of eWOM was measured by the average sentiment scores derived by the BERT(Bidirectional Encoder Representations from Transformers) model. As a result, it was recognized that we have to be careful in interpreting the analysis results using secondary data, because the units of measurement of the 4 dimensions(the number of reviews, average rating, rating variance, and average sentiment) of eWOM are not all same, and the 4 dimensions are highly correlated with each other. By comparing the estimation results of three analysis models: regression model for aggregate data, panel regression model and panel error correction model for weekly panel data, it was found that the effects of 4 dimensions of eWOM may differ depending on the analysis method. The analysis result from the panel error correction model showed that only the volume of eWOM(number of reviews) had a significant effect on the number of audiences. The results of this study empirically show that considering the characteristics of the data used and the selection of an appropriate research method are important for accurate analysis, especially when we are trying to analyze the effects of the dimensions of eWOM on the movie box office success using secondary data about online movie reviews."
온라인 비디오 개체 분할을 위한 동적 앵커 박스 기반 개체 디코딩과 위치 인지 개체 연결,2023,"['online video instance segmentation', 'transformer decoder', 'dynamic anchor box', 'deformable attention', 'instance association']",국문 초록 정보 없음,"Video instance segmentation (VIS) is a vision task that involves simultaneously detecting, classifying, segmenting, and tracking object instances in videos. In this study, we introduce dynamic anchor box and deformable attention for VIS (DAB-D-VIS), a novel transformer-based model for online VIS. To enhance the multilayer transformer-based instance decoding for each video frame, our proposed modeluses deformable attention mechanisms that focus on a small set of key sampling points. Additionally, dynamic anchor boxes are employed to explicitly represent the region of candidate instances. These two methods have already been proven to be effective for transformer-based object detection from images. Furthermore, to address the constraints of online VIS, our model incorporates a robust inter-frame instance association method. This method leverages both similarity in the contrastive embedding space and positional difference in the images between two instances. Extensive experiments conducted on the YouTube-VIS benchmark dataset validate the effectiveness of our proposed DAB-D-VIS model."
초거대 인공지능 프로세서 반도체 기술 개발 동향,2023,"['artificial intelligence', 'large language model', 'neural processing unit', 'transformer']",국문 초록 정보 없음,"The emergence of generative hyperscale artificial intelligence (AI) has enabled new services, such as image-generating AI and conversational AI based on large language models. Such services likely lead to the influx of numerous users, who cannot be handled using conventional AI models. Furthermore, the exponential increase in training data, computations, and high user demand of AI models has led to intensive hardware resource consumption, highlighting the need to develop domain-specific semiconductors for hyperscale AI. In this technical report, we describe development trends in technologies for hyperscale AI processors pursued by domestic and foreign semiconductor companies, such as NVIDIA, Graphcore, Tesla, Google, Meta, SAPEON, FuriosaAI, and Rebellions."
Universal Flux Balancing Control to Suppress Transient DC-Bias of Phase-Shift Modulated Multi-Active-Bridge Converters,2023,"['Multi-active bridge', 'phase-shift modulation', 'transient dc-bias elimination']",국문 초록 정보 없음,"As a multiport version of the bidirectional isolated dual-active bridge converter, multi-active bridge (MAB) has promising applications in energy routers for dc microgrids and more electric aircraft. The power flow control can be primarily realized by applying phase shifts between AC voltages of different ports, whereas the duty cycles of the voltages are additional degree of freedom to optimize the power loss. However, in the transient conditions, the varied phase shifts and duty cycles result in transient dcbias in the transformer winding currents and the magnetizing flux linkage, which not only imposes significant current stress on the semiconductor devices but also may saturate the high-frequency transformer. To address this issue, this paper develops a generic model of the MAB converter using the superposition theorem, and proposes a universal dynamic phase-shift control for the MAB converter to suppress the transient dc-bias. The highlights of the proposed method are 1) applicability in different modulation schemes and various transient conditions, 2) extendability to arbitrary number of ports for the MAB, and 3) simplicity with a unified analytical solution that can be implemented in an open-loop control. The effectiveness of the proposed control method is validated by simulation results of triple-active bridge and penta-active bridge converters."
비계설정 도구로서의 ChatGPT: 초등학생의 수학 논리 문제 해결 능력에 미치는 영향 평가,2023,"['Mathematical logic', 'problem-solving', 'scaffolding', 'large language model', 'generative pretrained transformer', 'ChatGPT', 'GPT-4']",국문 초록 정보 없음,"This study explored the effectiveness of personalized scaffolding using OpenAI’s large language model, ChatGPT-4.0, with the aim of enhancing elementary students’ mathematical logical problem-solving abilities. Based on Vygotsky’s socio-cultural learning theory, the influence of AI-based hint provision on students’ problem-solving abilities and self-regulated learning was analyzed. The results showed that the hints provided by ChatGPT effectively improved the students’ problem-solving levels, particularly utilizing restructuration and verification strategies efficiently. Furthermore, this AI model accurately identified the students’ errors and provided appropriate hints considering these errors. However, such effects varied depending on the difficulty of the problem and the students’ levels. These findings demonstrate the potential of AI as a tool for personalized educational support and suggest its applicability, especially in the field of mathematics education."
YOLOv8을 이용한 실시간 화재 검출 방법,2023,"['YOLOv8', 'Deep neural networks', 'Fire detection', 'Transformer', 'CNN']",국문 초록 정보 없음,"Since fires in uncontrolled environments pose serious risks to society and individuals, many researchers have been investigating technologies for early detection of fires that occur in everyday life. Recently, with the development of deep learning vision technology, research on fire detection models using neural network backbones such as Transformer and Convolution Natural Network has been actively conducted. Vision-based fire detection systems can solve many problems with physical sensor-based fire detection systems. This paper proposes a fire detection method using the latest YOLOv8, which improves the existing fire detection method. The proposed method develops a system that detects sparks and smoke from input images by training the Yolov8 model using a universal fire detection dataset. We also demonstrate the superiority of the proposed method through experiments by comparing it with existing methods."
A Sigma Converter for High-Voltage Bus Converter: Modeling and Control,2023,"['LCLCL-DCX', 'modeling', 'sigma converter', 'small signal model']",국문 초록 정보 없음,"In this article, a sigma converter for high-voltage bus converter is proposed. The sigma converter of input-series output-parallel consists of a LCLCL-DCX (DC Transformer) and a dual-transistor forward converter (DTFC). LCLCL-DCX transfers most of the energy, and it can deliver power with the fundamental component and the third harmonics, which makes it have a high efficiency. In this article, in order to further analyze the behavior of the converter and design control scheme, the small-signal model of proposed sigma converter is derived. And a voltage-mode control design is provided."
Potential Benefits and Perils of Incorporating ChatGPT to the Movement Disorders Clinic,2023,"['Benefits', 'Perils', 'ChatGPT', 'Movement Disorders Clinic']",국문 초록 정보 없음,"ChatGPT (chat.openai.com; Chat Generative Pre-trained Transformer) is an artificial intelligence (AI) language model created by the AI lab OpenAI (OpenAI Inc., San Francisco, CA, USA) that can generate contextually relevant text on many subjects. ChatGPT understands and answers to natural language input and is trained on massive amounts of text data, including books, articles, and websites"
Application of Informer for time-series NO2 prediction,2023,"['Deep learning', 'Prediction', 'LSTM', 'BI-LSTM', 'Transformer', 'Informer', '딥러닝', '예측', '장단기 메모리', '양방향 장단기 메모리', '변환기', '정보 제공자']","본 논문에서는 딥러닝 시계열 예측 모형을 평가한다. 최근 연구에 따르면 이 모형은 ARIMA와 같은 기존 예측 모형보다 성능이 우수하다고 결론짓는다. 그 중 히든 레이어에 이전 정보를 저장하는 순환 신경망이 이를 위한 예측 모형 중 하나이다. 네트워크의 그래디언트 소실 문제를 해결하기 위해 LSTM은 데이터 흐름의 반대 방향으로 숨겨진 레이어가 추가되는 BI-LSTM과 함께 순환 신경망 내부의 작은 메모리로 사용된다. 본 논문은 서울의 2018년 1월 1일부터 2022년도 1월 1일까지의 NO2 자료에 대해 Informer의 성능을 LSTM, BI-LSTM, Transformer와 비교하였다. 이에 실제 값과 예측값 사이의 평균 제곱근 오차와 평균 절대 오차를 구하였다. 그 결과 Test 데이터 (2021.09.01.~2022.01.01.)에 대해 Informer는 다른 방법에 비해 가장 높은 예측 정확도 (가장 낮은 예측 오차: 평균 제곱근 오차: 0.0167, 평균 절대 오차: 0.0138)를 보여 타 방법에 비해 그 우수성을 입증하였다. Informer는 당초 취지와 부합되게 다른 방법들이 갖고 있는 장기 시계열 예측에 있어서의 문제점을 개선하는 결과를 나타내고 있다.","In this paper, we evaluate deep learning time series forecasting models. Recent studies show that those models perform better than the traditional prediction model such as ARIMA. Among them, recurrent neural networks to store previous information in the hidden layer are one of the prediction models. In order to solve the gradient vanishing problem in the network, LSTM is used with small memory inside the recurrent neural network along with BI-LSTM in which the hidden layer is added in the reverse direction of the data flow.  In this paper, we compared the performance of Informer by comparing with other models (LSTM, BI-LSTM, and Transformer) for real Nitrogen dioxide (NO2) data. In order to evaluate the accuracy of each method, mean square root error and mean absolute error between the real value and the predicted value were obtained . Consequently, Informer has improved prediction accuracy compared with other methods."
TB-CompletionFormer : Two-branch Backbone 기반 향상된 깊이 완성 연구,2023,"['Depth completion(깊이 완성)', 'Vision Transformer(비전 트랜스포머)', 'Deep Learning (딥러닝)', 'Multi-modal Learning(멀티모달 러닝)', 'Deep Learning (딥러닝)', 'Residual Connection(잔차 연결)', 'Computer Vision (컴퓨터 비전)']",국문 초록 정보 없음,"Dense depth estimation about surrounding environments is one of the key components in autonomous driving applications such as object detection and SLAM. To predict accurate depth information, depth completion aims to predict a dense depth map from a sparse depth map by LiDAR and a RGB image by camera. In the depth completion, it is essential to effectively fuse both LiDAR and camera modalities to achieve good performance. In this paper, we propose a two-branch architecture based on Vision Transformer, which is a more powerful and accurate multi-modal backbone, to fuse different modalities from heterogeneous sensors. To fuse two modalities, the proposed model consists of two branch modules: a coarse-branch and a finebranch. Using an RGB image and a sparse depth map, the coarse-branch module generates a dense depth map focused on color information. The fine-branch module estimates a final dense depth map focused on depth information through the color-dominant depth map and the sparse depth map. Additionally, residual connections to the Joint Convolutional Attention and Transformer (JCAT) is added to preserve the feature information. As a result, experiments on NYUv2 dataset demonstrate that the proposed method outperforms the previous models."
KoGPT2를 활용한 P-tuning의 효과적 성능 향상 기법 연구,2023,"['GPT-2', 'P-tuning', 'BERT', 'Transformer', '자연어처리', '인공지능', 'GPT-2', 'P-tuning', 'BERT', 'transformer', 'natural language processing', 'AI']",국문 초록 정보 없음,"Recently, various models of natural language processing using deep learning have been introduced, and transformer-based pre-trained models, such as BERT and GPT, have become the basic models. Fine-tuning transformer-based deep learning models can achieve excellent performance by updating the parameters of the entire model. Meanwhile, the P-tuning method, which can improve performance by updating a small number of parameters, has been introduced. In this study, we propose a method of changing the prompt-encoder from the P-tuning method, which could achieve performance similar to the existing fine-tuning method, even if only a small number of parameters were updated by freezing the learning of the model parameters. KoGPT2 was used as the GPT-2 model for performance verification. As a result of classifying using NSMC and KorNLI datasets, the proposed method showed enhanced performance using NSMC and KorNLI datasets, with an improved accuracy of 4.56% and 11%, respectively, compared to the existing P-tuning method."
필터링이 적용된 이미지 증대 기법을 통한 태양전지 결함 탐지 시스템,2023,"['Filtering on Augment Image', 'Image Augmentation', 'Solar cell', 'Vision Transformer']",국문 초록 정보 없음,"The need for renewable energy is increasing due to problems such as environmental pollution and resource depletion. Solar energy is the largest type of renewable energy and is increasing. However, when a micro-crack is formed in a solar cell, it greatly affects the photovoltaic power generation system. This degrades the performance of the solar power system and can be easily damaged. In this paper, solar cell defects were detected through Electroluminescent (EL) images. The Vision Transformer-Huge model was used to extract solar cell defect features. In addition, the CIFAR-10 augmentation policy and Imagenet augmentation policy included in Auto-augment were used to increase the performance of the model for the small dataset. When using the augmentation policy, the characteristics of the solar cell defect may disappear from the data, so filtering was performed with the model learned from the original data. For filtering, only images with a prediction probability of 70% or more of augmented images through the Soft-max function were used. When comparing the performance of the proposed methods and the method learned with the original data, it showed excellent performance. As a result, the proposed model showed a performance improvement of about 5% based on accuracy, and a recall value of about 93% was obtained."
SAM Optimizer를 통한 위내시경 이미지 분류 CADx의 성능 향상 연구,2023,"['CADx', 'Gastric Diagnosis', 'Classification', 'Convolution Neural Network', 'Deep learning', 'Vision Transformer']",국문 초록 정보 없음,"Gastric cancer has a high incidence in East Asians, and the risk increases over time. Often, gastric cancer presents no early symptoms, leading to missed treatments. Consequently, in Korea, support is provided to individuals over 40 years of age who undergo gastroscopy. However, as the number of gastroscopy patients increases, doctors' fatigue rises, becoming a factor that can lead to misdiagnosis. Therefore, this paper proposes a CADx (Computer-Aided Diagnosis) system for gastric lesion classification based on ConvNeXt and ViT (Vision Transformer), applying the SAM (Sharpness Aware Minimization) optimizer. ConvNeXt is a network that achieves high performance by incorporating techniques from Swin Transformer and the latest advancements, with ResNet-50 as the base model. ViT divides the image into smaller patches and uses these patches as input to the Transformer. This allows for learning relationships between patches and ultimately leads to image classification. To address the issue of limited data in medical images, the gastric abnormal dataset was augmented using the AutoAugment policy. The SAM Optimizer is an optimization technique that detects and minimizes the ""sharpness"" of the loss function that may occur during the deep learning model's learning process. Using this method, the sensitivity of classifying abnormal and normal gastroscopy images in ConvNeXt increased from 0.7167 to 0.9583 for the original dataset and from 0.7583 to 0.9833 for the augmented dataset. ViT exhibited a significant decrease from 0.9500 to 0.7750 in the original dataset but increased from 0.9500 to 0.9583 in the augmented dataset. This demonstrates that the SAM Optimizer can effectively enhance CADx performance."
Investigation and Analysis of AI Language Models for the Development of the Nuclear Export and Import Control Associated Search System,2023,"['Nuclear export', 'Nuclear import', 'Associated search system', 'Language model', 'NEPS']",국문 초록 정보 없음,"The Nuclear Export and Import Control System (NEPS) is currently in operation for nuclear export and import control. To ensure consistent and efficient control, various computational systems are either already in place or being developed. With numerous scattered systems, it becomes crucial to integrate the databases from each to maximize their utility. In order to effectively utilize these scattered computer systems, it is necessary to integrate the databases of each system and develop an associated search system that can be used for integrated databases, so we investigated and analyzed the AI language model that can be applied to the associated search system. Language Models (LM) are primarily divided into two categories: understanding and generative. Understanding Language Models aim to precisely comprehend and analyze the provided text’s meaning. They consider the text’s bidirectional context to understand its deeper implications and are used in tasks such as text classification, sentiment analysis, question answering, and named entity recognition. In contrast, Generative Language Models focus on generating new text based on the given context. They produce new textual content continuously and are beneficial for text generation, machine translation, sentence completion, and storytelling. Given that the primary purpose of our associated search system is to comprehend user sentences or queries accurately, understanding language models are deemed more suitable. Among the understanding language models, we examined BERT and its derivatives, RoBERTa and DeBERTa. BERT (Bidirectional Encoder Representations from Transformers) uses a Bidirectional Transformer Encoder to understand the sentence context and engages in pre-training by predicting ‘MASKED’ segments. RoBERTa (A Robustly Optimized BERT Pre-training Approach) enhances BERT by optimizing its training methods and data processing. Although its core architecture is similar to BERT, it incorporates improvements such as eliminating the NSP (Next Sentence Prediction) task, introducing dynamic masking techniques, and refining training data volume, methodologies, and hyperparameters. DeBERTa (Decoding-enhanced BERT with disentangled attention) introduces a disentangled attention mechanism to the BERT architecture, calculating the relative importance score between word pairs to distribute attention more effectively and improve performance. In analyzing the three models, RoBERTa and DeBERTa demonstrated superior performance compared to BERT. However, considering factors like the acquisition and processing of training data, training time, and associated costs, these superior models may require additional efforts and resources. It’s therefore crucial to select a language model by evaluating the economic implications, objectives, training strategies, performance-assessing datasets, and hardware environments. Additionally, it was noted that by fine-tuning with methods from RoBERTa or DeBERTa based on pre-trained BERT models, the training speed could be significantly improved."
핀테크 분야에서 초거대 인공지능의 이해와 활용,2023,"['Hyperscale Artificial Intelligence', 'Fintech', 'ChatGPT', 'Social Network Analysis']",국문 초록 정보 없음,다국어 초록 정보 없음
An Attention-based Temporal Network for Parkinson's Disease Severity Rating using Gait Signals,2023,"[""Parkinson's disease"", 'Gait signals', 'Severity rating', 'Vertical ground reaction force', 'Transformer network']",국문 초록 정보 없음,"Parkinson's disease (PD) is a typical, chronic neurodegenerative disease involving the concentration of dopamine, which can disrupt motor activity and cause different degrees of gait disturbance relevant to PD severity in patients. As current clinical PD diagnosis is a complex, time-consuming, and challenging task that relays on physicians' subjective evaluation of visual observations, gait disturbance has been extensively explored to make automatic detection of PD diagnosis and severity rating and provides auxiliary information for physicians' decisions using gait data from various acquisition devices. Among them, wearable sensors have the advantage of flexibility since they do not limit the wearers' activity sphere in this application scenario. In this paper, an attention-based temporal network (ATN) is designed for the time series structure of gait data (vertical ground reaction force signals) from foot sensor systems, to learn the discriminative differences related to PD severity levels hidden in sequential data. The structure of the proposed method is illuminated by Transformer Network for its success in excavating temporal information, containing three modules: a preprocessing module to map intra-moment features, a feature extractor computing complicated gait characteristic of the whole signal sequence in the temporal dimension, and a classifier for the final decision-making about PD severity assessment. The experiment is conducted on the public dataset PDgait of VGRF signals to verify the proposed model's validity and show promising classification performance compared with several existing methods."
Assessing the performance of ChatGPT in answering questions regarding cirrhosis and hepatocellular carcinoma,2023,"['Artificial intelligence', 'Patient education as topic', 'Health communication', 'Telemedicine', 'Chronic disease management']",국문 초록 정보 없음,"Background/Aims: Patients with cirrhosis and hepatocellular carcinoma (HCC) require extensive and personalized care to improve outcomes. ChatGPT (Generative Pre-trained Transformer), a large language model, holds the potential to provide professional yet patient-friendly support. We aimed to examine the accuracy and reproducibility of ChatGPT in answering questions regarding knowledge, management, and emotional support for cirrhosis and HCC.Methods: ChatGPT’s responses to 164 questions were independently graded by two transplant hepatologists and resolved by a third reviewer. The performance of ChatGPT was also assessed using two published questionnaires and 26 questions formulated from the quality measures of cirrhosis management. Finally, its emotional support capacity was tested.Results: We showed that ChatGPT regurgitated extensive knowledge of cirrhosis (79.1% correct) and HCC (74.0% correct), but only small proportions (47.3% in cirrhosis, 41.1% in HCC) were labeled as comprehensive. The performance was better in basic knowledge, lifestyle, and treatment than in the domains of diagnosis and preventive medicine. For the quality measures, the model answered 76.9% of questions correctly but failed to specify decision-making cut-offs and treatment durations. ChatGPT lacked knowledge of regional guidelines variations, such as HCC screening criteria. However, it provided practical and multifaceted advice to patients and caregivers regarding the next steps and adjusting to a new diagnosis.Conclusions: We analyzed the areas of robustness and limitations of ChatGPT’s responses on the management of cirrhosis and HCC and relevant emotional support. ChatGPT may have a role as an adjunct informational tool for patients and physicians to improve outcomes."
정보성 동영상 요약 및 키워드 기반 영상검색 시스템,2023,"['정보성 동영상(informational video)', '동영상 요약(video summarization)', '질문 생성(question generation)', 'GPT 모델 (Generative Pre-trained Transformer)']",국문 초록 정보 없음,다국어 초록 정보 없음
EDAD: 도메인 적응과 지식 증류를 통합한 효율적 도메인 적응 증류,2023,"['Named Entity Recognition', 'Natural Language Processing', 'Domain Adaptation', 'Knowledge Distillation']",국문 초록 정보 없음,"In the field of natural language processing, a lot of progress has been made with the advent of Transformer having a self-attention mechanism. At the same time, the recently increasing model size causes difficulties in deploying the model for online serving that requires fast inference. To address this issue, one can employ model compression techniques when a target domain is coherent with the training corpus (i.e., a general domain) of pre-trained models such as BERT. However, the additional domain adaptation step is required along with model compression when we leverage such pre-trained models for special target domains such as medicine, law, finance, etc. In this paper, we propose an Efficient Domain Adaptive Distillation (EDAD) method to efficiently create a lightweight model capable of fast inference for a target domain by integrating knowledge distillation, which is one of the popular model compression methods, and domain adaptation processes. Experimental results demonstrate that EDAD can train a compact model for a target domain with much lower computational costs by integrating the two individual processes, adaptation and compression, into a single process and shows comparable performance with existing methods for named entity recognition (NER) tasks in the medical domain."
Comparative Global Loss Analysis in low Frequency Range of Three-Phase Diode Front End (DFE) and Active Front End (AFE) Rectifier Systems,2023,"['Global Loss', 'Active-Front-End rectifier', 'Diode-Front-End rectifier', 'Total Harmonic Distortion', 'Transformer model', 'Cable model']",국문 초록 정보 없음,"Both Diode Front End (DFE) and Active Front End (AFE) rectifiers are widely used in Adjustable Speed Drives (ASD). Besides the size and cost, another concern about AFE systems are more power losses in AFE rectifiers compared to DFE rectifiers. However, due to the low Total Harmonic Distortion (THD), when considering global losses, AFE systems can be advantageous. So far there is no work published on a comparative study of global losses between AFE and DFE systems. In this work, a simulation model in circuit level and a calculation approach are proposed to estimate the global losses of the whole system in both full load and part load conditions. The results of a 8 kW system reveal that AFE systems provide a better efficiency and both the simulation model and the calculation approach can be used to estimate overall losses."
주식데이터를 활용한 Attention과 LSTM의 성능 비교,2023,"['time series prediction', 'stock price prediction', 'dataset analysis', 'performance comparison']",국문 초록 정보 없음,"In recent years, there has been significant progress in the fields of Attention-based Transformers[1, 2]. In this paper, we compare the performance of an Attention encoder and an LSTM[3] for the stock prediction task, evaluating the performance based on the models size and input length. We utilize stock history data for Apple, Samsung, and Amazon, obtained from Kaggle. The Attention Encoder model outperforms the LSTM baseline model on all datasets. This demonstrates the superior performance of the Attention Encoder model. Additionally, increasing the input sequence dimension of the Attention Encoder network leads to improved performance. However, there is a concern regarding overfitting of the Attention Encoder model due to the rapid changes in the data."
Corroded and loosened bolt detection of steel bolted joint based on improved you only look once network and line segment detector,2023,"['corroded and loosened bolt detection', 'improved YOLOv5s', 'linear segment detector', 'steel bolted joints', 'vision transformer']",국문 초록 정보 없음,"Steel bolted joint is an important part of steel structure, and its damage directly affects the bearing capacity and durability of steel structure. Currently, the existing research mainly focuses on the identification of corroded bolts and corroded bolts respectively, and there are few studies on multiple states. A detection framework of corroded and loosened bolts is proposed in this study, and the innovations can be summarized as follows: (i) Vision Transformer (ViT) is introduced to replace the third and fourth C3 module of you-only-look-once version 5s (YOLOv5s) algorithm, which increases the attention weights of feature channels and the feature extraction capability. (ii) Three states of the steel bolts are considered, including corroded bolt, bolt missing and clean bolt. (iii) Line segment detector (LSD) is introduced for bolt rotation angle calculation, which realizes bolt looseness detection. The improved YOLOv5s model was validated on the dataset, and the mean average precision (mAP) was increased from 0.902 to 0.952. In terms of a lab-scale joint, the performance of the LSD algorithm and the Hough transform was compared from different perspective angles. The error value of bolt loosening angle of the LSD algorithm is controlled within 1.09%, less than 8.91% of the Hough transform. Furthermore, the proposed framework was applied to fullscale joints of a steel bridge in China. Synthetic images of loosened bolts were successfully identified and the multiple states were well detected. Therefore, the proposed framework can be alternative of monitoring steel bolted joints for management department."
주거환경에 대한 거주민의 만족도와 영향요인 분석 - 직방 아파트 리뷰 빅데이터와 딥러닝 기반 BERT 모형을 활용하여 -,2023,"['주거환경', '주거환경 만족 영향요인', '아파트 리뷰', '빅데이터', '감정분석', 'Residential Environment', 'Determining Factors of Residential Environment Satisfaction', 'Apartment Reviews', 'Big Data', 'Sentiment Analysis']",국문 초록 정보 없음,"Satisfaction on the residential environment is a major factor influencing the choice of residence and migration, and is directly related to the quality of life in the city. As online services of real estate increases, people’s evaluation on the residential environment can be easily checked and it is possible to analyze their satisfaction and its determining factors based on their evaluation. This means that a larger amount of evaluation can be used more efficiently than previously used methods such as surveys. This study analyzed the residential environment reviews of about 30,000 apartment residents collected from ‘Zigbang’, an online real estate service in Seoul. The apartment review of Zigbang consists of an evaluation grade on a 5-point scale and the evaluation content directly described by the dweller. At first, this study labeled apartment reviews as positive and negative based on the scores of recommended reviews that include comprehensive evaluation about apartment. Next, to classify them automatically, developed a model by using Bidirectional Encoder Representations from Transformers(BERT), a deep learning-based natural language processing model. After that, by using SHapley Additive exPlanation(SHAP), extract word tokens that play an important role in the classification of reviews, to derive determining factors of the evaluation of the residential environment. Furthermore, by analyzing related keywords using Word2Vec, priority considerations for improving satisfaction on the residential environment were suggested. This study is meaningful that suggested a model that automatically classifies satisfaction on the residential environment into positive and negative by using apartment review big data and deep learning, which are qualitative evaluation data of residents, so that it’s determining factors were derived. The result of analysis can be used as elementary data for improving the satisfaction on the residential environment, and can be used in the future evaluation of the residential environment near the apartment complex, and the design and evaluation of new complexes and infrastructure."
"Application of artificial intelligence chatbots, including ChatGPT, in education, scholarly work, programming, and content generation and its prospects: a narrative review",2023,"['Artificial intelligence', 'Literacy', 'Reproducibility of results', 'Search engine', 'Writing']",국문 초록 정보 없음,"This study aims to explore ChatGPT’s (GPT-3.5 version) functionalities, including reinforcement learning, diverse applications, and limitations. ChatGPT is an artificial intelligence (AI) chatbot powered by OpenAI’s Generative Pre-trained Transformer (GPT) model. The chatbot’s applications span education, programming, content generation, and more, demonstrating its versatility. ChatGPT can improve education by creating assignments and offering personalized feedback, as shown by its notable performance in medical exams and the United States Medical Licensing Exam. However, concerns include plagiarism, reliability, and educational disparities. It aids in various research tasks, from design to writing, and has shown proficiency in summarizing and suggesting titles. Its use in scientific writing and language translation is promising, but professional oversight is needed for accuracy and originality. It assists in programming tasks like writing code, debugging, and guiding installation and updates. It offers diverse applications, from cheering up individuals to generating creative content like essays, news articles, and business plans. Unlike search engines, ChatGPT provides interactive, generative responses and understands context, making it more akin to human conversation, in contrast to conventional search engines’ keyword-based, non-interactive nature. ChatGPT has limitations, such as potential bias, dependence on outdated data, and revenue generation challenges. Nonetheless, ChatGPT is considered to be a transformative AI tool poised to redefine the future of generative technology. In conclusion, advancements in AI, such as ChatGPT, are altering how knowledge is acquired and applied, marking a shift from search engines to creativity engines. This transformation highlights the increasing importance of AI literacy and the ability to effectively utilize AI in various domains of life."
"ChatGPT 시대, 의료 인공지능이 의사를 대신할 수 있을까?",2023,"['Artificial intelligence', 'ChatGPT', 'Evidence-based medicine', '의료 인공지능', '근거중심의학']",국문 초록 정보 없음,"Whether artificial intelligence (AI) can replace the role of doctors has seriously been discussed since the appearance of Chat Generative Pretrained Transformer (ChatGPT). Unlike past expectations that limit the role of AI in the medical field, doctors seem to both admire and be threatened by recent advances in AI models. Because previously developed and approved AI models have been managed and reviewed by medical experts during the preparation, refining, annotation, and verification of data, those systems can be considered to be based on evidence-based medicine. However, the current version of the ChatGPT model derives the most meaningful results from unverified open data. This approach enhances the accessibility to new information but is significantly different from the methodology of evidence-based medicine. Like all AI models developed to date, ChatGPT needs a system that can be rigorously verified and regulated by doctors to facilitate its use in the medical field. (Korean J Med 2023;98:99-101)"
A Study on Enhancing Keyword Extraction Productivity Using Natural Language Processing for Metadata Generation of FEPs,2023,"['Features', 'Events', 'and processes (FEPs)', 'Keyword extraction', 'Natural language processing', 'PAPiRUS']",국문 초록 정보 없음,"The development of Features, Events, and Processes (FEPs) and scenarios, which consider the longterm evolution of repository, is underway, along with the construction of input data and a model database for the adaptive process-based total system performance assessment framework, APro. PAPiRUS serves as an integrated information processing platform, enabling users to seamlessly access, search, and extract essential information. To enhance data usability, it is crucial to establish well-structured metadata for each dataset. Regarding FEPs, individual FEPs consist of extensive text-based data and sets of other short textual data. To enhance the searchability of these FEPs, precise keywords must be assigned to each FEP. For user convenience, the PAPiRUS FEP database contains several FEPs not only the long-term evolution FEPs developed by KAERI but also thousands of FEPs form the databases such as NEA PFEPs and Posiva FEPs. Generating keywords for thousands of FEPs proves to be a labor-intensive task. Consequently, this study explores natural language processing techniques for keyword analysis to boost the productivity of the keyword generation process. Specifically, we employ Generative Pretrained Transformer (GPT) models for keyword extraction. Our test results for keyword extraction demonstrate that, although not flawless, providing suitable prompts yields sufficiently useful keyword sets. We identified several optimal prompts and developed an Excel-based program to derive keywords from the existing FEP database using these prompts. By using the outcomes of this study, initial versions of keyword sets for thousands of FEPs can be rapidly produced and subsequently refined through expert review and editing. The generated keywords will serve as metadata within PAPiRUS."
Online Unstructured Data Analysis Models with KoBERT and Word2vec: A Study on Sentiment Analysis of Public Opinion in Korean,2023,"['KoBERT', 'Word2vec', 'Public opinion analysis', 'Sentiment classification']",국문 초록 정보 없음,"Online news articles and comments play a vital role in shaping public opinion. Numerous studies have conducted online opinion analyses using these as raw data. Bidirectional encoder representations from transformer (BERT)-based sentiment analysis of public opinion have recently attracted significant attention. However, owing to its limited linguistic versatility and low accuracy in domains with insufficient learning data, the application of BERT to Korean is challenging. Conventional public opinion analysis focuses on term frequency; hence, low-frequency words are likely to be excluded because their importance is underestimated. This study aimed to address these issues and facilitate the analysis of public opinion regarding Korean news articles and comments. We propose a method for analyzing public opinion using word2vec to increase the word-frequency-centered analytical limit in conjunction withKoBERT, which is optimized for Korean language by improving BERT. Naver news articles and comments were analyzed using a sentiment classification model developed on the KoBERT framework. The experiment demonstrated a sentiment classification accuracy of over 90%. Thus, it yields faster and more precise results than conventional methods. Words with a low frequency of occurrence, but high relevance, can be identified using word2vec."
Automatic Chinese-English Translation Algorithm based on Out-of-vocabulary Words in the Context of Cross-cultural Communication,2023,"['Cross-cultural communication', 'Out-of-vocabulary', 'Automatic translation', 'Neural network']",국문 초록 정보 없음,"In the context of cross-cultural communication, translation between languages has become increasingly important. Based on automatic Chinese–English translation, this study examined the processing of out-of-vocabulary (OOV) words. First, this paper briefly introduces two basic translation models: seq2seq and Transformer. Second, we propose a semantic-based OOV processing method, which replaces OOV words with the most similar words by calculating the semantic similarity of word vectors and then uses the source-language sentences with the replaced words to train a translation model. Compared to the seq2seq model, the Bilingual Evaluation Understudy (BLEU) values of the Transformer model were higher (37.26 for the NIST06 dataset and 30.75 for the NIST08 dataset). After OOV processing, retaining low-frequency OOV words was conducive to the improvement of BLEU scores, which were increased by 0.63 and 0.09 for NIST06 and NIST08 for the Transformer model, respectively. This shows the effectiveness of the OOV processing method. The OOV processing method could be applied to automatic Chinese–English translation."
딥러닝을 이용한 여행 리뷰 보조 작성 서비스 개발,2023,[],국문 초록 정보 없음,"In a digital age, the demand for comprehensive travel reviews on online platforms has surged. However, writing detailed reviews can be intimidating for users, impacting the quality of feedback. This paper proposes the services to assist users in creating in-depth and valuable travel reviews with Generative pre-trained transformer (GPT)model and OpenAI API. The solutions make the review writing process easier, offering topic suggestions and posing relevant questions. Therefore it not only enhances the quantity and quality of reviews but also lightens the load on users. Performance tests confirm increased efficiency, offering promising transformations for travelers, travel agencies, and product sellers. This innovative approach not only streamlines the review process but also fosters well-informed decision-making, ultimately enriching the overall travel experience."
딥러닝 기반 시설재배 생산량 예측 연구,2023,"['Paprika', 'Cucumber', 'Time-series forecasting', 'forecasting production', 'Look-back window', '.']",국문 초록 정보 없음,"Smart agriculture optimizes labor and resources and increases production efficiency through data-driven decision-making. The field of predicting production based on data collected in real time is an important technology for productivity improvement and automation. In this paper, a deep learning model for forecasting production is used using environmental and growth information of paprika and cucumber among facility horticulture smart farms. By applying MLP(Multi Layer Perceptron), RNN(Recurrent Neural Networks), LSTM(Long Short-Term Memory models), GRU(Gated Recurrent Unit), TCN(Temporal Convolution Network), Transformer, etc., which are mainly used for time-series forecasting, the size of the look-back window and the forecasted data size Various adjustments were made to analyze the results of forecasting performance according to the model. The data of the smartfarm datamart was used, and the relationship between the size of the look-back window and the size of the forecasted to be predicted was investigated for each model.Copyright Ⓒ 2023 Korean Institute of Broadcast and Media Engineers. All rights reserved.“This is an Open-Access article distributed under the terms of the Creative Commons BY-NC-ND (http://creativecommons.org/licenses/by-nc-nd/3.0) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited and not altered.”"
자율주행을 위한 정적 장면 컨텍스트 변조 기반 차량 궤적 예측 네트워크,2023,"['자율주행', '딥러닝', '궤적 예측', '판단', '컨텍스트 변조', 'Autonomous driving', 'Deep learning', 'Trajectory forecasting', 'Planning', 'Context modulation']",국문 초록 정보 없음,"In this paper, we are proposing a vehicle trajectory forecasting network based on static scene context modulation.First, in modeling the distribution over future trajectories efficiently via variational auto-encoder frameworks, we suggest using a transformer-based trajectory encoder that models the interaction between neighboring vehicles. The proposed encoder is trained to remove interaction between irrelevant vehicles, and model key interaction more efficiently. Moreover, to increase the diversity of generated trajectories, we propose using latent variables during the trajectory generation process in modulating static scene context. Then, we can use large-scale, real-world datasets like nuScenes in evaluating performance. Experimental results showed that the proposed model generates plausible and diverse future trajectories with the techniques proposed in this paper. Furthermore, it outperformed the baseline models in terms of prediction accuracy."
Window Attention 기반의 멀티 모달 객체 탐지,2023,[],국문 초록 정보 없음,"object detection is a widely used task in autonomous driving. In a real-world scenario, the object detection algorithm is strictly required to reliably operate in changing environments (i.e., weather, illumination, and seasonal changes to name a few) To overcome this problem, we propose a multi modal object detection model that utilizes both RGB and Infrared(IR) image. In particular, window attention mechanism is used to clearly derive the correlation between the two modalities. Feature map generated by each branch are fed into the transformer block. The transformer block generates attention maps by considering correlation between RGB and IR images, and they are used to refine features from each modality. Through this, we developed a transformer that attention map the location of objects using feature maps from each modality. Experimental results show that the proposed method is superior to previous method quantitatively and qualitatively."
"Unveiling the Power of Deep Learning: A Comparative Study of LSTM, BERT, and GRU for Disaster Tweet Classification",2023,"['Text mining', 'Text classification', 'Sentiment analysis', 'Supervised machine learning', 'BERT', 'GRU', 'LSTM']",국문 초록 정보 없음,"Disasters have serious effects on peoples lives and buildings. Therefore, social media platforms, such as Twitter, have become more critical. They are crucial tools for responding to and managing disasters effectively. This study examined the effectiveness of various deep learning models, such as bidirectional encoder representations from transformers (BERT), gated recurrent units (GRU), and long short-term memory (LSTM) for classifying disaster-related tweets. Twitter data related to different disasters were collected using hashtags. The data were then cleaned, preprocessed, and manually annotated by a team. The annotated data were divided into training, validation, and testing sets. The data were used to train three models based on BERT, GRU, and LSTM for the categorical classification of disaster tweets. Finally, the three models were evaluated and compared using the test data. BERT achieved an accuracy of 96.2%, making it the most effective model. In contrast, the LSTM and GRU models achieved an accuracy of 93.2% and 88.4%, respectively. These findings underscore the potential effectiveness of deep learning models in classifying disaster-related tweets, offering insights that could enhance disaster management strategies, refine social media monitoring processes, bolster public safety, and provide directions for future research."
A study on the effectiveness of intermediate features in deep learning on facial expression recognition,2023,"['Intermediate Feature', 'Artificial Intelligence', 'Facial Expression Recognition']",국문 초록 정보 없음,"The purpose of this study is to evaluate the impact of intermediate features on FER performance. To achieve this objective, intermediate features were extracted from the input images at specific layers (FM1~FM4) of the pre-trained network (Resnet-18). These extracted intermediate features and original images were used as inputs to the vision transformer (ViT), and the FER performance was compared. As a result, when using a single image as input, using intermediate features extracted from FM2 yielded the best performance (training accuracy: 94.35%, testing accuracy: 75.51%). When using the original image as input, the training accuracy was 91.32% and the testing accuracy was 74.68%. However, when combining the original image with intermediate features as input, the best FER performance was achieved by combining the original image with FM2, FM3, and FM4 (training accuracy: 97.88%, testing accuracy: 79.21%). These results imply that incorporating intermediate features alongside the original image can lead to superior performance. The findings can be referenced and utilized when designing the preprocessing stages of a deep learning model in FER. By considering the effectiveness of using intermediate features, practitioners can make informed decisions to enhance the performance of FER systems."
인터넷 댓글의 정책여론 대표성 평가: 가상자산 과세의 사례,2023,"['댓글', '인터넷 여론', '조세정책', '감성분류', '딥러닝', 'comments on internet article', 'internet public opinion', 'tax policy', 'sentiment analysis', 'deep learning']",본고는 가상자산 기타소득세 신설이라는 조세정책의 사례를 통해 인터넷 기사 댓글 내용이 조세정책에 대한 여론을 대표하는지 여부를 평가하였다. 조세정책과관련된 인터넷 기사는 누구나 쉽게 접할 수 있고 그에 대한 의견도 댓글로 자유롭게 표현할 수 있다. 따라서 기사에 대한 댓글들이 정책에 대한 국민들의 여론을나타낸다고 볼 수도 있을 것이다. 본 연구에서는 세 차례에 걸친 무작위 표본추출을 통한 주관식 설문조사를 바탕으로 트랜스포머 모형을 학습하여 가상자산 기타소득세 신설 정책에 대한 긍정/부정을 분류할 수 있는 감성분류기를 구축하고 해당 조세정책에 대한 기사 댓글에 대해 긍정/부정 비율을 구하였다. 그 결과 기사의 댓글에 나타난 정책반응은 부정의견의 비율이 전체 댓글의 80~90%로 나타났다. 이는 무작위 표본추출을 통한 주관식 설문조사의 부정비율 약 30%에 비해 상당한 수준의 편의를 보이는 것을 알 수 있다. 본 연구의 결과는 향후 조세정책뿐만 아니라 정부의 정책에 대한 국민들의 반응을 수렴하는 과정에서 기사의 댓글을 근거자료로 삼을 것인지 여부를 판단하는데 시사점을 준다.,"We evaluate whether the contents of comments on internet articles represent public opinion on tax policy through the case of income tax on cryptocurrency investment. Internet articles related to tax policy are easily accessible to anyone, and opinions can be freely expressed in comments. Therefore, it can be regarded that the comments on the article represent public opinion on the policy. In this study, based on text data collected by surveys through random sampling, we train a transformer model to build a sentiment analyzer that can classify positive/negative opinions on the new income tax policy on cryptocurrency and obtain the positive/negative ratio for the comments. As a result, we find that the rate of negative opinions in the policy responses to the comments on the articles was 80 to 90% of the total comments. It can be seen that this shows a considerable level of bias compared to the negative rate of about 30% of the open-ended survey through random sampling. The results shed light on the process of collecting public responses to government policies such as tax policies for determining whether or not to accept comments on articles as evidence of public opinion."
RelCurator: a text mining-based curation system for extracting gene–phenotype relationships specific to neurodegenerative disorders,2023,['Curation system · Deep learning · Gene–phenotype relationship · Neurodegenerative disorders'],국문 초록 정보 없음,"Background The identification of gene–phenotype relationships is important in medical genetics as it serves as a basis for precision medicine. However, most of the gene-phenotype relationship data are buried in the biomedical literature in textual form.Objective We propose RelCurator, a curation system that extracts sentences including both gene and phenotype entities related to specific disease categories from PubMed articles, provides rich additional information such as entity taggings, and predictions of gene–phenotype relationships.Methods We targeted neurodegenerative disorders and developed a deep learning model using Bidirectional Gated Recurrent Unit (BiGRU) networks and BioWordVec word embeddings for predicting gene–phenotype relationships from biomedical texts. The prediction model is trained with more than 130,000 labeled PubMed sentences including gene and phenotype entities, which are related to or unrelated to neurodegenerative disorders.Results We compared the performance of our deep learning model with those of Bidirectional Encoder Representations from Transformers (BERT), Support Vector Machine (SVM), and simple Recurrent Neural Network (simple RNN) models. Our model performed better with an F1-score of 0.96. Furthermore, the evaluation done using a few curation cases in the real scenario showed the effectiveness of our work. Therefore, we conclude that RelCurator can identify not only new causative genes, but also new genes associated with neurodegenerative disorders’ phenotype.Conclusion RelCurator is a user-friendly method for accessing deep learning-based supporting information and a concise web interface to assist curators while browsing the PubMed articles. Our curation process represents an important and broadly applicable improvement to the state of the art for the curation of gene–phenotype relationships."
Electrolytic capacitorless STATCOM with both inductive and capacitive VAR compensation modes,2023,"['STATCOM', 'Power decoupling', 'Flux cancellation method', 'VAR compensation']",국문 초록 정보 없음,"This paper proposes an effective VAR source established as a cascaded H-bridge (CHB) static compensator (STATCOM), which is based on the flux cancellation method. The conventional CHB-STATCOM uses a low-frequency large capacitor as a source. The required capacitance value of the capacitor increases as a function of the amount of VAR produced to compensate the power system. The proposed topology is based on flux cancellation. Therefore, the source of VAR in this case is not limited by the capacitors or the power decoupling transformer size. The sub-module (SM) of the proposed topology comprises of a CHB module with power decoupling circuits. The double-frequency ripple powers on the sub-module (SM) capacitors, which have a phase shift of 120° with respect to each other. The ripple powers are derived from each of the three phases toward a common magnetic core to cancel each other out. An isolated bidirectional triple port dual half-bridge converter is utilized for the flux cancellation process. In this converter, the main challenges are leakage inductances and the high voltage insulation among the three windings of the high-frequency transformer. An αβ-frame-based model is proposed, using the generalized state-space averaging method, for the flux cancellation circuit. Furthermore, the size is significantly reduced by the proposed method, since a small sub-module capacitance of a few microfarads is sufficient to operate the CHBSTATCOM. The analysis and the controller design process are presented, followed by simulation and hardware validations."
Least Squares Regression Based Ant Lion Optimizer to Solve Optimal Reactive Power and Economic Load Dispatch Problems,2023,"['ALO', 'CP line', 'ELD', 'IEEE 57 bus', 'LS-ALO', 'LVHM', 'ORPD', 'Positive sequence line', '3-Phase PI line']",국문 초록 정보 없음,"The main work of this paper is to describe the influence of transmission line modeling routine in ORPD and ELD problems by using the minimized sum of the squares of normal deviates ant lion optimization algorithm. This method applied on proposed IEEE 57 bus power systems in an effort to achieve the best performance with stable and secure operation of all systems. The proposed method in this paper has improved standard ant lion optimizer (ALO) in terms of both exploration and exploitation properties.The proposed LR-ALO method was applied to solve four versions of power system models, such as positive sequence, 3-Phase PI, and CP transmission lines based power systems, and also lumped PI lines based low voltage hardware model of power system. The effectiveness of proposed method on all power systems were used to determine the generator voltage, tap position of transformers, and reactive power injection subjected to power loss minimization in ORPD problem. Also, calculated voltage deviation, voltage stability index for propsed systems. Similarly, optimal allocation of active power generation among all generators in the system found to balance both load demand and losses subjected to minimization of total fuel cost of the power system in ELD problem. A comprehensive analysis of optimization results was shown that the possible solutions of control variables in both ORPD and ELD problems. Also, the comparison of results was shown the accuracy and effectiveness of proposed method over elitism phases of ALO and other meta heuristic algorithms."
딥러닝 기반 넙치 아가미 질병 이미지 분류 및 t-SNE를 이용한 시각화,2023,[],국문 초록 정보 없음,"Fish that live aquatic in farms are rapidly spreading the disease, so it is important to prevent the disease from occurring, and it is important to suppress it at an early stage. However, it is not easy to diagnose the disease without the help of a specialist. In this study, we deal with gill diseases that occur frequently in halibut, which is the most farmed fish in Korea. The classification performance of the latest classification models based on deep learning was compared and evaluated using halibut gill image data. As a result of the experiment, the CoatNet model, a classification model based on CNN and Transformer, showed the best performance with a test accuracy of 96.88%. In addition, t-SNE was applied to visualize and analyze the classification results."
적대적 생성 신경망 기반 비공기압 타이어 디자인 시스템,2023,"['Generative Adversarial Networks', 'Non-Pneumatic Tire', 'Design', 'OpenCV', 'AI']",국문 초록 정보 없음,"The design of non-pneumatic tires, which are created by filling the space between the wheel and the tread with elastomeric compounds or polygonal spokes, has become an important research topic in the automotive and aerospace industries. In this study, a system was designed for the design of non-pneumatic tires through the implementation of a generative adversarial network. We specifically examined factors that could impact the design, including the type of non-pneumatic tire, its intended usage environment, manufacturing techniques, distinctions from pneumatic tires, and how spoke design affects load distribution. Using OpenCV, various shapes and spoke configurations were generated as images, and a GAN model was trained on the projected GANs to generate shapes and spokes for non-pneumatic tire designs. The designed non-pneumatic tires were labeled as available or not, and a Vision Transformer image classification AI model was trained on these labels for classification purposes. Evaluation of the classification model show convergence to a near-zero loss and a 99% accuracy rate confirming the generation of non-pneumatic tire designs."
Generative Interactive Psychotherapy Expert (GIPE) Bot,2023,"['Machine Learning', 'Mental Health', 'Therapeutic Chatbot', 'Deep Learning Approaches', 'GPT-2']",국문 초록 정보 없음,"One of the objectives and aspirations of scientists and engineers ever since the development of computers has been to interact naturally with machines. Hence features of artificial intelligence (AI) like natural language processing and natural language generation were developed. The field of AI that is thought to be expanding the fastest is interactive conversational systems. Numerous businesses have created various Virtual Personal Assistants (VPAs) using these technologies, including Apple's Siri, Amazon's Alexa, and Google Assistant, among others. Even though many chatbots have been introduced through the years to diagnose or treat psychological disorders, we are yet to have a user-friendly chatbot available. A smart generative cognitive behavioral therapy with spoken dialogue systems support was then developed using a model Persona Perception (P2) bot with Generative Pre-trained Transformer-2 (GPT-2). The model was then implemented using modern technologies in VPAs like voice recognition, Natural Language Understanding (NLU), and text-to-speech. This system is a magnificent device to help with voice-based systems because it can have therapeutic discussions with the users utilizing text and vocal interactive user experience."
중량부품 지지대 보강에 의한 원자력 발전소 배터리 차져의 내진한계상태 비교,2023,"['Battery Charger(배터리 차져)', 'Electronic Cabinet(전기 캐비넷)', 'Seismic Amplification(지진 증폭)', 'High Frequency Motion(고주기진동)', 'Shaking Table Test(진동대 실험)']",국문 초록 정보 없음,"In South Korea, nuclear power plant equipments are designed and qualified according to the Nuclear Regulatory Commission (NRC) Regulatory Guide 1.60 (R.G. 1.60). However, the uniform hazard spectrum (UHS) at the nuclear site in Uljin area exhibits a higher response acceleration in the high frequency range in comparison with R.G. 1.60. In this study, a seismic limit state test considering different response spectra types was executed for battery chargers with and without reinforcement, which is a similar model to that using N.P.P. Battery charger using N.P.P. includes a massive transformer, which is normally fixed to the enclosure with brackets. Brackets should be under high seismic loads because of the transformer mass. Failure modes of the battery charger considered in this study were observed as the structural failure mode in transformer supports (bracket) and functional failure mode in major relay chattering. It seems that the reinforcement for structural failure mode can improve not only structural limit state but also reduce the chattering phenomenon under seismic conditions."
Analysis of the Parameters Affecting the Efficiency of the Wireless Power Transmission System Designed for New Generation Electric Vehicles,2023,"['Wireless power transfer (WPT)', 'Coupling factor', 'ANSYS-Maxwell', 'Efficiency']",국문 초록 정보 없음,"In magnetic resonance coupled wireless power transfer (WPT) systems, parameters were investigated in the WPT system to ensure maximum power transfer under the conditions of changing the distance between the receive coil and the transmit coil. When the distance between the transceiver coils is changed, the inductances of the system and the coupling coefficient for maximum power transfer were calculated with Maxwell-3D, which performed a solution based on the finite element method (FEM). In addition, the effect of the distance variation between the transmitter and receiver coils, the coupling coefficient (k) values, on the input inductance and power transmission was investigated. In the model developed in the ANSYS-Maxwell environment, it has been observed that the variation of the input inductance depending on the distance and therefore the common inductance between the transmitter and receiver coils can be analyzed. In addition, the effect of the coupling factor (k) on the WPT system has also been demonstrated. It has been shown that maximum power transfer can be sustained in WPT systems where the distance between the receive coil and the transmit coil varies within certain limits. Finally, the efficiency of the transformer for a close distance between the coils was also tested experimentally."
Derivation of Isolated Outputs from a Boost and Buck-Boost Topology,2023,"['Multi-port converters', 'LLC Filter', 'Pulse width modulated Swtch model', 'Pulse frequency modulation']",국문 초록 정보 없음,Multiport converters are used to power more than one load using a single conversion stage. Many applications require galvanic isolation between the input and output of the converter. This paper presents a technique using which a boost and buck-boost topology can be transformed to a multiport converter with isolated outputs. The implementation starts with identification of terminals at which transformer can be interfaced for these topologies. Selection and characterization of circuit for implementation are defined. Some design expressions on selection of components for maximum power transfer are also defined. The proposed method is experimentally validated using lab prototypes. the passive synchronization.
Magnetics Design Optimization for LLC Converter employing Machine Learning,2023,"['Bayesian optimization', 'LLC converter', 'magnetics design', 'multiphysics simulations']",국문 초록 정보 없음,"This paper proposes an intelligent and machine-learning based optimization method that targets to optimal windings layer setup for LLC converter transformer with small number of optimizing iterations. The research utilizes Bayesian optimization (BO) to guide designers toward winding layers setup that achieves the maximum efficiency at nominal operating point of LLC converters. Furthermore, a new multiphysics simulation platform is proposed for feeding efficiencies to optimizing iterations. This simulation platform is able to provide entire losses estimation, high simulation accuracy is verified by measurement. An LLC converter prototype with 33V input and 400V/250W output is implemented to demonstrate effectiveness of proposed optimization method."
The Status Quo of the English Past-Tense Debate,2023,"['English past tense', 'past-tense debate', 'words and rules', 'connectionism', 'Transformer-based modeling']",국문 초록 정보 없음,"This paper described phenomena related to the past tense in English, summarized the theories and empirical evidence to explain these phenomena. It aimed to present implications for the past tense and similar language phenomena in current deep learning models, such as ChatGPT, a modern large language model(LLM). The discussion related to the past tense in English dates back to when a special article called ""The Past-Tense Debate"" was published in Trends in Cognitive Sciences in 2002. As a result, seemingly simple language phenomena gained the interest of researchers in various fields such as linguistics, psychology, and computer science. Steven Pinker and Michael Ullman wrote a paper titled ""The past and future of the past tense,"" to which James McClelland and Karalyn Patterson responded with a paper stating, ""Words or Rules cannot exploit the regularity in exceptions."" McClelland and Patterson published a paper titled ""Rules or connections in past-tense inflections: what does the evidence rule out?"" and Pinker and Ullman countered with ""Combination and structure, not gradedness, is the issue."" At this point, twenty years later, in 2023, ChatGPT, a neural network-based AI system, has attracted global attention. In this paper, we examine past tense theories and explore whether recent neural networks like ChatGPT resolve the past tense debate or if the issues raised in that debate are still relevant in the current era of artificial intelligence."
Study on handwritten invoice recognition system,2023,"['Handwritten', 'Optical character recognition', 'Visual document understanding', 'Neural Network', 'Transformer']",국문 초록 정보 없음,"In recent years, the growing preference for contactless services has resulted in an increased demand for kiosk-based reception systems. Although several kiosks have been activated for product orders, the reception services that entail more intricate procedures and require extensive information, such as those in postal and logistics services, have not been adequately embraced. Furthermore, the conventional paper-based reception systems fail to suffice the modern requirements. Therefore, this study proposes an innovative system for automatically processing handwritten invoices. The proposed system accurately extracts invoice details from parcel images and automatically digitizes the sender and receiver information by employing a visual document understanding model. In particular, this study presents the essential construction of an optimal training dataset that is required during the fine-tuning process of neural network-based models. It is anticipated that this will dramatically enhance the performance and accuracy of systems that handle hand-written invoices, paving the way for innovative advancements in the forthcoming years."
Design Optimization and Performance Analysis of a Three-Phase Three-Level MVDC Bidirectional Isolator using Series-Connected 10kV SiC MOSFETs and 10kV SiC JBS diodes,2023,"['MVDC isolators', 'SiC MOSFETs', 'Dual Active Bridge', 'Dynamic Voltage Balancing', 'SiC JBS diodes', 'Snubber losses', 'HF Transformer']",국문 초록 정보 없음,"This research article presents a thorough investigation of the 3 Level - 3 Phase (3L-3P) Neutral Point Clamped (NPC) Dual Active Bridge (DAB) topology for designing Medium Voltage (MV) DC isolators. An equivalent circuit model is utilized to provide a detailed analysis of the switching transition at specific operating points, including α = 60° , 0 < φ < 90° (φ≠ 60° ) α = 60°, φ = 60°. These operating points not only ensure optimal performance but also minimize the Common Mode (CM) current injected into the operating environment. The article also employs the model to evaluate the minimum current required for Zero Voltage Switching (ZVS) operation, snubber current during turn transition, and the loss in the snubber resistor. Furthermore, the article proposes a dynamic voltage balancing algorithm that significantly reduces DC offset by 80% and switching losses by 50% through precise turn-off Vds mismatch across MOSFETs and Diodes within a specified limit of 100V. The accuracy of the model used in the proposed algorithm is validated through experimental turn-off waveforms of SiC MOSFET at 1.75kV. The article introduces experimental test benches to examine the impact of base plate capacitance (Cbs) on the voltage mismatch and snubber losses in the 3L pole, providing valuable insights into the behavior of the topology under various conditions. The experiments are conducted up to 6kV DC bus voltage, and the snubber loss and dynamic voltage mismatch across series connected MOSFETs and diodes are evaluated and validated through the theoretical model."
Understanding the Relationship Between Dependency of Generative AI Chatbot Service and Performance in Graduate Students : Focusing on the Moderating Effects of Hallucination,2023,"['Generative AI Chatbot', 'Hallucination', 'Habitual Behavior of GAI', 'Addictive Behavior of  GAI', 'Learning']",국문 초록 정보 없음,"This study aims to investigate generative AI (GAI) services, which are leading innovations in various domains and significantly affecting education with a range of positive and negative effects. Specifically, this study seeks to empirically examine the academic performance of graduate students who regularly use generative AI services such as ChatGPT (Generative Pre-trained Transformer). To establish the research framework and hypotheses, this study incorporates concepts from social cognitive theory and the concept of dependency. Also, this study introduces graduate students' personal and socio-environmental factors alongside the dependency variables of habitual and addictive behavior of GAI and their impact on learning and task performance. This study uses a structural methodology in order to test the hypotheses and a mixed approach that includes Artificial Neural Networks (ANN) to predict the most critical variables. We plan to collect data from 500 graduate students who actively utilize generative AI services to achieve this. Based on the outcomes of this research, this study tries to offer academic and practical implications for educational institutions and administration."
