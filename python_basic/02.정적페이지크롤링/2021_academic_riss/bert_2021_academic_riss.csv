title,date,keywords,abstract,multilingual_abstract
BERT를 활용한 초등학교 고학년의 욕설문장 자동 분류방안 연구,2021,"['사전언어학습모델', '욕설', '욕설문장', '자동분류', '초등학생', 'BERT', 'Profanity', 'Profanity Sentence', 'Automaic Classification', 'Elemrary School Student']","코로나19로 인해 초등학생이 온라인 환경에 머무는 시간이 증가함에 따라 작성하는 게시글, 댓글, 채팅의 양이 증가하였고, 타인의 감정을 상하게 하거나 욕설을 하는 등의 문제가 발생하고 있다. 네티켓을 초등학교에서 교육하고 있지만, 교육시간이 부족할 뿐 아니라 행동의 변화까지 기대하기는 어려움이 있어 자연어처리를 통한 기술적인 지원이 필요한 상황이다. 본 연구는 초등학생이 작성하는 문장에 사전언어학습 모델에 적용하여 자동으로 욕설문장을 필터링하는 실험을 진행하였다. 실험은 온라인 학습 플랫폼에서 초등학교 4-6학년의 채팅내역을 수집하였고, 채팅 내역중에 욕설로 신고되어 판정된 욕설문장을 함께 수집하여 사전학습된 언어모델을 통해 훈련하였다. 실험결과, 욕설문장을 분류한 결과 75%의 정확률을 보이는 것으로 분석되어 학습 데이터가 충분히 보완된다면, 초등학생이 사용하는 온라인 플랫폼에서 적용할 수 있음을 보여주었다.","As the amount of time that elementary school students spend online increased due to Corona 19, the amount of posts, comments, and chats they write increased, and problems such as offending others' feelings or using swear words are occurring. Netiquette is being educated in elementary school, but training time is insufficient. In addition, it is difficult to expect changes in student behavior. So, technical support through natural language processing is needed. In this study, an experiment was conducted to automatically filter profanity sentences by applying them to a pre-trained language model on sentences written by elementary school students. In the experiment, chat details of elementary school 4-6 graders were collected on an online learning platform, and general sentences and profanity sentences were trained through a pre-learned language model. As a result of the experiment, as a result of classifying profanity sentences, it was analyzed that the precision was 75%. It has been shown that if the learning data is sufficiently supplemented, it can be sufficiently applied to the online platform used by elementary school students."
BERT+ 알고리즘 기반 약물 리뷰를 활용한 약물 이상 반응 탐지,2021,"['Detection of ADRs', 'Drug Reviews', 'Sentiment Analysis', 'Named Entity Recognition', 'ADRs Dictionary', '약물 이상 반응 탐지', '약물 리뷰', '감성분석', '개체명 인식', '약물 이상 반응 사전']",,"In this paper, we present an approach for detection of adverse drug reactions from drug reviews to compensate limitations of the spontaneous adverse drug reactions reporting system. Considering negative reviews usually contain adverse drug reactions, sentiment analysis on drug reviews was performed and extracted negative reviews. After then, MedDRA dictionary and named entity recognition were applied to the negative reviews to detect adverse drug reactions. For the experiment, drug reviews of Celecoxib, Naproxen, and Ibuprofen from 5 drug review sites, and analyzed. Our results showed that detection of adverse drug reactions is able to compensate to limitation of under-reporting in the spontaneous adverse drugs reactions reporting system."
딥러닝 언어모델과 중국어 문법 ―BERT를 활용한 방향보어의 예측 모형을 중심으로,2021,"['Deep learning', 'Language model', 'BERT', 'Directional complement', 'Transfer learning', 'Pre-training', 'Fine-tuning']",,"In this study, we investigated how accurately the BERT model can predict Chinese directional complement. In addition, we analyzed which words the BERT model uses as an important clue in the Chinese directional complement inference process. According to the results of this study, it can be seen that the BERT model shows excellent performance in inferring distributional features and grammatical relationships based on transfer learning. Results of experiments with five Chinese directional complements show that the accuracy rate of predictions is quite high. In addition, as a result of analysis using the masked language model, it was found that the BERT model appropriately uses important clues to determine Chinese directional complement in context.We believe that this study is not only meaningful in the field of NLP, but also provides insight into Chinese grammar research or language education. If this methodology is properly utilized, it will be possible to establish an application system for Chinese grammar research and education. In Neural network models, sufficient language data learning allows us to predict which language expressions are more natural to use. Proper use of these advantages will give us insight into Chinese grammatical functions. This Chinese grammar prediction system will also help Chinese learners improve their skills by showing them what expressions are grammatically correct."
BERT를 활용한 상표 의견제출통지서 거절이유 분류모델 개발,2021,"['BERT', '자연어처리', '딥러닝', '워드 임베딩', '토크나이저', 'BERT', 'Natural Language Process', 'Deep Learning', 'Word Embedding', 'Tokenizer']","멀티미디어 콘텐츠의 증가와 스마트 기기의 보급으로 다양한 종류의 데이터가 폭발적으로 생산되고 있다. 특히, 텍스트 데이터는 오랜 시간 인류의 의사 표현수단이었으며, 텍스트 분석에 대한 수요 및 필요성은 다 양한 분야에서 지속적으로 증가하고 있다. 최근에는 다양한 분야에서 뛰어난 성능을 보이는 딥러닝과 텍스트 를 의미적으로 벡터화하는 워드 임베딩 (word embedding) 방식이 결합된 ELMo (embeddings from language model), GPT (generative pre-training of a language model), BERT (bidirectional encoder representations from transformers)와 같은 모델들이 개발되어왔다. 특히 구글이 개발한 BERT는 현재 자연 어처리 분야에서 가장 뛰어난 성능을 보이는 언어모델로 손꼽히고 있다. 하지만 ‘영어’나 소셜미디어 데이터 와 같은 ‘일반 텍스트’에 특화된 BERT는 ‘한국어’나 ‘R&D 문서, 지식재산권 문서 등’ 전문 분야에 특화된 텍 스트에서 최적의 성능이 구현되지 않기 때문에 전문 분야에 맞는 말뭉치(corpus)를 학습하는 등의 최적화 과 정을 통해 최적의 성능을 도출할 수 있다. 따라서 본 연구에서는 한글 상표 분야에 특화된 BERT를 개발하 기 위해 상표 의견제출통지서 내 거절이유 텍스트 말뭉치를 활용하여 상표 관련 전문 문서에 특화된 토크나 이저 (tokenizer) 모델을 학습하고 이를 BERT에 활용하였다. 제안 모델의 분류 정확도는 기존의 다국어 BERT 모델의 분류 정확도 성능보다 약 4.97%p 높은 96.89%를 기록하였다. 해당 결과를 통해 자연어 분야 에서 높은 성능을 보였던 BERT가 전문 용어로 구성된 말뭉치 데이터에서도 높은 분류 정확도를 보일 수 있 다는 것을 확인하였다.","The increase of multimedia contents and the spread of smart devices have resulted in explosive production of various kinds of data. In particular, text data has been a means of expressing human opinion for a long time, and the demand and necessity for text analysis are continuously increasing in various fields. Recently, models such as the ELMo (embeddings from language model), GPT (generating pre-training of a language model), BERT (bidirectional encoder representations from transformation) have been developed that combine deep learning, which is showing excellent performance in various fields, and the word embedding method that semantically vectorizes text. In particular, BERT developed by Google is considered to be one of the most outstanding language models in the field of natural language processing. However, BERT specialized in 'English' and plain text, such as social media data, does not implement optimal performance in specialized texts such as 'Korean' or 'R&D documents, intellectual property documents', so it can achieve optimal performance through optimization such as learning specialized corpus. Therefore, In this study, in order to develop a BERT specialized in the field of Korean trademarks, a tokenizer model specialized in trademark-related professional documents was learned using a text corpus for rejection in the trademark opinion submission notice and used for BERT. The classification accuracy of the proposed model was 96.89%, which is 4.97% higher than the classification accuracy performance of the existing multilingual BERT model. The results showed that BERT, which showed high performance in the field of natural language, could show high classification accuracy even in corpus data composed of specialized terms."
Improving BERT-based Sentiment Analysis Model using Graph-based Ranking Mechanism,2021,"['인공지능', '자연어 처리', '감정분석', '그래프 기반 메커니즘', '인공신경망', 'BERT', 'artificial intelligence', 'natural language processing', 'sentiment analysis', 'graph-based mechanism', 'neural network']","문서 처리의 자동화에 대한 필요성이 대두됨에 따라 인공지능을 통한 자연어 처리(Natural Language Processing) 분야에서 연구가 활발하게 진행되고 있다. 본 연구에서는 자연어 처리 분야 중 특히 감정분석(Sentiment Analysis) 분야에서 그래프 기반의 순위화 메커니즘을 통해 추출한 형태소, 또는 요약 기반의 벡터인 GRAB vector(GRAph-Based vector)를 제안하고 이를 통해 기존의 BERT(Bidirectional Embedding Representations from Transformers)모델에 적용한다. 이를 통하여 더욱 강인하고 성능이 향상된 GRAB-BERT 모델을 제안한다. 또한, GRAB vector가 모델에 미치는 영향을 분석하기 위하여 재귀적 인공신경망(Recurrent Neural Network) 기반 모델들과 BERT 기반 모델에 시퀀스 입력 길이를 각각 다르게 학습한 경우 GRAB vector의 적용 여부에 따른 성능을 한국어와 영어에 대하여 분석한다. 결과적으로 형태소 단위로 추출된 벡터가 BERT와 같은 병렬적으로 문자를 처리하는 모델의 경우, 더욱 강인한 학습이 가능하며 성능이 향상됨을 보인다. 추가로, BERT 기반의 모델과 반대로 재귀적 인공신경망 기반모델들의 경우 형태소 기반이 아닌 그래프 기반 요약문 추출을 통한 벡터를 적용한 경우가 더 효과적임을 보인다.","Due to the need for automated document processing, artificial intelligence research has been actively conducted in the field of natural language processing(NLP). In this paper, we propose the GRAB vector(GRAph-Based vector), which consists of vectorized keyword-based morphemes or summaries extracted from the graph-based ranking mechanism. Next, we applied the GRAB vector to the sentiment analysis task, which is an NLP task, and we proposed a more accurate and robust model, GRAB-BERT(GRAB vector-BERT model). Then, to analyze the effect of the GRAB vector on this model, we compared the performances of recurrent neural network models(RNNs) and BERT models with or without the application of the GRAB vector on both English and Korean text samples with different sequence sizes. Our results demonstrate that applying the GRAB vector to models such as BERT to process inputs in parallel improves the robustness of the model and its performance. Furthermore, unlike BERT-based models, RNN models are more effective when applying graph-based extracted summaries than when applying morpheme-based summaries."
외부 지식이 반영된 BERT를 활용한 검색 기반 대화 시스템,2021,"['딥 러닝', '자연어 처리', '검색 기반 대화 시스템', '사후 학습', '대화 응답 선택', 'deep learning', 'natural language processing', 'retrieval-based dialog system', 'BERT', 'post-training', 'response selection']","인간과 상호작용할 수 있는 대화시스템 개발은 인공지능 분야의 중요한 과제 중 하나이다. 이러한 문제를 해결하기 위해 대화 시스템에서 외부 지식을 활용하는 연구는 꾸준히 진행되어 왔다. 하지만 외부 지식을 학습하기 위해서는 구조화된 지식이 필요하며, 이를 생성하기 위해선 상당한 자원이 필요하다. 이러한 관점에서 본 연구는 검색 기반 대화 시스템에서 구조화 되지 않는 텍스트를 외부 지식으로 사용하는 모델을 제안한다. 기본 모델로 사전 학습된 언어 모델인 BERT를 사용하고 사후 학습을 통해 모델에 외부 지식을 학습시킨다. 이 후 사후 학습된 모델을 대화 응답 선택 태스크에 미세조정하여 문제를 해결한다. 기존 BERT 모델에 비해 외부 지식을 학습한 모델의 성능이 우분투 코퍼스에서 R10@1기준 1.3% 향상된 결과를 보였다.","Developing dialogue systems is a crucial aspect of artificial intelligence. Various studies have been advanced to improve the performance of the dialogue systems using external knowledge. However, well-structured external knowledge is required in order to be applied to the dialogue systems, and it needs a considerable amount of time and resources to be generated. From this point of view, this study proposes a model that uses unstructured texts as external knowledge for retrieval-based dialogue systems. We use a pre-trained language model, BERT as a base model for the response selection task, and then we train BERT with the external knowledge through post-training. Eventually, the post-trained model is fine-tuned for the dialogue response selection task. Compared to existing BERT models, the performance of the post-trained model with external knowledge is improved by 1.3% in R10@1 on ubuntu corpus."
감성 및 감정 단어 마스킹 기반 BERT와 GPT 파이프라인 방식을 통한 감정 문장 생성,2021,"['emotion analysis', 'transfer learning', 'BERT', 'GPT', 'random masking', 'sentence generation']",,"Recently, due to advances in Artificial Intelligence(AI), there have been active studies on AI Chatbot, which has reached a level where it can similarly express human emotions. If AI Chatbot can accurately understand human emotions and respond accordingly, natural conversations between chatbots and humans are possible. Large-capacity, high-quality training data for emotion analysis is needed to train a deep learning model that automatically generates sentences. In this study, we proposes a new BERT+GPT pipeline method in which an emotion word masking-based BERT is used to automatically generate large-capacity, high-quality training data as the input of GPT that is used to generate response sentences containing human emotions. Our experimental results show that the proposed method improved up to 12% accuracy, compared to the existing BERT model, showing that most response sentences generated by GPT were better in both emotion consistency and natural meaning."
Automated Essay Scoring Using Recurrence over BERT (RoBERT),2021,"['automated essay scoring', 'trait-specific essay scoring', 'essay evaluation', 'Recurrence over BERT (RoBERT)', 'hierarchical transformers']",,"This study aimed to build a system that could automate students"" English essay evaluation by using Recurrence over BERT (RoBERT), a state-of-the-art deep learning model. English essay evaluation is inherently time-consuming. It may reflect teacher bias. English teachers are usually burdened with the task of evaluating many essays in a short period of time. Automated essay scoring (AES) can solve these problems. It has the advantage of being able to evaluate essays in a short time and without bias. In this paper, the RoBERT model was trained and evaluated on Essay Set #8 of the Automated Student Assessment Prize (ASAP) dataset. The 5-fold cross validation evaluation method was used for fair comparison with the previously suggested AES models. As a result, the RoBERT model showed the highest agreement with the human raters’ resolved scores in 5 out of 6 trait scores than the previous evaluation models. The advantage of it is that it can use the pre-trained BERT model and deal with long inputs, overcoming the input size limit of the BERT model. It was confirmed that the RoBERT model works well for trait-specific evaluation of long essays. Thus, the RoBERT model can be used as an auxiliary means to automate the evaluation of students"" essays and reduce the excessive work of English teachers."
양방향 인재매칭을 위한 BERT 기반의 전이학습 모델,2021,"['HR Matching', 'Job Recommendation', 'BERT', 'Transfer Learning', 'Pre-trained Language Model', 'Fine-tuning Deep Learning Model']",,"While youth unemployment has recorded the lowest level since the global COVID-19 pandemic, SMEs(small and medium sized enterprises) are still struggling to fill vacancies. It is difficult for SMEs to find good candidates as well as for job seekers to find appropriate job offers due to information mismatch. To overcome information mismatch, this study proposes the fine-turning model for bidirectional HR matching based on a pre-learning language model called BERT(Bidirectional Encoder Representations from Transformers). The proposed model is capable to recommend job openings suitable for the applicant, or applicants appropriate for the job through sufficient pre-learning of terms including technical jargons. The results of the experiment demonstrate the superior performance of our model in terms of precision, recall, and f1-score compared to the existing content-based metric learning model. This study provides insights for developing practical models for job recommendations and offers suggestions for future research."
Zero-anaphora resolution in Korean based on deep language representation model: BERT,2021,"['attention', 'bidirectional encoder representations from transformers (BERT)', 'deep learning', 'language representation model', 'zero-anaphora resolution (ZAR)']",,"It is necessary to achieve high performance in the task of zero anaphora resolution (ZAR) for completely understanding the texts in Korean, Japanese, Chinese, and various other languages. Deep-learning-based models are being employed for building ZAR systems, owing to the success of deep learning in the recent years. However, the objective of building a high-quality ZAR system is far from being achieved even using these models. To enhance the current ZAR techniques, we fine-tuned a pretrained bidirectional encoder representations from transformers (BERT). Notably, BERT is a general language representation model that enables systems to utilize deep bidirectional contextual information in a natural language text. It extensively exploits the attention mechanism based upon the sequence-transduction model Transformer. In our model, classification is simultaneously performed for all the words in the input word sequence to decide whether each word can be an antecedent. We seek end-to-end learning by disallowing any use of hand-crafted or dependency-parsing features. Experimental results show that compared with other models, our approach can significantly improve the performance of ZAR."
A Leader’s Final Decision Classification Model Tested on Meeting Records with BERT,2021,"['분류', '리더십', '조선왕조실록', 'classification', 'leadership', 'RNN', 'BERT', 'annals of the Joseon dynasty']",,
BERT 기반 감성분석을 이용한 추천시스템,2021,"['딥러닝', '추천시스템', '감성분석', 'BERT', 'deep learning', 'recommender system', 'sentiment analysis', 'CRM']",,"If it is difficult for us to make decisions, we ask for advice from friends or people around us. When we decide to buy products online, we read anonymous reviews and buy them. With the advent of the Data-driven era, IT technologys development is spilling out many data from individuals to objects. Companies or individuals have accumulated, processed, and analyzed such a large amount of data that they can now make decisions or execute directly using data that used to depend on experts. Nowadays, the recommender system plays a vital role in determining the users preferences to purchase goods and uses a recommender system to induce clicks on web services (Facebook, Amazon, Netflix, Youtube). For example, Youtubes recommender system, which is used by 1 billion people worldwide every month, includes videos that users like, like and videos they watched. Recommended system research is deeply linked to practical business. Therefore, many researchers are interested in building better solutions. Recommender systems use the information obtained from their users to generate recommendations because the development of the provided recommender systems requires information on items that are likely to be preferred by the user. We began to trust patterns and rules derived from data rather than empirical intuition through the recommender systems. The capacity and development of data have led machine learning to develop deep learning. However, such recommender systems are not all solutions. Proceeding with the recommender systems, there should be no scarcity in all data and a sufficient amount. Also, it requires detailed information about the individual. The recommender systems work correctly when these conditions operate. The recommender systems become a complex problem for both consumers and sellers when the interaction log is insufficient. Because the sellers perspective needs to make recommendations at a personal level to the consumer and receive appropriate recommendations with reliable data from the consumers perspective.  In this paper, to improve the accuracy problem for appropriate recommendation to consumers, the recommender systems are proposed in combination with context-based deep learning. This research is to combine user-based data to create hybrid Recommender Systems. The hybrid approach developed is not a collaborative type of Recommender Systems, but a collaborative extension that integrates user data with deep learning. Customer review data were used for the data set. Consumers buy products in online shopping malls and then evaluate product reviews. Rating reviews are based on reviews from buyers who have already purchased, giving users confidence before purchasing the product. However, the recommendation system mainly uses scores or ratings rather than reviews to suggest items purchased by many users. In fact, consumer reviews include product opinions and user sentiment that will be spent on evaluation. By incorporating these parts into the study, this paper aims to improve the recommendation system. This study is an algorithm used when individuals have difficulty in selecting an item. Consumer reviews and record patterns made it possible to rely on recommendations appropriately. The algorithm implements a recommendation system through collaborative filtering. This studys predictive accuracy is measured by Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Netflix is strategically using the referral system in its programs through competitions that reduce RMSE every year, making fair use of predictive accuracy. Research on hybrid recommender systems combining the NLP approach for personalization recommender systems, deep learning base, etc. has been increasing.  Among NLP studies, sentiment analysis began to take shape in the mid-2000s as user review data increased. Sentiment analysis is a text classification task based on machine learning. The machine learning-based sentiment analysis has a disadvan"
SMERT: Single-stream Multimodal BERT for Sentiment Analysis and Emotion Detection,2021,"['자연어 처리', '멀티 모달', '감성 분석', '감정 탐지', '단일 입출력 트랜스포머', 'natural language processing', 'multimodal', 'sentiment analysis', 'emotion detection', 'single-stream transformer', 'BERT']",,
Combining Sentiment-Combined Model with Pre-Trained BERT Models for Sentiment Analysis,2021,"['사전학습모델', '감정 분석', '외적 결합', '지식 증류', '감정 자질', 'BERT', 'pre-trained model', 'sentiment analysis', 'external fusing', 'knowledge distillation', 'sentiment features']",,
"Using Vertical and Horizonal Hidden Vector of BERT, Attention-based Separated Transfer Learning Model for Dialog Response Selection",2021,"['대화 응답 선택 시스템', '전이학습 모델', 'BERT', '딥러닝', 'dialog response selection system', 'transfer learning model', 'deep learning']",,
Deep learning can contrast the minimal pairs of syntactic data,2021,"['deep learning', 'BERT', 'syntactic judgment', 'minimal pair', 'contrast']",,"The present work aims to assess the feasibility of using deep learning as a useful tool to investigate syntactic phenomena. To this end, the present study concerns three research questions: (i) whether deep learning can detect syntactically inappropriate constructions, (ii) whether deep learning’s acceptability judgments are accountable, and (iii) whether deep learning’s aspects of acceptability judgments are similar to human judgments. As a proxy for a deep learning language model, this study chooses BERT. The current paper comprises syntactically contrasted pairs of English sentences which come from the three test suites already available. The first one is 196 grammatical -ungrammatical minimal pairs from DeKeyser (2000). The second one is examples in four published syntax textbooks excerpted from Warstadt et al. (2019). The last one is extracted from Sprouse et al. (2013), which collects the examples reported in a theoretical linguistics journal, Linguistic Inquiry. The BERT models, base BERT and large BERT, are assessed by judging acceptability of items in the test suites with an evaluation metric, surprisal, which is used to measure how ‘surprised’ a model is when encountering a word in a sequence of words, i.e., a sentence. The results are analyzed in the two frameworks: directionality and repulsion. The results of directionality reveals that the two versions of BERT are overall competent at distinguishing ungrammatical sentences from grammatical ones. The statistical results of both repulsion and directionality also reveal that the two variants of BERT do not differ significantly. Regarding repulsion, correct judgments and incorrect ones are significantly different. Additionally, the repulsion of the first test suite, which is excerpted from the items for testing learners’ grammaticality judgments, is higher than the other test suites, which are excerpted from the syntax textbooks and published literature. This study compares BERT’s acceptability judgments with magnitude estimation results reported in Sprouse et al. (2013) in order to examine if deep learning’s syntactic knowledge is akin to human knowledge. The error analyses on incorrectly judged items reveal that there are some syntactic constructions that the two BERTs have trouble learning, which indicates that BERT’s acceptability judgments are distributed not randomly."
추가 사전학습 기반 지식 전이를 통한 국가 R&D 전문 언어모델 구축,2021,"['국가 R&D', '지식 전이', '사전학습 모델', 'BERT', '추가 사전학습', 'National R&D', 'Knowledge Transfer', 'Pre-trained Language Model', 'Further Pre-training']","최근 딥러닝 기술이 빠르게 발전함에 따라 국가 R&D 분야의 방대한 텍스트 문서를 다양한 관점에서 분석하기 위한 수요가 급증하고 있다. 특히 대용량의 말뭉치에 대해 사전학습을 수행한 BERT(Bidirectional Encoder Representations from Transformers) 언어모델의 활용에 대한 관심이 높아지고 있다. 하지만 국가 R&D와 같이 고도로 전문화된 분야에서 높은 빈도로 사용되는 전문어는 기본 BERT에서 충분히 학습이 이루어지지 않은 경우가 많으며, 이는 BERT를 통한 전문 분야 문서 이해의 한계로 지적되고 있다. 따라서 본 연구에서는 최근 활발하게 연구되고 있는 추가 사전학습을 활용하여, 기본 BERT에 국가 R&D 분야 지식을 전이한 R&D KoBERT 언어모델을 구축하는 방안을 제시한다. 또한 제안 모델의 성능 평가를 위해 보건의료, 정보통신 분야의 과제 약 116,000건을 대상으로 분류 분석을 수행한 결과, 제안 모델이 순수한 KoBERT 모델에 비해 정확도 측면에서 더 높은 성능을 나타내는 것을 확인하였다.","With the recent rapid development of deep learning technology, the demand for analyzing huge text documents in the national R&D field from various perspectives is rapidly increasing. In particular, interest in the application of a BERT(Bidirectional Encoder Representations from Transformers) language model that has pre-trained a large corpus is growing. However, the terminology used frequently in highly specialized fields such as national R&D are often not sufficiently learned in basic BERT. This is pointed out as a limitation of understanding documents in specialized fields through BERT. Therefore, this study proposes a method to build an R&D KoBERT language model that transfers national R&D field knowledge to basic BERT using further pre-training. In addition, in order to evaluate the performance of the proposed model, we performed classification analysis on about 116,000 R&D reports in the health care and information and communication fields. Experimental results showed that our proposed model showed higher performance in terms of accuracy compared to the pure KoBERT model."
Game Tag Extraction Model for Korean Game Classification,2021,"['Game Classification', 'Deep Learning', 'BERT', 'Word Embedding']","최근 소프트웨어 유통망(ESD)를 통해 유통되는 게임이 늘어남에 따라 사용자들이 원하는 게임을 찾는데 어려움을 겪고 있다. 이에 따라 사용자들이 원하는 게임을 찾기 쉽게 태그를 생성하는 모델의 필요성이 대두되고 있다. 본 논문에서는 태그를 생성하는 모델을 BERT를 통해 설계하였다. 태그 100개 중 가장 적합한 태그를 4개 추출하기 위해 입력된 문장에 대해 각 태그별로 이진분류를 수행하고 이진분류 당시의 Softmax값이 가장 컸던 태그 4개를 선택했다. 또한, 모델의 정확도를 위해서 약 33억 개의 다국어 단어로 학습한 pre-trained Multilingual BERT 모델과 약 5천만 개의 한국어 단어로 학습한 KoBERT 모델을 가져와 한국어 데이터로 학습(finetuning) 시켜 사용하였다. 실험에서 BERT 모델은 KoBERT 모델보다 F- 점수에서 9.19 % 더 나은 성능을 보입니다. 이는 언어 학습 데이터 세트의 크기가 특정 언어인 한국어 특성보다 더 중요하다는 것을 나타낸다.","As the number of games increases in the software distribution network (ESD), it is difficult to find the game that a user wants. Therefore, the game can be recommended based on some game keyword tags for the user. In this paper, we propose a method to automatically generate the game keyword tags from the game description with the deep learning model, BERT. To generate the appropriate game keyword tags, the proposed method extracts the 100 representative game keyword tags from a game publishing platform Steam, and it performs the binary classification per tag. Finally, it selects 4 game keyword tags with the highest Softmax scores. Considering the accuracy improvement, a Korean game description set is used for finetuning and optimization, so that it updates both the BERT model pretrained with approximately 3.3 billion multilingual words, and the KoBERT model pretrained with approximately 50 million Korean words. Experiments show that the BERT model performs 9.19 % at F-score better than the KoBERT model. It describes that the size of the training data set is much more important than the characteristics of the specific language."
Deep Learning-based Target Masking Scheme for Understanding Meaning of Newly Coined Words,2021,"['Target Masking', 'Deep Learning', 'BERT', 'Newly Coined Words', 'Sentiment Analysis', '표적 마스킹', '딥러닝', '신조어', '감성분석']","최근 대량의 텍스트 분석을 위해 딥 러닝(Deep Learning)을 활용하는 연구들이 활발히 수행되고 있으며, 특히 대량의 텍스트에 대한 학습 결과를 특정 도메인 텍스트의 분석에 적용하는 사전 학습 언어 모델(Pre-trained Language Model)이 주목받고 있다. 다양한 사전 학습 언어 모델 중 BERT(Bidirectional Encoder Representations from Transformers) 기반 모델이 가장 널리 활용되고 있으며, 최근에는 BERT의 MLM(Masked Language Model)을 활용한 추가 사전 학습(Further Pre-training)을 통해 분석 성능을 향상시키기 위한 방안이 모색되고 있다. 하지만 전통적인 MLM 방식은 신조어와 같이 새로운 단어가 포함된 문장의 의미를 충분히 명확하게 파악하기 어렵다는 한계를 갖는다. 이에 본 연구에서는 기존의 MLM을 보완하여 신조어에 대해서만 집중적으로 마스킹을 수행하는 신조어 표적 마스킹(NTM: Newly Coined Words Target Masking)을 새롭게 제안한다. 제안 방법론을 적용하여 포털 ‘N’사의 영화 리뷰 약 70만 건을 분석한 결과, 제안하는 신조어 표적 마스킹이 기존의 무작위 마스킹에 비해 감성 분석의 정확도 측면에서 우수한 성능을 보였다.","Recently, studies using deep learning to analyze a large amount of text are being actively conducted. In particular, a pre-trained language model that applies the learning results of a large amount of text to the analysis of a specific domain text is attracting attention. Among various pre-trained language models, BERT(Bidirectional Encoder Representations from Transformers)-based model is the most widely used. Recently, research to improve the performance of analysis is being conducted through further pre-training using BERT""s MLM(Masked Language Model). However, the traditional MLM has difficulties in clearly understands the meaning of sentences containing new words such as newly coined words. Therefore, in this study, we newly propose NTM(Newly coined words Target Masking), which performs masking only on new words. As a result of analyzing about 700,000 movie reviews of portal ""N"" by applying the proposed methodology, it was confirmed that the proposed NTM showed superior performance in terms of accuracy of sensitivity analysis compared to the existing random masking."
분류 정확도 향상을 위한 선택적 마스킹 기반 추가 사전 학습 기법,2021,"['감성 분석', '선택적 마스킹', '어텐션 메커니즘', 'sentiment analysis', 'BERT', 'MLM', 'selective masking', 'attention mechanism']","최근 여러 자연어 처리 분야에서 사전 학습 언어 모델인 BERT를 활용하여 분석 과제에 최적화된 텍스트 표현을 추출하려는 연구가 활발하게 이루어지고 있다. 특히 BERT의 학습 방식 중 하나인 MLM(Masked Language Model)을 활용하여 도메인 정보 또는 분석 과제 데이터를 추가 사전 학습(Further Pre-training)하는 시도가 이어지고 있다. 하지만 기존의 MLM 기법이 채택한 무작위 마스킹을 사용하여 감성 분류 과제에서 추가 사전 학습을 수행하는 경우, 분류 학습에 중요한 단서가 되는 단어가 마스킹될 수 있다는 가능성으로 인해 문장 전체에 대한 감성 정보 학습이 충분히 이루어지지 않는다는 한계가 있다. 이에 본 연구에서는 무작위 마스킹이 아닌 단서 단어를 제외하고 마스킹하는 선택적 마스킹을 통해 감성 분류 과제에 특화된 추가 사전 학습을 수행할 수 있는 방법을 제안한다. 더불어 주변 단어를 선택하기 위해 어텐션 메커니즘(Attention Mechanism)을 활용하여 단어의 감성 기여도를 측정하는 방안도 함께 제안한다. 제안 방법론을 실제 감성 댓글에 적용하여 문장 벡터를 추론하고 감성 분류 실험을 수행한 결과, 제안 방법론이 기존의 여러 비교 모델에 비해 분류 정확도 측면에서 우수한 성능을 나타냄을 확인하였다.","Recently, studies to extract text expressions optimized for analysis tasks by utilizing bidirectional encoder representations from transformers (BERT), which is a pre-training language model, are being actively conducted in various natural language processing fields. In particular, attempts are being made to further pre-train domain information or target data using masked language model (MLM), which is one of the BERT training methods. However, if further pre-training is performed with the existing random masking when performing sentiment classification, there is a limitation that sentimental nuance for the entire sentence may not be sufficiently learned if the words that are important clues to the sentiment classification are masked. Therefore, in this study, we propose an further pre-training method specialized for sentiment classification tasks which sufficiently reflect sentiment information in sentences by selective masking that excludes clue words from masking candidates. In addition, this study proposes a method to distinguish between clue words and surrounding words as the role of words by utilizing the attention mechanism. On inferring sentence vectors by applying the proposed methodology to actual sentiment comments and performing sentiment classification experiments, it was confirmed that the proposed methodology showed superior performance in terms of classification accuracy compared to several existing comparison models."
RoBERTa를 이용한 한국어 기계독해,2021,"['기계독해', '언어모델', '토큰 분리', 'RoBERTa', 'machine reading comprehension', 'language model', 'tokenizing']","기계독해는 문단에서 주어진 질문에 대한 답을 찾는 자연어처리 task이다. 최근 BERT와 같이 대량의 데이터로 학습한 언어모델을 자연어처리에 이용하는 연구가 진행되고 있다. 본 논문에서는 토크나이징 방식을 형태소와 자소 단위를 결합한 형태 등으로 변경하고 RoBERTa 학습 및 평가를 진행하여 토크나이징 방식에 따른 성능 변화를 보았다. 그리고 BERT를 수정한 RoBERTa 모델을 학습하고 기계독해를 위해 MCAF(Multi-level Co-Attention Fusion)를 결합한 모델을 제안한다. 한국어 기계독해 데이터 셋인 KorQuAD 데이터를 이용하여 실험한 결과 dev 셋에서 EM 87.62%, F1 94.61%의 성능을 보였다.","Machine reading comprehension is a natural language processing task that finds answers to a given question in a given paragraph. Currently, studies on language model trained with a large amount of data, such as BERT, for natural language processing are in progress. In this paper, we adapted a tokenizer capable of analyzing text in morpheme and grapheme level, conducted RoBERTa learning and evaluated the changes in benchmark scores depending on the variation of the tokenizing method. In addition, we have trained the RoBERTa model with a modified BERT and propose a model that combines the RoBERTa model with MCAF(Multi-level Co-Attention Fusion) for the purpose of machine reading comprehension. As a results, the experiments with KorQuAD, a korean machine reading comprehension development dataset, showed that EM is 87.62% and F1 is 94.61%."
DeepKLM - 통사 실험을 위한 전산 언어모델 라이브러리 -,2021,"['BERT', 'language model', 'surprisal', 'experimental syntax', 'corpus', '언어모델', '서프라이절', '실험통사론', '말뭉치']",,"This paper introduces DeepKLM, a deep learning library for syntactic experiments. The library enables researchers to use the state-of-the-art deep computational language model, based on BERT (Bidirectional Encoder Representations from Transformers). The library, written in Python, works to fill the masked part of a sentence with a specific token, similar to the Cloze task in the traditional language experiments. The output value of surprisal is related to human language processing in terms of speed and complexity. The library additionally provides two visualization tools of the heatmap and the attention head visualization. This article also provides two case studies of NPIs and reflexives employing the library. The library has room for improvement in that the BERT-based components are not entirely on par with those in human language sentences. Despite such limits, the case studies imply that the library enables us to assess human and deep learning machines’ language ability."
Sentiment analysis of Korean movie reviews using XLM-R,2021,"['Sentiment analysis', 'Transformer', 'BERT', 'XLM-R', 'Transfer learning', 'Fine tuning']",,"Sentiment refers to a person's thoughts, opinions, and feelings toward an object. Sentiment analysis is a process of collecting opinions on a specific target and classifying them according to their emotions, and applies to opinion mining that analyzes product reviews and reviews on the web. Companies and users can grasp the opinions of public opinion and come up with a way to do so. Recently, natural language processing models using the Transformer structure have appeared, and Google's BERT is a representative example. Afterwards, various models came out by remodeling the BERT. Among them, the Facebook AI team unveiled the XLM-R (XLM-RoBERTa), an upgraded XLM model. XLM-R solved the data limitation and the curse of multilinguality by training XLM with 2TB or more refined CC (CommonCrawl), not Wikipedia data. This model showed that the multilingual model has similar performance to the single language model when it is trained by adjusting the size of the model and the data required for training. Therefore, in this paper, we study the improvement of Korean sentiment analysis performed using a pre-trained XLM-R model that solved curse of multilinguality and improved performance."
Negative Reasons Classification for Twitter Data on US Airline Services,2021,"['Topic Classification', 'Deep Learning', 'Natural Language Processing (NLP)', 'Embedding', 'BERT']",,"Many companies try to analyze and utilize feedback on services. This can be used for improving service quality or marketing. Until now, most natural language processing studies have attempted to analyze emotions divided into positive, negative and neutral. However, in this work, specific negative reasons are extracted and classified. The dataset is a standard dataset from kaggle that uses tweet data for U.S. airline services. Tweets categorized as negative are labeled with 10 categories of negative reasons. The dataset was divided into train, validation, and test 8:1:1. The learning and classification process was largely divided into two stages. The first is to convert words and sentences into vector values. It is compared and analyzed using Doc2Vec and BERT (Bidirectional Encoder Representations from Transformers) models for embedding and vectorization. The second is to learn and classify sentences transformed into vectors by matching them with 10 negative reason classes. During this learning process, I converted the negative reason into a sentence and attached it to the back of the original text and made new data.I then used BERT's Next Sentence Prediction technique to allow further learning to be performed. This method was able to improve classification accuracy. For each dataset and classification method, metrics were computed, visualized, and compared."
Automatic Classification of the Korean Triage Acuity Scale in Simulated Emergency Rooms Using Speech Recognition and Natural Language Processing: a Proof of Concept Study,2021,"['Triage', 'Classification', 'Machine Learning', 'Natural Language Processing', 'Deep Learning']",,"Background: Rapid triage reduces the patients' stay time at an emergency department (ED). The Korean Triage Acuity Scale (KTAS) is mandatorily applied at EDs in South Korea.For rapid triage, we studied machine learning-based triage systems composed of a speech recognition model and natural language processing-based classification.Methods: We simulated 762 triage cases that consisted of 18 classes with six types of the main symptom (chest pain, dyspnea, fever, stroke, abdominal pain, and headache) and three levels of KTAS. In addition, we recorded conversations between emergency patients and clinicians during the simulation. We used speech recognition models to transcribe the conversation.Bidirectional Encoder Representation from Transformers (BERT), support vector machine (SVM), random forest (RF), and k-nearest neighbors (KNN) were used for KTAS and symptom classification. Additionally, we evaluated the Shapley Additive exPlanations (SHAP) values of features to interpret the classifiers.Results: The character error rate of the speech recognition model was reduced to 25.21% through transfer learning. With auto-transcribed scripts, support vector machine (area under the receiver operating characteristic curve [AUROC], 0.86; 95% confidence interval [CI], 0.81–0.9), KNN (AUROC, 0.89; 95% CI, 0.85–0.93), RF (AUROC, 0.86; 95% CI, 0.82–0.9) and BERT (AUROC, 0.82; 95% CI, 0.75–0.87) achieved excellent classification performance.Based on SHAP, we found “stress”, “pain score point”, “fever”, “breath”, “head” and “chest” were the important vocabularies for determining KTAS and symptoms.Conclusion: We demonstrated the potential of an automatic KTAS classification system using speech recognition models, machine learning and BERT-based classifiers."
Zero-Shot 기반 기계번역 품질 예측 연구,2021,"['기계번역 품질 예측', '인공신경망 기계번역', '제로샷', '언어 융합', '자연언어처리', 'Quality estimation', 'Neural machine translation', 'Zero-shot', 'Language convergence', 'Natural language processing']","최근 다언어모델(Cross-lingual language model)을 활용하여 한 번도 보지 못한 특정 언어의 하위 태스크를 수행하는 제로샷 교차언어 전이(Zero-shot cross-lingual transfer)에 대한 관심이 증가하고 있다. 본 논문은 기계번역 품질 예측(Quality Estimation, QE)을 학습하기 위한 데이터 구축적 측면에서의 한계점을 지적 하고, 데이터를 구축하기 어려운 상황에서도 QE를 수행할 수 있도록 제로샷 교차언어 전이를 수행한다. QE에서 제로샷을 다룬 연구는 드물며, 본 논문에서는 교차언어모델을 활용하여 영어-독일어 QE 데이터에 대해 미세조정을 실시한 후 다른 언어쌍으로의 제로샷 전이를 진행했고 이 과정에서 다양한 다언어모델을 활용하여 비교 연구를 수행했다. 또한 다양한 자원 크기로 구성된 언어쌍에 대해 제로샷 실험을 진행하고 실험 결과에 대해 언어별 언어 학적 특성 관점으로의 분석을 수행하였다. 실험결과 multilingual BART와 multillingual BERT에서 가장 높은 성능을 보였으며, 특정 언어쌍에 대해 QE 학습을 전혀 진행하지 않은 상황에서도 QE를 수행할 수 있도록 유도하였다.","Recently, there has been a growing interest in zero-shot cross-lingual transfer, which leverages cross-lingual language models (CLLMs) to perform downstream tasks that are not trained in a specific language. In this paper, we point out the limitations of the data-centric aspect of quality estimation (QE), and perform zero-shot cross-lingual transfer even in environments where it is difficult to construct QE data. Few studies have dealt with zero-shots in QE, and after fine-tuning the English-German QE dataset, we perform zero-shot transfer leveraging CLLMs. We conduct comparative analysis between various CLLMs. We also perform zero-shot transfer on language pairs with different sized resources and analyze results based on the linguistic characteristics of each language. Experimental results showed the highest performance in multilingual BART and multillingual BERT, and we induced QE to be performed even when QE learning for a specific language pair was not performed at all."
딥러닝을 활용한 모바일 어플리케이션 리뷰 분류에 관한 연구,2021,"['모바일 배달 어플리케이션', '사용자 리뷰 분류', '사용자 리뷰 분류법', 'Mobile Delivery Application', 'User review classification', 'User Reviews Taxonomy']","스마트폰과 태블릿과 같은 스마트 기기의 발달과 사용이 증가함에 따라, 모바일 기기를 기반으로 한 모바일 어플리케이션 시장이 급속도로 커지고 있다. 모바일 어플리케이션 사용자는 어플리케이션을 사용 경험을 공유하고자 리뷰를 남기는데, 이를 분석하면 소비자들의 다양한 니즈를 파악할 수 있고 어플리케이션 개발자들은 소비자들이 작성한 리뷰를 통해 애플리케이션의 개선을 위한 유용한 피드백을 받을 수 있다. 그러나 소비자들의 남기는 많은 양의 리뷰를 수작업으로 분석하기 위해서는 많은 시간과 비용을 지불해야하기 때문에 이를 최소화 할 방안을 마련할 필요성이 존재한다. 이에 본 연구에서는 구글 플레이스토어(Google PlayStore)의 배달 어플리케이션 사용자 리뷰를 수집한 후 머신러닝과 딥러닝 기법을 활용하여 어플리케이션 기능 장점, 단점, 기능 개선 요청, 버그 보고의 4가지 범주로 분류하는 방법을 제안한다. 연구 결과, Hugging Face의 pretrain된 BERT기반 Transformer모델의 성능의 경우 위의 4개의 범주에 대한 f1 score값은 차례대로 0.93, 0.51, 0.76, 0.83으로 LSTM, GRU보다 뛰어난 성능을 보인 것을 확인할 수 있었다.","With the development and use of smart devices such as smartphones and tablets increases, the mobile application market based on mobile devices is growing rapidly. Mobile application users write reviews to share their experience in using the application, which can identify consumers various needs and application developers can receive useful feedback on improving the application through reviews written by consumers. However, there is a need to come up with measures to minimize the amount of time and expense that consumers have to pay to manually analyze the large amount of reviews they leave. In this work, we propose to collect delivery application user reviews from Google PlayStore and then use machine learning and deep learning techniques to classify them into four categories like application feature advantages, disadvantages, feature improvement requests and bug report. In the case of the performance of the Hugging Face s pretrained BERT-based Transformer model, the f1 score values for the above four categories were 0.93, 0.51, 0.76, and 0.83, respectively, showing superior performance than LSTM and GRU."
Attention Capsule Network for Aspect-Level Sentiment Classification,2021,"['Capsule Network', 'Convolutional Neural Network', 'Aspect-level Sentiment Classification', 'Natural Language Processing', 'Attention Mechanism']",,"As a fine-grained classification problem, aspect-level sentiment classification predicts the sentiment polarity for different aspects in context. To address this issue, researchers have widely used attention mechanisms to abstract the relationship between context and aspects. Still, it is difficult to effectively obtain a more profound semantic representation, and the strong correlation between local context features and the aspect-based sentiment is rarely considered. In this paper, a hybrid attention capsule network for aspect-level sentiment classification (ABASCap) was proposed. In this model, the multi-head self-attention was improved, and a context mask mechanism based on adjustable context window was proposed, so as to effectively obtain the internal association between aspects and context. Moreover, the dynamic routing algorithm and activation function in capsule network were optimized to meet the task requirements. Finally, sufficient experiments were conducted on three benchmark datasets in different domains. Compared with other baseline models, ABASCap achieved better classification results, and outperformed the state-of-the-art methods in this task after incorporating pre-training BERT."
주제 어트리뷰트 모델을 이용한 주제 키워드 기반 한국어 문서 요약,2021,"['topic centric summarization', 'machine learning', 'pre-training', 'MASS', 'PPLM', '주제 키워드 기반 문서 요약', '머신 러닝', '사전 학습', 'MASS', 'PPLM']","문서 추상 요약은 요약 모델이 원문의 핵심 정보를 파악하여 새로운 요약문을 생성하는 작업이다. 이때 추상 요약 모델로 일반적인 Sequence-to-Sequence 모델을 많이 사용하였지만 여기에 핵심 정보를 잘 표현하고 요약문에 반영하기 위해 주제(topic)을 넣어 요약문을 생성하는 주제 중심 요약(Topic centric summarization)을 하는 연구가 최근에 진행되고 있다. 그러나 기존의 방법은 주제 분포(Topic distribution)를 반영하여 문장을 생성하기 위해 모델을 처음부터 학습해야 하기 때문에 사전 학습 언어 모델의 장점을 살리기 어렵다. 본 논문에서는 사전 학습 언어 모델의 장점을 살리면서 주제 키워드를 요약문에 반영하여 주제 중심 요약을 할 수 있는 방법을 제시한다. 제안하는 주제 중심 요약 방법은 기존 조건부 언어 모델(Conditional Language Model)에서 연구되었던 PPLM(Plug and Play Language Model)의 어트리뷰트 모델을 문서 요약에서 사용되는 사전 학습 Sequence-to-Sequence 모델인 MASS에 적용하여 ‘주제 키워드 기반 요약문’을 생성하는 방법이다. 제안하는 방법은 별도의 추가 학습을 요구하지 않기 때문에 MASS의 언어 능력과 파인 튜닝으로 학습한 요약 능력을 그대로 사용함과 동시에 특정 키워드를 등장시켜 주제에서 벗어나지 않는 요약문을 생성할 수 있게 한다. 제안하는 방법의 우수성을 보이기 위해 BERT+Transformer 디코더를 사용한 모델, PPLM을 적용하지 않은 MASS 모델과 한국어 요약성능을 비교하였으며 평균적으로 ROUGE와 BERTScore 모두 성능이 향상되는 것을 확인할 수 있었다.","Abstractive summarization takes original text as an input and generates a summary containing the core-information about the original text. The abstractive summarization model is mainly designed by the Sequence-to-Sequence model. To improve quality as well as coherence of summary, the topic-centric methods which contain the core information of the original text are recently proposed.However, the previous methods perform additional training steps which make it difficult to take advantage of the pre-trained language model. This paper proposes a topic-centric summarizer that can reflect topic words to a summary as well as retain the characteristics of language model by using PPLM. The proposed method does not require any additional training. To prove the effectiveness of the proposed summarizer, this paper performed summarization experiments with Korean newspaper data."
Is c-command Machine-learnable?,2021,"['c-command', 'deep learning', 'BERT', 'surprisal', 'NPI', 'reflexive anaphor']",,"Many psycholinguistic studies have tested whether pronouns and polarity items elicit additional processing cost when they are not c-commanded. The previous studies claim that the c-command constraint regulates the distribution of relevant syntactic objects. As such, the syntactic effects of the c-command relation are greatly affected by the types of licensing (e.g. quantificational binding) and reading comprehension patterns of subjects (e.g. linguistic illusion). The present study investigates the reading behavior of the language model BERT when the syntactic processing of relational information (i.e. X c-commands Y) is required. Specifically, our two experiments contrasted the BERT comprehension of a c-commanding licensor versus a non-c-commanding licensor with reflexive anaphora and negative polarity items. The analysis based on the information-theoretic measure of surprisal suggests that violations of the c-command constraint are unexpected for BERT representations. We conclude that deep learning models like BERT can learn the syntactic c-command restriction at least with respect to reflexive anaphors and negative polarity items. At the same time, BERT appeared to have some limitations in its flexibility to apply compensatory pragmatic reasoning when a non-c-commanding licensor intruded in the dependency structure."
How are Korean Neural Language Models ‘surprised’ Layerwisely?,2021,"['KR-BERT', 'KoBERT', 'linguistic anomaly', 'surprisal gap', 'layerwise', '한국어 신경망 언어모델', '언어학적 변칙', '‘놀라움’ 차이', '신경망 층별 분석']",,"Since the introduction of BERT, recent works have shown success in detecting when a word is anomalous given sentence context. Since likelihood score is not an appropriate tool in identifying the exact property of linguistic anomaly, Li et al. (2021) recently adopt Gaussian models for density estimation at intermediate layers of pretrained language models. They find that different English pretrained language models employ separate mechanisms to recognize different types of linguistic anomaly. In keeping with Li et al.‘s methodology, we probe whether Korean counterparts such as KoBERT and KR-BERT are sensitive to different levels of linguistic anomaly, just as English-based language models are. To investigate the issue concerned, we construct an experiment with a suite of test data involving morphosyntactic, semantic, and commonsense anomaly in Korean and apply the two Korean-based models to test relevant sentences. We find that KoBERT and KR-BERT show relatively higher surprisal gaps throughout layers when the anomaly is morphosyntactic than when the anomaly is semantic. By contrast, commonsense anomaly does not exhibit any surprisal gap in any layer. We thus report that, like their English counterparts, KoBERT and KR-BERT use different mechanisms to track the different types of linguistic anomaly."
부정적 감정 완화를 위한 BERGPT-chatbot,2021,"['Deep Learning', 'Sentiment Analysis', 'AI Chatbot', 'KR-BERT', 'KoGPT2-chatbot', '딥러닝', '감정 분석', 'AI챗봇']","본 연구에서는 ""레플리카""와 같은 텍스트 입력 기반의 부정적 감정 완화가 가능한 국내 인공지능 챗봇인 BERGPT-chatbot을 제안하고자 한다. BERGPT-chatbot은 KR-BERT와 KoGPT2-chatbot을 파이프라인으로 만들어 감정 완화 챗봇을 모델링하였다. KR-BERT를 통해 정제되지 않은 일상 데이터셋에 감정을 부여하고, 추가 데이터셋을 KoGPT2-chatbot을 통해 학습하는 방식이다. BERGPT-chatbot의 개발 배경은 다음과 같다. 현재 전 세계적으로 우울증 환자가 증가하고 있으며, 이는 COVID-19로 인해 장기적 실내 생활이나 대인 관계 제한으로 더욱 심각한 문제로 대두되었다. 그로 인해 부정적 감정 완화나 정신 건강 케어에 목적을 둔 국외의 인공지능 챗봇이 팬데믹 사태로 사용량이 증가하였다. 국내에서도 국외의 챗봇과 비슷한 심리 진단 챗봇이 서비스 되고 있으나, 국내의 챗봇은 텍스트 입력 기반 답변이 아닌 버튼형 답변 중심으로 국외 챗봇과 비교하였을 때 심리 진단 수준에 그쳐 아쉬운 실정이다. 따라서, BERGPT-chatbot을 통해 감정 완화에 도움을 주는 챗봇을 제안하였으며, BERGPT-chatbot과 KoGPT2-chatbot을 언어 모델의 내부 평가 지표인 ‘퍼플렉서티’를 통해 비교 분석하여 BERGPT-chatbot의 우수함을 보여주고자 한다.","In this paper, we propose a BERGPT-chatbot, a domestic AI chatbot that can alleviate negative emotions based on text input such as ‘Replika’. We made BERGPT-chatbot into a chatbot capable of mitigating negative emotions by pipelined two models, KR-BERT and KoGPT2-chatbot. We applied a creative method of giving emotions to unrefined everyday datasets through KR-BERT, and learning additional datasets through KoGPT2-chatbot. The development background of BERGPT-chatbot is as follows. Currently, the number of people with depression is increasing all over the world. This phenomenon is emerging as a more serious problem due to COVID-19, which causes people to increase long-term indoor living or limit interpersonal relationships. Overseas artificial intelligence chatbots aimed at relieving negative emotions or taking care of mental health care, have increased in use due to the pandemic. In Korea, Psychological diagnosis chatbots similar to those of overseas cases are being operated. However, as the domestic chatbot is a system that outputs a button-based answer rather than a text input-based answer, when compared to overseas chatbots, domestic chatbots remain at a low level of diagnosing human psychology. Therefore, we proposed a chatbot that helps mitigating negative emotions through BERGPT-chatbot. Finally, we compared BERGPT-chatbot and KoGPT2-chatbot through ‘Perplexity’, an internal evaluation metric for evaluating language models, and showed the superity of BERGPT-chatbot."
Personality Prediction Based on Text Analytics Using Bidirectional Encoder Representations from Transformers from English Twitter Dataset,2021,"['Personality prediction', 'Twitter', 'Big Five personality traits', 'BERT']",,"Personality traits can be inferred from a person’s behavioral patterns. One example is when writing posts on social media. Extracting information about individual personalities can yield enormous benefits for various applications such as recommendation systems, marketing, or hiring employees. The objective of this research is to build a personality prediction system that uses English texts from Twitter as a dataset to predict personality traits. This research uses the Big Five personality traits theory to analyze personality traits, which consist of openness, conscientiousness, extraversion, agreeableness, and neuroticism. Several classifiers were used in this research, such as support vector machine, convolutional neural network, and variants of bidirectional encoder representations from transformers (BERT). To improve the performance, we implemented several feature extraction techniques, such as N-gram, linguistic inquiry and word count (LIWC), word embedding, and data augmentation. The best results were obtained by fine-tuning the BERT model and using it as the main classifier of the personality prediction system. We conclude that the BERT performance could be improved by using individual tweets instead of concatenated ones."
Embedding Calculus with Nonword Properties Improves Word Sense Disambiguation,2021,"['Word Sense Disambiguation', 'Word Embedding', 'BERT', 'Embedding Calculus', 'Probing Task', 'U-WIN', 'Lexical Hierarchy']",,"The present study concerns word sense disambiguation in neural language models using the diagnostic classifiers and hierarchical lexical network. First, we conducted an experiment to see whether the neural models are capable of detecting ambiguous nouns and how they do so. Secondly, we carried out an experiment to verify whether the neural models can identify a specific sense of a lexeme and how they do so. For these experiments, we made use of Word2Vec and FastText as the fixed embedding models and BERT as the contextualized model. In addition, we examined the uniformed and weighted sum method by adding nonword properties (senses). In the case of ambiguity detection, BERT with the general embedding showed better performance than the other models. In regards to sense class detection, BERT with nonword properties showed the best performance on lexemes with numerous senses."
최소대립 문장쌍을 활용한 한국어 사전학습모델의 통사 연구 활용 가능성 검증,2021,"['deep learning', 'BERT', 'acceptability judgment', 'minimal pair', 'correlation coefficient']",,"Syntactic studies make use of the minimally pairwise sentences as an argumentation tool, because the pairs allow us to pay attention to the constraints of interest. Likewise, it is helpful to use a set of minimal pairs in deep learning-based experiments for assessing the syntactic ability of neural language models. In this context, this study verifies whether the deep learning Korean model has the ability to properly distinguish the well-formed expressions and the corresponding ill-formed expressions. In the meanwhile, this study serves to examine the feasibility of the language resource constructed by the Korean government for deep learning architecture. The research is three-fold. First, we conducted an acceptability judgment testing to verify whether and how the language resource used in this study is indeed trustworthy. The results indicate that the judgments provided in the language resource converge with the judgments of our own experiment well enough. Second, we employed four Korean models such as mBERT, KoBERT, KR-BERT, KorBERT in order to evaluate how the language resource has a potentiality to predict the well-formedness of Korean expressions. The different models yield different results, the reason of which is fully discussed. Third, we made use of an independent test-set for evaluating the deep learning systems. It turns out that the results are still challenging, which implies that the current Korean models may have room for improvement to understand the syntactic phenomena."
"A Sentiment Analysis of Men""s and Women""s Speech: Dictionary-based vs. Deep-learning-based Analysis",2021,"['BNC64', 'sentiment analysis', 'dictionary-based', 'GRU', 'BERT']",,"It is well known that the language women use is different from men""s use. In this paper, the researchers examine men""s and women""s speech through sentiment analysis. For the study of gender differences, this study uses the BNC64 corpus, whose files are selected from the British National Corpus (BNC). The BNC64 corpus is composed of 64 files, which are taken from the spoken part of the BNC. The corpus contains 32 files for male speakers and 32 files for female speakers that represent the characteristics of male vs. female differences. The study analyzed all 64 files using sentiment analysis and tried to discover the differences. It is known that there are roughly three types of approach to sentiment analysis: dictionary-based analysis, machine-learning-based analysis, and deep-learning-based analysis. This study takes the first and third kind of analyses. In the dictionary-based analysis, the researchers calculate the sentiment score (SS), sentiment word ratio (SR), and positive word ratio (PR). In the deep-learning analysis, the researchers take two different sorts of analyses: the GRU (Gated Recurrent Units) and the BERT (Bidirectional Encoder Representations from Transformers). Through the analyses, the study finds that (i) there is no significant differences in SS and SR between men and women, (ii) women usually use more positive words than men, and the differences are statistically significant, and (iii) the deep-learning-based analysis is much superior to modelling the gender differences in the sentiment analysis."
대형 사전학습 언어모델 연구에 대한 고찰,2021,"['pre-trained language model', 'fine-tuning', 'BERT', 'GPT', 'large-scale language model']",,"The pre-training language model(PLM), BERT has achieved great success in Natural Language Processing(NLP) by transferring knowledge from rich-resource pre-training task to low-resource downstream tasks. The focus of researchers’ attention on PLM is its application and performance. In this study, we were interested in large-scale PLM models with high performance. We tried to identify major issues through PLM-related researches, and to discuss on the present development situation of large-scale PLMs. First, we reviewed researches analyzing whether PLM acquires linguistic/world knowledge through pre-training task. PLM remains as a blackbox and researchers fail to identify the internal mechanism of PLM. The aim of a large-scale PLM is to make the model more general-purposed. The larger the number of parameter is, the more difficult to train the parameters. Instead of fine-tuning method, few-shot method was proposed, but further study remains to be done. The final question is whether making larger PLM is the right research direction. Due to the overconsumption of computing and energy resources by PLM, we propose the development of eco-friendly and explainable PLM. We hope this study will provide linguists with an opportunity to understand recent technological research trends and to find joint work in the field of NLP."
발화 의도 예측 및 슬롯 채우기 복합 처리를 위한 한국어 데이터셋 개발,2021,"['자연어처리', '발화 이해', '발화 의도 예측', '슬롯 채우기', '데이터셋', 'BERT', 'Natural Language Processing', 'Spoken Language Understanding', 'Intent Classification', 'Slot Filling', 'Dataset', 'BERT']","사람의 발화 내용을 이해하도록 하는 언어 인식 시스템은 주로 영어로 연구되어 왔다. 본 논문에서는 시스템과 사용자의 대화 내용을 수집한 말뭉치를 바탕으로 언어 인식 시스템을 훈련시키고 평가할 때 사용할 수 있는 한국어 데이터셋을 개발하고, 관련 통계를 제시한다. 본 데이터셋은 식당 예약이라는 고정된 주제 안에서 사용자의 발화 의도와 슬롯 채우기를 해야 하는 데이터셋이다. 본 데이터셋은 6857개의 한국어 문장으로 이루어져 있으며, 표기된 단어 슬롯 의 종류는 총 7개이다. 본 데이터셋에서 표기된 발화의 종류는 총 5개이며, 문장의 발화 내용에 따라 최대 2개까지 동시에 기입되어 있다. 영어권에서 연구된 모델을 본 데이터셋에 적용시켜 본 결과, 발화 의도 추측 정확도는 조금 하락 하였고, 슬롯 채우기 F1 점수는 크게 차이나는 모습을 보였다.","Spoken language understanding, which aims to understand utterance as naturally as human would, are mostly focused on English language. In this paper, we construct a Korean language dataset for spoken language understanding, which is based on a conversational corpus between reservation system and its user. The domain of conversation is limited to restaurant reservation. There are 7 types of slot tags and 5 types of intent tags in 6857 sentences. When a model proposed in English-based research is trained with our dataset, intent classification accuracy decreased a little, while slot filling F1 score decreased significantly."
CNN 보조 손실을 이용한 차원 기반 감성 분석,2021,"['온라인 리뷰 분석', '차원 기반 감성 분석', 'Online review analysis', 'ABSA', 'TASD', 'CNN', 'BERT']",,"Aspect Based Sentiment Analysis (ABSA), which analyzes sentiment based on aspects that appear in the text, is drawing attention because it can be used in various business industries. ABSA is a study that analyzes sentiment by aspects for multiple aspects that a text has. It is being studied in various forms depending on the purpose, such as analyzing all targets or just aspects and sentiments. Here, the aspect refers to the property of a target, and the target refers to the text that causes the sentiment. For example, for restaurant reviews, you could set the aspect into food taste, food price, quality of service, mood of the restaurant, etc. Also, if there is a review that says, The pasta was delicious, but the salad was not, the words steak and salad, which are directly mentioned in the sentence, become the “target.”  So far, in ABSA, most studies have analyzed sentiment only based on aspects or targets. However, even with the same aspects or targets, sentiment analysis may be inaccurate. Instances would be when aspects or sentiment are divided or when sentiment exists without a target. For example, sentences like, Pizza and the salad were good, but the steak was disappointing. Although the aspect of this sentence is limited to “food,” conflicting sentiments coexist. In addition, in the case of sentences such as Shrimp was delicious, but the price was extravagant, although the target here is “shrimp,” there are opposite sentiments coexisting that are dependent on the aspect. Finally, in sentences like The food arrived too late and is cold now. there is no target (NULL), but it transmits a negative sentiment toward the aspect service. Like this, failure to consider both aspects and targets – when sentiment or aspect is divided or when sentiment exists without a target – creates a dual dependency problem.  To address this problem, this research analyzes sentiment by considering both aspects and targets (Target-Aspect-Sentiment Detection, hereby TASD). This study detected the limitations of existing research in the field of TASD: local contexts are not fully captured, and the number of epochs and batch size dramatically lowers the F1-score. The current model excels in spotting overall context and relations between each word. However, it struggles with phrases in the local context and is relatively slow when learning. Therefore, this study tries to improve the models performance.  To achieve the objective of this research, we additionally used auxiliary loss in aspect-sentiment classification by constructing CNN(Convolutional Neural Network) layers parallel to existing models. If existing models have analyzed aspect-sentiment through BERT encoding, Pooler, and Linear layers, this research added CNN layer-adaptive average pooling to existing models, and learning was progressed by adding additional loss values for aspect-sentiment to existing loss. In other words, when learning, the auxiliary loss, computed through CNN layers, allowed the local context to be captured more fitted. After learning, the model is designed to do aspect-sentiment analysis through the existing method.  To evaluate the performance of this model, two datasets, SemEval-2015 task 12 and SemEval-2016 task 5, were used and the f1-score increased compared to the existing models. When the batch was 8 and epoch was 5, the difference was largest between the F1-score of existing models and this study with 29 and 45, respectively. Even when batch and epoch were adjusted, the F1-scores were higher than the existing models. It can be said that even when the batch and epoch numbers were small, they can be learned effectively compared to the existing models. Therefore, it can be useful in situations where resources are limited.  Through this study, aspect-based sentiments can be more accurately analyzed. Through various uses in business, such as development or establishing marketing strategies, both consumers and sellers will be able to"
ELMo 임베딩 기반 문장 중요도를 고려한 중심 문장 추출방법,2021,"['추출 요약', '중심 문장', '문장 중요도', '중심 문장 특성', '임베딩', 'Extractive Summarization', 'Topic Sentence', 'Sentence Importance', 'Topic Sentence Feature', 'Embedding']","본 연구는 뉴스 기사에서 기사문을 구성하는 문장별 중요도를 고려하여 요약문을 추출하는 방법에 관한 것으로 문장 중요도에 영향을 주는 특성으로 중심 문장(Topic Sentence)일 확률, 기사 제목 및 다른 문장과의 유사도, 문장 위치에 따른 가중치를 추출하여 문장 중요도를 계산하는 방법을 제안한다. 이때, 중심 문장(Topic Sentence)은 일반 문장과는 구별되는 특징을 가질 것이라는 가설을 세우고, 딥러닝 기반 분류 모델을 학습시켜 입력 문장에 대한 중심 문장 확률값을 구한다. 또한 사전학습된 ELMo 언어 모델을 활용하여 문맥 정보를 반영한 문장 벡터값을 기준으로 문장간 유사도를 계산하여 문장 특성으로 추출한다. LSTM 및 BERT 모델의 중심 문장 분류성능은 정확도 93%, 재현율 96.22%, 정밀도 89.5%로 높은 분석 결과가 나왔으며, 이렇게 추출된 문장 특성을 결합하여 문장별 중요도를 계산한 결과, 기존 TextRank 알고리즘과 비교하여 중심 문장 추출 성능이 10% 정도 개선된 것을 확인할 수 있었다.","This study is about a method of extracting a summary from a news article in consideration of the importance of each sentence constituting the article. We propose a method of calculating sentence importance by extracting the probabilities of topic sentence, similarity with article title and other sentences, and sentence position as characteristics that affect sentence importance. At this time, a hypothesis is established that the Topic Sentence will have a characteristic distinct from the general sentence, and a deep learning-based classification model is trained to obtain a topic sentence probability value for the input sentence. Also, using the pre-learned ELMo language model, the similarity between sentences is calculated based on the sentence vector value reflecting the context information and extracted as sentence characteristics. The topic sentence classification performance of the LSTM and BERT models was 93% accurate, 96.22% recall, and 89.5% precision, resulting in high analysis results. As a result of calculating the importance of each sentence by combining the extracted sentence characteristics, it was confirmed that the performance of extracting the topic sentence was improved by about 10% compared to the existing TextRank algorithm."
Simple and effective neural coreference resolution for Korean language,2021,"['coreference resolution', 'head‐final language', 'Korean', 'pretrained language model', 'recurrent neural network']",,"We propose an end‐to‐end neural coreference resolution for the Korean language that uses an attention mechanism to point to the same entity. Because Korean is a head‐final language, we focused on a method that uses a pointer network based on the head. The key idea is to consider all nouns in the document as candidates based on the head‐final characteristics of the Korean language and learn distributions over the referenced entity positions for each noun. Given the recent success of applications using bidirectional encoder representation from transformer (BERT) in natural language‐processing tasks, we employed BERT in the proposed model to create word representations based on contextual information. The experimental results indicated that the proposed model achieved state‐of‐the‐art performance in Korean language coreference resolution."
딥러닝 기반 단어 임베딩을 적용한 사진 자막 영작문 채점 시스템,2021,"['컴퓨터 언어보조학습', 'computer assisted language learning', '딥러닝', 'deep learning', '영작문', 'English writing', '단어 임베딩', 'word embedding', '준거참조검사', 'criterion-referenced test', '채점', 'scoring']",,"Since human grading of English writing requires substantial resources, many researchers in the area of Computer-Assisted Language Learning (CALL) have been focusing on automatic scoring systems based on natural language processing systems, machine learning, and other automatic processing mechanisms. English Testing Services (ETS) announced several automatic scoring systems for English writing. In this paper, we suggest using a deep learning based automatic scoring system for an English caption writing test. Our method involves using a sentence similarity measurement, which compares different levels of answer sentences with user writing input. We chose different word embedding types (Word2Vec, Word Mover‘s Distance (WMD), Bidirectional Encoder Representations from Transformers (BERT)) and Abstract Meaning Representation (AMR), a linguistic model for comparing semantic differences between two sentences based on semantic representation. Scoring systems should not only satisfy the requirements of complicated scoring rubrics but also meet the conditions of a language proficiency test. Our results show that BERT outperforms three competitive models in predicting accurate scoring levels and also shows the characteristics of the criterion reference which could theoretically express the standards of a language proficiency test."
Korean automatic spacing using pretrained transformer encoder and analysis,2021,"['attention', 'BERT', 'Korean automatic spacing', 'natural language processing', 'pretrained transformer encoder']",,"Automatic spacing in Korean is used to correct spacing units in a given input sentence. The demand for automatic spacing has been increasing owing to frequent incorrect spacing in recent media, such as the Internet and mobile networks. Therefore, herein, we propose a transformer encoder that reads a sentence bidirectionally and can be pretrained using an out‐of‐task corpus. Notably, our model exhibited the highest character accuracy (98.42%) among the existing automatic spacing models for Korean. We experimentally validated the effectiveness of bidirectional encoding and pretraining for automatic spacing in Korean. Moreover, we conclude that pretraining is more important than fine‐tuning and data size."
Effect of White Willow Bark on Delayed Onset Muscle Soreness Following Resistance Training: A Pilot Study,2021,"['Delayed Onset Muscle Soreness', 'Eccentric Exercise', 'Nutritional Supplements', 'White Willow Bark']",,"[OBJECTIVES] Delayed onset muscle soreness (DOMS) is discomfort that occurs within 8-24hrs following an unaccustomed bout of physical activity that peaks within 24-27hrs and slowly resolves on its own. White willow bark (WWB) is a nutritional supplement that is believed to have anti-inflammatory and analgesic properties like aspirin but without the risk of GI adverse effects. The purpose of this investigation is to determine the effectiveness of WWB on alleviating the symptoms of DOMS following exercise. [METHODS] Twenty-five individuals volunteered to participate and were randomly assigned to take WWB (798mg salicin) or placebo for 5 days following a lower body resistance training session which consisted of 5X10 lunges at 40% body weight (BW) and 3X fatigue leg press at 75%BW. Test procedures included visual analog scale (VAS), mid-thigh circumference and pressure pain threshold. VAS was measured pre, all five days of the supplementation period and day 6 (post-supplementation). All other variables were measured at pre, immediate, day 3(72hrs), and day 6 (post-supplementation). [RESULTS] No condition X time interaction was observed (p > 0.05) for any variable. However, VAS scores were lower in the WWB compared to the placebo for all time frames. There was a significant main effect of time for VAS scores indicating muscle soreness for hamstrings (p < 0.001), gluteal (p < 0.001), gastrocnemius (p < 0.001) and quadriceps (p < 0.001). In addition, there was a significant main effect of time for right midthigh pressure pain threshold (p = 0.02), mid-right (p < 0.001) and mid-left (p < 0.001) thigh circumference. [CONCLUSIONS] WWB may reduce subjective feelings of muscle soreness and appears to have analgesic properties."
딥러닝 언어모델의 한국어 학습자 말뭉치 원어민성 판단 결과 분석 연구,2021,"['deep learning', '딥러닝', 'BERT', '버트', 'nativelikeness judgment', '원어민성 판단', 'classification', '분류', 'Korean learner corpus', '한국어 학습자 말뭉치', 'error-tagged corpus', '오류 주석 말뭉치']",,"The present study aims to analyze how deep learning judges nativelikeness in the corpus of native Korean speakers and Korean learners. To this end, a deep learning model that classifies sentences of native Korean speakers and Korean learners was built, and the criteria for determining nativelikeness between deep learning and humans were compared and analyzed in terms of error analysis. As a result of the analysis, the accuracy of the deep learning model built in this study was found to be 91%, which means that 91 sentences out of 100 sentences were accurately classified whether they were written by the native speaker or by the learner. In addition, since the error annotation result of the learner corpus is a projected result of human judgment of nativelikeness, the similarities and differences of the criteria for determining nativelikeness were described in detail by comparing it with the test data verification result of deep learning. The results of this study will be an important basis for clarifying what the nativelikeness of native Korean speakers is and for objectively judging the nativelikeness of the language produced by Korean learners.(Yonsei University)"
흐름이 있는 문서에 적합한 비지도학습 추상 요약 방법,2021,"['NLP', 'Summarization', 'GAN', 'BERT', 'Transformer']",,"Recently, a breakthrough has been made in the NLP area by Transformer techniques based on encoder-decoder. However, this only can be used in mainstream languages where millions of dataset are well-equipped, such as English and Chinese, and there is a limitation that it cannot be used in non-mainstream languages where dataset are not established. In addition, there is a deflection problem that focuses on the beginning of the document in mechanical summarization. Therefore, these methods are not suitable for documents with flows such as fairy tales and novels. In this paper, we propose a hybrid summarization method that does not require a dataset and improves the deflection problem using GAN with two adaptive discriminators. We evaluate our model on the CNN/Daily Mail dataset to verify an objective validity. Also, we proved that the model has valid performance in Korean, one of the non-mainstream languages."
How are Korean Neural Language Models ‘surprised’ Layerwisely?,2021,"['한국어 신경망 언어모델', '언어학적 변칙', '‘놀라움’ 차이', '신경망 층별분석', 'KR-BERT', 'KoBERT', 'linguistic anomaly', 'surprisal gap', 'layerwise']",,
온라인 전시 행사에서 개인 맞춤형 추천 시스템 적용 사례 연구 : 산학협력 EXPO를 중심으로,2021,"['온라인 전시회', '추천 시스템', 'Matrix Factorization', 'TF-IDF', 'Sentence-BERT', 'Online exhibition', 'Recommendation system']",,"Due to the spread of Corona 19, many exhibitions and events are being conducted online, non-face-to-face. Compared to offline events, online exhibitions and events have the advantage of being able to watch anytime, anywhere without space and time constraints. On the other hand, in online exhibitions and events, content delivery is inferior because the amount of time for the audience to immerse is relatively small. Despite these shortcomings, the spread of Corona 19 is prolonged and online exhibitions and events are expanding. Also, MICE experts predict that even after the end of Corona 19, offline and online events will be held simultaneously. Therefore, in this study, as a strategy to increase the sense of immersion in the program in online exhibitions and events, we intend to propose a method that can maximize the effect of delivering content during a limited time of visitors stay a personalized artificial intelligence recommendation algorithm."
재난문자 분류를 위한 딥러닝 모델,2021,"['Disaster Alerts', 'Text Classification', 'Deep Learning', 'Word Embedding', 'BERT', '재난문자', '텍스트 분류', '딥러닝', '단어 임베딩']","재난문자는 재난 발생 시 국가에서 해당 지역에 있는 시민들에게 보내는 문자 메시지다. 재난문자의 발송 건수는 점점 증가하여, 불필요한 재난문자가 많이 수신됨에 따라 재난문자를 차단하는 사람들이 증가하고 있다. 이와 같은 문제를 해결하기 위하여, 본 연구에서는 재난문자를 재난 유형별로 자동으로 분류하고 수신자에 따라 필요한 재난의 재난문자만 수신하게 하는 딥러닝 모델을 제안한다. 제안 모델은 재난문자를 KoBERT를 통해 임베딩하고, LSTM을 통해 재난 유형별로 분류한다. [명사], [명사 + 형용사 + 동사], [모든 품사]의 3가지 품사 조합과 제안 모델, 키워드 분류, Word2Vec + 1D-CNN 및 KoBERT + FFNN의 4종류 분류 모델을 활용하여 재난문자를 분류한 결과, 제안 모델이 0.988954의 정확도로 가장 높은 성능을 달성하였다.","Disaster alerts are text messages sent by government to people in the area in the event of a disaster. Since the number of disaster alerts has increased, the number of people who block disaster alerts is increasing as many unnecessary disaster alerts are being received. To solve this problem, this study proposes a deep learning model that automatically classifies disaster alerts by disaster type, and allows only necessary disaster alerts to be received according to the recipient. The proposed model embeds disaster alerts via KoBERT and classifies them by disaster type with LSTM. As a result of classifying disaster alerts using 3 combinations of parts of speech: [Noun], [Noun + Adjective + Verb] and [All parts], and 4 classification models: Proposed model, Keyword classification, Word2Vec + 1D-CNN and KoBERT + FFNN, the proposed model achieved the highest performance with 0.988954 accuracy."
A Deep Learning-based Two-Steps Pipeline Model for Korean Morphological Analysis and Part-of-Speech Tagging,2021,"['형태소 분석', '형태소 품사 태깅', '파이프라인', 'morphological analysis', 'part-of-speech tagging', 'sequence-to-sequence', 'BERT', 'pipeline']",,
생성 요약 향상을 위한 가중치 문맥 벡터 기반 BART 모델,2021,"['abstractive summarization', 'sequence-to-sequence', 'pre-trained model', 'weighted context vector']",,"An abstractive summary is a short text containing only salient information from the original text. In our work, given an original text, the Extractor first classifies whether each sentence is salient or not. Then, in fine-tuning BART, by a weight, scaling the context vectors of the encoder corresponding to the salient sentences can give the effect of clustering by increasing the margin between two clusters - One contains salient sentences and the other non-salient ones. Consequently, our modified BART model can show high performance by learning these clustering patterns. Our experimental results show that BART using the proposed method improves up to 3% in ROUGE score and 0.2% in BERT score."
딥러닝 기반 한국어 개체명 인식의 평가와 오류 분석 연구,2021,"['named entity recognition', 'Korean language', 'natural language processing', 'proper name', 'terminology']",,"Named entity recognition is a natural language processing task that recognizes and classifies named entities in an unstructured text. The targets of NER are not limited to typical proper names for persons, locations and organizations, but also date, time and quantity expressions and can be further expanded to names of events, animals, plants, materials and other encyclopedic entities. A real-world NER system is also expected to be tuned to process domain-specific terminologies. In this study, the researchers built and tested a BERT based Korean NER system and proposed methods for evaluation and error analysis. The study trained the system with 140K word NER corpus and evaluated with 60K test. Error types are proposed to be categorized into four classes: detection, boundary, segmentation, and labelling. Error rates are found to vary greatly from 1% to 30% between entity labels, which are grouped into the most accurate time and quantity expressions, relatively accurate proper names, and highly erroneous terminologies. We expect that the error analysis will provide insights for finding a better way of data collection and post-processing correction."
Argument Facet Detection in Online Debates Based on Attention Weights and Clustering with Combined Similarity Matrices,2021,"['argument mining', 'argument facets', 'argument clustering', 'semantic similarities', 'debate texts', 'attention weights']",,"The purpose of argument mining research is to analyze and understand the stances, content, and structures of large argumentative texts, such as online debates. For our research, we collected a list of identified arguments from online debates and attempted to use unsupervised methods to create a list of common justifications for each argument stance in each domain. We propose a model that clusters arguments by subtopics, or justifications, and then extracts the list of representative words for argument facets from each cluster. We were able to improve clustering performance by using a combination of three different similarity matrices (cosine similarity between BERT sentence embeddings, semantic textual similarity, and similarity between topic probability distributions) for the clustering algorithm. Our clustering produced 5%p and 7.5%p of ARI and V-measure values on average, which outperforms previous work in two of four domains. Additionally, we used a Transformer model to utilize the attention weights to discover argument facets, and we observed better performances compared to the method without attention weights."
딥러닝 및 토픽모델링 기법을 활용한 소셜 미디어의 자살 경향 문헌 판별 및 분석,2021,[],,"This study aims to create a deep learning-based classification model to classify suicide tendency by suicide corpus constructed for the present study. Also, to analyze suicide factors, the study classified suicide tendency corpus into detailed topics by using topic modeling, an analysis technique that automatically extracts topics. For this purpose, 2,011 documents of the suicide-related corpus collected from social media naver knowledge iN were directly annotated into suicide-tendency documents or non-suicide-tendency documents based on suicide prevention education manual issued by the Central Suicide Prevention Center, and we also conducted the deep learning model(LSTM, BERT, ELECTRA) performance evaluation based on the classification model, using annotated corpus data. In addition, one of the topic modeling techniques, LDA identified suicide factors by classifying thematic literature, and co-word analysis and visualization were conducted to analyze the factors in-depth."
워드 임베딩 클러스터링을 활용한 리뷰 다중문서 요약기법,2021,"['Muti-document', 'Text Summarization', 'Transformer', 'Word Embedding', '다중문서', '텍스트 요약', '트랜스포머', '워드 임베딩']",,"Multi-document refers to a document consisting of various topics, not a single topic, and a typical example is online reviews. There have been several attempts to summarize online reviews because of their vast amounts of information. However, collective summarization of reviews through existing summary models creates a problem of losing the various topics that make up the reviews. Therefore, in this paper, we present method to summarize the review with minimal loss of the topic. The proposed method classify reviews through processes such as preprocessing, importance evaluation, embedding substitution using BERT, and embedding clustering. Furthermore, the classified sentences generate the final summary using the trained Transformer summary model. The performance evaluation of the proposed model was compared by evaluating the existing summary model, seq2seq model, and the cosine similarity with the ROUGE score, and performed a high performance summary compared to the existing summary model."
최신 기계번역 품질 예측 연구,2021,[],,"Quality estimation (QE) can evaluate the quality of machine translation output even for those who do not know the target language, and its high utilization highlights the need for QE. QE shared task is held every year at Conference on Machine Translation (WMT), and recently, researches applying Pretrained Language Model (PLM) are mainly being conducted. In this paper, we conduct a survey on the QE task and research trends, and we summarize the features of PLM. In addition, we used a multilingual BART model that has not yet been utilized and performed comparative analysis with the existing studies such as XLM, multilingual BERT, and XLM-RoBERTa. As a result of the experiment, we confirmed which PLM was most effective when applied to QE, and saw the possibility of applying the multilingual BART model to the QE task."
웹 말뭉치에 대한 문장 필터링 데이터 셋 구축 방법,2021,[],,"Pretrained models with high performance in various tasks within natural language processing have the advantage of learning the linguistic patterns of sentences using large corpus during the training, allowing each token in the input sentence to be represented with appropriate feature vectors. One of the methods of constructing a corpus required for a pre-trained model training is a collection method using web crawler. However, sentences that exist on web may contain unnecessary words in some or all of the sentences because they have various patterns. In this paper, we propose a dataset construction method for filtering sentences containing unnecessary words using neural network models for corpus collected from the web. As a result, we construct a dataset containing a total of 2,330 sentences. We also evaluated the performance of neural network models on the constructed dataset, and the BERT model showed the highest performance with an accuracy of 93.75%."
딥러닝 자동 분류 모델을 위한 공황장애 소셜미디어 코퍼스 구축 및 분석,2021,[],,"This study is to create a deep learning based classification model to examine the characteristics of panic disorder and to classify the panic disorder tendency literature by the panic disorder corpus constructed for the present study. For this purpose, 5,884 documents of the panic disorder corpus collected from social media were directly annotated based on the mental disease diagnosis manual and were classified into panic disorder-prone and non-panic-disorder documents. Then, TF-IDF scores were calculated and word co-occurrence analysis was performed to analyze the lexical characteristics of the corpus. In addition, the co-occurrence between the symptom frequency measurement and the annotated symptom was calculated to analyze the characteristics of panic disorder symptoms and the relationship between symptoms. We also conducted the performance evaluation for a deep learning based classification model. Three pre-trained models, BERT multi-lingual, KoBERT, and KcBERT, were adopted for classification model, and KcBERT showed the best performance among them. This study demonstrated that it can help early diagnosis and treatment of people suffering from related symptoms by examining the characteristics of panic disorder and expand the field of mental illness research to social media."
알렉상드르 뒤마의 1859년 살롱 비평 연구,2021,"['알렉상드르 뒤마', '살롱', '살롱 전시', '미술비평(살롱 비평)', '들라크루아', '밀레', '에베르', '트루와용', '색', '색채 주의자', '감각작용', '사실', 'Alexandre Dumas', 'Salon', 'critique d’art', 'salon de peinture', 'Delacroix', 'Millet', 'Troyon', 'couleur', 'coloriste', 'sensation', 'v&#233', 'rit&#233']","본 연구는 잘 알려지지 않은 알렉상드르 뒤마의 살롱 비평서인 󰡔1859년 살롱전에서의 미술과 동시대 작가들 L’art et les artistes contemporains au salon de 1859󰡕(1859)을 통해 뒤마의 1859년 살롱 비평의 주요 견해와 가치가 무엇인지 살펴보는 것을 주목적으로 한다. 무엇보다 먼저 우리는 작가 뒤마가 살롱 비평가로서 어떤 원칙(비평 정신)을 가지고 당대 살롱전에 대한 비평문을 썼는지 살펴보며 1859년까지 프랑스 미술대전인 살롱전의 현실, 즉 미술계의 동향을 분석할 것이다. 다음으로는 그가 비중 있게 다루는 화가 들라크루아에 대한 견해와 그가 다루고 있는 다수의 화가들 가운데 에베르와 트루와용, 밀레에 대한 비평을 통해 뒤마 미술비평의 실제를 확인할 것이다. 이제까지 논의되지 않은 뒤마의 살롱 비평 분석을 통해 우리는 작가 뒤마의 살롱 비평가로서의 위상은 물론 그의 1859년 살롱 비평의 특징과 의미를 확인할 것이다. 이는 뒤마의 살롱 비평이 지닌 문화사와 미술사적 가치 또한 발견하는 계기가 될 수 있을 것이다.","S appuyant sur L art et les artistes contemporains au salon de 1859 (autrement dit, le Salon de 1859) d Alexandre Dumas, cette étude vise à examiner les principales vues et valeurs du Salon de 1859 de Dumas. Pour cela, tout d’abord, nous envisageons l esprit critique chez Dumas en tant que critique du salon de peinture et analyserons également l’actualité des salons jusqu’en 1859. Ensuite, nous constatons les caractéristiques et le rô̂le du Salon de 1859 de Dumas, c’est à travers les réflecxions dumasiennes sur Delacroix et les autres peintres tels que Hébert, Troyons et Millet. Ce sera l occasion de vérifier le statut d’Alexandre Dumas en tant que critique d’art et la valeur de son Salon de 1859 dans la généalogie des Salons et l’histoire d’art du 19ème siècle."
한국어 문장 감성 추출 정확도 향상을 위한 BIO 태깅과 Triplet 방법론 연구,2021,"['Artificial Intelligence', 'Natural Language Processing', 'BIO Tagging', 'Aspect', 'Opinion', 'Sentiment', '인공지능', '자연어처리', 'BIO 태깅', '감성 주체', '감성 관점', '감성']",,"Thanks to the development of artificial intelligence technology, research in the field of communication between humans and computers has been vitalized, showing tangible results. Research on the processes in which a computer understands and translates human speech and generates and responds to a sentence, based on analysis and reasoning about it is still being actively conducted. However, in this process, the representative feature that the computer has not yet completely extracted from human language is the sentiment. The sentiments, contained in human language are expressed in all areas of society and used in various ways. For example, the author of an academic thesis expresses his own theories or his claims about existing theories with positive or negative sentiment in the text. We are actively expressing our sentiments on social media that we face and participate in in our daily lives, and influence economic activity through positive or negative evaluations of purchased products. In this respect, automatic extraction of human text sentiments is a very important research field, and it is a major technology that can create economic utility as a new business model and attracts attention as a new industrial field.In general, the sentiment contained in a sentence in the speech act performed by humans can be divided into positive, negative, and neutral. In this paper, we will study the triplet extraction method, which is newly introduced in the process of developing the technology to automatically extract the above three speaker sentiments in the sentence, and BIO tagging as a basic labeling technique for this. To this end, in this paper, we will analyze the triplet-related research results announced in 2020 and 2021 in depth, track their strengths and weaknesses, and apply them to the new triplet method we are developing. Our company has researched and developed Triplet that automatically extracts the emotion of a sentence based on Korean data. As shown in <Figure 11>, the head office built and tested the Triplets for the Hangul data predicted by the GTS model trained with the Multilingual Bert model. The accuracy of Aspect and Opinion terms is 0.7 and 0.6, respectively, which is not a very high score, but it is judged that the accuracy will increase with additional data acquisition and update. The triplet accuracy is 0.3, which is still a low score. It is speculated that the reason why the triplet accuracy is low is because the training was conducted with the ‘Multilingual’ model rather than the Hangul-specific model. Therefore, we plan to train with a Korean-specific model by building a Korean training data set, and it is expected to show high accuracy soon in the future."
Simulating TSN Traffic Scheduling and Shaping For Future Automotive Ethernet,2021,"['Automotive electronics', 'Ethernet networks', 'scheduling algorithms', 'simulation', 'time-sensitive networking.']",,"The broadening range of applications for vehicles hasmotivated the evolution of the automotive communication network.Ethernet has been deployed in production vehicles to build invehiclenetworks (IVN) by main manufacturers. To extent Ethernetwith real-time service for future E/E architecture, a time-sensitivenetworking (TSN) profile for automotive Ethernet has been created.This paper evaluates the implementation of multiple trafficscheduling and shaping mechanisms in the automotive Ethernet,respectively. And we especially focus on two solutions, namely thetime-aware shaping (TAS) and asynchronous traffic shaping (ATS).To investigate the performance, we introduce a TSN-based automotivegateway testing model in a simulation environment. Furthermore,another two methods, i.e., strict-priority and credit-basedshaper (CBS), as well as TAS and ATS are implemented in themodel and tested within a domain-based IVN scenario. The resultsshow that TAS guarantees the shortest worst-case latency ofhigh-priority streams, whereas it has a longer transmission latencyfor low-priority streams. ATS provides less determinism for highprioritystreams than TAS, but ensures a better average latency ofall streams."
Inhomogeneity of CoCrW powder products manufactured by SLM technology,2021,['· CoCrW dental alloy · Hardness · Porosity · Selective laser melting · Surface roughness'],,"The article deals with selective laser melting process using CoCrW powder. Our aim was to identify the influence of product position on the building platform and the re-use of powder on roughness, hardness, porosity and corrosion properties of materials, made under uniform process parameters. Roughness was tested with a stylus profilometer, corrosion resistance by electrochemical impedance spectroscopy and linear polarization. Porosity was assessed by image analysis on metallographic sections taken in two mutually perpendicular directions. The same metallographic sections were used to measure microhardness. The results confirmed a statistically significant dependence of the surface roughness with the position on the platform as well as with the recycling of the powder used. The corrosion resistance of the materials was excellent. Increased porosity values and lower hardness occurred in the middle of the building platform. The reason for the variation of the properties within the building platform can be seen in the unequal laser input energy in the individual positions of the plate."
사회보장과 조세 ― 공공부조와 사회보험을 중심으로 ―,2021,"['사회국가', 'Sozialstaat', '사회적 정의', 'Soziale Gerechtigkeit', '사회보장', 'Soziale Sicherheit', '조세', 'Steuern', '공공부조', '사회부조', 'Sozialhilfe', '사회보험', 'Sozialversicherung']","21세기가 20년이 지난 지금 민주주의는 새로운 도전에 직면해 있고, 글로벌 경쟁 속에서 경제질서 또한 제4차 산업혁명을 계기로 커다란 변화의 흐름을 타고 있다. 그런 가운데 사회국가의 실현, 특히 변화된 21세기적 상황 속에서 사회적 안전망을 어떻게 확대·강화할 것인지는 이러한 모든 문제와 맞물려 대한민국을 비롯한 선진국들의 중요한 과제로 부각되고 있다.국민 다수가 원하는 것이 무엇인지를 가장 중요시할 수밖에 없는 민주정치에서, 저출산·고령화 속에 경제활동인구가 줄고 “20대 80의 사회”가 점점 현실로 다가오는 시점에서 사회보장의 중요성은 일부 사회적 약자만의 문제가 아닌 국민 모두의 생존 문제와 직결되기 때문이다.오늘날 우리의 삶은 사회보장 없는 삶을 생각할 수 없게 되었고, 사회보장제도의 합리화, 특히 변화된 경제적 여건에 상응하여 사회적 안전망을 확대·강화하는 것은 삶의 질을 유지하기 위한 필수적인 조건이 되었다. 이에 대한 국민적 공감대가 형성되어 있는 것은 분명하지만, 문제는 한정된 재원 배분의 우선순위, 필요한 재원확보 방법 등과 관련하여서는 매우 날카로운 이해관계의 충돌과 대립으로 이어지고 있다는 점이다. 예컨대, 사회보장을 위한 재원확보를 위해 증세가 불가피하다면 누구를 대상으로 어떤 세목을 신설할 것인지, 아니면 기존 세목 가운데 어떤 세목에 대해 증세할 것인지, 구체적으로 세율을 인상할 것인지 아니면 각종 공제의 축소 등을 통해 증세할 것인지 등의 문제와 관련하여 논란이 있는 것이다. 그뿐만 아니라 증세가 경제활동을 위축시키고 그로 인해 실업률이 증가하는 등 악순환을 가져올 것이라는 우려도 만만치 않다.오늘날 사회국가는 민주주의와 법치주의의 큰 틀 안에서 그 보장의 정도와 방법을 결정하게 된다. 즉, 사회국가실현의 의미는 단순히 사회적 약자에 대한 배려에만 있는 것이 아니라 이를 통해 국민 전체가 사회적 정의라는 공통된 이념과 기준을 공유한다는 것에 있다. 이러한 기본이념과 원칙으로 국가질서 전체가 전체 사회보장 제도 및 이를 뒷받침하는 조세제도가 체계적이고 유기적인 관련성 속에서 정비되고 발전되어야 할 것이다.이 논문에서는 이러한 문제의식에 기초하여 사회보장과 조세의 내적 관련성에 대하여 거시적으로 개관하였다. 이를 위해 먼저, 사회적 정의에 기초한 실질적 자유, 실질적 평등의 실현이라는 사회국가의 이념을 조명 (Ⅱ.)한 후, 사회보장, 특히 공공부조와 사회보험을 통한 사회국가 이념의 실현을 개관(Ⅲ. 및 Ⅳ.)한 다음, 조세의 이중적 기능과 소득세의 특성에 따라 「사회국가의 실현을 위한 사회보장과 조세의 역할」(Ⅴ)을 검토하였다.","Heutzutage, 20 Jahre nach dem 21. Jahrhundert, steht die Demokratie vor neuen Herausforderungen, und auch die Wirtschaftsordnung im globalen Wettbewerb verändert sich nach der 4. industriellen Revolution stark. In der Zwischenzeit hebt sich die Frage der Verwirklichung des Sozialstaates hervor, insbesondere die Frage, wie das soziale Sicherheitsnetz in den veränderten Gegebenheiten des 21. Jahrhunderts ausgebaut und verstärkt werden kann, mit all diesen Fragen ineinandergreifend, als eine wichtige Aufgabe für fortgeschrittene Länder, einschließlich Korea.Dies liegt daran, dass die Bedeutung der sozialen Sicherheit in einer demokratischen Politik, in der die Mehrheit der Bürger am wichtigsten sein muss, und in dem Zeitpunkt, wann die wirtschaftlich aktive Bevölkerung bei einer niedrigen Geburtenrate und einer alternden Bevölkerung weiter abnimmt, und in dem Zeitpunkt, an dem die “Gesellschaft der 20er und 80er Jahre“ zunehmend an Realität gewinnt, in direktem Zusammenhang mit dem Überleben aller Menschen steht, nicht nur mit dem Überleben der einigen sozial Schwächeren.Heute ist unser Leben ohne soziale Sicherheit undenkbar geworden, und ist die Rationalisierung des Systems der sozialen Sicherheit, insbesondere die Erweiterung und Verstärkerung des sozialen Sicherheitsnetzes als Reaktion auf die veränderten wirtschaftlichen Bedingungen, zu einer wesentlichen Voraussetzung für die Aufrechterhaltung der Lebensqualität geworden.Es ist klar, dass diesbezüglich ein bürgerlicher Konsens wetergehend besteht. Aber das Problem ist, dass dies zu sehr scharfen Interessenkonflikten und Konfrontationen in Bezug auf die Prioritäten der begrenzten Mitteln und Art und Weise der erforderlichen Finanzeirungen führt. Zum Beispiel, wenn eine Steuererhöhung für die Finanzierung der sozialen sicherheit unvermeidbar ist, stellen sich heftige Auseinandersetzungen darübert, ob eine neue Steuerart geschaffen werden sollte oder ob und welche Belastung unter den bestehenden Steuerarten erhöht werden sollte und zwar durch Steuersatzerhöhung oder durch Reduzierungen der verschiedenen Abzüge usw. Darüber hinaus gibt es Bedenken, dass die Steurerhöhung zu einem Teufelskreis führen wird, beispielsweise zu einem Rückgang der Wirtschaftstätigkeit und einem Anstieg der Arbeitslosenquote.Heutigentags bestimmt der Sozialstaat das Niveau und den Weg seiner Gewährleistung im großen Rahmen von Demokratie und Rechtsstaatlichkeit. Mit anderen Worten, die Bedeutung der Verwirklichung des Sozialstaates liegt nicht nur in der Sorge für sozial Schwächeren, sondern auch darin, dass die gesamten Bürger dadurch eine gemeinsame idee und gemeinsame Kriterien der sozialer Gerechtigkeit teilen. Mit dieser Grundidee und diesen Prinzipen sollten das gesamte Sozialversicherungssystem und das Steuersystem, das jedes unterstützt, in einem systematischen und organischen Verhältnis überarbeitet und weiterentwickelt werden.Auf Basis diese Bewusstseins bietet diese Arbeit einen makroskopischen Überblick über die internen Beziehungen zwischen sozialer Sicherheit und Steuern. Zu diesem Zweck beleuchtet sie zunächst die Idee des Sozialstaates, die Verwirklichung von materialler Freiheit und materieller Gleichheit auf der Grundlage sozialer Gerechtigkeit(II.). Danach gibt sie dann einen Überblick über die Verwirklichung der Idee des Sozialstaates durch soziale Sicherheit, insbesondere Sozialhilfen und Sozialversicherungen(III und IV). Anschließend betrachtet sie die Rolle von Sozialversicherungen und Steuern für die Verwirklichung eines Sozialstaates(Ⅴ) gerectht der Doppelfunktion der Steuern und den Eigenarten der Einkommensteuer."
