title,date,keywords,abstract,multilingual_abstract
사전학습 언어 모델을 활용한 트랜스포머 기반 텍스트 요약,2021,"['딥러닝', '자동 문서 요약', '트랜스포머', '사전학습 언어 모델', 'Deep Learning', 'Automatic Text Summarization', 'Transformer', 'Pre-trained Language Model']","소셜 미디어의 등장으로 많은 양의 텍스트 데이터가 온라인상에서 생산 및 유통되면서, 정보 이용자가 방대한 정 보로부터 필요한 정보만을 추려내는 작업은 더욱 어려워지게 되었다. 이로 인해 많은 양의 텍스트를 자동으로 요약 하기 위한 다양한 시도가 이루어지고 있으며, 특히 최근에는 풍부한 표현의 요약문을 새롭게 생성할 수 있는 추상 요약 접근법에 대한 연구가 활발히 수행되고 있다. 추상 요약 분야에서는 신경망 기반의 트랜스포머 모델이 우수한 성능을 보이며 널리 활용되고 있지만, 충분한 양의 학습 데이터가 확보되지 않으면 트랜스포머를 구성하고 있는 매 개변수의 학습이 충분히 이루어지지 않아서 양질의 요약문을 생성하기 어렵다는 한계를 갖는다. 따라서 본 연구는 소량의 학습 데이터가 주어진 상황에서도 양질의 요약문을 생성하기 위해, 한국어 사전학습 언어 모델인 KoBERT의 일부 요소를 추출하여 트랜스포머 기반의 추상 요약 모델에 적용하는 문서 요약 방안을 제시한다. 제안 방법론의 우 수성을 검증하기 위해 Dacon의 한국어 문서 생성 요약 데이터 42,803건에 대한 요약 실험을 수행한 결과, 제안 방법 론이 비교 방법론에 비해 요약 품질을 평가하는 지표인 ROUGE 기준으로 우수한 성능을 보임을 확인하였다.","As a large amount of text data is produced and distributed online with the advent of social media, it has become more difficult for users to extract only necessary information from a vast amount of information. As a result, various attempts have been made to automatically summarize a large amount of text. In particular, the abstractive summarization approach is being actively studied because it can create a new summary with rich contextual expressions. In the abstractive summarization task, neural network-based transformer models show high-performance and are used widely. However, if a sufficient amount of training data is not provided, it is difficult to generate a high-quality summary because the parameters constituting the transformer are not sufficiently learned. In this work, we proposes a text summarization method that generates a high-quality summary given a small amount of training data. Specifically, we extract some elements of KoBERT and apply them to a transformer-based abstractive summarization model. We conducted an experiment on 42,803 cases of Dacon's summary data to evaluate the performance of the proposed methodology. As a result of the experiment, it was confirmed that the proposed methodology showed superior performance based on the ROUGE index compared to the comparative method."
단어 손실함수와 반복 페널티를 추가한 트랜스포머 인코더-디코더 제목 생성 모델,2021,"['트랜스포머 인코더-디코더 모델', '자동 제목 생성', '단어 손실함수', '반복 페널티', 'transformer encoder-decoder', 'automatic title generation', 'word loss', 'repeat penalty']","제목은 문서를 대표하는 어구 혹은 문장이라 정의할 수 있다. 우리는 문서의 제목을 생성하기 위해 트랜스포머 기반 인코더-디코더 구조를 제안한다. 대용량 문서를 이용하여 트랜스포머 인코더-디코더 구조의 사전학습(pre-training)을 진행하고 본문과 제목 쌍으로 이루어진 문서를 이용하여 미세조정(fine-tuning)을 진행하였다. 또한 제목 생성 태스크로 범위가 제한되는 미세조정 과정에서 입력 문서에 나타나는 어절의 생성 비율을 증가시키기 위해 단어 손실함수를 추가하고 토큰이 반복적으로 생성되는 문제를 개선하기 위한 반복 패널티를 모델 추가하는 방법을 제안한다. 25,564개의 논문 데이터를 사용한 실험에서 단어 손실함수와 반복 패널티를 개별적으로 적용시킨 모델의 성능이 기존 모델에 비해 개선되고, 두 제안 방법을 모두 적용한 모델에서는 Rouge-L의 성능이 2.7% 향상되는 것을 확인하였다.","The title can be defined as a phrase or sentence the represents the document. We propose a transformer encoder-decoder model to generate the title of the document. The transformer model is pre-trained based on a the usage of a large document, and fine-tuning is performed using the data comprising of the body and title. Also, in the fine-tuning process, the scope of which is limited to the title generation task, a Word Loss is added to increase the generation ratio of words appearing in the input document and ground truth title. We propose a method of adding a Repeat Penalty to the model to reduce the problem that tokens are repeatedly generated. In an experiment conducted using data from 25,564 papers, the performance of the model that individually applied the Word Loss and the Repeat Penalty was improved compared to the baseline. It was confirmed that Rouge-L""s performance improved by 2.7% in the model to which both the proposed methods were applied."
텍스트 트랜스포머 모델에서 어텐션 맵을 이용한 경사도 기반 화이트 박스 적대적 예제 생성 방안,2021,"['트랜스포머', '적대적 예제', '딥러닝', '텍스트 데이터', '화이트박스', 'Transformer', 'Adversarial example', 'Deep learning', 'Text data', 'Whitebox']",,"Abstract should be placed here. These instructions give you guidelines for preparing papers for JDCS. The method of generating Adversarial examples for text data of the transformer model was mostly a black box attack method because of the discrete characteristics of text data. Recently, a gradient-based white box attack method targeting text data of a transformer model has been announced, which has the disadvantage that it takes a long time and is not efficient because it learns one distribution for each generation of an example. This paper improves the efficiency of the existing white box attack method by proposing an attention constraint using the attention structure of the transformer model. Through experiments, it has been proven that the generation time can be shortened by about 6.5% and the diversity of generated adversarial examples can be increased by 2.4% compared to the previous research results."
효율적인 트랜스포머를 이용한 팩트체크 자동화 모델,2021,"['Automated fact checking', 'Locality sensitive hashing', 'Natural language processing', 'Transformer']",,"Nowadays, fake news from newspapers and social media is a serious issue in news credibility. Some of machine learning methods (such as LSTM, logistic regression, and Transformer) has been applied for fact checking. In this paper, we present Transformer-based fact checking model which improves computational efficiency. Locality Sensitive Hashing (LSH) is employed to efficiently compute attention value so that it can reduce the computation time. With LSH, model can group semantically similar words, and compute attention value within the group. The performance of proposed model is 75% for accuracy, 42.9% and 75% for Fl micro score and F1 macro score, respectively."
주제 어트리뷰트 모델을 이용한 주제 키워드 기반 한국어 문서 요약,2021,"['topic centric summarization', 'machine learning', 'pre-training', 'MASS', 'PPLM', '주제 키워드 기반 문서 요약', '머신 러닝', '사전 학습', 'MASS', 'PPLM']","문서 추상 요약은 요약 모델이 원문의 핵심 정보를 파악하여 새로운 요약문을 생성하는 작업이다. 이때 추상 요약 모델로 일반적인 Sequence-to-Sequence 모델을 많이 사용하였지만 여기에 핵심 정보를 잘 표현하고 요약문에 반영하기 위해 주제(topic)을 넣어 요약문을 생성하는 주제 중심 요약(Topic centric summarization)을 하는 연구가 최근에 진행되고 있다. 그러나 기존의 방법은 주제 분포(Topic distribution)를 반영하여 문장을 생성하기 위해 모델을 처음부터 학습해야 하기 때문에 사전 학습 언어 모델의 장점을 살리기 어렵다. 본 논문에서는 사전 학습 언어 모델의 장점을 살리면서 주제 키워드를 요약문에 반영하여 주제 중심 요약을 할 수 있는 방법을 제시한다. 제안하는 주제 중심 요약 방법은 기존 조건부 언어 모델(Conditional Language Model)에서 연구되었던 PPLM(Plug and Play Language Model)의 어트리뷰트 모델을 문서 요약에서 사용되는 사전 학습 Sequence-to-Sequence 모델인 MASS에 적용하여 ‘주제 키워드 기반 요약문’을 생성하는 방법이다. 제안하는 방법은 별도의 추가 학습을 요구하지 않기 때문에 MASS의 언어 능력과 파인 튜닝으로 학습한 요약 능력을 그대로 사용함과 동시에 특정 키워드를 등장시켜 주제에서 벗어나지 않는 요약문을 생성할 수 있게 한다. 제안하는 방법의 우수성을 보이기 위해 BERT+Transformer 디코더를 사용한 모델, PPLM을 적용하지 않은 MASS 모델과 한국어 요약성능을 비교하였으며 평균적으로 ROUGE와 BERTScore 모두 성능이 향상되는 것을 확인할 수 있었다.","Abstractive summarization takes original text as an input and generates a summary containing the core-information about the original text. The abstractive summarization model is mainly designed by the Sequence-to-Sequence model. To improve quality as well as coherence of summary, the topic-centric methods which contain the core information of the original text are recently proposed.However, the previous methods perform additional training steps which make it difficult to take advantage of the pre-trained language model. This paper proposes a topic-centric summarizer that can reflect topic words to a summary as well as retain the characteristics of language model by using PPLM. The proposed method does not require any additional training. To prove the effectiveness of the proposed summarizer, this paper performed summarization experiments with Korean newspaper data."
LIME 알고리즘을 이용한 한국어 감성 분류 모델 해석,2021,,"한국어 감성 분류 작업은 챗봇, 사용자의 물건 구매 평 분석 등 실 서비스에서 사용되고 있으며, 현재 딥러닝 기술의 발달로 높은 성능을 가진 신경망 모델을 활발히 사용하여 감성 분류 작업을 수행하고 있다. 하지만 신경망 모델은 입력 문장이 어떤 단어들로 인해 결과가 예측되었는지 해석하는 것이 쉽지 않으며, 최근 신경망 모델의 해석을 위한 모델 해석 방법들이 활발히 제안되어지고 있다. 본 논문에서는 모델 해석 방법 중 LIME 알고리즘을 이용하여 한국어 감성 분류 데이터 셋으로 학습된 모델들의 입력 문장 내 단어들 중 어떤 단어가 결과에 영향을 미쳤는지 해석하고자 한다. 그 결과, 85.23%의 성능을 보인 양방향 순환 신경망 모델의 해석 결과, 총 25,283개의 긍정, 부정 단어를 포함했지만, 상대적으로 낮은 성능을 보인 84.20%의 Transformer 모델의 해석 결과, 총 26,447개의 긍정, 부정 단어가 포함되어 있어 양방향 순환 신경망 모델보다 Transformer 모델이 신뢰할 수 있는 모델임을 확인할 수 있었다.",
CTR 예측을 위한 비전 트랜스포머 활용에 관한 연구,2021,"['클릭률', '심층신경망', '추천시스템', 'e-비즈니스', 'Click-Through Rate', 'Deep Neural Network', 'Recommender systems', 'e-business']","Click-Through Rate(CTR) 예측은 추천시스템에서 후보 항목의 순위를 결정하고 높은 순위의 항목들을 추천하여 고객의 정보 과부하를 줄임과 동시에 판매 촉진을 통한 수익 극대화를 달성할 수 있는 핵심 기능이다. 자연어 처리와 이미지 분류 분야는 심층신경망(deep neural network)의 활용을 통한 괄목한 성장을 하고 있다. 최근 이 분야의 주류를 이루던 모델과 차별화된 어텐션(attention) 메커니즘 기반의 트랜스포머(transformer) 모델이 제안되어 state-of-the-art를 달성하였다. 본 연구에서는 CTR 예측을 위한 트랜스포머 기반 모델의 성능 향상 방안을 제시한다. 자연어와 이미지 데이터와는 다른 이산적(discrete)이며 범주적(categorical)인 CTR 데이터 특성이 모델 성능에 미치는 영향력을 분석하기 위해 임베딩의 일반화(regularization)와 트랜스포머의 정규화(normalization)에 관한 실험을 수행한다. 실험 결과에 따르면, CTR 데이터 입력 처리를 위한 임베딩 과정에서 L2 일반화의 적용과 트랜스포머 모델의 기본 정규화 방법인 레이어 정규화 대신 배치 정규화를 적용할 때 예측 성능이 크게 향상됨을 확인하였다.","Click-Through Rate (CTR) prediction is a key function that determines the ranking of candidate items in the recommendation system and recommends high-ranking items to reduce customer information overload and achieve profit maximization through sales promotion. The fields of natural language processing and image classification are achieving remarkable growth through the use of deep neural networks. Recently, a transformer model based on an attention mechanism, differentiated from the mainstream models in the fields of natural language processing and image classification, has been proposed to achieve state-of-the-art in this field. In this study, we present a method for improving the performance of a transformer model for CTR prediction. In order to analyze the effect of discrete and categorical CTR data characteristics different from natural language and image data on performance, experiments on embedding regularization and transformer normalization are performed. According to the experimental results, it was confirmed that the prediction performance of the transformer was significantly improved when the L2 generalization was applied in the embedding process for CTR data input processing and when batch normalization was applied instead of layer normalization, which is the default regularization method, to the transformer model."
특허문서 자동분류를 위한 딥러닝 개별 모델 분류기와 앙상블 분류기의 성능비교,2021,"['Patent classification', 'Ensemble', 'Deep learning']",,
추가 사전학습 기반 지식 전이를 통한 국가 R&D 전문 언어모델 구축,2021,"['국가 R&D', '지식 전이', '사전학습 모델', 'BERT', '추가 사전학습', 'National R&D', 'Knowledge Transfer', 'Pre-trained Language Model', 'Further Pre-training']","최근 딥러닝 기술이 빠르게 발전함에 따라 국가 R&D 분야의 방대한 텍스트 문서를 다양한 관점에서 분석하기 위한 수요가 급증하고 있다. 특히 대용량의 말뭉치에 대해 사전학습을 수행한 BERT(Bidirectional Encoder Representations from Transformers) 언어모델의 활용에 대한 관심이 높아지고 있다. 하지만 국가 R&D와 같이 고도로 전문화된 분야에서 높은 빈도로 사용되는 전문어는 기본 BERT에서 충분히 학습이 이루어지지 않은 경우가 많으며, 이는 BERT를 통한 전문 분야 문서 이해의 한계로 지적되고 있다. 따라서 본 연구에서는 최근 활발하게 연구되고 있는 추가 사전학습을 활용하여, 기본 BERT에 국가 R&D 분야 지식을 전이한 R&D KoBERT 언어모델을 구축하는 방안을 제시한다. 또한 제안 모델의 성능 평가를 위해 보건의료, 정보통신 분야의 과제 약 116,000건을 대상으로 분류 분석을 수행한 결과, 제안 모델이 순수한 KoBERT 모델에 비해 정확도 측면에서 더 높은 성능을 나타내는 것을 확인하였다.","With the recent rapid development of deep learning technology, the demand for analyzing huge text documents in the national R&D field from various perspectives is rapidly increasing. In particular, interest in the application of a BERT(Bidirectional Encoder Representations from Transformers) language model that has pre-trained a large corpus is growing. However, the terminology used frequently in highly specialized fields such as national R&D are often not sufficiently learned in basic BERT. This is pointed out as a limitation of understanding documents in specialized fields through BERT. Therefore, this study proposes a method to build an R&D KoBERT language model that transfers national R&D field knowledge to basic BERT using further pre-training. In addition, in order to evaluate the performance of the proposed model, we performed classification analysis on about 116,000 R&D reports in the health care and information and communication fields. Experimental results showed that our proposed model showed higher performance in terms of accuracy compared to the pure KoBERT model."
BERT를 활용한 상표 의견제출통지서 거절이유 분류모델 개발,2021,"['BERT', '자연어처리', '딥러닝', '워드 임베딩', '토크나이저', 'BERT', 'Natural Language Process', 'Deep Learning', 'Word Embedding', 'Tokenizer']","멀티미디어 콘텐츠의 증가와 스마트 기기의 보급으로 다양한 종류의 데이터가 폭발적으로 생산되고 있다. 특히, 텍스트 데이터는 오랜 시간 인류의 의사 표현수단이었으며, 텍스트 분석에 대한 수요 및 필요성은 다 양한 분야에서 지속적으로 증가하고 있다. 최근에는 다양한 분야에서 뛰어난 성능을 보이는 딥러닝과 텍스트 를 의미적으로 벡터화하는 워드 임베딩 (word embedding) 방식이 결합된 ELMo (embeddings from language model), GPT (generative pre-training of a language model), BERT (bidirectional encoder representations from transformers)와 같은 모델들이 개발되어왔다. 특히 구글이 개발한 BERT는 현재 자연 어처리 분야에서 가장 뛰어난 성능을 보이는 언어모델로 손꼽히고 있다. 하지만 ‘영어’나 소셜미디어 데이터 와 같은 ‘일반 텍스트’에 특화된 BERT는 ‘한국어’나 ‘R&D 문서, 지식재산권 문서 등’ 전문 분야에 특화된 텍 스트에서 최적의 성능이 구현되지 않기 때문에 전문 분야에 맞는 말뭉치(corpus)를 학습하는 등의 최적화 과 정을 통해 최적의 성능을 도출할 수 있다. 따라서 본 연구에서는 한글 상표 분야에 특화된 BERT를 개발하 기 위해 상표 의견제출통지서 내 거절이유 텍스트 말뭉치를 활용하여 상표 관련 전문 문서에 특화된 토크나 이저 (tokenizer) 모델을 학습하고 이를 BERT에 활용하였다. 제안 모델의 분류 정확도는 기존의 다국어 BERT 모델의 분류 정확도 성능보다 약 4.97%p 높은 96.89%를 기록하였다. 해당 결과를 통해 자연어 분야 에서 높은 성능을 보였던 BERT가 전문 용어로 구성된 말뭉치 데이터에서도 높은 분류 정확도를 보일 수 있 다는 것을 확인하였다.","The increase of multimedia contents and the spread of smart devices have resulted in explosive production of various kinds of data. In particular, text data has been a means of expressing human opinion for a long time, and the demand and necessity for text analysis are continuously increasing in various fields. Recently, models such as the ELMo (embeddings from language model), GPT (generating pre-training of a language model), BERT (bidirectional encoder representations from transformation) have been developed that combine deep learning, which is showing excellent performance in various fields, and the word embedding method that semantically vectorizes text. In particular, BERT developed by Google is considered to be one of the most outstanding language models in the field of natural language processing. However, BERT specialized in 'English' and plain text, such as social media data, does not implement optimal performance in specialized texts such as 'Korean' or 'R&D documents, intellectual property documents', so it can achieve optimal performance through optimization such as learning specialized corpus. Therefore, In this study, in order to develop a BERT specialized in the field of Korean trademarks, a tokenizer model specialized in trademark-related professional documents was learned using a text corpus for rejection in the trademark opinion submission notice and used for BERT. The classification accuracy of the proposed model was 96.89%, which is 4.97% higher than the classification accuracy performance of the existing multilingual BERT model. The results showed that BERT, which showed high performance in the field of natural language, could show high classification accuracy even in corpus data composed of specialized terms."
효과적인 비전 트랜스포머를 통한 화재 감지,2021,"['컨볼루션 뉴럴 네트워크', '화재 감지', '포그기아의 데이터 세트', '산불', '스마트시티', '비전 변압기', 'Convolution Neural Network', 'Fire Detection', 'Foggia’s Dataset', 'Forest Fire', 'Smart Cities', 'Vision Transformers']","오늘날 현대사회에서 스마트하고 안전한 도시는 연구 커뮤니티의 주요 관심사 중 하나이다. 도시들은 개방된 지역, 농경지, 숲으로 둘러싸여 있으며, 화재 발생은 인간의 삶을 위협하고 그들의 재산도 손상시킬 수 있다. 최근 비전 센서 기반 화재 감지 기술은 컴퓨터 비전 분야의 전문가들을 통해, 최신 문헌에서 다양한 컨볼루션 신경 네트워크 (CNN)을 대한 최고의 성능을 달성하고 있다. 그러나 이러한 기술은 변환 불변이고, 지역성에 민감하며, 이미지에 대한 전체적인 이해가 부족하다. 또한 CNN 기반 모델은 계산 비용을 줄이기 위해 차원 축소를 위한 풀링 레이어 전략을 사용했지만, 가장 활동적인 특징 검출기의 정확한 위치와 같은 많은 의미 있는 정보를 손실한다. 이러한 문 제를 극복하기 위해 본 연구에서는 비전 트랜스포머(ViT)기반 화재 감지 모델을 개발하였다. ViT는 입력 이미지를 이미지 패치로 분할한 다음 워드 임베딩과 유사한 시퀀스 구조로 트랜스포머에 제공한다. 우리는 벤치마크 화재 데 이터 세트에서 제안된 작업의 성능을 평가하고 최신(SOTA) CNN 방법과 비교할 때 좋은 결과를 달성한다.","In today's modern age, smart and safe cities are one of the major concerns of the research community. The cities are surrounded by open areas, agricultural land, and forests, where fire incidence can make human lives threatening, damaging their properties as well. Recently, vision sensors-based fire detection has attracted computer vision domain experts, where the leading performance is achieved by a variety of convolution neural networks(CNN) in the recent literature. However, these techniques are translation invariant, locality-sensitive, and lacking a global understanding of images. Furthermore, CNN-based models use the pooling layers strategy for dimensionality reduction to reduce the computational cost but it also loses a lot of meaningful information such as the precise location of the most active feature detector. To overcome these problems, in this work, we developed Vision Transformers(ViT) based model for fire detection. The ViT split the input image into image patches and then feed these patches to the transformer in a sequence structure similar to word embeddings. We evaluate the performance of the proposed work on the benchmark fire dataset and achieve good results when compared to state-of-the-art(SOTA) fire detection CNN models."
오토인코더 기반의 생성 모델을 이용한 유입변압기 수명 예측 시스템,2021,"['Oil transformer', 'Neural network', 'Autoencoder', 'Data augmentation', 'Life estimation', '유입변압기', '신경회로망', '오토인코더', '데이터 증강', '수명예측']",,
태그 라벨과 트랜스포머를 이용한 시맨틱 분할,2021,"['Deep learning', 'Transformer', 'Semantic Segmentation']",,"Currently, research on artificial intelligence that autonomously interacts with surroundings without human management has attracted research attention in vehicle and robot-related fields. The recognition of the surrounding environment is the basis for artificial intelligence that requires interaction with the surroundings, which means that research on object detection is necessary. In general, object detection proceeds in the same way as detection and segmentation. Among them, in the case of segmentation, the size of the model is smaller, and more information can be obtained than detection using anchors. However, the inferior detection performance and generalization ability of this method for small objects has limited its further application. In this paper, a modified transformer structure with different configuration of training data from the existing label data is presented to improve the performance of segmentation."
시계열 예측을 위한 스타일 기반 트랜스포머,2021,"['Time Series Forecasting', 'Transformer', 'Generative Decoder', 'Style Transfer', '시계열 예측', '트랜스포머', '생성 디코더', '스타일 변환']",,"Time series forecasting refers to predicting future time information based on past time information. Accurately predicting future information is crucial because it is used for establishing strategies or making policy decisions in various fields. Recently, a transformer model has been mainly studied for a time series prediction model. However, the existing transformer model has a limitation in that it has an auto-regressive structure in which the output result is input again when the prediction sequence is output. This limitation causes a problem in that accuracy is lowered when predicting a distant time point. This paper proposes a sequential decoding model focusing on the style transformation technique to handle these problems and make more precise time series forecasting. The proposed model has a structure in which the contents of past data are extracted from the transformer-encoder and reflected in the style-based decoder to generate the predictive sequence. Unlike the decoder structure of the conventional auto-regressive transformer, this structure has the advantage of being able to more accurately predict information from a distant view because the prediction sequence is output all at once. As a result of conducting a prediction experiment with various time series datasets with different data characteristics, it was shown that the model presented in this paper has better prediction accuracy than other existing time series prediction models."
언어 학습자로서의 신경망 언어모델: 영어 주어-동사 일치를 중심으로,2021,"['language model', 'GPT-2', 'linguistic generalization', 'subject-verb agreement', 'interference effect', 'plural-singular asymmetry', '언어모델', '언어(학) 일반화', '주어-동사 일치', '간섭효과', '복수-단수 비대칭']",,"This paper assesses the language-processing ability of the neural language model (L2-GPT-2) trained on data sets of English textbooks published in Korea using the Generative Pre-trained Transformer (GPT)-2. Assuming that the language model (LM) is also an (artificial) language learner, we test it focusing on subject-verb agreement in English. It is a well-established fact that L1 speakers exhibit facilitatory interference effects in ungrammatical sentences with a plural subject and a singular form of verb. Unlike human native speakers, L2-GPT-2 as well as L1-GPT-2 display such effects in ungrammatical sentences either with a plural subject and a singular form of verb, or with a singular subject and a plural form of verb. Though there is a significant difference between human speakers and neural LMs in processing subject-verb agreement, the two LMs’ sensitivity to interference by a distractor NP points to the fact that they can attain a remarkably human-like linguistic generalization on subject-verb agreement."
FEM Simulation and Test Verification of PD Ultrasonic Signal Propagation in a Power Transformer Model,2021,"['Power transformers', 'Partial discharge', 'FEM', 'Propagation property']",,"Ultrasonic signals will be generated when partial discharge occurs in internal insulation faults in large oil immersed power transformers: because the ultrasonic signal has strong anti-interference ability and has no direct electromagnetic contact with the equipment, it is widely used in transformer fault detection and positioning. In this paper, the fi nite element method (FEM) is used to simulate the ultrasonic signal in a 35 kV power transformer. The infl uence of transformer case on ultrasonic signal propagation is considered, and the propagation law of the ultrasonic signal inside the transformer is obtained. Fabry–Pérot (F–P) fi bre acoustic sensors with a centre frequency of 28 kHz were fabricated. A partial discharge detection test was carried out in a 35 kV transformer winding model using the F–P sensors. The test results show that the ultrasonic waveform detected by the F–P sensors are in good agreement with the simulation results, and the propagation of the ultrasonic wave inside the transformer is verifi ed. It lays a foundation for detecting and locating PDs in power transformer by F–P acoustic sensors."
Analytical loss model of series-resonant indirect-matrix-type power electronics transformers using MOSFETs,2021,"['Analytical loss model', 'MOSFET', 'Series-resonant dc-dc converter', 'Indirect-matrix converter', 'Power electronics transformer', 'High efficiency']",,"A MOSFET-based series-resonant indirect-matrix-type power electronic transformer (PET) consists of line-frequency-switching folding-unfolding bridges and series-resonant dc-dc converters (SRCs), which is an attractive choice to achieve ac-ac conversion due to its high efficiency and high power density. However, the conduction losses, switching losses, and core losses in each of the high-frequency switching cycles T<sub>s</sub> are different due to the pulsating dc voltage |ac|, which introduces difficulties in the calculation of losses. In addition, the small dc capacitors also participate in the resonance, and the resonant current shape deviates from a pure sine. Therefore, the conventional loss model is not suitable for PETs. In this paper, the time-domain analysis of SRC resonant current considering the dc capacitor is developed. Then, the analytical expression of the resonant current RMS value in grid cycle T<sub>g</sub> is derived, which is more accurate in calculating conduction loss than the conventional model. In addition, it avoids the calculating losses in each T<sub>s</sub>. Next, an analytical switching loss model is developed based on the curve fitting of the capacitance-voltage relationship. A simplified analytical core loss model is provided. Each part of the loss of the PET is verified by thermal simulations, and the total losses are verified by efficiency tests on an experiment prototype."
양방향 인재매칭을 위한 BERT 기반의 전이학습 모델,2021,"['HR Matching', 'Job Recommendation', 'BERT', 'Transfer Learning', 'Pre-trained Language Model', 'Fine-tuning Deep Learning Model']",,"While youth unemployment has recorded the lowest level since the global COVID-19 pandemic, SMEs(small and medium sized enterprises) are still struggling to fill vacancies. It is difficult for SMEs to find good candidates as well as for job seekers to find appropriate job offers due to information mismatch. To overcome information mismatch, this study proposes the fine-turning model for bidirectional HR matching based on a pre-learning language model called BERT(Bidirectional Encoder Representations from Transformers). The proposed model is capable to recommend job openings suitable for the applicant, or applicants appropriate for the job through sufficient pre-learning of terms including technical jargons. The results of the experiment demonstrate the superior performance of our model in terms of precision, recall, and f1-score compared to the existing content-based metric learning model. This study provides insights for developing practical models for job recommendations and offers suggestions for future research."
Analyzing the Impact of Sequential Context Learning on the Transformer Based Korean Text Summarization Model,2021,"['한국어 텍스트 요약', '제목 생성', '어텐션 메커니즘', '트랜스포머 모델', 'Korean text summarization', 'headline generation', 'attention mechanism', 'transformer model']",,
음성감정인식 성능 향상을 위한 트랜스포머 기반 전이학습 및 다중작업학습,2021,,,"It is hard to prepare sufficient training data for speech emotion recognition due to the difficulty of emotion labeling. In this paper, we apply transfer learning with large-scale training data for speech recognition on a transformer-based model to improve the performance of speech emotion recognition. In addition, we propose a method to utilize context information without decoding by multi-task learning with speech recognition. According to the speech emotion recognition experiments using the IEMOCAP dataset, our model achieves a weighted accuracy of 70.6 % and an unweighted accuracy of 71.6 %, which shows that the proposed method is effective in improving the performance of speech emotion recognition."
DeepKLM - 통사 실험을 위한 전산 언어모델 라이브러리 -,2021,"['BERT', 'language model', 'surprisal', 'experimental syntax', 'corpus', '언어모델', '서프라이절', '실험통사론', '말뭉치']",,"This paper introduces DeepKLM, a deep learning library for syntactic experiments. The library enables researchers to use the state-of-the-art deep computational language model, based on BERT (Bidirectional Encoder Representations from Transformers). The library, written in Python, works to fill the masked part of a sentence with a specific token, similar to the Cloze task in the traditional language experiments. The output value of surprisal is related to human language processing in terms of speed and complexity. The library additionally provides two visualization tools of the heatmap and the attention head visualization. This article also provides two case studies of NPIs and reflexives employing the library. The library has room for improvement in that the BERT-based components are not entirely on par with those in human language sentences. Despite such limits, the case studies imply that the library enables us to assess human and deep learning machines’ language ability."
분산전원용 PCS의 누설전류 등가회로 모델 및 분석,2021,"['Leakage current', 'Common-mode Voltage', 'Equivalent circuit', 'Parasitic capacitance']",,"Due to continuous environmental problems around the world and an increase in energy consumption, interest in solar power, wind power, and hydropower is increasing. Among them, photovoltaics is the most used in industries and homes because it is easy to repair and can be used for a long time. In photovoltaic PCS, parasitic capacitance is generated between the photovoltaic module and the ground due to the installation structure of the photovoltaic module. This parasitic capacitance, VCM, which is a common mode voltage generated by switching of PCS, is applied to CP, and leakage current occurs. In this paper, the common mode voltage characteristics at three-phase three-level, and three-phase two-level were compared. Proposed an equivalent circuit that was equalized from unaffected by grid impedance the point of view of the common-mode by dividing it into a transformerless type PCS connected to the grid ground and a transformer type PCS applying a transformer. The validity of the proposed equivalent circuit was verified by constructing a three-phase three-level PCS and comparing the equivalent circuit using the experiment measurements and PSIM. In the transformerless PCS, the leakage current is equivalent to 1.13A in the equivalent circuit and 1.14A in the experiment. In the transformer type PCS, the equivalent circuit is 68mA and the experiment 70mA. In addition, for the validity of the proposed equivalent circuit, FFT analysis of leakage current, which is the switching frequency, was performed."
Number Normalization in Korean Using the Transformer Model,2021,"['텍스트 정규화', '숫자 정규화', '시퀀스 투 시퀀스', '트랜스포머', '웹 텍스트 데이터', 'text normalization', 'number normalization', 'sequence to sequence', 'transformer', 'web text data']",,
Improving BERT-based Sentiment Analysis Model using Graph-based Ranking Mechanism,2021,"['인공지능', '자연어 처리', '감정분석', '그래프 기반 메커니즘', '인공신경망', 'BERT', 'artificial intelligence', 'natural language processing', 'sentiment analysis', 'graph-based mechanism', 'neural network']","문서 처리의 자동화에 대한 필요성이 대두됨에 따라 인공지능을 통한 자연어 처리(Natural Language Processing) 분야에서 연구가 활발하게 진행되고 있다. 본 연구에서는 자연어 처리 분야 중 특히 감정분석(Sentiment Analysis) 분야에서 그래프 기반의 순위화 메커니즘을 통해 추출한 형태소, 또는 요약 기반의 벡터인 GRAB vector(GRAph-Based vector)를 제안하고 이를 통해 기존의 BERT(Bidirectional Embedding Representations from Transformers)모델에 적용한다. 이를 통하여 더욱 강인하고 성능이 향상된 GRAB-BERT 모델을 제안한다. 또한, GRAB vector가 모델에 미치는 영향을 분석하기 위하여 재귀적 인공신경망(Recurrent Neural Network) 기반 모델들과 BERT 기반 모델에 시퀀스 입력 길이를 각각 다르게 학습한 경우 GRAB vector의 적용 여부에 따른 성능을 한국어와 영어에 대하여 분석한다. 결과적으로 형태소 단위로 추출된 벡터가 BERT와 같은 병렬적으로 문자를 처리하는 모델의 경우, 더욱 강인한 학습이 가능하며 성능이 향상됨을 보인다. 추가로, BERT 기반의 모델과 반대로 재귀적 인공신경망 기반모델들의 경우 형태소 기반이 아닌 그래프 기반 요약문 추출을 통한 벡터를 적용한 경우가 더 효과적임을 보인다.","Due to the need for automated document processing, artificial intelligence research has been actively conducted in the field of natural language processing(NLP). In this paper, we propose the GRAB vector(GRAph-Based vector), which consists of vectorized keyword-based morphemes or summaries extracted from the graph-based ranking mechanism. Next, we applied the GRAB vector to the sentiment analysis task, which is an NLP task, and we proposed a more accurate and robust model, GRAB-BERT(GRAB vector-BERT model). Then, to analyze the effect of the GRAB vector on this model, we compared the performances of recurrent neural network models(RNNs) and BERT models with or without the application of the GRAB vector on both English and Korean text samples with different sequence sizes. Our results demonstrate that applying the GRAB vector to models such as BERT to process inputs in parallel improves the robustness of the model and its performance. Furthermore, unlike BERT-based models, RNN models are more effective when applying graph-based extracted summaries than when applying morpheme-based summaries."
Transformer 기반 비윤리적 문장 탐지,2021,"['인공지능', '문장분류', '자연어처리', '비윤리적 문장', '딥러닝', 'artificial intelligence', 'sentence classification', 'natural language processing', 'unethical sentence', 'deep learning']",,"Social network services (SNS) have spread due to the development of information and communication technology, but at the same time, they have caused serious social problems such as malicious comments. The number of arrests and incidents of cyber defamation and insults increased sharply from 8,880 in 2014 to 16,633 in 2019, and measures are required to solve the problem. However, existing regulations such as IP blacklist and slang filters make it difficult to detect malicious comments. Therefore, we need an artificial intelligence model optimized for unethical sentence detection. This paper proposes a Transformer-based unethical sentence detection model that has shown high performance in natural language processing. The model showed accuracy of 95.03% and will be utilized as an unethical sentence detection model. Also, it will be applied in various fields such as streaming services as well as comments on SNS."
Self-revising Transformer with Multi-view for Image Captioning,2021,"['자연어 처리', '이미지 캡션 생성', '멀티-헤드 주의 기제 기법', '다중 관점 인코더', 'natural language processing', 'image captioning', 'multi-head attention', 'multi-view encoder']",,
무인 감시 Transformer,2021,"['Deep learning', 'Transformer', 'Segmentation', 'Visual surveillance']",,"In a visual surveillance system, even the same object should exhibit different detection results depending on the surrounding environment configuration. To this end, the model for visual surveillance needs to detect an object by understanding the state of the object according to the environment on the image. In this study, for such visual surveillance, an object segmentation model applied with a transformer structure suitable for image processing was used to divide objects inside the image into foreground and background. A modified attention structure was presented for the corresponding transformer structure, and the results of object segmentation models according to the type of input data were compared."
Design and Modeling of Octagonal Planar Inductor and Transformer in Monolithic Technology for RF Systems,2021,['Forward converter  · Inductor  · Integration  · Octagonal  · Planar  · RF  · Transformer'],,"This paper presents the design and modeling of planar inductor and transformer for their integration in a Forward converter.The windings are of octagonal spiral planar topology. Basing on Wheeler method, we evaluate the inductance values of planar coils. All parasitic eff ects generated by stacking of diff erent material layers are summarized perfectly in the π-electrical model of both on chip inductor and transformer. The electromagnetic simulation, by using COMSOL Multiphysics 5.3 software, illustrates the distribution of magnetic fi eld lines, the current density and the temperature distribution in our integrated micro coils. To validate the diff erent geometric and electrical parameters values, we have simulated the circuit of the Forward converter containing the π-electrical circuit of on chip integrated inductor and transformer, using the software TINA 9.0."
Zero-anaphora resolution in Korean based on deep language representation model: BERT,2021,"['attention', 'bidirectional encoder representations from transformers (BERT)', 'deep learning', 'language representation model', 'zero-anaphora resolution (ZAR)']",,"It is necessary to achieve high performance in the task of zero anaphora resolution (ZAR) for completely understanding the texts in Korean, Japanese, Chinese, and various other languages. Deep-learning-based models are being employed for building ZAR systems, owing to the success of deep learning in the recent years. However, the objective of building a high-quality ZAR system is far from being achieved even using these models. To enhance the current ZAR techniques, we fine-tuned a pretrained bidirectional encoder representations from transformers (BERT). Notably, BERT is a general language representation model that enables systems to utilize deep bidirectional contextual information in a natural language text. It extensively exploits the attention mechanism based upon the sequence-transduction model Transformer. In our model, classification is simultaneously performed for all the words in the input word sequence to decide whether each word can be an antecedent. We seek end-to-end learning by disallowing any use of hand-crafted or dependency-parsing features. Experimental results show that compared with other models, our approach can significantly improve the performance of ZAR."
Korean automatic spacing using pretrained transformer encoder and analysis,2021,"['attention', 'BERT', 'Korean automatic spacing', 'natural language processing', 'pretrained transformer encoder']",,"Automatic spacing in Korean is used to correct spacing units in a given input sentence. The demand for automatic spacing has been increasing owing to frequent incorrect spacing in recent media, such as the Internet and mobile networks. Therefore, herein, we propose a transformer encoder that reads a sentence bidirectionally and can be pretrained using an out‐of‐task corpus. Notably, our model exhibited the highest character accuracy (98.42%) among the existing automatic spacing models for Korean. We experimentally validated the effectiveness of bidirectional encoding and pretraining for automatic spacing in Korean. Moreover, we conclude that pretraining is more important than fine‐tuning and data size."
Transmission Distance Improvement of a Two-Coil Magnetic Resonance Wireless Power Transmission System Using Transformers,2021,"['Magnetic Resonance Modeling', 'Q-Factor', 'Wireless Power Transfer']",,"In this paper, a two-coil magnetic resonance wireless transmission system is studied to improve the transmission distance using transformers. A conventional two-coil and four-coil wireless power transmission (WPT) system as well as a two-coil WPT system with transformers are analyzed comparatively via circuit simulations and experiments. Circuit analysis was used to predict the transmission distance with the highest efficiency. To verify the improvement in the transmission distance of the proposed system, transformers with inductance values of 80, 100, and 140 μH were fabricated and analyzed through experiments and simulations. A maximum S21 parameter of 0.76 was noted when the inductance was 80 μH and the transmitting distance was 4 cm. The experimental results almost matched the simulation results. From the experiments, it was shown that the transmitting distance of a WPT system can be adjusted by using transformers. Additionally, it was found that the transmitting distance is inversely proportional to the transformer inductance, and the efficiency of the WPT system decreases with the transmitting distance."
Personality Prediction Based on Text Analytics Using Bidirectional Encoder Representations from Transformers from English Twitter Dataset,2021,"['Personality prediction', 'Twitter', 'Big Five personality traits', 'BERT']",,"Personality traits can be inferred from a person’s behavioral patterns. One example is when writing posts on social media. Extracting information about individual personalities can yield enormous benefits for various applications such as recommendation systems, marketing, or hiring employees. The objective of this research is to build a personality prediction system that uses English texts from Twitter as a dataset to predict personality traits. This research uses the Big Five personality traits theory to analyze personality traits, which consist of openness, conscientiousness, extraversion, agreeableness, and neuroticism. Several classifiers were used in this research, such as support vector machine, convolutional neural network, and variants of bidirectional encoder representations from transformers (BERT). To improve the performance, we implemented several feature extraction techniques, such as N-gram, linguistic inquiry and word count (LIWC), word embedding, and data augmentation. The best results were obtained by fine-tuning the BERT model and using it as the main classifier of the personality prediction system. We conclude that the BERT performance could be improved by using individual tweets instead of concatenated ones."
음질 및 속도 향상을 위한 선형 스펙트로그램 활용 Text-to-speech,2021,"['speech synthesis', 'machine learning', 'artificial intelligence', 'text-to-speech (TTS)', '음성 합성', '기계 학습', '인공지능']","인공신경망에 기반한 대부분의 음성 합성 모델은 고음질의 자연스러운 발화를 생성하기 위해 보코더 모델을 사용한다. 보코더 모델은 멜 스펙트로그램 예측 모델과 결합하여 멜 스펙트로그램을 음성으로 변환한다. 그러나 보코더 모델을 사용할 경우에는 많은 양의 컴퓨터 메모리와 훈련 시간이 필요하며, GPU가 제공되지 않는 실제 서비스환경에서 음성 합성이 오래 걸린다는 단점이 있다. 기존의 선형 스펙트로그램 예측 모델에서는 보코더 모델을 사용하지 않으므로 이 문제가 발생하지 않지만, 대신에 고품질의 음성을 생성하지 못한다. 본 논문은 뉴럴넷 기반 보코더를 사용하지 않으면서도 양질의 음성을 생성하는 Tacotron 2 & Transformer 기반의 선형 스펙트로그램 예측 모델을 제시한다. 본 모델의 성능과 속도 측정 실험을 진행한 결과, 보코더 기반 모델에 비해 성능과 속도 면에서 조금 더 우세한 점을 보였으며, 따라서 고품질의 음성을 빠른 속도로 생성하는 음성 합성 모델 연구의 발판 역할을 할 것으로 기대한다.","Most neural-network-based speech synthesis models utilize neural vocoders to convert mel-scaled spectrograms into high-quality, human-like voices. However, neural vocoders combined with mel-scaled spectrogram prediction models demand considerable computer memory and time during the training phase and are subject to slow inference speeds in an environment where GPU is not used. This problem does not arise in linear spectrogram prediction models, as they do not use neural vocoders, but these models suffer from low voice quality. As a solution, this paper proposes a Tacotron 2 and Transformer-based linear spectrogram prediction model that produces high-quality speech and does not use neural vocoders. Experiments suggest that this model can serve as the foundation of a high-quality text-to-speech model with fast inference speed."
면진장치의 종류에 따른 변압기의 지진거동 분석,2021,"['Transformers', 'Seismic Isolator', 'Artificial Seismic Wave', 'Dynamic Analysis', 'Response Acceleration']","본 연구에서는 변압기 내진성능을 확보하기 위해 면진장치에 따른 지진 저감성능과 특성을 평가하기 위한 수치해석을 수행하였다. 해석에 적용한 면진장치는 마찰진자면진장치(FIP)와 납-고무면진장치(LRB)이며 각 면진장치를 유한요소해석 프로그램을 사용하여 수치해석 모델화하였다. 또한, 변압기의 형상적 특성을 고려하기 위해 수치해석 모델링한 변압기의 형상과 하중이 실제 변압기와 유사하도록 모델링을 수행하였다. 변압기 해석모델에 마찰진자면진장치와 납-고무면진장치를 적용하여 동적해석을 수행하기 위해 내진설계기준 공통적용사항과 ICC-ES AC156의 응답스펙트럼을 참고하여 생성한 인공지진파를 해석에 적용하였다. 해석결과 면진장치가 적용되지 않은 해석과 비교하였을 때 마찰진자면진장치를 적용한 변압기에서 발생한 가속도 감쇠율은 75%로 나타났으며 납-고무면진장치를 적용한 변압기의 응답가속도 감쇠율은 최소 42%로 나타나 마찰진자면진장치가 납-고무면진장치에 비해 가속도 응답면에서 우수한 결과를 보였다. 또한, 면진장치를 적용하였을 때 발생하는 변압기의 회전을 검토하기 위해 가속도가 작용할 때 변압기 좌우측의 상대변위를 측정하였다. 이때 납-고무면진장치가 마찰진자면진장치에 비해 변압기의 회전이 큰 것으로 나타나 납-고무면진장치의 설치를 고려할 때 면진 대상구조물의 회전에 대한 적절한 대응책의 마련이 필요할 것으로 보인다.","In this study, a numerical analysis was performed to evaluate the seismic reduction performance and characteristics of a seismic isolator used to ensure the performance of the transformer under seismic conditions. The seismic isolators used in this analysis were the Friction Isolation Pendulum(FIP) and the Lead-Rubber Bearing (LRB). Each seismic isolator was modeled using a numerical analysis program. To consider the characteristics of the transformer, the model in the simulation was made similar to the actual shape of the transformer. To perform dynamic analysis, artificial seismic waves were generated and targeted onto the transformer by referring to the response spectrums for common applications from the seismic design standards and the ICC-ES AC156. In the dynamic analysis of the model, the FIP showed better results in terms of acceleration response than the LRB. Also, in the analysis, the LRB showed bigger rotational deformation as compared to the FIP. Therefore, appropriate countermeasures against the rotational deformation of the structure are required while installing the LRB."
Deep Learning-based Target Masking Scheme for Understanding Meaning of Newly Coined Words,2021,"['Target Masking', 'Deep Learning', 'BERT', 'Newly Coined Words', 'Sentiment Analysis', '표적 마스킹', '딥러닝', '신조어', '감성분석']","최근 대량의 텍스트 분석을 위해 딥 러닝(Deep Learning)을 활용하는 연구들이 활발히 수행되고 있으며, 특히 대량의 텍스트에 대한 학습 결과를 특정 도메인 텍스트의 분석에 적용하는 사전 학습 언어 모델(Pre-trained Language Model)이 주목받고 있다. 다양한 사전 학습 언어 모델 중 BERT(Bidirectional Encoder Representations from Transformers) 기반 모델이 가장 널리 활용되고 있으며, 최근에는 BERT의 MLM(Masked Language Model)을 활용한 추가 사전 학습(Further Pre-training)을 통해 분석 성능을 향상시키기 위한 방안이 모색되고 있다. 하지만 전통적인 MLM 방식은 신조어와 같이 새로운 단어가 포함된 문장의 의미를 충분히 명확하게 파악하기 어렵다는 한계를 갖는다. 이에 본 연구에서는 기존의 MLM을 보완하여 신조어에 대해서만 집중적으로 마스킹을 수행하는 신조어 표적 마스킹(NTM: Newly Coined Words Target Masking)을 새롭게 제안한다. 제안 방법론을 적용하여 포털 ‘N’사의 영화 리뷰 약 70만 건을 분석한 결과, 제안하는 신조어 표적 마스킹이 기존의 무작위 마스킹에 비해 감성 분석의 정확도 측면에서 우수한 성능을 보였다.","Recently, studies using deep learning to analyze a large amount of text are being actively conducted. In particular, a pre-trained language model that applies the learning results of a large amount of text to the analysis of a specific domain text is attracting attention. Among various pre-trained language models, BERT(Bidirectional Encoder Representations from Transformers)-based model is the most widely used. Recently, research to improve the performance of analysis is being conducted through further pre-training using BERT""s MLM(Masked Language Model). However, the traditional MLM has difficulties in clearly understands the meaning of sentences containing new words such as newly coined words. Therefore, in this study, we newly propose NTM(Newly coined words Target Masking), which performs masking only on new words. As a result of analyzing about 700,000 movie reviews of portal ""N"" by applying the proposed methodology, it was confirmed that the proposed NTM showed superior performance in terms of accuracy of sensitivity analysis compared to the existing random masking."
분류 정확도 향상을 위한 선택적 마스킹 기반 추가 사전 학습 기법,2021,"['감성 분석', '선택적 마스킹', '어텐션 메커니즘', 'sentiment analysis', 'BERT', 'MLM', 'selective masking', 'attention mechanism']","최근 여러 자연어 처리 분야에서 사전 학습 언어 모델인 BERT를 활용하여 분석 과제에 최적화된 텍스트 표현을 추출하려는 연구가 활발하게 이루어지고 있다. 특히 BERT의 학습 방식 중 하나인 MLM(Masked Language Model)을 활용하여 도메인 정보 또는 분석 과제 데이터를 추가 사전 학습(Further Pre-training)하는 시도가 이어지고 있다. 하지만 기존의 MLM 기법이 채택한 무작위 마스킹을 사용하여 감성 분류 과제에서 추가 사전 학습을 수행하는 경우, 분류 학습에 중요한 단서가 되는 단어가 마스킹될 수 있다는 가능성으로 인해 문장 전체에 대한 감성 정보 학습이 충분히 이루어지지 않는다는 한계가 있다. 이에 본 연구에서는 무작위 마스킹이 아닌 단서 단어를 제외하고 마스킹하는 선택적 마스킹을 통해 감성 분류 과제에 특화된 추가 사전 학습을 수행할 수 있는 방법을 제안한다. 더불어 주변 단어를 선택하기 위해 어텐션 메커니즘(Attention Mechanism)을 활용하여 단어의 감성 기여도를 측정하는 방안도 함께 제안한다. 제안 방법론을 실제 감성 댓글에 적용하여 문장 벡터를 추론하고 감성 분류 실험을 수행한 결과, 제안 방법론이 기존의 여러 비교 모델에 비해 분류 정확도 측면에서 우수한 성능을 나타냄을 확인하였다.","Recently, studies to extract text expressions optimized for analysis tasks by utilizing bidirectional encoder representations from transformers (BERT), which is a pre-training language model, are being actively conducted in various natural language processing fields. In particular, attempts are being made to further pre-train domain information or target data using masked language model (MLM), which is one of the BERT training methods. However, if further pre-training is performed with the existing random masking when performing sentiment classification, there is a limitation that sentimental nuance for the entire sentence may not be sufficiently learned if the words that are important clues to the sentiment classification are masked. Therefore, in this study, we propose an further pre-training method specialized for sentiment classification tasks which sufficiently reflect sentiment information in sentences by selective masking that excludes clue words from masking candidates. In addition, this study proposes a method to distinguish between clue words and surrounding words as the role of words by utilizing the attention mechanism. On inferring sentence vectors by applying the proposed methodology to actual sentiment comments and performing sentiment classification experiments, it was confirmed that the proposed methodology showed superior performance in terms of classification accuracy compared to several existing comparison models."
기도 신학 : 욕망(desire)의 변화(transformation)를 중심으로,2021,"['기도', '욕망의 변화', '삼위일체', '사라 코클리', '닛사의 그레고리우스', 'Prayer', 'Transformation', 'Trinity', 'Sarah Coakley', 'Gregory of Nyssa']","기도하는 방법(Lex orandi)은 믿는 방식(lex credendi)과 별개가 아니다. 기도와 신 학적 교리는 결코 떨어져 생각할 수 없다. 기도란 무엇인가에 대한 응답은 여러 차원 에서 논의할 수 있다. 그러나 필자는 기도의 역할, 기도의 결과에 대해 집중한다. 왜 냐하면, 기도에 대한 정의는 기도를 통한 삶의 변화에 따라 달라질 수 있기 때문이고, 나아가 기도에 대한 ‘정의’(definition)와 ‘실천’(praxis)의 이분법적 접근을 피할 수 있 기 때문이다. 사라 코클리(Sarah Coakley)는 기도를 삶에서 나타나는 실천 또는 생활 방식으로 분류한다. 기도는 기도하는 사람의 내적인 변화와 외적인 삶의 변화를 통해 완성된다. ‘성자’ 예수 그리스도의 이름으로 ‘성부’ 하나님께 드려진 기도는 변화 (transformation)를 이루는 원동력인데, 여기서 ‘성령’이 이끄는 힘에 의지한 우리들의 실천도 간과될 수 없다. 이를 통해 보았을 때, 기도는 삼위일체 교리와 밀접한 관계가 있다. 삼위일체 교리는 기도를 통해서 형성되었고 동시에 기도를 위한 것이다. 사라 코클리(Sarah Coakley)는 성부와 성자 중심의 전통적 삼위일체를 “위계적 모델”이라 명명하고, 이를 넘어서 성령의 역할을 재조명함으로써 새로운 삼위일체 모델을 제시 한다. 성령은 유한과 무한, 타자와 자기를 잇는 사랑의 끈이요 매개이다. 성령의 중보와 도움으로 유한한 인간이 삼위일체 하나님의 경륜적 삶에 동참할 수 있고, 나아가 하나님과 연합한 존재로 변화된 삶을 살 수 있다. 따라서, 코클리가 주장하는 바, “성 령 주도적” 삼위일체론(Spirit-led incorporative Trinity)은 기도를 통한 우리들의 인 식과 존재의 변화를 위해 형성된 교리이다. 이를 위한 코클리의 신학적 방법론은 닛 사의 그레고리우스(Gregory of Nyssa)로부터 영향을 받았고, 그 핵심에 ‘욕망(desire)’ 의 ‘변화’(transformation)라는 개념이 있다. 프로이드 이후 형성된 욕망에 대한 개념 은 수정되어야 한다. 플라톤이 사용한 에로스(eros)와 욕망(desire)은 성적인 합일을 위한 욕구 불만의 표출이 아니라, 신적인 것을 향한 갈망이었다. 그레고리우스는 영성 신학적 관점에서 이를 수정 보완하였는데, 거룩한 하나님과 연합하고자 하는 갈망이 다. 욕망의 변화는 기도를 통해 가능하다. 성령 안에서, 성령을 통해 기도할 때, 우리 가 성부 하나님의 사랑을 알 수 있고, 성자 예수 그리스도를 본받아 하나님과 연합할 뿐 아니라, 변화된 존재로 살아갈 수 있다. 따라서, 본 논고는 삼위일체 교리와 기도 의 연관성에 대해 논증할 뿐 아니라, 기도 신학은 인간에 대한 이해와 더불어 구원론 에 대한 새로운 길을 제시한다고 주장한다.","Lex orandi, lex credendi; the law of what is to be prayed is never to be separated from the law of what is to be believed. It is not easy to define what Christian prayer is. Still, the author focuses on the role and result of prayer, keeping away from separating the relationship between definition and praxis in praying. According to Sarah Coakley, prayer itself is life and the way to transform lives, which is constructing with and in the doctrine of Trinity: praying in the Son, Jesus Christ, praying to the Father God, and through the Holy Spirit. Sarah Coakley shows the strong relationship between the way of prayer and Christian doctrine by delving into a new model of the Trinity. Coakley argues convincingly that the Spirit plays a pivotal role in our understanding of the being and life of God. The Holy Spirit is to ensure liaison between the Father and the Son in love. In so doing, the finite, human being, can participate in the infinite’s salvific life. This new approach is referred to as a Spirit-led incorporative model for the Trinity. Coakley derives her understanding of the Trinity from the thought of Gregory of Nyssa, especially the concept of desire and transformation. The concept of desire, after Post-Freudian, should be modified. In his work, Symposium, Plato asserts that eros and desire are toward the infinite or divine. But, in Gregory's theology of spirituality, eros is employed, meaning to unite with God in and through praying. Prayer is how transformation occurs, leading us to be in union with God. Praying in and through the Holy Spirit, all Christians can realize what God the Father’s love is, unite with Jesus Christ by imitatio Christi, and finally live as the transformed and the transformer. Thus, the doctrine of Trinity and praying are in a strong relationship, developing a theology of prayer, which gives us new eyes that enable us to see new theological anthropology and soteriology"
Sentiment analysis of Korean movie reviews using XLM-R,2021,"['Sentiment analysis', 'Transformer', 'BERT', 'XLM-R', 'Transfer learning', 'Fine tuning']",,"Sentiment refers to a person's thoughts, opinions, and feelings toward an object. Sentiment analysis is a process of collecting opinions on a specific target and classifying them according to their emotions, and applies to opinion mining that analyzes product reviews and reviews on the web. Companies and users can grasp the opinions of public opinion and come up with a way to do so. Recently, natural language processing models using the Transformer structure have appeared, and Google's BERT is a representative example. Afterwards, various models came out by remodeling the BERT. Among them, the Facebook AI team unveiled the XLM-R (XLM-RoBERTa), an upgraded XLM model. XLM-R solved the data limitation and the curse of multilinguality by training XLM with 2TB or more refined CC (CommonCrawl), not Wikipedia data. This model showed that the multilingual model has similar performance to the single language model when it is trained by adjusting the size of the model and the data required for training. Therefore, in this paper, we study the improvement of Korean sentiment analysis performed using a pre-trained XLM-R model that solved curse of multilinguality and improved performance."
워드 임베딩 클러스터링을 활용한 리뷰 다중문서 요약기법,2021,"['Muti-document', 'Text Summarization', 'Transformer', 'Word Embedding', '다중문서', '텍스트 요약', '트랜스포머', '워드 임베딩']",,"Multi-document refers to a document consisting of various topics, not a single topic, and a typical example is online reviews. There have been several attempts to summarize online reviews because of their vast amounts of information. However, collective summarization of reviews through existing summary models creates a problem of losing the various topics that make up the reviews. Therefore, in this paper, we present method to summarize the review with minimal loss of the topic. The proposed method classify reviews through processes such as preprocessing, importance evaluation, embedding substitution using BERT, and embedding clustering. Furthermore, the classified sentences generate the final summary using the trained Transformer summary model. The performance evaluation of the proposed model was compared by evaluating the existing summary model, seq2seq model, and the cosine similarity with the ROUGE score, and performed a high performance summary compared to the existing summary model."
Hyperparameter experiments on end-to-end automatic speech recognition,2021,"['automatic speech recognition', 'transformer', 'neural network', 'hyperparameters', 'optimization']",,"End-to-end (E2E) automatic speech recognition (ASR) has achieved promising performance gains with the introduced self-attention network, Transformer. However, due to training time and the number of hyperparameters, finding the optimal hyperparameter set is computationally expensive. This paper investigates the impact of hyperparameters in the Transformer network to answer two questions: which hyperparameter plays a critical role in the task performance and training speed. The Transformer network for training has two encoder and decoder networks combined with Connectionist Temporal Classification (CTC). We have trained the model with Wall Street Journal (WSJ) SI-284 and tested on devl93 and eval92. Seventeen hyperparameters were selected from the ESPnet training configuration, and varying ranges of values were used for experiments. The result shows that “num blocks” and “linear units” hyperparameters in the encoder and decoder networks reduce Word Error Rate (WER) significantly. However, performance gain is more prominent when they are altered in the encoder network. Training duration also linearly increased as “num blocks” and “linear units” hyperparameters’ values grow. Based on the experimental results, we collected the optimal values from each hyperparameter and reduced the WER up to 2.9/1.9 from dev93 and eval93 respectively."
Finite Element Analysis of the Effects of Process and Material Parameters on the LVDT Output Characteristics,2021,"['LVDT(선형가변차동변압기)', 'Output Characteristic(출력 특성)', 'Supply Power(공급 전원)', 'Core Material(코어 재질)', 'Finite Element Analysis(유한요소해석)']",,"Linear variable differential transformer (LVDT) is a displacement sensor and is commonly used owing to its wide measurement range, excellent linearity, high sensitivity, and precision. To improve the output characteristics of LVDT, a few studies have been conducted to analyze the output using a theoretical method or a finite element method. However, the material properties of the core and the electromagnetic force acting on the core were not considered in the previous studies. In this study, a finite element analysis model was proposed considering the characteristics of the LVDT composed of coils, core, magnetic shell and electric circuit, and the core displacement. Using the proposed model, changes in sensitivity and linear region of LVDT according to changes in process and material parameters were analyzed. The outputs of the LVDT model were compared with those of the theoretical analysis, and then, the proposed analysis model was validated. When the electrical conductivity of the core was high and the relative magnetic permeability was low, the decrease in sensitivity was large. Additionally, an increase in the frequency of the power led to further decrease in sensitivity. The electromagnetic force applied on the core increased as the voltage increased, the frequency decreased, and the core displacement increased."
흐름이 있는 문서에 적합한 비지도학습 추상 요약 방법,2021,"['NLP', 'Summarization', 'GAN', 'BERT', 'Transformer']",,"Recently, a breakthrough has been made in the NLP area by Transformer techniques based on encoder-decoder. However, this only can be used in mainstream languages where millions of dataset are well-equipped, such as English and Chinese, and there is a limitation that it cannot be used in non-mainstream languages where dataset are not established. In addition, there is a deflection problem that focuses on the beginning of the document in mechanical summarization. Therefore, these methods are not suitable for documents with flows such as fairy tales and novels. In this paper, we propose a hybrid summarization method that does not require a dataset and improves the deflection problem using GAN with two adaptive discriminators. We evaluate our model on the CNN/Daily Mail dataset to verify an objective validity. Also, we proved that the model has valid performance in Korean, one of the non-mainstream languages."
SMERT: Single-stream Multimodal BERT for Sentiment Analysis and Emotion Detection,2021,"['자연어 처리', '멀티 모달', '감성 분석', '감정 탐지', '단일 입출력 트랜스포머', 'natural language processing', 'multimodal', 'sentiment analysis', 'emotion detection', 'single-stream transformer', 'BERT']",,
Simple and effective neural coreference resolution for Korean language,2021,"['coreference resolution', 'head‐final language', 'Korean', 'pretrained language model', 'recurrent neural network']",,"We propose an end‐to‐end neural coreference resolution for the Korean language that uses an attention mechanism to point to the same entity. Because Korean is a head‐final language, we focused on a method that uses a pointer network based on the head. The key idea is to consider all nouns in the document as candidates based on the head‐final characteristics of the Korean language and learn distributions over the referenced entity positions for each noun. Given the recent success of applications using bidirectional encoder representation from transformer (BERT) in natural language‐processing tasks, we employed BERT in the proposed model to create word representations based on contextual information. The experimental results indicated that the proposed model achieved state‐of‐the‐art performance in Korean language coreference resolution."
콘포머 기반 한국어 음성인식,2021,,,"We propose a speech recognition system based on conformer. Conformer is known to be convolution-augmented transformer, which combines transfer model for capturing global information with Convolution Neural Network (CNN) for exploiting local feature effectively. The baseline system is developed to be a transfer-based speech recognition using Long Short-Term Memory (LSTM)-based language model. The proposed system is a system which uses conformer instead of transformer with transformer-based language model. When Electronics and Telecommunications Research Institute (ETRI) speech corpus in AI-Hub is used for our evaluation, the proposed system yields 5.7 % of Character Error Rate (CER) while the baseline system results in 11.8 % of CER. Even though speech corpus is extended into other domain of AI-hub such as NHNdiguest speech corpus, the proposed system makes a robust performance for two domains. Throughout those experiments, we can prove a validation of the proposed system."
Automatic Classification of the Korean Triage Acuity Scale in Simulated Emergency Rooms Using Speech Recognition and Natural Language Processing: a Proof of Concept Study,2021,"['Triage', 'Classification', 'Machine Learning', 'Natural Language Processing', 'Deep Learning']",,"Background: Rapid triage reduces the patients' stay time at an emergency department (ED). The Korean Triage Acuity Scale (KTAS) is mandatorily applied at EDs in South Korea.For rapid triage, we studied machine learning-based triage systems composed of a speech recognition model and natural language processing-based classification.Methods: We simulated 762 triage cases that consisted of 18 classes with six types of the main symptom (chest pain, dyspnea, fever, stroke, abdominal pain, and headache) and three levels of KTAS. In addition, we recorded conversations between emergency patients and clinicians during the simulation. We used speech recognition models to transcribe the conversation.Bidirectional Encoder Representation from Transformers (BERT), support vector machine (SVM), random forest (RF), and k-nearest neighbors (KNN) were used for KTAS and symptom classification. Additionally, we evaluated the Shapley Additive exPlanations (SHAP) values of features to interpret the classifiers.Results: The character error rate of the speech recognition model was reduced to 25.21% through transfer learning. With auto-transcribed scripts, support vector machine (area under the receiver operating characteristic curve [AUROC], 0.86; 95% confidence interval [CI], 0.81–0.9), KNN (AUROC, 0.89; 95% CI, 0.85–0.93), RF (AUROC, 0.86; 95% CI, 0.82–0.9) and BERT (AUROC, 0.82; 95% CI, 0.75–0.87) achieved excellent classification performance.Based on SHAP, we found “stress”, “pain score point”, “fever”, “breath”, “head” and “chest” were the important vocabularies for determining KTAS and symptoms.Conclusion: We demonstrated the potential of an automatic KTAS classification system using speech recognition models, machine learning and BERT-based classifiers."
유사도 기반 이미지 캡션을 이용한 시각질의응답 연구,2021,,,"Visual Question Answering (VQA) and image captioning are tasks that require understanding of the features of images and linguistic features of text. Therefore, co-attention may be the key to both tasks, which can connect image and text. In this paper, we propose a model to achieve high performance for VQA by image caption generated using a pretrained standard transformer model based on MSCOCO dataset. Captions unrelated to the question can rather interfere with answering, so some captions similar to the question were selected to use based on a similarity to the question. In addition, stopwords in the caption could not affect or interfere with answering, so the experiment was conducted after removing stopwords. Experiments were conducted on VQA-v2 data to compare the proposed model with the deep modular co-attention network (MCAN) model, which showed good performance by using co-attention between images and text. As a result, the proposed model outperformed the MCAN model."
Argument Facet Detection in Online Debates Based on Attention Weights and Clustering with Combined Similarity Matrices,2021,"['argument mining', 'argument facets', 'argument clustering', 'semantic similarities', 'debate texts', 'attention weights']",,"The purpose of argument mining research is to analyze and understand the stances, content, and structures of large argumentative texts, such as online debates. For our research, we collected a list of identified arguments from online debates and attempted to use unsupervised methods to create a list of common justifications for each argument stance in each domain. We propose a model that clusters arguments by subtopics, or justifications, and then extracts the list of representative words for argument facets from each cluster. We were able to improve clustering performance by using a combination of three different similarity matrices (cosine similarity between BERT sentence embeddings, semantic textual similarity, and similarity between topic probability distributions) for the clustering algorithm. Our clustering produced 5%p and 7.5%p of ARI and V-measure values on average, which outperforms previous work in two of four domains. Additionally, we used a Transformer model to utilize the attention weights to discover argument facets, and we observed better performances compared to the method without attention weights."
Adaptive Distance Protection Scheme for Mutually Coupled Line,2021,['Adaptive relaying  · Distance protection  · Mutual coupling  · Mho characteristic  · Artifi cial neural networks'],,"The availability of zero-sequence current, under normal circumstances, determines the accuracy of the operation of a distance relay which is connected to a mutually coupled parallel line. When this is not available, the system adopts a diff erent compensation factor which if, not properly calculated introduces errors in the relay operation. The proposed adaptive protection scheme, described in this paper, consists of three modular artifi cial neural networks model (ANN). This is developed using the feed-forward nonlinear backpropagation Levenberg–Marquardt algorithm that determines the actual status of the mutually coupled lines. The remote terminal units connected to the current and voltage transformers are used to acquire the appropriate data. The proposed scheme also carefully determines the ground distance element reach settings by calculating the apparent impedance while considering mutual coupling for all practical system confi gurations from the ANN; this eliminates the need for a compensation factor. The results of the apparent impedance (R + jX) calculated by the proposed adaptive and the conventional schemes, showed an average percentage error of (0.06% and 0.02%) and (15% and 41.5%) respectively. Having obtained this result, the performance of the proposed adaptive scheme showed the exact fault location with a higher accuracy when compared with a compensated conventional scheme"
딥러닝 기반 단어 임베딩을 적용한 사진 자막 영작문 채점 시스템,2021,"['컴퓨터 언어보조학습', 'computer assisted language learning', '딥러닝', 'deep learning', '영작문', 'English writing', '단어 임베딩', 'word embedding', '준거참조검사', 'criterion-referenced test', '채점', 'scoring']",,"Since human grading of English writing requires substantial resources, many researchers in the area of Computer-Assisted Language Learning (CALL) have been focusing on automatic scoring systems based on natural language processing systems, machine learning, and other automatic processing mechanisms. English Testing Services (ETS) announced several automatic scoring systems for English writing. In this paper, we suggest using a deep learning based automatic scoring system for an English caption writing test. Our method involves using a sentence similarity measurement, which compares different levels of answer sentences with user writing input. We chose different word embedding types (Word2Vec, Word Mover‘s Distance (WMD), Bidirectional Encoder Representations from Transformers (BERT)) and Abstract Meaning Representation (AMR), a linguistic model for comparing semantic differences between two sentences based on semantic representation. Scoring systems should not only satisfy the requirements of complicated scoring rubrics but also meet the conditions of a language proficiency test. Our results show that BERT outperforms three competitive models in predicting accurate scoring levels and also shows the characteristics of the criterion reference which could theoretically express the standards of a language proficiency test."
Characteristic analysis of new hybrid compensation topology for wireless charging circuits,2021,"['Wireless power transfer', 'Constant-current output', 'Constant-voltage output', 'Electric vehicle charging']",,"Wireless power transfer (WPT) has the advantages of flexibility, safety, and high reliability. Thus, it is widely used in portable electronic equipment, electric vehicles (EV), medical equipment, and other fields. The resonance compensation method of wireless charging directly affects the gain characteristics of the output current and voltage. As a result, this is one of the main research focuses of wireless power transmission technology. A new hybrid compensation topology circuit is proposed in this paper, which is based on the EV constant-current (CC) and constant-voltage (CV) charging mode. In this hybrid topology circuit, an equivalent loosely coupled transformer T model is established for the primary and secondary coils. Through an analysis of the circuit principle, it is concluded that the wireless power transmission circuit can realize CC and CV outputs under a dynamic load change. The output power efficiency characteristics of the serial/parallel (S/P) compensation and serial/serial (S/S) compensation circuits are analyzed. In addition, the mutual inductance parameters are optimized by an efficiency power product method to achieve the overall optimal parameter design in terms of the circuit output power and efficiency. A simulation model and an experimental prototype of the wireless power transmission circuit are established. The design process of the circuit parameters under a given target current and voltage is also presented. The simulation verifies the correctness of the constant current and constant voltage output of the S/P-S/S hybrid compensation circuit. The experimental platform verifies the CC output under the S/P compensation topology, and the CV output under the serial/serial (S/S) compensation topology."
Adaptive Distance Protection Scheme Setting in Presence of SVC Using Remote Terminal Unit,2021,['Remote Terminal Unit  · FACTS Devices  · Adaptive Protection  · Relay Setting  · Transmission lines'],,FACTS devices are presently used to improve the power transfer capability of a transmission line and voltage stability of a power system network. Shunt injected current (I sh ) by the Static Var Compensators (SVC) causes underreach or overreach of distance relay when not considered during relay calibration of protection system. This paper presents an adaptive relay setting procedure in the presence of a SVC connected at the midpoint of a transmission line. The remote terminal unit RTU is connected to the SVC terminal via a current transformer (CT) measures the injected shunt current when the SVC is the switch in or out of the network and transferred the measured value to the local station via fi ber optic. The PSCAD/ EMTDC software is used to model an adaptive relay that implements the conventional and adaptive relay system settings.The proposed scheme presents a hybrid distance protection system whose setting is based on the prevailing SVC switching conditions; the relay system was implemented using mho characteristics relay. The results obtained show that the proposed scheme has a high accurate setting
Quasi resonant converter for autonomous power supply,2021,"['DC-DC power converters', 'Pulse width modulation', 'Quasiresonant converters', 'Shunt transistor']",,"Quasiresonant converters (QRCs) are increasingly being used in autonomous power supply systems. These converters are efficient, have small dimensions, and operate stably when the load changes. This study is devoted to the development of a zero-current switching QRC with improved characteristics. The main advantages of this QRC are a wide range of regulation and a low level of output voltage ripple. These advantages are achieved by the recuperation of excess energy due to a transistor shunting the primary winding of a transformer. In this work, a mathematical model of advanced QRC (AQRC) is developed, and the quantitative relationship between the parameters of the AQRC power circuit elements is established. In addition, the electromagnetic processes occurring in the AQRC are studied on the Simulink model. Moreover, the AQRC characteristics for various operating modes are studied, and the rational parameters of its components are determined. Then, an AQRC prototype is created. The tests of the prototype are in good agreement with the models. Results show that the AQRC allows for the adjustment of the output voltage ranging from 0 to 100% of the rated value and has a standard level of electromagnetic compatibility."
딥러닝을 활용한 모바일 어플리케이션 리뷰 분류에 관한 연구,2021,"['모바일 배달 어플리케이션', '사용자 리뷰 분류', '사용자 리뷰 분류법', 'Mobile Delivery Application', 'User review classification', 'User Reviews Taxonomy']","스마트폰과 태블릿과 같은 스마트 기기의 발달과 사용이 증가함에 따라, 모바일 기기를 기반으로 한 모바일 어플리케이션 시장이 급속도로 커지고 있다. 모바일 어플리케이션 사용자는 어플리케이션을 사용 경험을 공유하고자 리뷰를 남기는데, 이를 분석하면 소비자들의 다양한 니즈를 파악할 수 있고 어플리케이션 개발자들은 소비자들이 작성한 리뷰를 통해 애플리케이션의 개선을 위한 유용한 피드백을 받을 수 있다. 그러나 소비자들의 남기는 많은 양의 리뷰를 수작업으로 분석하기 위해서는 많은 시간과 비용을 지불해야하기 때문에 이를 최소화 할 방안을 마련할 필요성이 존재한다. 이에 본 연구에서는 구글 플레이스토어(Google PlayStore)의 배달 어플리케이션 사용자 리뷰를 수집한 후 머신러닝과 딥러닝 기법을 활용하여 어플리케이션 기능 장점, 단점, 기능 개선 요청, 버그 보고의 4가지 범주로 분류하는 방법을 제안한다. 연구 결과, Hugging Face의 pretrain된 BERT기반 Transformer모델의 성능의 경우 위의 4개의 범주에 대한 f1 score값은 차례대로 0.93, 0.51, 0.76, 0.83으로 LSTM, GRU보다 뛰어난 성능을 보인 것을 확인할 수 있었다.","With the development and use of smart devices such as smartphones and tablets increases, the mobile application market based on mobile devices is growing rapidly. Mobile application users write reviews to share their experience in using the application, which can identify consumers various needs and application developers can receive useful feedback on improving the application through reviews written by consumers. However, there is a need to come up with measures to minimize the amount of time and expense that consumers have to pay to manually analyze the large amount of reviews they leave. In this work, we propose to collect delivery application user reviews from Google PlayStore and then use machine learning and deep learning techniques to classify them into four categories like application feature advantages, disadvantages, feature improvement requests and bug report. In the case of the performance of the Hugging Face s pretrained BERT-based Transformer model, the f1 score values for the above four categories were 0.93, 0.51, 0.76, and 0.83, respectively, showing superior performance than LSTM and GRU."
열화상 이미지 분석을 통한 배전 설비 공정능력지수 감지 시스템 개발,2021,"['Object Detection', 'Process Capability Index', 'Thermal Imaging Technology']",,"Purpose: The purpose of this study is to propose a system predicting whether an electricity distribution system is abnormal by analyzing the temperature of the deteriorated system. Traditional electricity distribution system abnormality diagnosis was mainly limited to post-inspection. This research presents a remote monitoring system for detecting thermal images of the deteriorated electricity distribution system efficiently hereby providing safe and efficient abnormal diagnosis to electricians.Methods: In this study, an object detection algorithm (YOLOv5) is performed using 16,866 thermal images of electricity distribution systems provided by KEPCO(Korea Electric Power Corporation). Abnormality/ Normality of the extracted system images from the algorithm are classified via the limit temperature. Each classification model, Random Forest, Support Vector Machine, XGBOOST is performed to explore 463,053 temperature datasets. The process capability index is employed to indicate the quality of the electricity distribution system.Results: This research performs case study with transformers representing the electricity distribution systems. The case study shows the following states: accuracy 100%, precision 100%, recall 100%, F1-score 100%. Also the case study shows the process capability index of the transformers with the following states: steady state 99.47%, caution state 0.16%, and risk state 0.37%.Conclusion: The sum of caution and risk state is 0.53%, which is higher than the actual failure rate. Also most transformer abnormalities can be detected through this monitoring system."
ESS용 변압기의 접지방식에 의한 CMV 모델링 및 특성에 관한 연구,2021,"['CMV', 'ESS', 'PCS', 'IGBT', 'Noise', 'Y-△ winding method', '△-Y winding method', 'Grounding', 'Non-grounding', 'PSCAD/EMTDC software']","2017년을 시작으로 2020년 6월까지 총 29건의 화재사고가 발생하여 많은 재산피해가 보고되고 있으며, 전기적인 위해요인중의 하나인 공통모드 전압(CMV: common mode voltage)이 화재원인으로 추정되고 있다. 즉, ESS가 설치되어 있는 수용가의 연계용 변압기는 분산전원연계기준에 따라 Y-△결선방식을 채용해야 하지만, 일부 수용가들은 기존의 △-Y 결선방식을 적용하고 있으며, 실제 ESS 운용현장에서 배터리 측 절연레벨을 초과하는 CMV가 발생한 사례가 보고되고 있다. 따라서, 본 논문에서는 실제 ESS가 운용되는 사이트에서 발생하는 CMV의 특성을 분석하고, 이를 검증하기 위하여 배전계통 상용해석 프로그램인 PSCAD/EMTDC를 사용하여 AC전원부, PCS부, 배터리 부로 구성된 ESS 사이트의 모델링을 수행한다. 상기의 모델링을 바탕으로 시뮬레이션을 수행한 결과, 실제 측정 결과와 유사하게 PCS용 내부변압기 중성점의 접지방식에 따라 CMV의 특성이 크게 달라지고, 중성점이 접지된 경우 CMV의 값이 정격전압을 초과하여 배터리 측 절연레벨에 심각한 악영향을 줄 수 있음을 확인하였다. 또한, PCS용 내부변압기의 중성점을 비접지로 운용한 경우, CMV가 크게 감소하여, 전기설비기준의 절연레벨을 만족하는 것을 알 수 있었다.","Since 2017, a total of 29 fire accidents have occurred in energy storage systems (ESSs) as of June 2020. The common mode voltage (CMV) is one of the electrical hazards that is assumed to be a cause of those fire accidents. Several cases of CMV that violate the allowable insulation level of a battery section are being reported in actual ESS operation sites with △-Y winding connections. Thus, this paper evaluates the characteristics of CMV. An ESS site was modeled with an AC grid, PCS, and battery sections using PSCAD/EMTDC software. As a result of a simulation based on the proposed model, it was confirmed that characteristics of CMV vary significantly and are similar to actual measurements, depending on the grounding method of the internal transformer for PCS. The insulation level of the battery section may be severely degraded as the value of CMV exceeds the rated voltage in case of a grounding connection. It was found that the value of CMV dramatically declines when the internal transformer for PCS is operated as non-grounding connection, so it meets the standard insulation level."
Automated Essay Scoring Using Recurrence over BERT (RoBERT),2021,"['automated essay scoring', 'trait-specific essay scoring', 'essay evaluation', 'Recurrence over BERT (RoBERT)', 'hierarchical transformers']",,"This study aimed to build a system that could automate students"" English essay evaluation by using Recurrence over BERT (RoBERT), a state-of-the-art deep learning model. English essay evaluation is inherently time-consuming. It may reflect teacher bias. English teachers are usually burdened with the task of evaluating many essays in a short period of time. Automated essay scoring (AES) can solve these problems. It has the advantage of being able to evaluate essays in a short time and without bias. In this paper, the RoBERT model was trained and evaluated on Essay Set #8 of the Automated Student Assessment Prize (ASAP) dataset. The 5-fold cross validation evaluation method was used for fair comparison with the previously suggested AES models. As a result, the RoBERT model showed the highest agreement with the human raters’ resolved scores in 5 out of 6 trait scores than the previous evaluation models. The advantage of it is that it can use the pre-trained BERT model and deal with long inputs, overcoming the input size limit of the BERT model. It was confirmed that the RoBERT model works well for trait-specific evaluation of long essays. Thus, the RoBERT model can be used as an auxiliary means to automate the evaluation of students"" essays and reduce the excessive work of English teachers."
