title,date,keywords,abstract,multilingual_abstract
트랜스포머를 이용한 GVQA 모델의 성능 개선에 관한 연구,2021,[],"오늘날 인공지능(Artificial Intelligence, AI) 분야에서 가장 구현하기 어려운 분야 중 하나는 추론이다. 근래 추론 분야에서 영상과 언어가 결합한 다중 모드(Multi-modal) 환경에서 영상 기반의 질의 응답(Visual Question Answering, VQA) 과업에 대한 AI 모델이 발표됐다. 얼마 지나지 않아 VQA 모델의 성능을 개선한 GVQA(Grounded Visual Question Answering) 모델도 발표됐다. 하지만 아직 GVQA 모델도 완벽한 성능을 내진 못한다. 본 논문에서는 GVQA 모델의 성능 개선을 위해 VCC(Visual Concept Classifier) 모델을 ViT-G(Vision Transformer-Giant)/14로 변경하고, ACP(Answer Cluster Predictor) 모델을 GPT(Generative Pretrained Transformer)-3으로 변경한다. 이와 같은 방법들은 성능을 개선하는 데 큰 도움이 될 수 있다고 사료된다.",다국어 초록 정보 없음
사전학습 언어 모델을 활용한 트랜스포머 기반 텍스트 요약,2021,"['딥러닝', '자동 문서 요약', '트랜스포머', '사전학습 언어 모델', 'Deep Learning', 'Automatic Text Summarization', 'Transformer', 'Pre-trained Language Model']","소셜 미디어의 등장으로 많은 양의 텍스트 데이터가 온라인상에서 생산 및 유통되면서, 정보 이용자가 방대한 정 보로부터 필요한 정보만을 추려내는 작업은 더욱 어려워지게 되었다. 이로 인해 많은 양의 텍스트를 자동으로 요약 하기 위한 다양한 시도가 이루어지고 있으며, 특히 최근에는 풍부한 표현의 요약문을 새롭게 생성할 수 있는 추상 요약 접근법에 대한 연구가 활발히 수행되고 있다. 추상 요약 분야에서는 신경망 기반의 트랜스포머 모델이 우수한 성능을 보이며 널리 활용되고 있지만, 충분한 양의 학습 데이터가 확보되지 않으면 트랜스포머를 구성하고 있는 매 개변수의 학습이 충분히 이루어지지 않아서 양질의 요약문을 생성하기 어렵다는 한계를 갖는다. 따라서 본 연구는 소량의 학습 데이터가 주어진 상황에서도 양질의 요약문을 생성하기 위해, 한국어 사전학습 언어 모델인 KoBERT의 일부 요소를 추출하여 트랜스포머 기반의 추상 요약 모델에 적용하는 문서 요약 방안을 제시한다. 제안 방법론의 우 수성을 검증하기 위해 Dacon의 한국어 문서 생성 요약 데이터 42,803건에 대한 요약 실험을 수행한 결과, 제안 방법 론이 비교 방법론에 비해 요약 품질을 평가하는 지표인 ROUGE 기준으로 우수한 성능을 보임을 확인하였다.","As a large amount of text data is produced and distributed online with the advent of social media, it has become more difficult for users to extract only necessary information from a vast amount of information. As a result, various attempts have been made to automatically summarize a large amount of text. In particular, the abstractive summarization approach is being actively studied because it can create a new summary with rich contextual expressions. In the abstractive summarization task, neural network-based transformer models show high-performance and are used widely. However, if a sufficient amount of training data is not provided, it is difficult to generate a high-quality summary because the parameters constituting the transformer are not sufficiently learned. In this work, we proposes a text summarization method that generates a high-quality summary given a small amount of training data. Specifically, we extract some elements of KoBERT and apply them to a transformer-based abstractive summarization model. We conducted an experiment on 42,803 cases of Dacon's summary data to evaluate the performance of the proposed methodology. As a result of the experiment, it was confirmed that the proposed methodology showed superior performance based on the ROUGE index compared to the comparative method."
단어 손실함수와 반복 페널티를 추가한 트랜스포머 인코더-디코더 제목 생성 모델,2021,"['트랜스포머 인코더-디코더 모델', '자동 제목 생성', '단어 손실함수', '반복 페널티', 'transformer encoder-decoder', 'automatic title generation', 'word loss', 'repeat penalty']","제목은 문서를 대표하는 어구 혹은 문장이라 정의할 수 있다. 우리는 문서의 제목을 생성하기 위해 트랜스포머 기반 인코더-디코더 구조를 제안한다. 대용량 문서를 이용하여 트랜스포머 인코더-디코더 구조의 사전학습(pre-training)을 진행하고 본문과 제목 쌍으로 이루어진 문서를 이용하여 미세조정(fine-tuning)을 진행하였다. 또한 제목 생성 태스크로 범위가 제한되는 미세조정 과정에서 입력 문서에 나타나는 어절의 생성 비율을 증가시키기 위해 단어 손실함수를 추가하고 토큰이 반복적으로 생성되는 문제를 개선하기 위한 반복 패널티를 모델 추가하는 방법을 제안한다. 25,564개의 논문 데이터를 사용한 실험에서 단어 손실함수와 반복 패널티를 개별적으로 적용시킨 모델의 성능이 기존 모델에 비해 개선되고, 두 제안 방법을 모두 적용한 모델에서는 Rouge-L의 성능이 2.7% 향상되는 것을 확인하였다.","The title can be defined as a phrase or sentence the represents the document. We propose a transformer encoder-decoder model to generate the title of the document. The transformer model is pre-trained based on a the usage of a large document, and fine-tuning is performed using the data comprising of the body and title. Also, in the fine-tuning process, the scope of which is limited to the title generation task, a Word Loss is added to increase the generation ratio of words appearing in the input document and ground truth title. We propose a method of adding a Repeat Penalty to the model to reduce the problem that tokens are repeatedly generated. In an experiment conducted using data from 25,564 papers, the performance of the model that individually applied the Word Loss and the Repeat Penalty was improved compared to the baseline. It was confirmed that Rouge-L""s performance improved by 2.7% in the model to which both the proposed methods were applied."
투사적 그림검사 분야로의 트랜스포머 기반의 이미지 분류 모델 적용에 관한 연구,2021,"['미술치료', '인간과 컴퓨터 상호작용', '딥러닝', '이미지 분류', '트랜스포머']","본 논문에서는 심리진단을 위한 그림검사(Drawing Test) 과정에서 미술치료사의 객관적인 의사결정을 지원하기 위한 이미지 분류 모델인 VQ-ViT(Vector Quantization-Vision Transformer)을 제안한다. 사전학습을 위해 대규모의 레이블링 데이터셋이 필요한 기존의 이미지 분류 모델(e.g., Resnet, Vision Transformer)에 비해 VQ-ViT는 레이블링되지 않은 데이터에 대하여 비지도학습으로 임베딩을 진행한다. 우리는 CIFAR-10, TU-Berlin 그리고 직접 수집한 빗 속의 사람 그림 검사(PITR, Person-In-The-Rain) 데이터셋에 대하여 VQ-ViT와 기존의 이미지 분류 모델들과의 성능 비교실험을 진행하였다. 그 결과 VQ-ViT는 비지도 학습의 이미지 임베딩 기법과 적은 파라미터 수로 그림검사 분야에서의 발전 가능성을 보였다. 이를 기반으로 우리는 딥러닝 기반의 이미지 분류 모델의 한계점 파악 및 발전 방향을 논의한다.",다국어 초록 정보 없음
텍스트 트랜스포머 모델에서 어텐션 맵을 이용한 경사도 기반 화이트 박스 적대적 예제 생성 방안,2021,"['트랜스포머', '적대적 예제', '딥러닝', '텍스트 데이터', '화이트박스', 'Transformer', 'Adversarial example', 'Deep learning', 'Text data', 'Whitebox']",국문 초록 정보 없음,"Abstract should be placed here. These instructions give you guidelines for preparing papers for JDCS. The method of generating Adversarial examples for text data of the transformer model was mostly a black box attack method because of the discrete characteristics of text data. Recently, a gradient-based white box attack method targeting text data of a transformer model has been announced, which has the disadvantage that it takes a long time and is not efficient because it learns one distribution for each generation of an example. This paper improves the efficiency of the existing white box attack method by proposing an attention constraint using the attention structure of the transformer model. Through experiments, it has been proven that the generation time can be shortened by about 6.5% and the diversity of generated adversarial examples can be increased by 2.4% compared to the previous research results."
효율적인 트랜스포머를 이용한 팩트체크 자동화 모델,2021,"['Automated fact checking', 'Locality sensitive hashing', 'Natural language processing', 'Transformer']",국문 초록 정보 없음,"Nowadays, fake news from newspapers and social media is a serious issue in news credibility. Some of machine learning methods (such as LSTM, logistic regression, and Transformer) has been applied for fact checking. In this paper, we present Transformer-based fact checking model which improves computational efficiency. Locality Sensitive Hashing (LSH) is employed to efficiently compute attention value so that it can reduce the computation time. With LSH, model can group semantically similar words, and compute attention value within the group. The performance of proposed model is 75% for accuracy, 42.9% and 75% for Fl micro score and F1 macro score, respectively."
효율적인 트랜스포머에 기반한 설명 가능한 팩트체크 모델,2021,"['Attention Mechanism', 'Transformer', 'Explainable Fact Checking', 'Hashing Function']","본 논문에서는 어텐션 메커니즘에 기반하여 정보 판단에 대한 근거를 제공하는, 이른바 설명 가능한 팩트체크 모델을 제안할 것이다. 최근 미디어의 발달에 따라 각종 뉴스가 쏟아지고 있는 바, 이와 더불어 뉴스에 대한 진위 여부 판단, 즉 팩트체크가 주목받고 있는 상황이다. 하지만 현재 팩트체크는 언론인이나 시민 단체 일원들의 검색 능력에 의존하고 있어서, 이를 자동적으로 하는 모델에 대한 연구가 진행되고 있다. 이에 본 논문에서 설명 가능한 자동 팩트체크 모델을 제안하고자 한다.","In this paper, we introduce the model so-called Explainable Fact-Checking model based on attention mechanism which shows both the result of fact check of the news and the evidence of verdict. Recently, several news surge on media, so fact check attracts much attentions. However, in present fact check relies on the search made by journalists and members of fact check organization, so there is some researches about automated fact checking. Therefore in this paper we propose explainable automated fact checking model."
트랜스포머 모델을 활용한 드노보 펩타이드 서열 분석,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 모델에 제로카피를 적용한 서빙 기법,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
PatentQA: 트랜스포머 모델을 이용한 특허 질의응답 신경망 검색시스템,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
돌연변이 탐지를 위한 트랜스포머 모델과 RNN 모델의 성능비교,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 기반 다중 센서를 활용한 EPS 실차 소음 수준 예측 모델,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 기반 딥러닝 맵매칭 모델 개발,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 기반 압타머-단백질 상호작용 예측 분류 모델 설계,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
감성 사전을 활용한 트랜스포머 모델 기반의 감정분석,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
5G Dual Connectivity 환경을 위한 트랜스포머 모델 기반 상향 링크 자원 예측 연구,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
Temporal Fusion Transformer 모델을 활용한 다층 수평 시계열 데이터 분석,2021,[],"시계열 형태의 데이터는 다양한 분야에서 수집되고 응용되기 때문에 정확한 시계열 예측은 많은 분야에서 운영 효율성을 높일 수 있는 중요한 분석 방법으로 고려된다. 그중 다층 수평 예측은 사용자에게 전반적인 시계열 데이터 경향성을 제공할 수 있다. 하지만 다양한 정보를 포함하는 시계열 데이터는 데이터에 내재한 이질성(heterogeneity)까지 포괄적으로 고려한 방법을 통해서만 정확한 예측을 할 수 있다. 하지만 지금까지 많은 시계열 분석 모델들이 데이터의 이질성을 반영하지 못했다. 이러한 한계를 보완하고자 우리는 Temporal Fusion Transformer 모델을 사용하여 실생활과 밀접한 관련이 있는 데이터에 적용하여 이질성을 고려한 향상된 예측을 수행하였다. 실제, 주식 데이터와 미세 먼지 데이터와 같은 실생활 시계열 데이터에 적용하였고 실험 결과 기존 모델보다 Mean Squared Error(MSE)가 0.3487 낮은 것을 확인하였다.",다국어 초록 정보 없음
KoEPT: Transformer 기반 생성 모델을 사용한 한국어 수학 문장제 문제 자동 풀이,2021,[],"이 논문에서는 자연어로 구성된 수학 문장제 문제를 자동으로 풀이하기 위한 Transformer 기반의 생성모델인 KoEPT를 제안한다. 수학 문장제 문제는 일상 상황을 수학적 형식으로 표현한 자연어 문제로, 문장제 문제 풀이 기술은 실생활에 응용 가능성이 많아 국내외에서 다양하게 연구된 바 있다. 한국어의 경우 지금까지의 연구는 문제를 유형으로 분류하여 풀이하는 기법들이 주로 시도되었으나, 이러한 기법은 다양한 수식을 포괄하여 분류 난도가 높은 데이터셋에 적용하기 어렵다는 한계가 있다. 본 논문은 이를 해결하기 위해 우선 현존하는 한국어 수학 문장제 문제 데이터셋인 CC, IL, ALG514의 분류 난도를 측정한 후 5겹교차 검증 기법을 사용하여 KoEPT의 성능을 평가하였다. 평가에 사용된 한국어 데이터셋들에 대하여, KoEPT는 CC에서는 기존 최고 성능과 대등한 99.1%, IL과 ALG514에서 각각 89.3%, 80.5%로 새로운 최고 성능을 얻었다. 뿐만 아니라 평가 결과 KoEPT는 분류 난도가 높은 데이터셋에 대해 상대적으로 개선된 성능을 보였다.",다국어 초록 정보 없음
Transformer 모델을 이용한 감정 분석,2021,"['Sentiment analysis', 'Transformer']",국문 초록 정보 없음,다국어 초록 정보 없음
Shared Attention Network와 Multi-Query Attention을 이용한 Transformer 모델의 디코더 속도 개선,2021,"['기계번역', 'NMT', 'Transformer', 'Shared Attention Networks', 'Multi- Query Attention']",국문 초록 정보 없음,다국어 초록 정보 없음
위협 탐지 및 교통량 예측을 위한 장기 시계열 데이터에서의 Transformer 와 LSTM 모델 성능 비교,2021,"['Transformer', 'LSTM', 'Time-series data', 'Security Threat detection']",국문 초록 정보 없음,"Deep learning research to analyze industrial time-series data has been an active research topic. Recent studies have attempted to borrow the models for natural language processing(NLP) to handle time dependency issues. However, industrial data have different properties compared with NLP data: strongly dependent on a time axis. Moreover, because industrial information is continuously accumulating while the machine is running, it has a much longer sequence than other sequential data. In this study, we compare the performance of widely used natural language models, LSTM and Transformer, on such long-time series industrial data. For comparison, we performed experiments to detect an attack on a water treatment management system and to predict traffic flow on a highway. We confirmed that the Transformer using the attention mechanism showed better performance than the LSTM."
신약개발을 위한 Transformer기반의 딥러닝 모델,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
비전 트랜스포머를 활용한 이미지 분류 시스템 설계,2021,"['Image Classification', 'Vision Transformer', 'CIFAR-10', 'Convolutional Neural Network']","트랜스포머(Transformer)는 자연어 처리 분야에서 괄목할 만한 성과를 내어 왔다. 이러한 트랜스포머의 기능을 트랜스포머를 이미지 분류와 같은 다른 분야에 적용하는 연구가 최근 진행되고 있다. 본 연구에서는 이미지 데이터를 학습하여 해당 이미지를 분류하는 비전 트랜스포머 모델을 설계하고 구현하였다. 비전 트랜스포머(Vision Transformer ; ViT) 모델은 합성곱 신경망(Convolutional Neural Network ; CNN)을 사용하는 대신 어텐션 메커니즘 (Attention Mechanism)을 이미지 분류에 적용하여 계산비용을 크게 줄였다. 비전 트랜스포머의 설계를 검증하기 위해, CIFAR-10 데이터와 CIFAR-100 데이터를 벤치마크 데이터로 사용하였다. 실험 결과, 설계된 비전 트랜스포머를 향후 이미지 분류에 활용할 만한 가능성을 발견할 수 있었다.","We have seen remarkable achievements of Transformer in natural language processing fields. Recently, there have been research conducted for applying Transformer for other fields like image classification. In this paper, we design and implement vision transformer model that trains image data for classification. Vision Transformer (ViT) model significantly reduces the computational cost of training image classifiers by applying attention mechanism instead of using a convolutional neural network (CNN). To evaluate our design of Vision Transformer, we use CIFAR-10 and CIFAR-100 as benchmark datasets. From the experiment results, we discover the possibility of utilizing our Vision Transformer design for image classification tasks in the future."
주제 어트리뷰트 모델을 이용한 주제 키워드 기반 한국어 문서 요약,2021,"['topic centric summarization', 'machine learning', 'pre-training', 'MASS', 'PPLM', '주제 키워드 기반 문서 요약', '머신 러닝', '사전 학습', 'MASS', 'PPLM']","문서 추상 요약은 요약 모델이 원문의 핵심 정보를 파악하여 새로운 요약문을 생성하는 작업이다. 이때 추상 요약 모델로 일반적인 Sequence-to-Sequence 모델을 많이 사용하였지만 여기에 핵심 정보를 잘 표현하고 요약문에 반영하기 위해 주제(topic)을 넣어 요약문을 생성하는 주제 중심 요약(Topic centric summarization)을 하는 연구가 최근에 진행되고 있다. 그러나 기존의 방법은 주제 분포(Topic distribution)를 반영하여 문장을 생성하기 위해 모델을 처음부터 학습해야 하기 때문에 사전 학습 언어 모델의 장점을 살리기 어렵다. 본 논문에서는 사전 학습 언어 모델의 장점을 살리면서 주제 키워드를 요약문에 반영하여 주제 중심 요약을 할 수 있는 방법을 제시한다. 제안하는 주제 중심 요약 방법은 기존 조건부 언어 모델(Conditional Language Model)에서 연구되었던 PPLM(Plug and Play Language Model)의 어트리뷰트 모델을 문서 요약에서 사용되는 사전 학습 Sequence-to-Sequence 모델인 MASS에 적용하여 ‘주제 키워드 기반 요약문’을 생성하는 방법이다. 제안하는 방법은 별도의 추가 학습을 요구하지 않기 때문에 MASS의 언어 능력과 파인 튜닝으로 학습한 요약 능력을 그대로 사용함과 동시에 특정 키워드를 등장시켜 주제에서 벗어나지 않는 요약문을 생성할 수 있게 한다. 제안하는 방법의 우수성을 보이기 위해 BERT+Transformer 디코더를 사용한 모델, PPLM을 적용하지 않은 MASS 모델과 한국어 요약성능을 비교하였으며 평균적으로 ROUGE와 BERTScore 모두 성능이 향상되는 것을 확인할 수 있었다.","Abstractive summarization takes original text as an input and generates a summary containing the core-information about the original text. The abstractive summarization model is mainly designed by the Sequence-to-Sequence model. To improve quality as well as coherence of summary, the topic-centric methods which contain the core information of the original text are recently proposed.However, the previous methods perform additional training steps which make it difficult to take advantage of the pre-trained language model. This paper proposes a topic-centric summarizer that can reflect topic words to a summary as well as retain the characteristics of language model by using PPLM. The proposed method does not require any additional training. To prove the effectiveness of the proposed summarizer, this paper performed summarization experiments with Korean newspaper data."
LIME 알고리즘을 이용한 한국어 감성 분류 모델 해석,2021,[],"한국어 감성 분류 작업은 챗봇, 사용자의 물건 구매 평 분석 등 실 서비스에서 사용되고 있으며, 현재 딥러닝 기술의 발달로 높은 성능을 가진 신경망 모델을 활발히 사용하여 감성 분류 작업을 수행하고 있다. 하지만 신경망 모델은 입력 문장이 어떤 단어들로 인해 결과가 예측되었는지 해석하는 것이 쉽지 않으며, 최근 신경망 모델의 해석을 위한 모델 해석 방법들이 활발히 제안되어지고 있다. 본 논문에서는 모델 해석 방법 중 LIME 알고리즘을 이용하여 한국어 감성 분류 데이터 셋으로 학습된 모델들의 입력 문장 내 단어들 중 어떤 단어가 결과에 영향을 미쳤는지 해석하고자 한다. 그 결과, 85.23%의 성능을 보인 양방향 순환 신경망 모델의 해석 결과, 총 25,283개의 긍정, 부정 단어를 포함했지만, 상대적으로 낮은 성능을 보인 84.20%의 Transformer 모델의 해석 결과, 총 26,447개의 긍정, 부정 단어가 포함되어 있어 양방향 순환 신경망 모델보다 Transformer 모델이 신뢰할 수 있는 모델임을 확인할 수 있었다.",다국어 초록 정보 없음
CTR 예측을 위한 비전 트랜스포머 활용에 관한 연구,2021,"['클릭률', '심층신경망', '추천시스템', 'e-비즈니스', 'Click-Through Rate', 'Deep Neural Network', 'Recommender systems', 'e-business']","Click-Through Rate(CTR) 예측은 추천시스템에서 후보 항목의 순위를 결정하고 높은 순위의 항목들을 추천하여 고객의 정보 과부하를 줄임과 동시에 판매 촉진을 통한 수익 극대화를 달성할 수 있는 핵심 기능이다. 자연어 처리와 이미지 분류 분야는 심층신경망(deep neural network)의 활용을 통한 괄목한 성장을 하고 있다. 최근 이 분야의 주류를 이루던 모델과 차별화된 어텐션(attention) 메커니즘 기반의 트랜스포머(transformer) 모델이 제안되어 state-of-the-art를 달성하였다. 본 연구에서는 CTR 예측을 위한 트랜스포머 기반 모델의 성능 향상 방안을 제시한다. 자연어와 이미지 데이터와는 다른 이산적(discrete)이며 범주적(categorical)인 CTR 데이터 특성이 모델 성능에 미치는 영향력을 분석하기 위해 임베딩의 일반화(regularization)와 트랜스포머의 정규화(normalization)에 관한 실험을 수행한다. 실험 결과에 따르면, CTR 데이터 입력 처리를 위한 임베딩 과정에서 L2 일반화의 적용과 트랜스포머 모델의 기본 정규화 방법인 레이어 정규화 대신 배치 정규화를 적용할 때 예측 성능이 크게 향상됨을 확인하였다.","Click-Through Rate (CTR) prediction is a key function that determines the ranking of candidate items in the recommendation system and recommends high-ranking items to reduce customer information overload and achieve profit maximization through sales promotion. The fields of natural language processing and image classification are achieving remarkable growth through the use of deep neural networks. Recently, a transformer model based on an attention mechanism, differentiated from the mainstream models in the fields of natural language processing and image classification, has been proposed to achieve state-of-the-art in this field. In this study, we present a method for improving the performance of a transformer model for CTR prediction. In order to analyze the effect of discrete and categorical CTR data characteristics different from natural language and image data on performance, experiments on embedding regularization and transformer normalization are performed. According to the experimental results, it was confirmed that the prediction performance of the transformer was significantly improved when the L2 generalization was applied in the embedding process for CTR data input processing and when batch normalization was applied instead of layer normalization, which is the default regularization method, to the transformer model."
특허문서 자동분류를 위한 딥러닝 개별 모델 분류기와 앙상블 분류기의 성능비교,2021,"['Patent classification', 'Ensemble', 'Deep learning']",국문 초록 정보 없음,다국어 초록 정보 없음
추가 사전학습 기반 지식 전이를 통한 국가 R&D 전문 언어모델 구축,2021,"['국가 R&D', '지식 전이', '사전학습 모델', 'BERT', '추가 사전학습', 'National R&D', 'Knowledge Transfer', 'Pre-trained Language Model', 'Further Pre-training']","최근 딥러닝 기술이 빠르게 발전함에 따라 국가 R&D 분야의 방대한 텍스트 문서를 다양한 관점에서 분석하기 위한 수요가 급증하고 있다. 특히 대용량의 말뭉치에 대해 사전학습을 수행한 BERT(Bidirectional Encoder Representations from Transformers) 언어모델의 활용에 대한 관심이 높아지고 있다. 하지만 국가 R&D와 같이 고도로 전문화된 분야에서 높은 빈도로 사용되는 전문어는 기본 BERT에서 충분히 학습이 이루어지지 않은 경우가 많으며, 이는 BERT를 통한 전문 분야 문서 이해의 한계로 지적되고 있다. 따라서 본 연구에서는 최근 활발하게 연구되고 있는 추가 사전학습을 활용하여, 기본 BERT에 국가 R&D 분야 지식을 전이한 R&D KoBERT 언어모델을 구축하는 방안을 제시한다. 또한 제안 모델의 성능 평가를 위해 보건의료, 정보통신 분야의 과제 약 116,000건을 대상으로 분류 분석을 수행한 결과, 제안 모델이 순수한 KoBERT 모델에 비해 정확도 측면에서 더 높은 성능을 나타내는 것을 확인하였다.","With the recent rapid development of deep learning technology, the demand for analyzing huge text documents in the national R&D field from various perspectives is rapidly increasing. In particular, interest in the application of a BERT(Bidirectional Encoder Representations from Transformers) language model that has pre-trained a large corpus is growing. However, the terminology used frequently in highly specialized fields such as national R&D are often not sufficiently learned in basic BERT. This is pointed out as a limitation of understanding documents in specialized fields through BERT. Therefore, this study proposes a method to build an R&D KoBERT language model that transfers national R&D field knowledge to basic BERT using further pre-training. In addition, in order to evaluate the performance of the proposed model, we performed classification analysis on about 116,000 R&D reports in the health care and information and communication fields. Experimental results showed that our proposed model showed higher performance in terms of accuracy compared to the pure KoBERT model."
BERT를 활용한 상표 의견제출통지서 거절이유 분류모델 개발,2021,"['BERT', '자연어처리', '딥러닝', '워드 임베딩', '토크나이저', 'BERT', 'Natural Language Process', 'Deep Learning', 'Word Embedding', 'Tokenizer']","멀티미디어 콘텐츠의 증가와 스마트 기기의 보급으로 다양한 종류의 데이터가 폭발적으로 생산되고 있다. 특히, 텍스트 데이터는 오랜 시간 인류의 의사 표현수단이었으며, 텍스트 분석에 대한 수요 및 필요성은 다 양한 분야에서 지속적으로 증가하고 있다. 최근에는 다양한 분야에서 뛰어난 성능을 보이는 딥러닝과 텍스트 를 의미적으로 벡터화하는 워드 임베딩 (word embedding) 방식이 결합된 ELMo (embeddings from language model), GPT (generative pre-training of a language model), BERT (bidirectional encoder representations from transformers)와 같은 모델들이 개발되어왔다. 특히 구글이 개발한 BERT는 현재 자연 어처리 분야에서 가장 뛰어난 성능을 보이는 언어모델로 손꼽히고 있다. 하지만 ‘영어’나 소셜미디어 데이터 와 같은 ‘일반 텍스트’에 특화된 BERT는 ‘한국어’나 ‘R&D 문서, 지식재산권 문서 등’ 전문 분야에 특화된 텍 스트에서 최적의 성능이 구현되지 않기 때문에 전문 분야에 맞는 말뭉치(corpus)를 학습하는 등의 최적화 과 정을 통해 최적의 성능을 도출할 수 있다. 따라서 본 연구에서는 한글 상표 분야에 특화된 BERT를 개발하 기 위해 상표 의견제출통지서 내 거절이유 텍스트 말뭉치를 활용하여 상표 관련 전문 문서에 특화된 토크나 이저 (tokenizer) 모델을 학습하고 이를 BERT에 활용하였다. 제안 모델의 분류 정확도는 기존의 다국어 BERT 모델의 분류 정확도 성능보다 약 4.97%p 높은 96.89%를 기록하였다. 해당 결과를 통해 자연어 분야 에서 높은 성능을 보였던 BERT가 전문 용어로 구성된 말뭉치 데이터에서도 높은 분류 정확도를 보일 수 있 다는 것을 확인하였다.","The increase of multimedia contents and the spread of smart devices have resulted in explosive production of various kinds of data. In particular, text data has been a means of expressing human opinion for a long time, and the demand and necessity for text analysis are continuously increasing in various fields. Recently, models such as the ELMo (embeddings from language model), GPT (generating pre-training of a language model), BERT (bidirectional encoder representations from transformation) have been developed that combine deep learning, which is showing excellent performance in various fields, and the word embedding method that semantically vectorizes text. In particular, BERT developed by Google is considered to be one of the most outstanding language models in the field of natural language processing. However, BERT specialized in 'English' and plain text, such as social media data, does not implement optimal performance in specialized texts such as 'Korean' or 'R&D documents, intellectual property documents', so it can achieve optimal performance through optimization such as learning specialized corpus. Therefore, In this study, in order to develop a BERT specialized in the field of Korean trademarks, a tokenizer model specialized in trademark-related professional documents was learned using a text corpus for rejection in the trademark opinion submission notice and used for BERT. The classification accuracy of the proposed model was 96.89%, which is 4.97% higher than the classification accuracy performance of the existing multilingual BERT model. The results showed that BERT, which showed high performance in the field of natural language, could show high classification accuracy even in corpus data composed of specialized terms."
경량화 기법을 적용한 트랜스포머 기반의 한국어 음성인식 연구,2021,[],"최근 Transformer 모델이 여러 도메인에 적용되어 기존의 성능을 뛰어넘는 결과를 보이고 있다. 음성인식 분야에도 Transformer 음향모델이 다른 모델 성능 대비 우수한 결과치를 보이고 있다. 하지만, Transformer의 고질적인 연산량 문제의 원인인 Feed Forward 연산을 사용하는 Self-attention 방식을 음성 인식 도메인에 적합하도록 개선하였다. 해당, Light-weight 기법을 적용한 Transformer 음향모델을 한국어 기반의 데이터 셋에 적용하여 연구를 진행하였고 최종적으로 17% 의 연산량 감소를 확인할 수 있었다.",다국어 초록 정보 없음
비전 트랜스포머를 이용한 휴대 수하물 보안검색 위험화물 검출 연구,2021,[],"본 연구에서는 최근 각광받고 있는 비전 트랜스포머 백본 기반 객체 검출 알고리즘을 항공 및 지하철 등 휴대 수하물 보안검색에 적용하기 위해 x-선 투과 영상 내 위험화물 검출 모델을 구성하여 학습 및 검증한 결과를 보고하고자 한다. 최근 MS COCO 객체 검출 벤치마크 결과 가장 뛰어난 검출 성능이 발표된 바 있는 Swin 트랜스포머(Transformer) 백본과 Cascade R-CNN 검출기를 휴대 수하물 보안검색 x-선 촬영 영상 내 위험화물 검출에 적용하였으며, 휴대 수하물 보안검색 x-선 촬영 영상으로 구성된 공개 데이터셋인 SIXray-10에 대해 지도학습을 진행한 결과, 기존 합성곱 신경망 백본을 이용한 검출결과와 비교하여 개선된 63.8 ㎃P의 정확도를 얻을 수 있었다.",다국어 초록 정보 없음
비전 트랜스포머를 이용한 품질검사 관련 연구,2021,[],"본 논문은 개선된 트랜스포머 기반 이미지 이상 탐지 및 위치 파악 네트워크를 제안한다. 우리가 제안하는 모델은 재구성 기반 접근 방식과 패치 임베딩의 조합이다. 트랜스포머 네트워크를 사용하면 임베디드 패치의 공간 정보를 보존하는 데 도움이 되며, 이는 나중에 가우스 혼합 밀도 네트워크에 의해 처리되어 변칙 영역을 지역화한다. 또한 트랜스포머에 증류토큰을 활용한 전이학습을 적용한다. 제안된 네트워크는 기존의 VT-ADL 보다 정확도와 이미지 처리 속도 모두에서 향상된 결과를 보여주었다.",다국어 초록 정보 없음
인공지능 언어모델에 통사구조와 한국어 특징에 대한 지식을 주입하는 방법,2021,[],국문 초록 정보 없음,다국어 초록 정보 없음
효과적인 비전 트랜스포머를 통한 화재 감지,2021,"['컨볼루션 뉴럴 네트워크', '화재 감지', '포그기아의 데이터 세트', '산불', '스마트시티', '비전 변압기', 'Convolution Neural Network', 'Fire Detection', 'Foggia’s Dataset', 'Forest Fire', 'Smart Cities', 'Vision Transformers']","오늘날 현대사회에서 스마트하고 안전한 도시는 연구 커뮤니티의 주요 관심사 중 하나이다. 도시들은 개방된 지역, 농경지, 숲으로 둘러싸여 있으며, 화재 발생은 인간의 삶을 위협하고 그들의 재산도 손상시킬 수 있다. 최근 비전 센서 기반 화재 감지 기술은 컴퓨터 비전 분야의 전문가들을 통해, 최신 문헌에서 다양한 컨볼루션 신경 네트워크 (CNN)을 대한 최고의 성능을 달성하고 있다. 그러나 이러한 기술은 변환 불변이고, 지역성에 민감하며, 이미지에 대한 전체적인 이해가 부족하다. 또한 CNN 기반 모델은 계산 비용을 줄이기 위해 차원 축소를 위한 풀링 레이어 전략을 사용했지만, 가장 활동적인 특징 검출기의 정확한 위치와 같은 많은 의미 있는 정보를 손실한다. 이러한 문 제를 극복하기 위해 본 연구에서는 비전 트랜스포머(ViT)기반 화재 감지 모델을 개발하였다. ViT는 입력 이미지를 이미지 패치로 분할한 다음 워드 임베딩과 유사한 시퀀스 구조로 트랜스포머에 제공한다. 우리는 벤치마크 화재 데 이터 세트에서 제안된 작업의 성능을 평가하고 최신(SOTA) CNN 방법과 비교할 때 좋은 결과를 달성한다.","In today's modern age, smart and safe cities are one of the major concerns of the research community. The cities are surrounded by open areas, agricultural land, and forests, where fire incidence can make human lives threatening, damaging their properties as well. Recently, vision sensors-based fire detection has attracted computer vision domain experts, where the leading performance is achieved by a variety of convolution neural networks(CNN) in the recent literature. However, these techniques are translation invariant, locality-sensitive, and lacking a global understanding of images. Furthermore, CNN-based models use the pooling layers strategy for dimensionality reduction to reduce the computational cost but it also loses a lot of meaningful information such as the precise location of the most active feature detector. To overcome these problems, in this work, we developed Vision Transformers(ViT) based model for fire detection. The ViT split the input image into image patches and then feed these patches to the transformer in a sequence structure similar to word embeddings. We evaluate the performance of the proposed work on the benchmark fire dataset and achieve good results when compared to state-of-the-art(SOTA) fire detection CNN models."
객체 검출을 위한 트랜스포머와 공간 피라미드 풀링 기반의 YOLO 네트워크,2021,[],"일반적으로 딥러닝 기반의 객체 검출(Object Detection)기법은 합성곱 신경망(Convolutional Neural Network, CNN)을 통해 입력된 영상의 특징(Feature)을 추출하여 이를 통해 객체 검출을 수행한다. 최근 자연어 처리 분야에서 획기적인 성능을 보인 트랜스포머(Transformer)가 영상 분류, 객체 검출과 같은 컴퓨터 비전 작업을 수행하는데 있어 경쟁력이 있음이 드러나고 있다. 본 논문에서는 YOLOv4-CSP의 CSP 블록을 개선한 one-stage 방식의 객체 검출 네트워크를 제안한다. 개선된 CSP 블록은 트랜스포머(Transformer)의 멀티 헤드 어텐션(Multi-Head Attention)과 CSP 형태의 공간 피라미드 풀링(Spatial Pyramid Pooling, SPP) 연산을 기반으로 네트워크의 Backbone과 Neck에서의 feature 학습을 돕는다. 본 실험은 MSCOCO test-dev2017 데이터 셋으로 평가하였으며 제안하는 네트워크는 YOLOv4-CSP의 경량화 모델인 YOLOv4s-mish에 대하여 평균 정밀도(Average Precision, AP)기준 2.7% 향상된 검출 정확도를 보인다.",다국어 초록 정보 없음
다중스케일 기반 비전 트랜스포머를 이용한 깊이 추정,2021,"['vision transform', 'artificial neural network', 'depth estimation']","본 연구는 광범위한 컴퓨터 비전 문제를 성공적으로 해결한 새로운 아키텍쳐인 Visual Transformer(ViT)를 사용하여 단일 이미지로부터 깊이 정보를 추정하는 새로운 접근 방식을 제안한다. 인코더의 자기주의 메커니즘을 통해 모델은 넓은 이미지 영역에서 깊이 정보를 고려하는 여러 이미지 패치 간의 상관관계를 확인한다. 상대적으로 전역 인식에 집중하는 자기주의의 국소 영역 인식 성능 개선을 위해서 다중스케일 기반 특징맵을 학습하여 깊이 정보를 추정한다. 논문에서 제안한 프레임워크는 간단하면서도 효과적이며 단일 ViT를 사용한 모델보다 성능이 우수하다. 정량적, 정성적 결과는 디코더의 추가적인 특징을 학습한 네트워크가 다양한 평가지표, 시각화 측면에서 개선된 성능을 보여준다.","In this study, we proposes a new approach to estimating depth information from a single image using Visual Transformer (ViT), a new architecture that successfully solves a wide range of computer vision problems. Through the encoder""s self-attention mechanism, the model verifies the correlation between multiple image patches that take depth information into account over a large image area. In order to improve the local area recognition performance of global recognition through self-attention, depth information is estimated by learning multi-scaling based feature maps. The framework proposed in the paper is simple and effective, and has better performance than a model using only ViT. Quantitative and qualitative results show that the network learning additional features of the decoder has improved performance in terms of various evaluation indicators and visualization."
중공업 도메인 특화 언어 모델 : HeavyBERT,2021,[],"최근 자연어 처리 연구에서 대용량 말뭉치와 양방향 트랜스포머 기반 맥락을 고려한 BERT와 같은 언어 모델을 사전 학습하고, 응용 태스크에 미세 조정하여 높은 성능을 달성하였다. 그러나 이러한 언어 모델은 위키피디아나 뉴스 등, 일반적인 도메인에 대하여 사전 학습한 것이기 때문에 중공업 도메인과 같은 특수한 도메인의 태스크에 적합하지 않다는 문제점이 존재한다. 본 논문에서는 중공업 도메인의 말뭉치와 BERT를 기반으로 사전 학습한 중공업 도메인에 특화된 언어 모델인 HeavyBERT를제안한다.",다국어 초록 정보 없음
트랜스포머를 이용한 중국어 NER 관련 문자와 단어 통합 임배딩,2021,[],국문 초록 정보 없음,"Since the words and words in Chinese sentences are continuous and the length of vocabulary is huge, Chinese NER(Named Entity Recognition) always based on character representation. In recent years, many Chinese research has been reconsidered how to integrate the word information into the Chinese NER model. However, the traditional sequence model has complex structure, the slow inference speed, and an additional dictionary information is needed, which is difficult to implement in the industry. The approach in this paper has the state of the art and parallelizable, which is integrated the char-word embeddings, so that the model learns word information. The proposed model is easy to implement, and outperforms traditional model in terms of speed and efficiency, which is improved f1-score on two dataset."
오토인코더 기반의 생성 모델을 이용한 유입변압기 수명 예측 시스템,2021,"['Oil transformer', 'Neural network', 'Autoencoder', 'Data augmentation', 'Life estimation', '유입변압기', '신경회로망', '오토인코더', '데이터 증강', '수명예측']",국문 초록 정보 없음,다국어 초록 정보 없음
태그 라벨과 트랜스포머를 이용한 시맨틱 분할,2021,"['Deep learning', 'Transformer', 'Semantic Segmentation']",국문 초록 정보 없음,"Currently, research on artificial intelligence that autonomously interacts with surroundings without human management has attracted research attention in vehicle and robot-related fields. The recognition of the surrounding environment is the basis for artificial intelligence that requires interaction with the surroundings, which means that research on object detection is necessary. In general, object detection proceeds in the same way as detection and segmentation. Among them, in the case of segmentation, the size of the model is smaller, and more information can be obtained than detection using anchors. However, the inferior detection performance and generalization ability of this method for small objects has limited its further application. In this paper, a modified transformer structure with different configuration of training data from the existing label data is presented to improve the performance of segmentation."
시계열 예측을 위한 스타일 기반 트랜스포머,2021,"['Time Series Forecasting', 'Transformer', 'Generative Decoder', 'Style Transfer', '시계열 예측', '트랜스포머', '생성 디코더', '스타일 변환']",국문 초록 정보 없음,"Time series forecasting refers to predicting future time information based on past time information. Accurately predicting future information is crucial because it is used for establishing strategies or making policy decisions in various fields. Recently, a transformer model has been mainly studied for a time series prediction model. However, the existing transformer model has a limitation in that it has an auto-regressive structure in which the output result is input again when the prediction sequence is output. This limitation causes a problem in that accuracy is lowered when predicting a distant time point. This paper proposes a sequential decoding model focusing on the style transformation technique to handle these problems and make more precise time series forecasting. The proposed model has a structure in which the contents of past data are extracted from the transformer-encoder and reflected in the style-based decoder to generate the predictive sequence. Unlike the decoder structure of the conventional auto-regressive transformer, this structure has the advantage of being able to more accurately predict information from a distant view because the prediction sequence is output all at once. As a result of conducting a prediction experiment with various time series datasets with different data characteristics, it was shown that the model presented in this paper has better prediction accuracy than other existing time series prediction models."
트랜스포머 인코더와 시암넷 결합한 시맨틱 유사도 알고리즘,2021,[],국문 초록 정보 없음,"To solve the problem that existing computing methods cannot adequately represent the semantic features of sentences, Siamese TRAT, a semantic feature extraction model based on Transformer encoder is proposed. The transformer model is used to fully extract the semantic information within sentences and carry out deep semantic coding for sentences. In addition, the interactive attention mechanism is introduced to extract the similar features of the association between two sentences, which makes the model better at capturing the important semantic information inside the sentence. As a result, it improves the semantic understanding and generalization ability of the model. The experimental results show that the proposed model can improve the accuracy significantly for the semantic similarity calculation task of English and Chinese, and is more effective than the existing methods."
언어 학습자로서의 신경망 언어모델: 영어 주어-동사 일치를 중심으로,2021,"['language model', 'GPT-2', 'linguistic generalization', 'subject-verb agreement', 'interference effect', 'plural-singular asymmetry', '언어모델', '언어(학) 일반화', '주어-동사 일치', '간섭효과', '복수-단수 비대칭']",국문 초록 정보 없음,"This paper assesses the language-processing ability of the neural language model (L2-GPT-2) trained on data sets of English textbooks published in Korea using the Generative Pre-trained Transformer (GPT)-2. Assuming that the language model (LM) is also an (artificial) language learner, we test it focusing on subject-verb agreement in English. It is a well-established fact that L1 speakers exhibit facilitatory interference effects in ungrammatical sentences with a plural subject and a singular form of verb. Unlike human native speakers, L2-GPT-2 as well as L1-GPT-2 display such effects in ungrammatical sentences either with a plural subject and a singular form of verb, or with a singular subject and a plural form of verb. Though there is a significant difference between human speakers and neural LMs in processing subject-verb agreement, the two LMs’ sensitivity to interference by a distractor NP points to the fact that they can attain a remarkably human-like linguistic generalization on subject-verb agreement."
FEM Simulation and Test Verification of PD Ultrasonic Signal Propagation in a Power Transformer Model,2021,"['Power transformers', 'Partial discharge', 'FEM', 'Propagation property']",국문 초록 정보 없음,"Ultrasonic signals will be generated when partial discharge occurs in internal insulation faults in large oil immersed power transformers: because the ultrasonic signal has strong anti-interference ability and has no direct electromagnetic contact with the equipment, it is widely used in transformer fault detection and positioning. In this paper, the fi nite element method (FEM) is used to simulate the ultrasonic signal in a 35 kV power transformer. The infl uence of transformer case on ultrasonic signal propagation is considered, and the propagation law of the ultrasonic signal inside the transformer is obtained. Fabry–Pérot (F–P) fi bre acoustic sensors with a centre frequency of 28 kHz were fabricated. A partial discharge detection test was carried out in a 35 kV transformer winding model using the F–P sensors. The test results show that the ultrasonic waveform detected by the F–P sensors are in good agreement with the simulation results, and the propagation of the ultrasonic wave inside the transformer is verifi ed. It lays a foundation for detecting and locating PDs in power transformer by F–P acoustic sensors."
Analytical loss model of series-resonant indirect-matrix-type power electronics transformers using MOSFETs,2021,"['Analytical loss model', 'MOSFET', 'Series-resonant dc-dc converter', 'Indirect-matrix converter', 'Power electronics transformer', 'High efficiency']",국문 초록 정보 없음,"A MOSFET-based series-resonant indirect-matrix-type power electronic transformer (PET) consists of line-frequency-switching folding-unfolding bridges and series-resonant dc-dc converters (SRCs), which is an attractive choice to achieve ac-ac conversion due to its high efficiency and high power density. However, the conduction losses, switching losses, and core losses in each of the high-frequency switching cycles T<sub>s</sub> are different due to the pulsating dc voltage |ac|, which introduces difficulties in the calculation of losses. In addition, the small dc capacitors also participate in the resonance, and the resonant current shape deviates from a pure sine. Therefore, the conventional loss model is not suitable for PETs. In this paper, the time-domain analysis of SRC resonant current considering the dc capacitor is developed. Then, the analytical expression of the resonant current RMS value in grid cycle T<sub>g</sub> is derived, which is more accurate in calculating conduction loss than the conventional model. In addition, it avoids the calculating losses in each T<sub>s</sub>. Next, an analytical switching loss model is developed based on the curve fitting of the capacitance-voltage relationship. A simplified analytical core loss model is provided. Each part of the loss of the PET is verified by thermal simulations, and the total losses are verified by efficiency tests on an experiment prototype."
양방향 인재매칭을 위한 BERT 기반의 전이학습 모델,2021,"['HR Matching', 'Job Recommendation', 'BERT', 'Transfer Learning', 'Pre-trained Language Model', 'Fine-tuning Deep Learning Model']",국문 초록 정보 없음,"While youth unemployment has recorded the lowest level since the global COVID-19 pandemic, SMEs(small and medium sized enterprises) are still struggling to fill vacancies. It is difficult for SMEs to find good candidates as well as for job seekers to find appropriate job offers due to information mismatch. To overcome information mismatch, this study proposes the fine-turning model for bidirectional HR matching based on a pre-learning language model called BERT(Bidirectional Encoder Representations from Transformers). The proposed model is capable to recommend job openings suitable for the applicant, or applicants appropriate for the job through sufficient pre-learning of terms including technical jargons. The results of the experiment demonstrate the superior performance of our model in terms of precision, recall, and f1-score compared to the existing content-based metric learning model. This study provides insights for developing practical models for job recommendations and offers suggestions for future research."
Analyzing the Impact of Sequential Context Learning on the Transformer Based Korean Text Summarization Model,2021,"['한국어 텍스트 요약', '제목 생성', '어텐션 메커니즘', '트랜스포머 모델', 'Korean text summarization', 'headline generation', 'attention mechanism', 'transformer model']",국문 초록 정보 없음,다국어 초록 정보 없음
음성감정인식 성능 향상을 위한 트랜스포머 기반 전이학습 및 다중작업학습,2021,[],국문 초록 정보 없음,"It is hard to prepare sufficient training data for speech emotion recognition due to the difficulty of emotion labeling. In this paper, we apply transfer learning with large-scale training data for speech recognition on a transformer-based model to improve the performance of speech emotion recognition. In addition, we propose a method to utilize context information without decoding by multi-task learning with speech recognition. According to the speech emotion recognition experiments using the IEMOCAP dataset, our model achieves a weighted accuracy of 70.6 % and an unweighted accuracy of 71.6 %, which shows that the proposed method is effective in improving the performance of speech emotion recognition."
자동화된 용종 분할을 위한 트랜스포머 기반의 네트워크,2021,"['Transformer', 'Image Segmentation', 'Polyp Segmentation']",국문 초록 정보 없음,"In recent years, UNet architecture has shown to be a standard network for medical image segmentation. However, it suffers from some severe limitations. It loses localization ability for low-level details followed by the inability of long-range dependencies. Motivated by this, we explore transformer-based architectures that exploit global context by modeling long-range spatial dependencies, which are essential for accurate polyp segmentation. In this paper, we propose an attention-based transformer encoded UNet model. This hybrid model inherits both characteristics of CNN block as well as attention block. We perform various experiments in existing architectures like UNet, ResUNet, ResUNet-Mod and our proposed method. The proposed method achieved a 0.645 mIOU score took an unassailable lead over prior methods."
Development Of Smart Soft-Sensor And Key Variable Selection For Determining Total Nitrogen And Total Phosphorus In Rivers Using Deep Transformer Ai Model,2021,"['Deep transformer models', 'Multi-ahead prediction', 'Smart soft-sensor', 'total phosphorus (TP)', 'total nitrogen (TN)']",국문 초록 정보 없음,다국어 초록 정보 없음
DeepKLM - 통사 실험을 위한 전산 언어모델 라이브러리 -,2021,"['BERT', 'language model', 'surprisal', 'experimental syntax', 'corpus', '언어모델', '서프라이절', '실험통사론', '말뭉치']",국문 초록 정보 없음,"This paper introduces DeepKLM, a deep learning library for syntactic experiments. The library enables researchers to use the state-of-the-art deep computational language model, based on BERT (Bidirectional Encoder Representations from Transformers). The library, written in Python, works to fill the masked part of a sentence with a specific token, similar to the Cloze task in the traditional language experiments. The output value of surprisal is related to human language processing in terms of speed and complexity. The library additionally provides two visualization tools of the heatmap and the attention head visualization. This article also provides two case studies of NPIs and reflexives employing the library. The library has room for improvement in that the BERT-based components are not entirely on par with those in human language sentences. Despite such limits, the case studies imply that the library enables us to assess human and deep learning machines’ language ability."
머신러닝 기법을 활용한 변압기 주파수 응답 모델 파라미터 추정,2021,[],국문 초록 정보 없음,"Examining power transformer fault is crucial for maintaining the power system reliability. The most popular methods for detecting power transformer fault includes thermal analysis, vibration analysis, partial discharge analysis, dissolved gas analysis(DGA) and sweep frequenct response analysis(SFRA). Especially, SFRA test plays an important role in detecting transformer internal fault such as winding fault. Simulation-level frequency response analysis enables inspection on power transformer before connecting to the grid. This paper proposes parameter estimation method using machine learning for the power transformer frequency response equivalent model."
분산전원용 PCS의 누설전류 등가회로 모델 및 분석,2021,"['Leakage current', 'Common-mode Voltage', 'Equivalent circuit', 'Parasitic capacitance']",국문 초록 정보 없음,"Due to continuous environmental problems around the world and an increase in energy consumption, interest in solar power, wind power, and hydropower is increasing. Among them, photovoltaics is the most used in industries and homes because it is easy to repair and can be used for a long time. In photovoltaic PCS, parasitic capacitance is generated between the photovoltaic module and the ground due to the installation structure of the photovoltaic module. This parasitic capacitance, VCM, which is a common mode voltage generated by switching of PCS, is applied to CP, and leakage current occurs. In this paper, the common mode voltage characteristics at three-phase three-level, and three-phase two-level were compared. Proposed an equivalent circuit that was equalized from unaffected by grid impedance the point of view of the common-mode by dividing it into a transformerless type PCS connected to the grid ground and a transformer type PCS applying a transformer. The validity of the proposed equivalent circuit was verified by constructing a three-phase three-level PCS and comparing the equivalent circuit using the experiment measurements and PSIM. In the transformerless PCS, the leakage current is equivalent to 1.13A in the equivalent circuit and 1.14A in the experiment. In the transformer type PCS, the equivalent circuit is 68mA and the experiment 70mA. In addition, for the validity of the proposed equivalent circuit, FFT analysis of leakage current, which is the switching frequency, was performed."
Number Normalization in Korean Using the Transformer Model,2021,"['텍스트 정규화', '숫자 정규화', '시퀀스 투 시퀀스', '트랜스포머', '웹 텍스트 데이터', 'text normalization', 'number normalization', 'sequence to sequence', 'transformer', 'web text data']",국문 초록 정보 없음,다국어 초록 정보 없음
Improving BERT-based Sentiment Analysis Model using Graph-based Ranking Mechanism,2021,"['인공지능', '자연어 처리', '감정분석', '그래프 기반 메커니즘', '인공신경망', 'BERT', 'artificial intelligence', 'natural language processing', 'sentiment analysis', 'graph-based mechanism', 'neural network']","문서 처리의 자동화에 대한 필요성이 대두됨에 따라 인공지능을 통한 자연어 처리(Natural Language Processing) 분야에서 연구가 활발하게 진행되고 있다. 본 연구에서는 자연어 처리 분야 중 특히 감정분석(Sentiment Analysis) 분야에서 그래프 기반의 순위화 메커니즘을 통해 추출한 형태소, 또는 요약 기반의 벡터인 GRAB vector(GRAph-Based vector)를 제안하고 이를 통해 기존의 BERT(Bidirectional Embedding Representations from Transformers)모델에 적용한다. 이를 통하여 더욱 강인하고 성능이 향상된 GRAB-BERT 모델을 제안한다. 또한, GRAB vector가 모델에 미치는 영향을 분석하기 위하여 재귀적 인공신경망(Recurrent Neural Network) 기반 모델들과 BERT 기반 모델에 시퀀스 입력 길이를 각각 다르게 학습한 경우 GRAB vector의 적용 여부에 따른 성능을 한국어와 영어에 대하여 분석한다. 결과적으로 형태소 단위로 추출된 벡터가 BERT와 같은 병렬적으로 문자를 처리하는 모델의 경우, 더욱 강인한 학습이 가능하며 성능이 향상됨을 보인다. 추가로, BERT 기반의 모델과 반대로 재귀적 인공신경망 기반모델들의 경우 형태소 기반이 아닌 그래프 기반 요약문 추출을 통한 벡터를 적용한 경우가 더 효과적임을 보인다.","Due to the need for automated document processing, artificial intelligence research has been actively conducted in the field of natural language processing(NLP). In this paper, we propose the GRAB vector(GRAph-Based vector), which consists of vectorized keyword-based morphemes or summaries extracted from the graph-based ranking mechanism. Next, we applied the GRAB vector to the sentiment analysis task, which is an NLP task, and we proposed a more accurate and robust model, GRAB-BERT(GRAB vector-BERT model). Then, to analyze the effect of the GRAB vector on this model, we compared the performances of recurrent neural network models(RNNs) and BERT models with or without the application of the GRAB vector on both English and Korean text samples with different sequence sizes. Our results demonstrate that applying the GRAB vector to models such as BERT to process inputs in parallel improves the robustness of the model and its performance. Furthermore, unlike BERT-based models, RNN models are more effective when applying graph-based extracted summaries than when applying morpheme-based summaries."
Swin Transformer를 이용한 항공사진에서 다중클래스 차량 검출,2021,"['artificial intelligence', 'computer vision', 'object detection', 'instance segmentation']","도시 상태를 탐지하기 위해서는 운송 수단 수, 교통 흐름등이 필수적으로 파악되어야 할 요소이다. 본 논문에서는 기존의 Mask R-CNN을 이용하여 다양한 차량의 형태를 학습하고, 드론으로 촬영한 도시 항공 영상에서 특정 유형의 차량 들을 검출하는 시스템을 오늘날 NLP 분야에서 널리 쓰이게 된 Transformer 모델을 컴퓨터 비전 문제에 도입하여 기존의 컨볼루션 신경망보다 높은 성능을 보여준 Swin Transformer 모델을 이용하여 기존의 연구에서 보여주었던 검출 시스템 능력을 향상시켰다.","In order to detect urban conditions, the number of means of transportation and traffic flow are essential factors to be identified. This paper improved the detection system capabilities shown in previous studies using the SwinTransformer model, which showed higher performance than existing convolutional neural networks, by learning various vehicle types using existing Mask R-CNN and introducing today’s widely used transformer model to detect certain types of vehicles in urban aerial images."
Multi-Label Image Classification Using The Base Vision Transformer,2021,"['Vision Transformers', 'Multi-label Classification', 'Neural Networks']",국문 초록 정보 없음,"As the images usually contain multiple objects and traditional approaches of classification suppose that the image has a dominant class, we propose a multi-label classification model based on the state-of-the-art vision transformers (ViT). The proposed model exposes all the object labels that exist in an image rather than the dominant object. The use of the vision transformer model allows the global understanding of the image rather than the convolutional approach which allows only the local understanding of the image based on the convolutional window size. We train and test our model on the challenging PASCAL VOC2012 dataset showing that our approach attains higher mean classification accuracy (97.2%) than the popular convolution-based neural network models."
Linear Video Transformer Network,2021,"['Video Recognition', 'Video Classification', 'Action Recognition', 'Linear Transformers', 'Transformers', 'Visual Transformers', 'Artificial Intelligence', 'Deep Learning']",국문 초록 정보 없음,"This paper presents a video classification model built using linear complexity transformers. This method utilizes attention mechanism both in spatial and temporal domain but using efficient Transformers (namely the Linformer and Longformer) to lower complexity from quadratic to linear in both domains. By leveraging low rankness of self-attention (Linformer) and patterns of local attention within fixed window and some global attention (Longformer), we can lower quadratic complexity of full self-attention (in both temporal and spatial domain) to linear complexity, which saves us memory usage and training/inference time drastically. Transformer based models have shown in a literature comparable results and superior speed to 3D Convolution based models (namely I3D and SlowFast).  Experiments show the comparable results on Kinetics-400 dataset. By comparing pros and cons of Longformer and Linformer on temporal /spatial domain, we conclude which model has best accuracy and/or inference speed."
Transformer 기반 비윤리적 문장 탐지,2021,"['인공지능', '문장분류', '자연어처리', '비윤리적 문장', '딥러닝', 'artificial intelligence', 'sentence classification', 'natural language processing', 'unethical sentence', 'deep learning']",국문 초록 정보 없음,"Social network services (SNS) have spread due to the development of information and communication technology, but at the same time, they have caused serious social problems such as malicious comments. The number of arrests and incidents of cyber defamation and insults increased sharply from 8,880 in 2014 to 16,633 in 2019, and measures are required to solve the problem. However, existing regulations such as IP blacklist and slang filters make it difficult to detect malicious comments. Therefore, we need an artificial intelligence model optimized for unethical sentence detection. This paper proposes a Transformer-based unethical sentence detection model that has shown high performance in natural language processing. The model showed accuracy of 95.03% and will be utilized as an unethical sentence detection model. Also, it will be applied in various fields such as streaming services as well as comments on SNS."
Self-revising Transformer with Multi-view for Image Captioning,2021,"['자연어 처리', '이미지 캡션 생성', '멀티-헤드 주의 기제 기법', '다중 관점 인코더', 'natural language processing', 'image captioning', 'multi-head attention', 'multi-view encoder']",국문 초록 정보 없음,다국어 초록 정보 없음
무인 감시 Transformer,2021,"['Deep learning', 'Transformer', 'Segmentation', 'Visual surveillance']",국문 초록 정보 없음,"In a visual surveillance system, even the same object should exhibit different detection results depending on the surrounding environment configuration. To this end, the model for visual surveillance needs to detect an object by understanding the state of the object according to the environment on the image. In this study, for such visual surveillance, an object segmentation model applied with a transformer structure suitable for image processing was used to divide objects inside the image into foreground and background. A modified attention structure was presented for the corresponding transformer structure, and the results of object segmentation models according to the type of input data were compared."
Visual surveillance transformer,2021,"['Deep learning', 'Transformer', 'Segmentation', 'Visual surveillance']",국문 초록 정보 없음,"In the case of the unmanned surveillance system field, even if it is the same object, the detection result will be different depending on the state of the object and the configuration of the surrounding environment. Therefore, artificial intelligence for unmanned surveillance needs to understand the environment on the image, understand the state of the object within the image, and understand the relationship between them. For this purpose, in this study, a transformed transformer structure that can receive a single image, which is 2D data, as an input, unlike splitting one image into a certain size and using it as an input, is presented, and the effect between neighboring pixels is considered by using a segmentation model to which it is applied. A possible background classification model was constructed."
Segmentation applying TAG type label data and Transformer,2021,"['Deep learning', 'Transformer', 'Segmentation', 'Visual surveillance']",국문 초록 정보 없음,"Autonomous driving of vehicles or robots using artificial intelligence is being studied the most. The recognition of the surrounding environment is the basis for artificial intelligence that requires interaction with the surroundings, which means that research on object detection is necessary. The size of the model is smaller, and more information can be obtained than detection using anchors, but the accuracy of segmentation is generally lower. In this paper, to improve this point, a transformed transformer structure is applied to improve the performance of segmentation, and it is proposed to use data in a format different from the existing label data. By using a single image as an input, there is no loss of location information, and a lighter model is presented by obtaining a segmentation image without going through a separate process. At the same time, to improve generalization performance, a method of assigning one label to one characteristic rather than assigning one label to one object was applied to the composition of the label data, and the difference in generalization ability was compared."
Robust and Explainable Sewer Crack Detection based on a Transformer,2021,"['crack detection', 'sewer defect', 'transformer', 'deep learning']",국문 초록 정보 없음,"Sewer pipes are an essential public infrastructure of countries worldwide. They support wastewater transportation for processing or disposal. The harsh environments inside the sewer pipes can lead to the occurrence of various defects. Current crack detection approaches mainly focus on the surveillance camera (CCTV) to assess the condition of the sewer pipes. This process is considered a tiresome and laborious process. Therefore, a robust and efficient sewer defect detection system based on the transformer architecture is introduced in this manuscript. In addition, the system can provide explainable visualization for its predictions using the transformer's attention."
Zero-anaphora resolution in Korean based on deep language representation model: BERT,2021,"['attention', 'bidirectional encoder representations from transformers (BERT)', 'deep learning', 'language representation model', 'zero-anaphora resolution (ZAR)']",국문 초록 정보 없음,"It is necessary to achieve high performance in the task of zero anaphora resolution (ZAR) for completely understanding the texts in Korean, Japanese, Chinese, and various other languages. Deep-learning-based models are being employed for building ZAR systems, owing to the success of deep learning in the recent years. However, the objective of building a high-quality ZAR system is far from being achieved even using these models. To enhance the current ZAR techniques, we fine-tuned a pretrained bidirectional encoder representations from transformers (BERT). Notably, BERT is a general language representation model that enables systems to utilize deep bidirectional contextual information in a natural language text. It extensively exploits the attention mechanism based upon the sequence-transduction model Transformer. In our model, classification is simultaneously performed for all the words in the input word sequence to decide whether each word can be an antecedent. We seek end-to-end learning by disallowing any use of hand-crafted or dependency-parsing features. Experimental results show that compared with other models, our approach can significantly improve the performance of ZAR."
Design and Modeling of Octagonal Planar Inductor and Transformer in Monolithic Technology for RF Systems,2021,['Forward converter  · Inductor  · Integration  · Octagonal  · Planar  · RF  · Transformer'],국문 초록 정보 없음,"This paper presents the design and modeling of planar inductor and transformer for their integration in a Forward converter.The windings are of octagonal spiral planar topology. Basing on Wheeler method, we evaluate the inductance values of planar coils. All parasitic eff ects generated by stacking of diff erent material layers are summarized perfectly in the π-electrical model of both on chip inductor and transformer. The electromagnetic simulation, by using COMSOL Multiphysics 5.3 software, illustrates the distribution of magnetic fi eld lines, the current density and the temperature distribution in our integrated micro coils. To validate the diff erent geometric and electrical parameters values, we have simulated the circuit of the Forward converter containing the π-electrical circuit of on chip integrated inductor and transformer, using the software TINA 9.0."
Korean automatic spacing using pretrained transformer encoder and analysis,2021,"['attention', 'BERT', 'Korean automatic spacing', 'natural language processing', 'pretrained transformer encoder']",국문 초록 정보 없음,"Automatic spacing in Korean is used to correct spacing units in a given input sentence. The demand for automatic spacing has been increasing owing to frequent incorrect spacing in recent media, such as the Internet and mobile networks. Therefore, herein, we propose a transformer encoder that reads a sentence bidirectionally and can be pretrained using an out‐of‐task corpus. Notably, our model exhibited the highest character accuracy (98.42%) among the existing automatic spacing models for Korean. We experimentally validated the effectiveness of bidirectional encoding and pretraining for automatic spacing in Korean. Moreover, we conclude that pretraining is more important than fine‐tuning and data size."
Personality Prediction Based on Text Analytics Using Bidirectional Encoder Representations from Transformers from English Twitter Dataset,2021,"['Personality prediction', 'Twitter', 'Big Five personality traits', 'BERT']",국문 초록 정보 없음,"Personality traits can be inferred from a person’s behavioral patterns. One example is when writing posts on social media. Extracting information about individual personalities can yield enormous benefits for various applications such as recommendation systems, marketing, or hiring employees. The objective of this research is to build a personality prediction system that uses English texts from Twitter as a dataset to predict personality traits. This research uses the Big Five personality traits theory to analyze personality traits, which consist of openness, conscientiousness, extraversion, agreeableness, and neuroticism. Several classifiers were used in this research, such as support vector machine, convolutional neural network, and variants of bidirectional encoder representations from transformers (BERT). To improve the performance, we implemented several feature extraction techniques, such as N-gram, linguistic inquiry and word count (LIWC), word embedding, and data augmentation. The best results were obtained by fine-tuning the BERT model and using it as the main classifier of the personality prediction system. We conclude that the BERT performance could be improved by using individual tweets instead of concatenated ones."
Transmission Distance Improvement of a Two-Coil Magnetic Resonance Wireless Power Transmission System Using Transformers,2021,"['Magnetic Resonance Modeling', 'Q-Factor', 'Wireless Power Transfer']",국문 초록 정보 없음,"In this paper, a two-coil magnetic resonance wireless transmission system is studied to improve the transmission distance using transformers. A conventional two-coil and four-coil wireless power transmission (WPT) system as well as a two-coil WPT system with transformers are analyzed comparatively via circuit simulations and experiments. Circuit analysis was used to predict the transmission distance with the highest efficiency. To verify the improvement in the transmission distance of the proposed system, transformers with inductance values of 80, 100, and 140 μH were fabricated and analyzed through experiments and simulations. A maximum S21 parameter of 0.76 was noted when the inductance was 80 μH and the transmitting distance was 4 cm. The experimental results almost matched the simulation results. From the experiments, it was shown that the transmitting distance of a WPT system can be adjusted by using transformers. Additionally, it was found that the transmitting distance is inversely proportional to the transformer inductance, and the efficiency of the WPT system decreases with the transmitting distance."
음질 및 속도 향상을 위한 선형 스펙트로그램 활용 Text-to-speech,2021,"['speech synthesis', 'machine learning', 'artificial intelligence', 'text-to-speech (TTS)', '음성 합성', '기계 학습', '인공지능']","인공신경망에 기반한 대부분의 음성 합성 모델은 고음질의 자연스러운 발화를 생성하기 위해 보코더 모델을 사용한다. 보코더 모델은 멜 스펙트로그램 예측 모델과 결합하여 멜 스펙트로그램을 음성으로 변환한다. 그러나 보코더 모델을 사용할 경우에는 많은 양의 컴퓨터 메모리와 훈련 시간이 필요하며, GPU가 제공되지 않는 실제 서비스환경에서 음성 합성이 오래 걸린다는 단점이 있다. 기존의 선형 스펙트로그램 예측 모델에서는 보코더 모델을 사용하지 않으므로 이 문제가 발생하지 않지만, 대신에 고품질의 음성을 생성하지 못한다. 본 논문은 뉴럴넷 기반 보코더를 사용하지 않으면서도 양질의 음성을 생성하는 Tacotron 2 & Transformer 기반의 선형 스펙트로그램 예측 모델을 제시한다. 본 모델의 성능과 속도 측정 실험을 진행한 결과, 보코더 기반 모델에 비해 성능과 속도 면에서 조금 더 우세한 점을 보였으며, 따라서 고품질의 음성을 빠른 속도로 생성하는 음성 합성 모델 연구의 발판 역할을 할 것으로 기대한다.","Most neural-network-based speech synthesis models utilize neural vocoders to convert mel-scaled spectrograms into high-quality, human-like voices. However, neural vocoders combined with mel-scaled spectrogram prediction models demand considerable computer memory and time during the training phase and are subject to slow inference speeds in an environment where GPU is not used. This problem does not arise in linear spectrogram prediction models, as they do not use neural vocoders, but these models suffer from low voice quality. As a solution, this paper proposes a Tacotron 2 and Transformer-based linear spectrogram prediction model that produces high-quality speech and does not use neural vocoders. Experiments suggest that this model can serve as the foundation of a high-quality text-to-speech model with fast inference speed."
면진장치의 종류에 따른 변압기의 지진거동 분석,2021,"['Transformers', 'Seismic Isolator', 'Artificial Seismic Wave', 'Dynamic Analysis', 'Response Acceleration']","본 연구에서는 변압기 내진성능을 확보하기 위해 면진장치에 따른 지진 저감성능과 특성을 평가하기 위한 수치해석을 수행하였다. 해석에 적용한 면진장치는 마찰진자면진장치(FIP)와 납-고무면진장치(LRB)이며 각 면진장치를 유한요소해석 프로그램을 사용하여 수치해석 모델화하였다. 또한, 변압기의 형상적 특성을 고려하기 위해 수치해석 모델링한 변압기의 형상과 하중이 실제 변압기와 유사하도록 모델링을 수행하였다. 변압기 해석모델에 마찰진자면진장치와 납-고무면진장치를 적용하여 동적해석을 수행하기 위해 내진설계기준 공통적용사항과 ICC-ES AC156의 응답스펙트럼을 참고하여 생성한 인공지진파를 해석에 적용하였다. 해석결과 면진장치가 적용되지 않은 해석과 비교하였을 때 마찰진자면진장치를 적용한 변압기에서 발생한 가속도 감쇠율은 75%로 나타났으며 납-고무면진장치를 적용한 변압기의 응답가속도 감쇠율은 최소 42%로 나타나 마찰진자면진장치가 납-고무면진장치에 비해 가속도 응답면에서 우수한 결과를 보였다. 또한, 면진장치를 적용하였을 때 발생하는 변압기의 회전을 검토하기 위해 가속도가 작용할 때 변압기 좌우측의 상대변위를 측정하였다. 이때 납-고무면진장치가 마찰진자면진장치에 비해 변압기의 회전이 큰 것으로 나타나 납-고무면진장치의 설치를 고려할 때 면진 대상구조물의 회전에 대한 적절한 대응책의 마련이 필요할 것으로 보인다.","In this study, a numerical analysis was performed to evaluate the seismic reduction performance and characteristics of a seismic isolator used to ensure the performance of the transformer under seismic conditions. The seismic isolators used in this analysis were the Friction Isolation Pendulum(FIP) and the Lead-Rubber Bearing (LRB). Each seismic isolator was modeled using a numerical analysis program. To consider the characteristics of the transformer, the model in the simulation was made similar to the actual shape of the transformer. To perform dynamic analysis, artificial seismic waves were generated and targeted onto the transformer by referring to the response spectrums for common applications from the seismic design standards and the ICC-ES AC156. In the dynamic analysis of the model, the FIP showed better results in terms of acceleration response than the LRB. Also, in the analysis, the LRB showed bigger rotational deformation as compared to the FIP. Therefore, appropriate countermeasures against the rotational deformation of the structure are required while installing the LRB."
고화질 얼굴 이미지 생성 시스템,2021,[],"본 논문은 VQGAN 과 트랜스포머를 이용한 얼굴 이미지 생성 시스템을 제시한다. 먼저 VQGAN 모델을 사용하여 이미지를 표현할 수 있는 블록 기반의 양자화된 코드북을 학습하고, 학습된 코드북을 기반으로 자기회귀모형을 이용하여 트랜스포머를 학습한다. 직접 수집한 3 만 장의 한국인 얼굴 데이터셋을 학습에 사용하여 실험한 결과 제안한 모델을 이용하여 고화질의 한국인 얼굴 이미지를 생성할 수 있었다.",다국어 초록 정보 없음
"SpanBERT를 이용한한국어 자연어처리: 기계 독해, 개체 연결, 의존 파싱",2021,[],"최근 자연어 처리 연구에서 대용량 말뭉치와 양방향 트랜스포머 기반 맥락을 고려한 BERT 와 같은 언어 모델을 사전 학습하고, 응용 태스크에 미세 조정하여 높은 성능을 달성하였다. 본 연구에서는 개별 토큰이 아닌 연속적인 범위(Span)의 토큰을 마스킹(Masking)한다. 그리고 개별 토큰 표현에 의존하지 않고 범위 경계(Span Boundary) 표상을 학습하여 마스킹 된 범위의 전체 내용을 예측 함으로서 기존의 한국어 RoBERTa[1] 모델을 확장한다. 이를 바탕으로 기계 독해, 개체 연결, 의존 파싱과 같은 응용 태스크 일부에서 기존 언어 모델보다 향상된 성능을 거두었다.",다국어 초록 정보 없음
전이학습 기반의 해충 영상 분류 성능 비교,2021,"['Pest Classification', 'Deep Learning', 'CNN', 'Transfer Learning']","수많은 종류의 해충은 종류에 따라 물리적인 피해, 작물 수확의 피해 그리고 생태계 보전 가치를 해치는 피해를 준다. 하지만 해충은 작고 비슷하게 생겨 전문가가 아닌 사람이 육안으로 해충의 종류를 알고 분류하기란 쉽지 않다. 따라서 본 논문은 해충 영상을 분류를 위해 6개의 딥러닝 모델 ResNet50, VGG16, SqueezeNet, FPN(Feature Pyramid Network), Attention Gated Network, PVT(Pyramid Vision Transformer)의 분류 성능에 대한 비교 분석을 하려 한다 이를 통해 해충 영상 분류에 가장 우수한 모델을 찾아 해충의 종류에 따른 적합한 방제 시스템을 구축하고자 한다.","Numerous types of pests cause physical damage, damage to crop harvest, and damage that harms ecosystem conservation value depending on the type. However, since the pests are small and similar, it is not easy for non-experts to recognize and classify the types of pests with the naked eye. Therefore, this paper want to perform a comparative analysis on the classification performance of six deep learning models ResNet50, VGG16, SqueezeNet, FPN(Feature Pyramid Network), Attention Gated Network, and PVT(Pyramid Vision Transformer) to classify pest images. Through this, we want to find the best model for pest image classification and build an appropriate control system according to the pest type."
Deep Learning-based Target Masking Scheme for Understanding Meaning of Newly Coined Words,2021,"['Target Masking', 'Deep Learning', 'BERT', 'Newly Coined Words', 'Sentiment Analysis', '표적 마스킹', '딥러닝', '신조어', '감성분석']","최근 대량의 텍스트 분석을 위해 딥 러닝(Deep Learning)을 활용하는 연구들이 활발히 수행되고 있으며, 특히 대량의 텍스트에 대한 학습 결과를 특정 도메인 텍스트의 분석에 적용하는 사전 학습 언어 모델(Pre-trained Language Model)이 주목받고 있다. 다양한 사전 학습 언어 모델 중 BERT(Bidirectional Encoder Representations from Transformers) 기반 모델이 가장 널리 활용되고 있으며, 최근에는 BERT의 MLM(Masked Language Model)을 활용한 추가 사전 학습(Further Pre-training)을 통해 분석 성능을 향상시키기 위한 방안이 모색되고 있다. 하지만 전통적인 MLM 방식은 신조어와 같이 새로운 단어가 포함된 문장의 의미를 충분히 명확하게 파악하기 어렵다는 한계를 갖는다. 이에 본 연구에서는 기존의 MLM을 보완하여 신조어에 대해서만 집중적으로 마스킹을 수행하는 신조어 표적 마스킹(NTM: Newly Coined Words Target Masking)을 새롭게 제안한다. 제안 방법론을 적용하여 포털 ‘N’사의 영화 리뷰 약 70만 건을 분석한 결과, 제안하는 신조어 표적 마스킹이 기존의 무작위 마스킹에 비해 감성 분석의 정확도 측면에서 우수한 성능을 보였다.","Recently, studies using deep learning to analyze a large amount of text are being actively conducted. In particular, a pre-trained language model that applies the learning results of a large amount of text to the analysis of a specific domain text is attracting attention. Among various pre-trained language models, BERT(Bidirectional Encoder Representations from Transformers)-based model is the most widely used. Recently, research to improve the performance of analysis is being conducted through further pre-training using BERT""s MLM(Masked Language Model). However, the traditional MLM has difficulties in clearly understands the meaning of sentences containing new words such as newly coined words. Therefore, in this study, we newly propose NTM(Newly coined words Target Masking), which performs masking only on new words. As a result of analyzing about 700,000 movie reviews of portal ""N"" by applying the proposed methodology, it was confirmed that the proposed NTM showed superior performance in terms of accuracy of sensitivity analysis compared to the existing random masking."
소셜 감성 데이터를 이용한 딥러닝 기반의 암호화폐 가격 예측,2021,"['Cryptocurrency', 'Price Prediction', 'Social Sentiment Data', 'BERT', 'Deep Learning']","소셜 감성 데이터를 활용하여 암호화폐의 가격을 예측하는 기존의 연구들은 데이터 수집의 출처가 제한적이어서 다양한 사회적 분위기를 반영하기 어렵다는 한계가 존재했으며, 사전 기반 언어 모델을 사용했기 때문에 최근의 유행어를 분석할 수 없었다. 본 연구에서는 상기 한계를 보완하기 위해 다양한 출처의 소셜 감성 데이터를 딥러닝 기반 언어 모델 BERT (Bidirectional Encoder Representations from Transformers)로 분석한다. 분석이 완료된 후, 분석 결과를 시계열 종속 변수로 변환하는 본 연구의 제안 방법을 통해 비트코인의 한 시간 단위 종가를 예측한다.  실험을 통해 제안 방법은 가격 예측 오차 감소에 효과가 있는 것으로 검증되었다.","Existing studies that predict the cryptocurrency price using social sentiment data have difficulty in considering various social atmospheres due to the limited sources of data collection. In addition, it was not possible to analyze buzzwords because they use a dictionary-based language model. In this study, to resolve the above problems, we analyze social sentiment data from various sources using BERT (Bidirectional Encoder Representations from Transformers), a deep learning-based language model. Then, the proposed method converts the result of the sentiment analysis into time-series independent variables to predict the hourly closing price of Bitcoin. Numerical experiments demonstrate the proposed method is effective in the reduction of price prediction error."
한국어 FastSpeech2의 하이퍼파라미터 조절을 통한 최적화 연구,2021,[],"본 논문은 트랜스포머 구조를 채택한 FastSpeech2에 Korean Single speaker Speech Dataset (KSS) 데이터셋을 이용한 한국어 음성합성시스템의 최적화에 대해 다룬다. 병렬화가 불가능하여 학습 속도가 느리다는 내재적인 단점을 가진 recurrent neural network를 이용한 기존 음성합성시스템과는 달리 FastSpeech2는 트랜스포머 구조를 채택하였기에 병렬화가 가능하여 학습 속도가 빠를 뿐만 아니라, variance adaptor를 모델에 포함해 음성합성시스템의 고질적인 문제였던 일 대 다수 문제에 대한 해법을 제시하였다. 본 논문에서는 FastSpeech2의 일부 하이퍼파라미터를 조정한다면 이러한 FastSpeech2의 장점을 살릴 수 있을 뿐만 아니라 한국어 화자 음성 정보를 활용한 한국어 음성합성시스템 성능 향상의 가설을 바탕으로 실험을 진행하였다. 독립변수는 인코더 블록의 개수와 각 블록 내부에 존재하는 멀티헤드의 개수이며, 각 실험의 결과는 훈련을 통해 나온 손실함수 그래프와 28명의 피실험자를 대상으로 한 평균 의견 점수를 통해 정량적, 정성적으로 분석하였다.",다국어 초록 정보 없음
일관된 대화 생성 챗봇: 성격 정보의 반영한 강인함에 대한 기법,2021,"['Personalized', 'Chit-chat', 'Transformer', 'Preprocessing']","챗봇은 학술적인 분야에 국한되지 않고 다양한 분야에서 연구되고 사용되고 있다. 그럼에도 불구하고 대부분의 챗봇은 성격이 일관적이지 않다는 문제점이 있었다. 본 연구는 해당 문제를 해결하고자 새로운 전처리 방식과 학습 방법론을 제안한다. 제안된 방법론의 효용성을 검증하기 위해서 PERSONA-CHAT 데이터셋 중 validation 데이터셋으로 성능을 비교하는 실험을 진행하였고, 같은 데이터셋을 활용한 다른 모델에 비해 Perplexity의 지표에서 가장 높은 성능을 달성했다.",다국어 초록 정보 없음
비접촉식 여자시스템을 위한 무선 전력 전송 회전 변압기의 주파수에 따른 효율 분석,2021,[],"본 논문에서는 하나의 축을 공유하는 회전형 변압기(Rotary-Transformer)의 주파수에 따른 효율 특성에 대해 연구하였다. Ansys Maxwell Simulation Tool을 이용하여 특정 주파수에서, 1차측에서 2차측으로 무선 전달되는 전압 및 전류의 비를 시뮬레이션으로 관찰하였다. 이것을 바탕으로 실제 모델을 만들고 주파수에 따른 전력 전달 비를 비교 및 분석하였다.",다국어 초록 정보 없음
분류 정확도 향상을 위한 선택적 마스킹 기반 추가 사전 학습 기법,2021,"['감성 분석', '선택적 마스킹', '어텐션 메커니즘', 'sentiment analysis', 'BERT', 'MLM', 'selective masking', 'attention mechanism']","최근 여러 자연어 처리 분야에서 사전 학습 언어 모델인 BERT를 활용하여 분석 과제에 최적화된 텍스트 표현을 추출하려는 연구가 활발하게 이루어지고 있다. 특히 BERT의 학습 방식 중 하나인 MLM(Masked Language Model)을 활용하여 도메인 정보 또는 분석 과제 데이터를 추가 사전 학습(Further Pre-training)하는 시도가 이어지고 있다. 하지만 기존의 MLM 기법이 채택한 무작위 마스킹을 사용하여 감성 분류 과제에서 추가 사전 학습을 수행하는 경우, 분류 학습에 중요한 단서가 되는 단어가 마스킹될 수 있다는 가능성으로 인해 문장 전체에 대한 감성 정보 학습이 충분히 이루어지지 않는다는 한계가 있다. 이에 본 연구에서는 무작위 마스킹이 아닌 단서 단어를 제외하고 마스킹하는 선택적 마스킹을 통해 감성 분류 과제에 특화된 추가 사전 학습을 수행할 수 있는 방법을 제안한다. 더불어 주변 단어를 선택하기 위해 어텐션 메커니즘(Attention Mechanism)을 활용하여 단어의 감성 기여도를 측정하는 방안도 함께 제안한다. 제안 방법론을 실제 감성 댓글에 적용하여 문장 벡터를 추론하고 감성 분류 실험을 수행한 결과, 제안 방법론이 기존의 여러 비교 모델에 비해 분류 정확도 측면에서 우수한 성능을 나타냄을 확인하였다.","Recently, studies to extract text expressions optimized for analysis tasks by utilizing bidirectional encoder representations from transformers (BERT), which is a pre-training language model, are being actively conducted in various natural language processing fields. In particular, attempts are being made to further pre-train domain information or target data using masked language model (MLM), which is one of the BERT training methods. However, if further pre-training is performed with the existing random masking when performing sentiment classification, there is a limitation that sentimental nuance for the entire sentence may not be sufficiently learned if the words that are important clues to the sentiment classification are masked. Therefore, in this study, we propose an further pre-training method specialized for sentiment classification tasks which sufficiently reflect sentiment information in sentences by selective masking that excludes clue words from masking candidates. In addition, this study proposes a method to distinguish between clue words and surrounding words as the role of words by utilizing the attention mechanism. On inferring sentence vectors by applying the proposed methodology to actual sentiment comments and performing sentiment classification experiments, it was confirmed that the proposed methodology showed superior performance in terms of classification accuracy compared to several existing comparison models."
기도 신학 : 욕망(desire)의 변화(transformation)를 중심으로,2021,"['기도', '욕망의 변화', '삼위일체', '사라 코클리', '닛사의 그레고리우스', 'Prayer', 'Transformation', 'Trinity', 'Sarah Coakley', 'Gregory of Nyssa']","기도하는 방법(Lex orandi)은 믿는 방식(lex credendi)과 별개가 아니다. 기도와 신 학적 교리는 결코 떨어져 생각할 수 없다. 기도란 무엇인가에 대한 응답은 여러 차원 에서 논의할 수 있다. 그러나 필자는 기도의 역할, 기도의 결과에 대해 집중한다. 왜 냐하면, 기도에 대한 정의는 기도를 통한 삶의 변화에 따라 달라질 수 있기 때문이고, 나아가 기도에 대한 ‘정의’(definition)와 ‘실천’(praxis)의 이분법적 접근을 피할 수 있 기 때문이다. 사라 코클리(Sarah Coakley)는 기도를 삶에서 나타나는 실천 또는 생활 방식으로 분류한다. 기도는 기도하는 사람의 내적인 변화와 외적인 삶의 변화를 통해 완성된다. ‘성자’ 예수 그리스도의 이름으로 ‘성부’ 하나님께 드려진 기도는 변화 (transformation)를 이루는 원동력인데, 여기서 ‘성령’이 이끄는 힘에 의지한 우리들의 실천도 간과될 수 없다. 이를 통해 보았을 때, 기도는 삼위일체 교리와 밀접한 관계가 있다. 삼위일체 교리는 기도를 통해서 형성되었고 동시에 기도를 위한 것이다. 사라 코클리(Sarah Coakley)는 성부와 성자 중심의 전통적 삼위일체를 “위계적 모델”이라 명명하고, 이를 넘어서 성령의 역할을 재조명함으로써 새로운 삼위일체 모델을 제시 한다. 성령은 유한과 무한, 타자와 자기를 잇는 사랑의 끈이요 매개이다. 성령의 중보와 도움으로 유한한 인간이 삼위일체 하나님의 경륜적 삶에 동참할 수 있고, 나아가 하나님과 연합한 존재로 변화된 삶을 살 수 있다. 따라서, 코클리가 주장하는 바, “성 령 주도적” 삼위일체론(Spirit-led incorporative Trinity)은 기도를 통한 우리들의 인 식과 존재의 변화를 위해 형성된 교리이다. 이를 위한 코클리의 신학적 방법론은 닛 사의 그레고리우스(Gregory of Nyssa)로부터 영향을 받았고, 그 핵심에 ‘욕망(desire)’ 의 ‘변화’(transformation)라는 개념이 있다. 프로이드 이후 형성된 욕망에 대한 개념 은 수정되어야 한다. 플라톤이 사용한 에로스(eros)와 욕망(desire)은 성적인 합일을 위한 욕구 불만의 표출이 아니라, 신적인 것을 향한 갈망이었다. 그레고리우스는 영성 신학적 관점에서 이를 수정 보완하였는데, 거룩한 하나님과 연합하고자 하는 갈망이 다. 욕망의 변화는 기도를 통해 가능하다. 성령 안에서, 성령을 통해 기도할 때, 우리 가 성부 하나님의 사랑을 알 수 있고, 성자 예수 그리스도를 본받아 하나님과 연합할 뿐 아니라, 변화된 존재로 살아갈 수 있다. 따라서, 본 논고는 삼위일체 교리와 기도 의 연관성에 대해 논증할 뿐 아니라, 기도 신학은 인간에 대한 이해와 더불어 구원론 에 대한 새로운 길을 제시한다고 주장한다.","Lex orandi, lex credendi; the law of what is to be prayed is never to be separated from the law of what is to be believed. It is not easy to define what Christian prayer is. Still, the author focuses on the role and result of prayer, keeping away from separating the relationship between definition and praxis in praying. According to Sarah Coakley, prayer itself is life and the way to transform lives, which is constructing with and in the doctrine of Trinity: praying in the Son, Jesus Christ, praying to the Father God, and through the Holy Spirit. Sarah Coakley shows the strong relationship between the way of prayer and Christian doctrine by delving into a new model of the Trinity. Coakley argues convincingly that the Spirit plays a pivotal role in our understanding of the being and life of God. The Holy Spirit is to ensure liaison between the Father and the Son in love. In so doing, the finite, human being, can participate in the infinite’s salvific life. This new approach is referred to as a Spirit-led incorporative model for the Trinity. Coakley derives her understanding of the Trinity from the thought of Gregory of Nyssa, especially the concept of desire and transformation. The concept of desire, after Post-Freudian, should be modified. In his work, Symposium, Plato asserts that eros and desire are toward the infinite or divine. But, in Gregory's theology of spirituality, eros is employed, meaning to unite with God in and through praying. Prayer is how transformation occurs, leading us to be in union with God. Praying in and through the Holy Spirit, all Christians can realize what God the Father’s love is, unite with Jesus Christ by imitatio Christi, and finally live as the transformed and the transformer. Thus, the doctrine of Trinity and praying are in a strong relationship, developing a theology of prayer, which gives us new eyes that enable us to see new theological anthropology and soteriology"
Sentiment analysis of Korean movie reviews using XLM-R,2021,"['Sentiment analysis', 'Transformer', 'BERT', 'XLM-R', 'Transfer learning', 'Fine tuning']",국문 초록 정보 없음,"Sentiment refers to a person's thoughts, opinions, and feelings toward an object. Sentiment analysis is a process of collecting opinions on a specific target and classifying them according to their emotions, and applies to opinion mining that analyzes product reviews and reviews on the web. Companies and users can grasp the opinions of public opinion and come up with a way to do so. Recently, natural language processing models using the Transformer structure have appeared, and Google's BERT is a representative example. Afterwards, various models came out by remodeling the BERT. Among them, the Facebook AI team unveiled the XLM-R (XLM-RoBERTa), an upgraded XLM model. XLM-R solved the data limitation and the curse of multilinguality by training XLM with 2TB or more refined CC (CommonCrawl), not Wikipedia data. This model showed that the multilingual model has similar performance to the single language model when it is trained by adjusting the size of the model and the data required for training. Therefore, in this paper, we study the improvement of Korean sentiment analysis performed using a pre-trained XLM-R model that solved curse of multilinguality and improved performance."
워드 임베딩 클러스터링을 활용한 리뷰 다중문서 요약기법,2021,"['Muti-document', 'Text Summarization', 'Transformer', 'Word Embedding', '다중문서', '텍스트 요약', '트랜스포머', '워드 임베딩']",국문 초록 정보 없음,"Multi-document refers to a document consisting of various topics, not a single topic, and a typical example is online reviews. There have been several attempts to summarize online reviews because of their vast amounts of information. However, collective summarization of reviews through existing summary models creates a problem of losing the various topics that make up the reviews. Therefore, in this paper, we present method to summarize the review with minimal loss of the topic. The proposed method classify reviews through processes such as preprocessing, importance evaluation, embedding substitution using BERT, and embedding clustering. Furthermore, the classified sentences generate the final summary using the trained Transformer summary model. The performance evaluation of the proposed model was compared by evaluating the existing summary model, seq2seq model, and the cosine similarity with the ROUGE score, and performed a high performance summary compared to the existing summary model."
Hyperparameter experiments on end-to-end automatic speech recognition,2021,"['automatic speech recognition', 'transformer', 'neural network', 'hyperparameters', 'optimization']",국문 초록 정보 없음,"End-to-end (E2E) automatic speech recognition (ASR) has achieved promising performance gains with the introduced self-attention network, Transformer. However, due to training time and the number of hyperparameters, finding the optimal hyperparameter set is computationally expensive. This paper investigates the impact of hyperparameters in the Transformer network to answer two questions: which hyperparameter plays a critical role in the task performance and training speed. The Transformer network for training has two encoder and decoder networks combined with Connectionist Temporal Classification (CTC). We have trained the model with Wall Street Journal (WSJ) SI-284 and tested on devl93 and eval92. Seventeen hyperparameters were selected from the ESPnet training configuration, and varying ranges of values were used for experiments. The result shows that “num blocks” and “linear units” hyperparameters in the encoder and decoder networks reduce Word Error Rate (WER) significantly. However, performance gain is more prominent when they are altered in the encoder network. Training duration also linearly increased as “num blocks” and “linear units” hyperparameters’ values grow. Based on the experimental results, we collected the optimal values from each hyperparameter and reduced the WER up to 2.9/1.9 from dev93 and eval93 respectively."
흐름이 있는 문서에 적합한 비지도학습 추상 요약 방법,2021,"['NLP', 'Summarization', 'GAN', 'BERT', 'Transformer']",국문 초록 정보 없음,"Recently, a breakthrough has been made in the NLP area by Transformer techniques based on encoder-decoder. However, this only can be used in mainstream languages where millions of dataset are well-equipped, such as English and Chinese, and there is a limitation that it cannot be used in non-mainstream languages where dataset are not established. In addition, there is a deflection problem that focuses on the beginning of the document in mechanical summarization. Therefore, these methods are not suitable for documents with flows such as fairy tales and novels. In this paper, we propose a hybrid summarization method that does not require a dataset and improves the deflection problem using GAN with two adaptive discriminators. We evaluate our model on the CNN/Daily Mail dataset to verify an objective validity. Also, we proved that the model has valid performance in Korean, one of the non-mainstream languages."
Finite Element Analysis of the Effects of Process and Material Parameters on the LVDT Output Characteristics,2021,"['LVDT(선형가변차동변압기)', 'Output Characteristic(출력 특성)', 'Supply Power(공급 전원)', 'Core Material(코어 재질)', 'Finite Element Analysis(유한요소해석)']",국문 초록 정보 없음,"Linear variable differential transformer (LVDT) is a displacement sensor and is commonly used owing to its wide measurement range, excellent linearity, high sensitivity, and precision. To improve the output characteristics of LVDT, a few studies have been conducted to analyze the output using a theoretical method or a finite element method. However, the material properties of the core and the electromagnetic force acting on the core were not considered in the previous studies. In this study, a finite element analysis model was proposed considering the characteristics of the LVDT composed of coils, core, magnetic shell and electric circuit, and the core displacement. Using the proposed model, changes in sensitivity and linear region of LVDT according to changes in process and material parameters were analyzed. The outputs of the LVDT model were compared with those of the theoretical analysis, and then, the proposed analysis model was validated. When the electrical conductivity of the core was high and the relative magnetic permeability was low, the decrease in sensitivity was large. Additionally, an increase in the frequency of the power led to further decrease in sensitivity. The electromagnetic force applied on the core increased as the voltage increased, the frequency decreased, and the core displacement increased."
SMERT: Single-stream Multimodal BERT for Sentiment Analysis and Emotion Detection,2021,"['자연어 처리', '멀티 모달', '감성 분석', '감정 탐지', '단일 입출력 트랜스포머', 'natural language processing', 'multimodal', 'sentiment analysis', 'emotion detection', 'single-stream transformer', 'BERT']",국문 초록 정보 없음,다국어 초록 정보 없음
Automatic Categorization of Islamic Jurisprudential Legal Questions using Hierarchical Deep Learning Text Classifier,2021,"['Islamic Fatwa', 'Natural Language Processing', 'Text Classification', 'Question Answering', 'Recurrent Neural Networks', 'Transformers']",국문 초록 정보 없음,"The Islamic jurisprudential legal system represents an essential component of the Islamic religion, that governs many aspects of Muslims' daily lives. This creates many questions that require interpretations by qualified specialists, or Muftis according to the main sources of legislation in Islam. The Islamic jurisprudence is usually classified into branches, according to which the questions can be categorized and classified. Such categorization has many applications in automated question-answering systems, and in manual systems in routing the questions to a specialized Mufti to answer specific topics. In this work we tackle the problem of automatic categorisation of Islamic jurisprudential legal questions using deep learning techniques. In this paper, we build a hierarchical deep learning model that first extracts the question text features at two levels: word and sentence representation, followed by a text classifier that acts upon the question representation. To evaluate our model, we build and release the largest publicly available dataset of Islamic questions and answers, along with their topics, for 52 topic categories. We evaluate different state-of-the art deep learning models, both for word and sentence embeddings, comparing recurrent and transformer-based techniques, and performing extensive ablation studies to show the effect of each model choice. Our hierarchical model is based on pre-trained models, taking advantage of the recent advancement of transfer learning techniques, focused on Arabic language."
Simple and effective neural coreference resolution for Korean language,2021,"['coreference resolution', 'head‐final language', 'Korean', 'pretrained language model', 'recurrent neural network']",국문 초록 정보 없음,"We propose an end‐to‐end neural coreference resolution for the Korean language that uses an attention mechanism to point to the same entity. Because Korean is a head‐final language, we focused on a method that uses a pointer network based on the head. The key idea is to consider all nouns in the document as candidates based on the head‐final characteristics of the Korean language and learn distributions over the referenced entity positions for each noun. Given the recent success of applications using bidirectional encoder representation from transformer (BERT) in natural language‐processing tasks, we employed BERT in the proposed model to create word representations based on contextual information. The experimental results indicated that the proposed model achieved state‐of‐the‐art performance in Korean language coreference resolution."
콘포머 기반 한국어 음성인식,2021,[],국문 초록 정보 없음,"We propose a speech recognition system based on conformer. Conformer is known to be convolution-augmented transformer, which combines transfer model for capturing global information with Convolution Neural Network (CNN) for exploiting local feature effectively. The baseline system is developed to be a transfer-based speech recognition using Long Short-Term Memory (LSTM)-based language model. The proposed system is a system which uses conformer instead of transformer with transformer-based language model. When Electronics and Telecommunications Research Institute (ETRI) speech corpus in AI-Hub is used for our evaluation, the proposed system yields 5.7 % of Character Error Rate (CER) while the baseline system results in 11.8 % of CER. Even though speech corpus is extended into other domain of AI-hub such as NHNdiguest speech corpus, the proposed system makes a robust performance for two domains. Throughout those experiments, we can prove a validation of the proposed system."
Explaining the translation error factors of Korean-Chinese machine translation services using self-attention visualization,2021,"['Self-attention', 'Machine translation', 'XAI', 'exBERT']",국문 초록 정보 없음,"This study analyzed the Korean-Chinese translation error factors of machine translation services such as Papago and Google translate through self-attention path visualization. Self-Attention is a key method of the Transformer and Bert NLP model and recently widely used in machine translation. The study analyzes the difference in the Attention path between ST (source text) and ST' which meaning does not change and the translation output appears more accurately. Understanding these attention path pattern differences and each translation result TT (target text), and TT', the study summarized three types of errors: symbols missing error, grammar error, and multiple meaning words error. The study analyzed using the xlm-ReBerta multilingual NLP model provided by exBERT, for self-attention visualization, and suggesting some suggestions for error resolution. Through the study, it was found that the cause of translation error can be well identified using the Self-Attention visualization method, and through the explanation, machine translation algorithm developers can use it to improve service quality, and researchers can more understand machine translation algorithms."
유사도 기반 이미지 캡션을 이용한 시각질의응답 연구,2021,[],국문 초록 정보 없음,"Visual Question Answering (VQA) and image captioning are tasks that require understanding of the features of images and linguistic features of text. Therefore, co-attention may be the key to both tasks, which can connect image and text. In this paper, we propose a model to achieve high performance for VQA by image caption generated using a pretrained standard transformer model based on MSCOCO dataset. Captions unrelated to the question can rather interfere with answering, so some captions similar to the question were selected to use based on a similarity to the question. In addition, stopwords in the caption could not affect or interfere with answering, so the experiment was conducted after removing stopwords. Experiments were conducted on VQA-v2 data to compare the proposed model with the deep modular co-attention network (MCAN) model, which showed good performance by using co-attention between images and text. As a result, the proposed model outperformed the MCAN model."
Automatic Classification of the Korean Triage Acuity Scale in Simulated Emergency Rooms Using Speech Recognition and Natural Language Processing: a Proof of Concept Study,2021,"['Triage', 'Classification', 'Machine Learning', 'Natural Language Processing', 'Deep Learning']",국문 초록 정보 없음,"Background: Rapid triage reduces the patients' stay time at an emergency department (ED). The Korean Triage Acuity Scale (KTAS) is mandatorily applied at EDs in South Korea.For rapid triage, we studied machine learning-based triage systems composed of a speech recognition model and natural language processing-based classification.Methods: We simulated 762 triage cases that consisted of 18 classes with six types of the main symptom (chest pain, dyspnea, fever, stroke, abdominal pain, and headache) and three levels of KTAS. In addition, we recorded conversations between emergency patients and clinicians during the simulation. We used speech recognition models to transcribe the conversation.Bidirectional Encoder Representation from Transformers (BERT), support vector machine (SVM), random forest (RF), and k-nearest neighbors (KNN) were used for KTAS and symptom classification. Additionally, we evaluated the Shapley Additive exPlanations (SHAP) values of features to interpret the classifiers.Results: The character error rate of the speech recognition model was reduced to 25.21% through transfer learning. With auto-transcribed scripts, support vector machine (area under the receiver operating characteristic curve [AUROC], 0.86; 95% confidence interval [CI], 0.81–0.9), KNN (AUROC, 0.89; 95% CI, 0.85–0.93), RF (AUROC, 0.86; 95% CI, 0.82–0.9) and BERT (AUROC, 0.82; 95% CI, 0.75–0.87) achieved excellent classification performance.Based on SHAP, we found “stress”, “pain score point”, “fever”, “breath”, “head” and “chest” were the important vocabularies for determining KTAS and symptoms.Conclusion: We demonstrated the potential of an automatic KTAS classification system using speech recognition models, machine learning and BERT-based classifiers."
Scene Text Recognition with Multi-decoders,2021,"['Scene text recognition', 'End to end frame', 'CTC decoder module', 'Attention decoder module']",국문 초록 정보 없음,"In this article, we focus on the scene text recognition problem, which is one of the challenging sub-files of computer vision because of the random existence of scene text. Recently, scene text recognition has achieved state-of-art performance because of the improvement of deep learning. At present, encoder-decoder architecture was widely used for scene recognition tasks, which consist of feature extractor, sequence module. Specifically, at the decoder part, connectionist temporal classification(CTC), attention mechanism, and transformer(self-attention) are three main approaches used in recent research. CTC decoder is flexible and can handle sequences with large changes in length for its align sequences features with labels in a frame-wise manner. Attention decoder can learn better and deeper feature expression and get the better position information of each character. Attention decoder can get more robust and accurate performance for both regular and irregular scene text. Moreover, a novel decoder mechanism is introduced in our study. The proposed architecture has several advantages: the model can be trained using the end-to-end manner under the condition of multi decoders, and can deal with the sequences of arbitrary length and the images of arbitrary shape. Extensive experiments on standard benchmarks demonstrate that our model’s performance is improved for regular and irregular text recognition."
Argument Facet Detection in Online Debates Based on Attention Weights and Clustering with Combined Similarity Matrices,2021,"['argument mining', 'argument facets', 'argument clustering', 'semantic similarities', 'debate texts', 'attention weights']",국문 초록 정보 없음,"The purpose of argument mining research is to analyze and understand the stances, content, and structures of large argumentative texts, such as online debates. For our research, we collected a list of identified arguments from online debates and attempted to use unsupervised methods to create a list of common justifications for each argument stance in each domain. We propose a model that clusters arguments by subtopics, or justifications, and then extracts the list of representative words for argument facets from each cluster. We were able to improve clustering performance by using a combination of three different similarity matrices (cosine similarity between BERT sentence embeddings, semantic textual similarity, and similarity between topic probability distributions) for the clustering algorithm. Our clustering produced 5%p and 7.5%p of ARI and V-measure values on average, which outperforms previous work in two of four domains. Additionally, we used a Transformer model to utilize the attention weights to discover argument facets, and we observed better performances compared to the method without attention weights."
Adaptive Distance Protection Scheme for Mutually Coupled Line,2021,['Adaptive relaying  · Distance protection  · Mutual coupling  · Mho characteristic  · Artifi cial neural networks'],국문 초록 정보 없음,"The availability of zero-sequence current, under normal circumstances, determines the accuracy of the operation of a distance relay which is connected to a mutually coupled parallel line. When this is not available, the system adopts a diff erent compensation factor which if, not properly calculated introduces errors in the relay operation. The proposed adaptive protection scheme, described in this paper, consists of three modular artifi cial neural networks model (ANN). This is developed using the feed-forward nonlinear backpropagation Levenberg–Marquardt algorithm that determines the actual status of the mutually coupled lines. The remote terminal units connected to the current and voltage transformers are used to acquire the appropriate data. The proposed scheme also carefully determines the ground distance element reach settings by calculating the apparent impedance while considering mutual coupling for all practical system confi gurations from the ANN; this eliminates the need for a compensation factor. The results of the apparent impedance (R + jX) calculated by the proposed adaptive and the conventional schemes, showed an average percentage error of (0.06% and 0.02%) and (15% and 41.5%) respectively. Having obtained this result, the performance of the proposed adaptive scheme showed the exact fault location with a higher accuracy when compared with a compensated conventional scheme"
Quasi resonant converter for autonomous power supply,2021,"['DC-DC power converters', 'Pulse width modulation', 'Quasiresonant converters', 'Shunt transistor']",국문 초록 정보 없음,"Quasiresonant converters (QRCs) are increasingly being used in autonomous power supply systems. These converters are efficient, have small dimensions, and operate stably when the load changes. This study is devoted to the development of a zero-current switching QRC with improved characteristics. The main advantages of this QRC are a wide range of regulation and a low level of output voltage ripple. These advantages are achieved by the recuperation of excess energy due to a transistor shunting the primary winding of a transformer. In this work, a mathematical model of advanced QRC (AQRC) is developed, and the quantitative relationship between the parameters of the AQRC power circuit elements is established. In addition, the electromagnetic processes occurring in the AQRC are studied on the Simulink model. Moreover, the AQRC characteristics for various operating modes are studied, and the rational parameters of its components are determined. Then, an AQRC prototype is created. The tests of the prototype are in good agreement with the models. Results show that the AQRC allows for the adjustment of the output voltage ranging from 0 to 100% of the rated value and has a standard level of electromagnetic compatibility."
열화상 이미지 분석을 통한 배전 설비 공정능력지수 감지 시스템 개발,2021,"['Object Detection', 'Process Capability Index', 'Thermal Imaging Technology']",국문 초록 정보 없음,"Purpose: The purpose of this study is to propose a system predicting whether an electricity distribution system is abnormal by analyzing the temperature of the deteriorated system. Traditional electricity distribution system abnormality diagnosis was mainly limited to post-inspection. This research presents a remote monitoring system for detecting thermal images of the deteriorated electricity distribution system efficiently hereby providing safe and efficient abnormal diagnosis to electricians.Methods: In this study, an object detection algorithm (YOLOv5) is performed using 16,866 thermal images of electricity distribution systems provided by KEPCO(Korea Electric Power Corporation). Abnormality/ Normality of the extracted system images from the algorithm are classified via the limit temperature. Each classification model, Random Forest, Support Vector Machine, XGBOOST is performed to explore 463,053 temperature datasets. The process capability index is employed to indicate the quality of the electricity distribution system.Results: This research performs case study with transformers representing the electricity distribution systems. The case study shows the following states: accuracy 100%, precision 100%, recall 100%, F1-score 100%. Also the case study shows the process capability index of the transformers with the following states: steady state 99.47%, caution state 0.16%, and risk state 0.37%.Conclusion: The sum of caution and risk state is 0.53%, which is higher than the actual failure rate. Also most transformer abnormalities can be detected through this monitoring system."
Characteristic analysis of new hybrid compensation topology for wireless charging circuits,2021,"['Wireless power transfer', 'Constant-current output', 'Constant-voltage output', 'Electric vehicle charging']",국문 초록 정보 없음,"Wireless power transfer (WPT) has the advantages of flexibility, safety, and high reliability. Thus, it is widely used in portable electronic equipment, electric vehicles (EV), medical equipment, and other fields. The resonance compensation method of wireless charging directly affects the gain characteristics of the output current and voltage. As a result, this is one of the main research focuses of wireless power transmission technology. A new hybrid compensation topology circuit is proposed in this paper, which is based on the EV constant-current (CC) and constant-voltage (CV) charging mode. In this hybrid topology circuit, an equivalent loosely coupled transformer T model is established for the primary and secondary coils. Through an analysis of the circuit principle, it is concluded that the wireless power transmission circuit can realize CC and CV outputs under a dynamic load change. The output power efficiency characteristics of the serial/parallel (S/P) compensation and serial/serial (S/S) compensation circuits are analyzed. In addition, the mutual inductance parameters are optimized by an efficiency power product method to achieve the overall optimal parameter design in terms of the circuit output power and efficiency. A simulation model and an experimental prototype of the wireless power transmission circuit are established. The design process of the circuit parameters under a given target current and voltage is also presented. The simulation verifies the correctness of the constant current and constant voltage output of the S/P-S/S hybrid compensation circuit. The experimental platform verifies the CC output under the S/P compensation topology, and the CV output under the serial/serial (S/S) compensation topology."
딥러닝 기반 단어 임베딩을 적용한 사진 자막 영작문 채점 시스템,2021,"['컴퓨터 언어보조학습', 'computer assisted language learning', '딥러닝', 'deep learning', '영작문', 'English writing', '단어 임베딩', 'word embedding', '준거참조검사', 'criterion-referenced test', '채점', 'scoring']",국문 초록 정보 없음,"Since human grading of English writing requires substantial resources, many researchers in the area of Computer-Assisted Language Learning (CALL) have been focusing on automatic scoring systems based on natural language processing systems, machine learning, and other automatic processing mechanisms. English Testing Services (ETS) announced several automatic scoring systems for English writing. In this paper, we suggest using a deep learning based automatic scoring system for an English caption writing test. Our method involves using a sentence similarity measurement, which compares different levels of answer sentences with user writing input. We chose different word embedding types (Word2Vec, Word Mover‘s Distance (WMD), Bidirectional Encoder Representations from Transformers (BERT)) and Abstract Meaning Representation (AMR), a linguistic model for comparing semantic differences between two sentences based on semantic representation. Scoring systems should not only satisfy the requirements of complicated scoring rubrics but also meet the conditions of a language proficiency test. Our results show that BERT outperforms three competitive models in predicting accurate scoring levels and also shows the characteristics of the criterion reference which could theoretically express the standards of a language proficiency test."
딥러닝을 활용한 모바일 어플리케이션 리뷰 분류에 관한 연구,2021,"['모바일 배달 어플리케이션', '사용자 리뷰 분류', '사용자 리뷰 분류법', 'Mobile Delivery Application', 'User review classification', 'User Reviews Taxonomy']","스마트폰과 태블릿과 같은 스마트 기기의 발달과 사용이 증가함에 따라, 모바일 기기를 기반으로 한 모바일 어플리케이션 시장이 급속도로 커지고 있다. 모바일 어플리케이션 사용자는 어플리케이션을 사용 경험을 공유하고자 리뷰를 남기는데, 이를 분석하면 소비자들의 다양한 니즈를 파악할 수 있고 어플리케이션 개발자들은 소비자들이 작성한 리뷰를 통해 애플리케이션의 개선을 위한 유용한 피드백을 받을 수 있다. 그러나 소비자들의 남기는 많은 양의 리뷰를 수작업으로 분석하기 위해서는 많은 시간과 비용을 지불해야하기 때문에 이를 최소화 할 방안을 마련할 필요성이 존재한다. 이에 본 연구에서는 구글 플레이스토어(Google PlayStore)의 배달 어플리케이션 사용자 리뷰를 수집한 후 머신러닝과 딥러닝 기법을 활용하여 어플리케이션 기능 장점, 단점, 기능 개선 요청, 버그 보고의 4가지 범주로 분류하는 방법을 제안한다. 연구 결과, Hugging Face의 pretrain된 BERT기반 Transformer모델의 성능의 경우 위의 4개의 범주에 대한 f1 score값은 차례대로 0.93, 0.51, 0.76, 0.83으로 LSTM, GRU보다 뛰어난 성능을 보인 것을 확인할 수 있었다.","With the development and use of smart devices such as smartphones and tablets increases, the mobile application market based on mobile devices is growing rapidly. Mobile application users write reviews to share their experience in using the application, which can identify consumers various needs and application developers can receive useful feedback on improving the application through reviews written by consumers. However, there is a need to come up with measures to minimize the amount of time and expense that consumers have to pay to manually analyze the large amount of reviews they leave. In this work, we propose to collect delivery application user reviews from Google PlayStore and then use machine learning and deep learning techniques to classify them into four categories like application feature advantages, disadvantages, feature improvement requests and bug report. In the case of the performance of the Hugging Face s pretrained BERT-based Transformer model, the f1 score values for the above four categories were 0.93, 0.51, 0.76, and 0.83, respectively, showing superior performance than LSTM and GRU."
Adaptive Distance Protection Scheme Setting in Presence of SVC Using Remote Terminal Unit,2021,['Remote Terminal Unit  · FACTS Devices  · Adaptive Protection  · Relay Setting  · Transmission lines'],국문 초록 정보 없음,FACTS devices are presently used to improve the power transfer capability of a transmission line and voltage stability of a power system network. Shunt injected current (I sh ) by the Static Var Compensators (SVC) causes underreach or overreach of distance relay when not considered during relay calibration of protection system. This paper presents an adaptive relay setting procedure in the presence of a SVC connected at the midpoint of a transmission line. The remote terminal unit RTU is connected to the SVC terminal via a current transformer (CT) measures the injected shunt current when the SVC is the switch in or out of the network and transferred the measured value to the local station via fi ber optic. The PSCAD/ EMTDC software is used to model an adaptive relay that implements the conventional and adaptive relay system settings.The proposed scheme presents a hybrid distance protection system whose setting is based on the prevailing SVC switching conditions; the relay system was implemented using mho characteristics relay. The results obtained show that the proposed scheme has a high accurate setting
ESS용 변압기의 접지방식에 의한 CMV 모델링 및 특성에 관한 연구,2021,"['CMV', 'ESS', 'PCS', 'IGBT', 'Noise', 'Y-△ winding method', '△-Y winding method', 'Grounding', 'Non-grounding', 'PSCAD/EMTDC software']","2017년을 시작으로 2020년 6월까지 총 29건의 화재사고가 발생하여 많은 재산피해가 보고되고 있으며, 전기적인 위해요인중의 하나인 공통모드 전압(CMV: common mode voltage)이 화재원인으로 추정되고 있다. 즉, ESS가 설치되어 있는 수용가의 연계용 변압기는 분산전원연계기준에 따라 Y-△결선방식을 채용해야 하지만, 일부 수용가들은 기존의 △-Y 결선방식을 적용하고 있으며, 실제 ESS 운용현장에서 배터리 측 절연레벨을 초과하는 CMV가 발생한 사례가 보고되고 있다. 따라서, 본 논문에서는 실제 ESS가 운용되는 사이트에서 발생하는 CMV의 특성을 분석하고, 이를 검증하기 위하여 배전계통 상용해석 프로그램인 PSCAD/EMTDC를 사용하여 AC전원부, PCS부, 배터리 부로 구성된 ESS 사이트의 모델링을 수행한다. 상기의 모델링을 바탕으로 시뮬레이션을 수행한 결과, 실제 측정 결과와 유사하게 PCS용 내부변압기 중성점의 접지방식에 따라 CMV의 특성이 크게 달라지고, 중성점이 접지된 경우 CMV의 값이 정격전압을 초과하여 배터리 측 절연레벨에 심각한 악영향을 줄 수 있음을 확인하였다. 또한, PCS용 내부변압기의 중성점을 비접지로 운용한 경우, CMV가 크게 감소하여, 전기설비기준의 절연레벨을 만족하는 것을 알 수 있었다.","Since 2017, a total of 29 fire accidents have occurred in energy storage systems (ESSs) as of June 2020. The common mode voltage (CMV) is one of the electrical hazards that is assumed to be a cause of those fire accidents. Several cases of CMV that violate the allowable insulation level of a battery section are being reported in actual ESS operation sites with △-Y winding connections. Thus, this paper evaluates the characteristics of CMV. An ESS site was modeled with an AC grid, PCS, and battery sections using PSCAD/EMTDC software. As a result of a simulation based on the proposed model, it was confirmed that characteristics of CMV vary significantly and are similar to actual measurements, depending on the grounding method of the internal transformer for PCS. The insulation level of the battery section may be severely degraded as the value of CMV exceeds the rated voltage in case of a grounding connection. It was found that the value of CMV dramatically declines when the internal transformer for PCS is operated as non-grounding connection, so it meets the standard insulation level."
Quarriable Knowledge Creation Framework from Unstructured Scientific Documents,2021,"['knowledge graph', 'natural language processing', 'deep learning', 'word embeddings', 'transformer.']",국문 초록 정보 없음,"Knowledge graphs (KGs) play a pivotal role in modern applications such as decision-making systems, question answering systems, and searching and retrieval systems. However, the automatic construction of a knowledge graph from unstructured text is a challenging task. Moreover, traditional dictionary-, rule-based and supervised machine learning approaches are not reasonably practical due to their dependency on human-expert annotated resources. It is especially true when a knowledge graph is generated from domain-specific information, updated frequently, such as COVID-19 related information resources. This paper uses a pre-trained embedding model (BERT) to create word vectors from COVID-19 research articles. The proposed model is employed at two levels: entity extraction from the text and querying the knowledge stored in KG."
Automated Essay Scoring Using Recurrence over BERT (RoBERT),2021,"['automated essay scoring', 'trait-specific essay scoring', 'essay evaluation', 'Recurrence over BERT (RoBERT)', 'hierarchical transformers']",국문 초록 정보 없음,"This study aimed to build a system that could automate students"" English essay evaluation by using Recurrence over BERT (RoBERT), a state-of-the-art deep learning model. English essay evaluation is inherently time-consuming. It may reflect teacher bias. English teachers are usually burdened with the task of evaluating many essays in a short period of time. Automated essay scoring (AES) can solve these problems. It has the advantage of being able to evaluate essays in a short time and without bias. In this paper, the RoBERT model was trained and evaluated on Essay Set #8 of the Automated Student Assessment Prize (ASAP) dataset. The 5-fold cross validation evaluation method was used for fair comparison with the previously suggested AES models. As a result, the RoBERT model showed the highest agreement with the human raters’ resolved scores in 5 out of 6 trait scores than the previous evaluation models. The advantage of it is that it can use the pre-trained BERT model and deal with long inputs, overcoming the input size limit of the BERT model. It was confirmed that the RoBERT model works well for trait-specific evaluation of long essays. Thus, the RoBERT model can be used as an auxiliary means to automate the evaluation of students"" essays and reduce the excessive work of English teachers."
