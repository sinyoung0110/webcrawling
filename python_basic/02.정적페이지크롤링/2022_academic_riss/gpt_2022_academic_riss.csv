title,date,keywords,abstract,multilingual_abstract
Transformer기반의 언어모델 Bert와 GPT-2 성능 비교 연구,2022,[],"최근 자연어처리 분야에서는 Bert, GPT 등 Transformer기반의 언어모델 연구가 활발히 이뤄지고 있다. 이러한 언어모델은 대용량의 말뭉치 데이터와 많은 파라미터를 이용하여 사전학습을 진행하여 다양한 자연어처리 테스트에서 높은 성능을 보여주고 있다. 이에 본 논문에서는 Transformer기반의 언어모델인 Bert와 GPT-2의 성능평가를 진행한다. 성능평가는 ‘네이버 영화 리뷰’ 데이터 셋을 통해 긍정 부정의 정확도와 학습시간을 측정한다. 측정결과 정확도에서는 GPT-2가 Bert보다 최소 4.16%에서 최대 5.32% 높은 정확도를 나타내었지만 학습시간에서는 Bert가 GPT-2보다 최소 104초에서 116초 빠르게 나타났다. 향후 성능 비교는 더 많은 데이터와 다양한 조건을 통해 구체적인 성능 비교가 필요하다.",다국어 초록 정보 없음
캐릭터 MBTI 기반 GPT-2 활용 뮤지컬 가사 생성 시스템,2022,"['가사생성', '인공지능', '뮤지컬', '마이어스-브릭스 유형지표', '자연어처리', 'Lyrics generation', 'Artificial intelligence', 'Musical', 'MBTI', 'Natural language processing']",국문 초록 정보 없음,"Recently, various cultural and artistic content using AI (Artificial Intelligence) technology have been spreading. In the field of natural language processing that write lyrics for songs, a lot of research has been focused on how to create lyric-like sentences through using rhyme. However, if the lyrics are created by simply considering the rhyme, the song in the musical may be poor at telling the mood on the flow of the play because it can not reflect the characters or the situation in the play. In this paper, we propose a GPT-2 (Generative Pre-trained Transformer-2)-based musical lyrics generation system that fits the MBTI (Myers-Briggs Type Indicator) of characters in musical plays. Character tendency in the play is analyzed using MBTI, and song playlist that match the character MBTI are collected to generate learning data, and musical lyrics that match the character’s tendency are generated through the GPT-2 algorithm based on the learned model. In this paper, it was shown that it is possible to generate musical lyrics suitable for character tendency and atmosphere in the play by proposing the relevant song search keywords for each MBTI and hyper-parameter values of the GPT-2 model suitable for lyrics generation."
양방향 GPT 네트워크를 이용한 VMS 메시지 이상 탐지,2022,"['VMS', 'GPT', 'NLL', '딥러닝', '이상탐지', 'VMS', 'GPT', 'NLL', 'Deep Learning', 'Anomaly Detection']","VMS (variable message signs) 시스템이 악의적인 공격에 노출되어 교통안전과 관련된 거짓정보를 출력하게 된다면 운전자에게 심각한 위험을 초래할 수 있다. 이러한 경우를 방지하기위해 VMS 시스템에 사용되는 메시지들을 수집하여 평상시의 패턴을 학습한다면 VMS 시스템에 출력될 수 있는 이상 메시지를 빠르게 감지하고 이에 대한 대응을 할 수 있을 것이다. 본논문에서는 양방향 GPT (generative pre-trained transformer) 모델을 이용하여 VMS 메시지의 평상 시 패턴을 학습한 후 이상 메시지를 탐지하는 기법을 제안한다. 구체적으로, 제안된 기법에VMS 메시지 및 시스템 파라미터를 입력 하고 이에 대한 NLL (negative log likelihood) 값을 최소화하도록 학습한다. 학습이 완료되면 판정해야 할 대상의 NLL 값을 계산한 후, 문턱치 값이상일 경우 이를 이상으로 판정한다. 실험 결과를 통해, 공격에 의한 악의적인 메시지 탐지뿐만 아니라 시스템의 오류가 발생하는 상황에 대한 탐지도 가능함을 보였다.","When a variable message signs (VMS) system displays false information related to traffic safety caused by malicious attacks, it could pose a serious risk to drivers. If the normal message patterns displayed on the VMS system are learned, it would be possible to detect and respond to the anomalous messages quickly. This paper proposes a method for detecting anomalous messages by learning the normal patterns of messages using a bi-directional generative pre-trained transformer (GPT) network. In particular, the proposed method was trained using the normal messages and their system parameters to minimize the corresponding negative log-likelihood (NLL) values. After adequate training, the proposed method could detect an anomalous message when its NLL value was larger than a pre-specified threshold value. The experiment results showed that the proposed method could detect malicious messages and cases when the system error occurs."
정원길(garden path) 문장 처리에 관한 GPT-2 신경망 언어 모델과 인간의 비교 연구,2022,"['artificial neural-network language model', 'garden path', 'humans', 'language learning', 'sentence processing']",국문 초록 정보 없음,다국어 초록 정보 없음
The Detection of Online Manipulated Reviews Using Machine Learning and GPT-3,2022,"['텍스트 마이닝', '온라인 리뷰', '조작된 리뷰 탐지', '텍스트 생성', '클래스 불균형 문제', 'Text Mining', 'Online Reviews', 'Manipulated Reviews Detection', 'Text Generation', 'Class Imbalance Problem.']",국문 초록 정보 없음,"Fraudulent companies or sellers strategically manipulate reviews to influence customers’ purchase decisions; therefore, the reliability of reviews has become crucial for customer decision-making. Since customers increasingly rely on online reviews to search for more detailed information about products or services before purchasing, many researchers focus on detecting manipulated reviews. However, the main problem in detecting manipulated reviews is the difficulties with obtaining data with manipulated reviews to utilize machine learning techniques with sufficient data. Also, the number of manipulated reviews is insufficient compared with the number of non-manipulated reviews, so the class imbalance problem occurs. The class with fewer examples is under-represented and can hamper a model’s accuracy, so machine learning methods suffer from the class imbalance problem and solving the class imbalance problem is important to build an accurate model for detecting manipulated reviews. Thus, we propose an OpenAI-based reviews generation model to solve the manipulated reviews imbalance problem, thereby enhancing the accuracy of manipulated reviews detection. In this research, we applied the novel autoregressive language model -GPT-3 to generate reviews based on manipulated reviews. Moreover, we found that applying GPT-3 model for oversampling manipulated reviews can recover a satisfactory portion of performance losses and shows better performance in classification (logit, decision tree, neural networks) than traditional oversampling models such as random oversampling and SMOTE."
GPT-3를 활용한 시나리오 생성 보조 시스템,2022,"['자연어 처리(Natural Language Processing)', '시나리오(Scenario)', 'GPT(Generative Pre-trained Transformer)', '문장 생성(Text Generation)']",국문 초록 정보 없음,다국어 초록 정보 없음
GPT3는 공감 대화를 생성할 수 있는가? 공감 대화 생성에서 제로샷 능력에 관하여,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
GPT-2 모델의 PIM 적용 가능성 탐색을 위한 성능 분석,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
근접 이웃 검색을 결합한 GPT-2 기반 언어모델링 및 감성분류,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
A Study on Improving P-tuning Performance with GPT2,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
Manipulated Reviews Detection by Using Machine Learning and Fine-tuned OpenAI GPT-3,2022,"['Mining', 'Online Reviews', 'Manipulated Reviews Detection', 'Text Generation', 'Class Imbalance Problem']",국문 초록 정보 없음,다국어 초록 정보 없음
BERT 기반 욕설 필터링 및 GPT 기반 감성 대화 챗봇 개발,2022,"['Bert', 'GPT', '자연어분류', '자연어생성', '챗봇']",국문 초록 정보 없음,다국어 초록 정보 없음
영어 보충어 강요 문장 처리 연구 : 전산 심리언어학 관점에서,2022,"['강요 효과', '보충어 강요', '예측', 'GPT-2', '서프라이절 이론', 'coercion effect', 'complement coercion', 'expectation', 'GPT-2', 'Surprisal theory']",국문 초록 정보 없음,"This paper investigated complement coercion in English from computational psycholinguistic perspectives. In previous psycholinguistic behavioral studies, when humans processed sentences such as The author began the book, it took longer in reading time to process entity noun phrases (e.g., the book) after the next coercing verbs (e.g., begin) that require event arguments (e.g., writing the book) than the ones after non-coercing verbs (e.g., write) that do not. We leveraged one of the recent neural language models, GPT-2, to examine how it as a computational manifestation of the human mind engages in processing complement coercion in English. We found that GPT-2 yielded the higher surprisal value for coerced complements than for non-coerced complements, which is in keeping with the results from previous psycholinguistic behavioral studies. This finding shows that there is a close link between reading time in humans and the informational-theoretic measure of surprisal in GPT-2 during processing sentences containing complement coercion in English, and that in light of their correlation, GPT-2 seems to have learned relevant linguistic information bearing on complement coercion in English."
자연어 처리 모델을 활용한 블록 코드 생성 및 추천 모델 개발,2022,"['자연어 처리', '자연어 생성', '블록 프로그래밍', '코드 생성', 'GPT-2', 'BLEU', 'ROUGE', 'Natural  Language  Process', 'Natural  Language  Generation', 'Block-based  Programming', 'Code  Generation']","본 논문에서는 코딩 학습 중 학습자의 인지 부하 감소를 목적으로 자연어 처리 모델을 이용하여 전이학습 및 미세조정을 통해 블록 프로그래밍 환경에서 이미 이루어진 학습자의 블록을 학습하여 학습자에게 다음 단계에서 선택가능한 블록을 생성하고 추천해주는 머신러닝 기반 블록 코드 생성 및 추천 모델을 개발하였다. 모델 개발을 위해 훈련용 데이터셋은 블록 프로그래밍 언어인 ‘엔트리’ 사이트의 인기 프로젝트 50개의 블록 코드를 전 처리하여 제작하였으며, 훈련 데이터셋과 검증 데이터셋 및 테스트 데이터셋으로 나누어 LSTM, Seq2Seq, GPT-2 모델을 기반으로 블록 코드를 생성하는 모델을 개발하였다. 개발된 모델의 성능 평가 결과, GPT-2가 LSTM과 Seq2Seq 모델보다 문장의 유사도를 측정하는 BLEU와 ROUGE 지표에서 더 높은 성능을 보였다. GPT-2 모델을 통해 실제 생성된 데이터를 확인한 결과 블록의 개수가 1개 또는 17개인 경우를 제외하면 BLEU와 ROUGE 점수에서 비교적 유사한 성능을 내는 것을 알 수 있었다.","In this paper, we develop a machine learning based block code generation and  recommendation model for the purpose of reducing cognitive load of learners during coding education that learns the learners block that has been made in the block programming environment using natural processing model and fine-tuning and then gen­erates and recommends the selectable blocks for the next step. To develop the model, the training dataset was produced by pre-processing 50 block codes that were on the popular block programming language web site ‘Entry’. Also, after dividing the pre-processed blocks into training dataset, verification dataset and test dataset, we developed a model that generates block codes based on LSTM, Seq2Seq, and GPT-2 model. In the results of the performance evaluation of the developed model, GPT-2 showed a higher performance than the LSTM and Seq2Seq model in the BLEU and ROUGE scores which measure sentence similarity. The data results generated through the GPT-2 model, show that the performance was relatively similar in the BLEU and ROUGE scores ex­cept for the case where the number of blocks was 1 or 17."
L2 영어 교과서를 ‘학습’한 L2-신경망 언어 모델의 문법 일반화 양상,2022,"['언어학적 일반화', '신경망 언어 모델', 'LSTM', 'GPT-2', 'L2-신경망 언어 모델', 'linguistic generalization', 'neural language model', 'LSTM', 'GPT-2', 'L2-language models']",국문 초록 정보 없음,.
Feature Analysis for Detecting Mobile Application Review Generated by AI-Based Language Model,2022,"['Artificial Intelligence', 'Fake Review', 'GPT-2', 'Language Model', 'Machine Learning', 'Software Engineering']",국문 초록 정보 없음,"Mobile applications can be easily downloaded and installed via markets. However, malware and maliciousapplications containing unwanted advertisements exist in these application markets. Therefore, smartphoneusers install applications with reference to the application review to avoid such malicious applications. Anapplication review typically comprises contents for evaluation; however, a false review with a specific purposecan be included. Such false reviews are known as fake reviews, and they can be generated using artificialintelligence (AI)-based text-generating models. Recently, AI-based text-generating models have beendeveloped rapidly and demonstrate high-quality generated texts. Herein, we analyze the features of fake reviewsgenerated from Generative Pre-Training-2 (GPT-2), an AI-based text-generating model and create a model todetect those fake reviews. First, we collect a real human-written application review from Kaggle. Subsequently,we identify features of the fake review using natural language processing and statistical analysis. Next, wegenerate fake review detection models using five types of machine-learning models trained using identifiedfeatures. In terms of the performances of the fake review detection models, we achieved average F1-scores of0.738, 0.723, and 0.730 for the fake review, real review, and overall classifications, respectively."
An Intensity Controlled Review Dataset Construction for Automatic Review Generation,2022,"['text-generation', 'auto-labeling', 'review', 'GPT2', 'BART', 'segementation']",국문 초록 정보 없음,"Most users refer to existing online reviews to see if other users were previously satisfied or not about the products (or foods) they want to buy. Meanwhile, some users do not want to write fruitful, realistic reviews because it is annoying and bothering. To minimize users’ writing costs, we are interested in implementing an automatic review generator to create a complete review with only scores and a few seed keywords by enabling intensity control tailored to the user’s needs. To this end, we propose an intensity controlled review data construction method for online review. Moreover, we employ GPT-2 and BART models popular for text generation tasks for the review generation experiments. Our automatic and manual evaluations for randomly sampled generation results prove the quality of our constructed dataset."
An Experimental Investigation of Discourse Expectations in Neural Language Models,2022,"['discourse expectation', 'implicit causality bias', 'neural language model', 'BERT', 'GPT-2', 'LSTM', 'next sentence prediction', 'coreference resolution', 'surprisal']",국문 초록 정보 없음,"The present study reports on three language processing experiments with most up-to-date neural language models from a psycholinguistic perspective. We investigated whether and how discourse expectations demonstrated in the psycholinguistics literature are manifested in neural language models, using the language models whose architectures and assumptions are considered most appropriate for the given language processing tasks. We first attempted to perform a general assessment of a neural model’s discourse expectations about story continuity or coherence (Experiment 1), based on the next sentence prediction module of the bidirectional transformer-based model BERT (Devlin et al. 2019). We also studied language models’ expectations about reference continuity in discursive contexts in both comprehension (Experiment 2) and production (Experiment 3) settings, based on so-called Implicit Causality biases. We used the unidirectional (or left-to-right) RNN-based model LSTM (Hochreiter and Schmidhuber 1997) and the transformer-based generation model GPT-2 (Radford et al. 2019), respectively. The results of the three experiments showed, first, that neural language models are highly successful in distinguishing between reasonably expected and unexpected story continuations in human communication and also that they exhibit human-like bias patterns in reference expectations in both comprehension and production contexts. The results of the present study suggest language models can closely simulate the discourse processing features observed in psycholinguistic experiments with human speakers. The results also suggest language models can, beyond simply functioning as a technology for practical purposes, serve as a useful research tool and/or object for the study of human discourse processing."
디자인 인공지능을 활용하기 위한 조형 언어 연구 -베이비 스키마 조형 언어를 중심으로-,2022,"['Baby schema', 'Artificial intelligence design', 'Digital design language', 'GPT3', '베이비 스키마', '인공지능 디자인', '디지털 조형언어', 'GPT3']",국문 초록 정보 없음,"(Background and purpose of research) With the emergence of a new design system known as artificial intelligence (AI) design, various discussions and studies beyond creative applications are being actively conducted. This study aims to discuss correct AI-based design research through the automatic generation of shapes based on a baby schema for cuteness and linguistic definitions of shape elements. Focusing on the visual and formative characteristics of the baby schema, which expresses cuteness considering universal aesthetic factors, the precise meaning of formative language and correct formative design methods are explained. From the perspective of evolutionary aesthetics and neuroaesthetics, we therefore explore the relationship between the digital language of AI and the supernormal stimulation and peak-shift cuteness of spatial modeling elements, aiming to derive and quantify human modeling language as a digital language for AI. (Method) To study the visual form of baby schemas using a digital formative language, first, through research on baby schemas as universal aesthetics, the likability mechanism behind cuteness is defined from an evolutionary aesthetic viewpoint. Second, through prior neurological research, the close relationship between the aesthetic sense of sculpture and language is examined. Third, the possibility of transforming Babiskima’s reduced plastic language into the digital plastic language of artificial intelligence is confirmed. Fourth, the baby schema’s eight digital modeling elements were verified through AI modeling experiments. (Results) To reproduce the aesthetic sense of sculpture with AI, we attempted to reconstruct the linguistic structure and found there is a difference in perception between humans and AI in interpreting phenomena. We also found that for the correct presentation, we needed to use the reduced-structure aesthetic language that composes the baby schema. This is a reminder that not only AI but also designers themselves must exercise correct cognitive thinking.(Conclusion) Cuteness can be defined as a concept of supernormal stimulus because it is an essential mechanism for human survival, which we reduced to a language of form. Pretty shapes, textures, colors, and environments serve as supernormal stimuli for the cognitive mechanisms of our aesthetic intelligence. Then, through a defined formative language, AI can be used to structurally reproduce the form of cuteness as a grammar of cuteness. This study will hopefully serve to guide the correct use and imagination of formative language as a means of expressing designers’ aesthetic sense."
신경망 언어 모델과 인간 언어 사용자의 주어와 목적어 관계절 처리 비교 연구,2022,"['subject/object relative clause', 'PP modification', 'reading time', 'surprisal', 'BERT', 'GPT-2', '주어/목적어 관계절', '전치사구 수식', '읽기 시간', '서프라이절']",국문 초록 정보 없음,"This paper is to investigate the distinct aspects of processing subject and object relative clauses (SRC and ORC) in neural language models (LMs) and humans using the materials, which are constructed by manipulating two RC types (SRC vs ORC) and two intervening PP types (locative vs temporal). Surprise values are collected from the transformer GPT-2 and BERT language models. Reading time data for humans are taken from Lowder and Gordon’s (2021) eye-tracking experiment. According to Lowder and Gordon (ibid.) that take as a critical region the matrix verb after the RC in the subject position, for humans the locative PP contained in the RC register longer reading times ORCs than SRCs, while the temporal PP does so for SRCs than ORCs. By contrast, for the two neural LMs, surprisals are higher for ORCs than SRCs regardless of whether the PP in question is locative or temporal. There was no statistically significant linear fit between human and the LMs’ responses. The result shows that to the extent that the distinction between locative and temporal PPs in RCs is syntactico-semantic, neither of the two neural language models is able to acquire human-like sensitivity to such a distinction."
Neural Network Language Models as Psycholinguistic Subjects: Focusing on Reflexive Dependency,2022,"['reflexive dependency', 'filler-gap dependency', 'gender mismatch effect', 'neural network language model', 'surprisal']",국문 초록 정보 없음,"The purpose of this study is to investigate the reflexive-antecedent dependency resolution accompanying the wh-filler-gap dependency resolution in neural network language models (LMs)’ sentence processing, comparing the processing result of LMs to the one of humans. To do so, we adopt the psycholinguistic methodology that Fraizer et al. (2015) used for humans. The neural-network language models employed in this study are four LMs: the Long Short-Term Memory (LSTM) trained on large datasets, the Generative Pre-trained Transformer-2 (GPT-2) trained on large datasets, an LSTM trained on small datasets (L2 datasets), and the GPT-2 trained on small datasets (L2 datasets). We found that only the LMs trained on large datasets were sensitive to the dependency between a reflexive and its antecedent matching in gender, but all of the four neural LMs failed to learn reflexive-antecedent dependency accompanying wh-filler-gap dependency. Furthermore, we also found that the neural LMs have a learning bias in gender mismatch."
Computer Science Research Ideas Generation Using Neural Networks,2022,"['Neural Networks', 'Artificial Intelligence', 'Computer Science Research Ideas', 'Topic Generation', 'GPT2']",국문 초록 정보 없음,"The number of published journals, conferences, and research papers in computer science is increasing rapidly, which has led to a challenge in coming up with new and unique ideas for research. To alleviate the issue, this paper uses artificial neural networks (ANNs) to generate new computer science research ideas. It does so by using a dataset collected from IEEE published journals and conferences to train an ANN model. The results reveal that the model has a 14% success rate in generating usable ideas. The outcome of this paper has implications for helping both new and experienced researchers come up with novel research topics."
언어 인공지능의 상식추론과 평가 체계 현황,2022,"['인공지능', '신경망 언어 모형', '상식추론', '평가 체계', '튜링 테스트', 'Artificial intelligence', 'word embedding', 'commonsense reasoning', 'evaluation benchmark', 'data curation']","최근 인공지능의 성능이 고도로 향상됨에 따라, 인공지능이 인간의 언어 구사 능력에 가까워졌다는 주장이 제기되었다. 예컨대, 인공지능 GPT-3는 인간의 작문 능력과 구별되지 않는 성능을 보이는 것처럼 알려졌다. 그러나, 구체적인 평가의 영역에 따라 인공지능과 인간이 큰 격차를 보인다. 대표적인 것이 상식추론이다. 예를 들어, 영희가 책가방을 메고 학교에 가는지, 아니면 나이트클럽을 가는지는 논리가 아닌 상식에 비추어 자명하다. 특히, 상식추론은 경험세계에 대한 광범위한 지식이 필요하다는 점에서, 문자열의 분포적 정보로부터 사실적인 지식을 이끌어내야 하는 인공지능에게 매우 도전적인 과제이다. 이 점에 착안하여 최근 인공지능이 상식추론을 학습하였는지 평가하기 위한 정량적 평가 체계 또는 벤치마크가 공개되고 있다. 튜링 테스트에서 출발한 벤치마크는 일종의 수만 건의 문제은행으로서,정확도와 유사도를 기반으로 인공지능의 상식추론을 정량적으로 검증한다. 이에 본고는 인공지능 상식추론과 평가 체계의 현황을 폭넓게 검토하고, 인문사회학적 관점에서 비판적인 이해를 시도한다. 구체적으로, 자연어처리 분야의 신경망 언어 모형 또는 워드 임베딩이 어떻게 문자열을 학습하는지 개념적으로 이해한다. 이와 함께, 인공지능이 학습한 추론 지식을 검증하는 평가 체계 또는 자연어처리 벤치마크의 구축 방법론과 예시 문장을 분석한다. 이를 위하여 최근 공개한 한국어 인공지능 벤치마크인 KLUE를 사례로 분석을 제시한다. 또한, 대표적인 벤치마크인 SWAG, CosmosQA, 그리고 CommonGen을 분석한다. 이와 함께, 최근의 대규모 인공지능의 개발이 내포하는 환경적, 경제적, 윤리적 우려가 커지고 있음을 지적하고, 언어학적 튜링 테스트를 중심으로 정량적 평가 체계의 본질적인 한계를 논의한다.","Recent advances in artificial intelligence (AI) showed that language model, i.e., probability estimator of word occurrences in contexts, may capture the human reasoning ability. This surprising finding is built upon the previous research on the empirical evaluation of the reasoning ability of AIs. The empirical evaluation measures the performance gap between the AIs and human speakers on “well-curated” datasets, often called as NLP benchmarks. Recently, many researchers propose new NLP benchmarks testing the commonsense reasoning, e.g, what should you do if you encounter a grizzly bear chasing the baby?. The question is tricky because the decision depends on situations, where baby may refer to the human baby or simply the baby cow. Thus, it has been noted that benchmarking the commonsense reasoning is critical for AI’s human-like performance in intelligent tasks. In this paper, we review some types of commonsense reasoning and newly released evaluation benchmarks, suggesting that the reliability of dataset is hinged upon the data curation method. We first briefly introduce how language model using neural networks learns or predicts the word probability. We then proceed to review how workers or annotators curate the NLP benchmarks, focusing on the collection of human intuitions regarding the text examples. Although these curation methods empirically show the diverse reasoning ability of AI, there are concerns about the negative social impacts of extremely large AIs. Importantly, NLP benchmarks are sometimes misleading because AI simply captures the shallow surface structure of language, which denotes that AI successfully mapped texts to texts. Overall, we suggest that constructing the colossal AIs is not a silver bullet to commonsense reasoning since AIs are not free from data bias."
데이터 증강을 이용한 커뮤니티 질의응답 모델 학습,2022,"['커뮤니티 질의응답', '데이터 증강', '언어모델', '문장생성', 'BERT']","커뮤니티 질의응답(cQA) 시스템은 사용자가 요구하는 정보를 담은 질문을 이용해 그 목적에 맞는 답변을 찾아내는 것으로, 자주 찾는 질문(FAQ)와 같은 소규모 시스템에도 사용할 수 있다. 기존 cQA 시스템으로는 tf-idf 기반 방법, 워드벡터와 CNN 을 이용한 방법 등이 개발되었다. 본 논문에서는 BERT 기반 딥러닝 모델을 cQA 시스템으로 개발하였다. 그러나, 소규모의 FAQ 용 cQA 시스템의 초기에는 사용자와 컴퓨터의 상호작용이 거의 없기 때문에 다양한 사용 예를 이용한 학습 효과를 충분히 내기 어렵다. 따라서, 이 문제를 해결하기 위해 GPT2 기반의 문장생성 모델인 DINO 를 한국어에 맞게 학습한 KoGPT2-DINO 를 이용하여 학습데이터를 증강하고, 증강된 데이터를 BERT 기반의 단일 문장 레이블링 모델로 학습하였다. 그 결과, 원본데이터 만으로 학습한 모델의 성능보다 증강한 데이터로 학습한 모델의 성능이 최대 6.57% 포인트 증가하였다.",다국어 초록 정보 없음
Genetic dissection of grain yield traits in a large collection of spring wheat (Triticum aestivum L.) germplasm,2022,"['Common wheat', 'Correlation analysis', 'Genetic advance', 'Heritability', 'Grain yield']",국문 초록 정보 없음,"Understanding genetic architecture of a crop germplasm is necessary for designing a successful breeding program. Herein, we evaluated a large collection of 500 spring wheat accessions for 2 crop seasons to dissect the genetics of 11 yield components and their direct and indirect contributions to grain yield (GY). The genetic estimates of broad sense heritability (h2 ), genetic advance (GA), phenotypic correlation (rp), genotypic correlation (rg), and path coefcient analysis were performed.Signifcant genetic variation was observed for all yield traits suggesting that GY can be improved by exploiting the studied yield traits. Phenotypic coefcient of variation (Vp) was greater than genotypic coefcient of variation (Vg) for all studied traits. Higher broad sense h2 and GA were observed for grains per plant (GpP), spikes per plant (SpP), spikelets per spike (Spt/S) and grain yield (GY). The GY exhibited signifcant and positive correlation with all studied traits except with spikelet density (SptD). The GpP and TGW exhibited positive direct efect on increasing grain yield. Taking together, SpP, GpS, TGW and SDW are the major contributors to improving genetic yield potential of bread wheat with spring growth habit. The negative correlation between GY and SptD was dissected in path coefcient analysis as negative indirect efect of SptD on GY through reduced GpS and TGW. Our study provides new insights on the association of GpP, TGW and SDW in bread wheat. The GpP, TGW and SDW are infuenced by SpP, Gpt/S, G/Spt, SptD, AL and PH with indirect efects on GY. To improve yield potential in wheat, the traits with direct efects (GpP, TGW and SDW) and indirect efects (e.g., SptD) can be used as selection criteria."
언어 인공지능의 상식추론과 평가 체계 현황,2022,"['인공지능', '신경망 언어 모형', '상식추론', '평가 체계', '튜링 테스트', 'Artificial intelligence', 'word embedding', 'commonsense reasoning', 'evaluation benchmark', 'data curation']","최근 인공지능의 성능이 고도로 향상됨에 따라, 인공지능이 인간의 언어 구사 능력에 가까워졌다는 주장이 제기되었다. 예컨대, 인공지능 GPT-3는 인간의 작문 능력과 구별되지 않는 성능을 보이는 것처럼 알려졌다. 그러나, 구체적인 평가의 영역에 따라 인공지능과 인간이 큰 격차를 보인다. 대표적인 것이 상식추론이다. 예를 들어, 영희가 책가방을 메고 학교에 가는지, 아니면 나이트클럽을 가는지는 논리가 아닌 상식에 비추어 자명하다. 특히, 상식추론은 경험세계에 대한 광범위한 지식이 필요하다는 점에서, 문자열의 분포적 정보로부터 사실적인 지식을 이끌어내야 하는 인공지능에게 매우 도전적인 과제이다. 이 점에 착안하여 최근 인공지능이 상식추론을 학습하였는지 평가하기 위한 정량적  평가 체계 또는 벤치마크가 공개되고 있다. 튜링 테스트에서 출발한 벤치마크는 일종의 수만 건의 문제은행으로서,정확도와 유사도를 기반으로 인공지능의 상식추론을 정량적으로 검증한다. 이에 본고는 인공지능 상식추론과 평가 체계의 현황을 폭넓게 검토하고, 인문사회학적 관점에서 비판적인 이해를 시도한다. 구체적으로, 자연어처리 분야의 신경망 언어 모형 또는 워드 임베딩이 어떻게 문자열을 학습하는지 개념적으로 이해한다. 이와 함께, 인공지능이 학습한 추론 지식을 검증하는 평가 체계 또는 자연어처리 벤치마크의 구축 방법론과 예시 문장을 분석한다. 이를 위하여 최근 공개한 한국어 인공지능 벤치마크인 KLUE를 사례로 분석을 제시한다. 또한, 대표적인 벤치마크인 SWAG, CosmosQA, 그리고 CommonGen을 분석한다. 이와 함께, 최근의 대규모 인공지능의 개발이 내포하는  환경적, 경제적, 윤리적 우려가 커지고 있음을 지적하고, 언어학적 튜링 테스트를 중심으로 정량적 평가 체계의 본질적인 한계를 논의한다.","Recent advances in artificial intelligence (AI) showed that language model, i.e., probability estimator of word occurrences in contexts, may capture the human reasoning ability. This surprising finding is built upon the previous research on the empirical evaluation of the reasoning ability of AIs. The empirical evaluation measures the performance gap between the AIs and human speakers on “well-curated” datasets, often called as NLP benchmarks. Recently, many researchers propose new NLP benchmarks testing the commonsense reasoning, e.g, what should you do if you encounter a grizzly bear chasing the baby?. The question is tricky because the decision depends on situations, where baby may refer to the human baby or simply the baby cow. Thus, it has been noted that benchmarking the commonsense reasoning is critical for AI’s human-like performance in intelligent tasks. In this paper, we review some types of commonsense reasoning and newly released evaluation benchmarks, suggesting that the reliability of dataset is hinged upon the data curation method. We first briefly introduce how language model using neural networks learns or predicts the word probability. We then proceed to review how workers or annotators curate the NLP benchmarks, focusing on the collection of human intuitions regarding the text examples. Although these curation methods empirically show the diverse reasoning ability of AI, there are concerns about the negative social impacts of extremely large AIs. Importantly, NLP benchmarks are sometimes misleading because AI simply captures the shallow surface structure of language, which denotes that AI successfully mapped texts to texts. Overall, we suggest that constructing the colossal AIs is not a silver bullet to commonsense reasoning since AIs are not free from data bias."
한국어 자연어생성에 적합한 사전훈련 언어모델 특성 연구,2022,"['Pre-train Language Model', 'Transformer', 'Abstractive text summarization', 'BART', 'GPT', '사전훈련 언어모델', '트랜스포머', '문서 생성요약', 'BART', 'GPT']",국문 초록 정보 없음,"This study empirically analyzed a Korean pre-trained language models (PLMs) designed for natural language generation. The performance of two PLMs – BART and GPT – at the task of abstractive text summarization was compared. To investigate how performance depends on the characteristics of the inference data, ten different document types, containing six types of informational content and creation content, were considered. It was found that BART (which can both generate and understand natural language) performed better than GPT (which can only generate). Upon more detailed examination of the effect of inference data characteristics, the performance of GPT was found to be proportional to the length of the input text. However, even for the longest documents (with optimal GPT performance), BART still out-performed GPT, suggesting that the greatest influence on downstream performance is not the size of the training data or PLMs parameters but the structural suitability of the PLMs for the applied downstream task. The performance of different PLMs was also compared through analyzing parts of speech (POS) shares. BART’s performance was inversely related to the proportion of prefixes, adjectives, adverbs and verbs but positively related to that of nouns. This result emphasizes the importance of taking the inference data’s characteristics into account when fine-tuning a PLMs for its intended downstream task."
부유사 농도에 따른 넙치와 조피볼락의 생리학적 특성,2022,"['Suspended load', 'AST/GOT', 'ALT/GPT', 'ALP', 'physiological change', '.']","부유사가 어류에 미치는 생리학적 특성을 알아보기 위해 대조구 (0 mg/L), 100 mg/L, 250 mg/L, 500 mg/L, 1,000 mg/L인 농도 구간에 넙치와 조피볼락을 노출시킨 후 AST/GOT, ALT/GPT, ALP와 cortisol을 분석하였다. 분석 결과 부유사 농도 구간에서 AST/GOT, AL T/GPT, ALP와 cortisol 값이 대조구에서 보다 높게 나타났다. 부유사에 노출된 넙치에는 S. parauberis, 조피볼락에는 V. harveyi를 인위감염하였을 때 생존율은 부유사 농도 구간에서보다 대조구에서 더 높게 나타났다. 이러한 결과는 이전에 연구되었던 부유사가 어류에 악영향을 미친다는 결과와 연관되어지며, 어류의 건강도에 부유사가 영향을 주는 것으로 시사된다.","After exposing Olive flounder and Korean rockfish to the concentration ranges of control (0 mg/L), 100 mg/L, 250 mg/L, 500 mg/L, and 1,000 mg/L, to investigate the physiological properties of suspended load on fish AST/GOT, ALT/GPT, ALP and cortisol were analyzed. As a result of the analysis, the AST/GOT, ALT/GPT, ALP and cortisol values were higher in the suspended load concentration section than in the control group. After artificial infection with S. parauberis for Olive flounder and V. harveyi for Korean rockfish exposed to suspended load, the survival rate was higher in the control group than in the suspended load concentration section. These results are related to the previously study results of suspended load that adversely affect fish, suggesting that suspended load affects the health of fish."
"사전운동보충제 섭취가 직업 트레이너의 신체조성, 1RM 및혈액변인에 미치는 영향",2022,"['pre-workout and supplementation', 'professional trainers', 'body composition', '1RM', 'blood variables.']",국문 초록 정보 없음,"The purpose of this study was to changes in body composition, 1RM and blood variables to professional trainers following four weeks of pre-workout and supplementation. The subjects were divided into a pre-workout supplement intake group (PWG) and a comparison group without intake(WG) of 4 people each. The measurement variables of this study were body composition (muscle mass, body fat), 1RM, and blood variables (ammonia, creatine phosphate, GOT, GPT). The mean and standard deviation for all variables were calculated using the SPSS program, and independent t-test, ANCOVA, and two-way repeated ANOVA were performed, and the statistical significance level was α=.05. The results are as follows.First, Body fat mass decreased significantly in PWG, but skeletal muscle mass did not change significantly. Second, the PWG increased significantly in 1RM after treatment in dead-lift and bench press excluding squats. Third, In both groups, there was no significant difference in creatine phosphate, GOT, and GPT, but ammonia was significantly increased. In conclusion, four weeks of high-intensity training due to pre-workout supplement intake had no effect on skeletal muscle mass, creatine phosphate, GOT and GPT, but had an effect on 1RM and ammonia increase."
감정에 기반한 가상인간의 대화 및 표정 실시간 생성 시스템 구현,2022,"['가상인간', '멀티모달 대화', '유니티', '감정기반 대화', 'GPT-2', 'RoBERTa', 'Virtual Human', 'Multi-Modal Dialogue', 'Unity', 'Dialogue based on Emotions', 'GPT-2', 'RoBERTa']","가상인간은 가상공간(가상 현실, 혼합 현실, 메타버스 등)에서 Unity와 같은 3D Engine 전용 모델링 도구로 구현된다. 실제 사람과 유사한 외모, 목소리, 표정이나 행동 등을 구현하기 위해 다양한 가상인간 모델링 도구가도입되었고, 어느 정도 수준까지 인간과 의사소통이 가능한 가상인간을 구현할 수 있게 되었다. 하지만, 지금까지의 가상인간 의사소통 방식은 대부분 텍스트 혹은 스피치만을 사용하는 단일모달에 머물러 있다. 최근 AI 기술이 발전함에 따라 가상인간의 의사소통 방식은 과거 기계 중심의 텍스트 기반 시스템에서 인간 중심의 자연스러운 멀티모달 의사소통 방식으로 변화할 수 있게 되었다. 본 논문에서는 다양한 대화 데이터셋으로 미세조정한 인공신경망을 사용해 사용자와 자연스럽게 대화 할 수 있는 가상인간을 구현하고, 해당 가상인간이 생성하는 문장의 감정값을 분석하여 이에 맞는 표정을 발화 중에 나타내는 시스템을 구현하여 사용자와 가상인간간의 실시간 멀티모달 대화가 가능하게 하였다.","Virtual humans are implemented with dedicated modeling tools like Unity 3D Engine in virtual space (virtual reality, mixed reality, metaverse, etc.). Various human modeling tools have been introduced to implement virtual human-like appearance, voice, expression, and behavior similar to real people, and virtual humans implemented via these tools can communicate with users to some extent. However, most of the virtual humans so far have stayed unimodal using only text or speech. As AI technologies advance, the outdated machine-centered dialogue system is now changing to a human-centered, natural multi-modal system. By using several pre-trained networks, we implemented an emotion-based multi-modal dialogue system, which generates human-like utterances and displays appropriate facial expressions in real-time."
딥러닝 기반 사전학습 언어모델에 대한 이해와 현황,2022,"['NLP', 'deep learning', 'language model', 'Transformer', 'BERT', 'GPT', '자연어 처리', '딥러닝', '언어모델', '트랜스포머', 'BERT', 'GPT']","사전학습 언어모델은 자연어 처리 작업에서 가장 중요하고 많이 활용되는 도구로, 대량의 말뭉치를 대상으로 사전학습이 되어있어 적은 수의 데이터를 이용한 미세조정학습으로도 높은 성능을 기대할 수 있으며, 사전학습된 토크나이저과 딥러닝 모형 등 구현에 필요한 요소들이 함께 배포되기 때문에 자연어 처리 작업에 소요되는 비용과 기간을 크게 단축시켰다. 트랜스포머 변형 모형은 이와 같은 장점을 제공하는 사전학습 언어모델 중에서 최근 가장 많이 사용되고 있는 모형으로, 번역을 비롯하여 문서 요약, 챗봇과 같은 질의 응답, 자연스러운 문장의 생성 및 문서의 분류 등 다양한 자연어 처리 작업에 활용되고 있으며 컴퓨터 비전 분야와 오디오 관련 분야 등 다른 분야에서도 활발하게 활용되고 있다. 본 논문은 연구자들이 보다 쉽게 사전학습 언어모델에 대해 이해하고 자연어 처리 작업에 활용할 수 있도록 하기 위해, 언어모델과 사전학습 언어모델의 정의로부터 시작하여 사전학습 언어모델의 발전과정과 다양한 트랜스포머 변형 모형에 대해 조사하고 정리하였다.",다국어 초록 정보 없음
가볍다차(茶)가 고지방식이로 유도된 비만 마우스에서 항염증에 미치는 효과,2022,"['Gabyeobda tea', 'Anti-inflammatory effect', 'High-fat diet', 'C57BL/6 mice']",국문 초록 정보 없음,"Objectives: The purpose of this study was to investigate the effects of Gabyeobda tea (GT) on anti-inflammation in ice induced high fat diet (HFD).Methods: The C57BL/6 mice fed HFD were administrated with GT once daily for 8 weeks. The changes of body weight, calorie intake levels were measured in mice. The level of serum total cholesterol, triglyceride, high density lipoprotein cholesterol, glutamic oxaloacetic transaminase (GOT), and glutamic pyruvic transaminase (GPT) were measured in mice by enzyme-based assay. It was also observed the histological changes of liver, and fat tissues with hematoxylin and eosin staining. Further real-time polymerase chain reaction (PCR) and enzyme-linked immunosorbent assay were employed to detect inflammatory cytokine levels such as tumor necrosis factor (TNF)-α, interleukin (IL)-6, and IL-1β.Results: HFD+GT group, which was administered with GT with HFD, showed no body weight gain compared with HFD group. However, levels of GOT, GPT, and inflammatory cytokines such as TNF-α, IL-6, and IL-1β in the blood of HFD+GT group were significantly reduced compared with HFD group. In addition, the messenger RNA (mRNA) expression level of the IL-12 gene was significantly reduced and the mRNA expression level of the IL-10 was increased in the liver.Conclusions: It suggests that Gabyeobda tea can alleviate inflammatory responses induced by high fat diet by inhibiting inflammatory cytokines production."
적조 Cochlodinium polykrikoides 노출에 따른 양식산 참돔과 넙치의 생리학적 반응,2022,"['Cochlodinium polykrikoides', 'Physiological response', 'Red tide', 'Reactive oxygen', 'Survival rate', '생리적 반응', '적조', '활성산소', '생존율']","본 연구는 양식산 참돔과 넙치를 Cochlodinium polykrikoides 적조에 노출시켜 노출 시간에 따른 생존율, 호흡수, 혈중 스트레스 지표 및 조직학적 변화를 비교 조사하였다. 대조구는 자연해수를 사용하였고, 실험구는 C. polykrikoides 밀도를 5,500±200 cells/ml로 설정하였다. 그 결과, 참돔은 적조 노출 1시간 이내, 넙치는 적조 노출 5시간 이내 전량 폐사하였다. 생리학적 반응을 분석한 결과, 참돔은 적조 노출 후 혈중 Glucose 농도가 감소하였으며, 혈중 GOT 및 GPT 농도는 증가하였고, 혈중 SOD 농도는 감소하고, CAT 및 GPx 농도는 증가하는 경향을 보였다.넙치는 적조 노출 후 혈중 Cortisol 및 GOT, GPT 농도가 증가하였고, 혈중 Glucose 농도는 적조노출 1시간째 증가한 이후 감소하였으며, 혈중 SOD, CAT, GPx 농도는 노출 1시간째 감소한이후 증가하였다. 조직학적 분석 결과, 참돔과 넙치의 아가미에 구조적인 손상이 발생하였다.결론적으로 5,500 cells/ml 밀도의 C. polykrikoides 적조 노출은 양식산 참돔과 넙치에게 산화적 스트레스로 작용하여 체내 항산화 방어 기작을 활성화하고, 간과 아가미의 손상을 발생시키는 것으로 나타났다.",다국어 초록 정보 없음
아스파라거스 부산물 추출물의 숙취 해소 및 통풍완화 효과,2022,"['Asparagus officinalis L.', 'Anti-hangover Effect', 'Anti-gout Effect']",국문 초록 정보 없음,"Background: Given the reported pharmacological effects of rutin from asparagus stems and roots, here, we assessed the anti-hangover and anti-gout effects of asparagus stem and root extracts.Methods and Results: We found that rutin amount in ethanol extracts of asparagus stems was 10.15 ㎎/g, and in hydrothermal extracts of asparagus roots it was 3.28 ㎎/g. Alcohol dehydrogenase and aldehyde dehydrogenase activities in asparagus-stem extracts were 80% and 40% of those in the control group, respectively. Following hangover induction in rats using alcohol, an injection of 400 ㎎/㎏ body weight (BW) of the extract significantly reduced the blood alcohol concentration compared to that of the control group. Additionally, blood gamma-glutamyl transpeptidase (GGT), glutamic oxaloacetic transaminase (GOT), and glutamic pyruvic transaminase (GPT) concentrations measured 5 h after alcohol administration revealed that treatment with 400 ㎎/㎏ BW of extract significantly reduced GGT and GOT concentrations compared to the control group, whereas changes in GPT were not significant. Moreover, xanthine oxidase activity after treatment with hydrothermal extracts of asparagus roots was 44.01 ± 0.32%, showing 53.3% inhibition compared to the positive control (allopurinol; 82.50 ± 4.98%). To assess the anti-gout effect of hydrothermal extracts of asparagus roots, we induced gout in rats by intraperitoneal injection of potassium oxonate and measured blood uric acid, creatinine, and blood urea nitrogen concentrations at 3 h post-administration of the extract. The results showed a significant reduction in creatinine and uric acid levels in the treated group compared to the control.Conclusions: These findings demonstrated the potential of asparagus stem and root extracts as functional ingredients."
인공지능 시대의 도래와 글쓰기 윤리,2022,"['인공지능', '글쓰기 윤리', '놀이', '스콜라', '표절', 'artificial intelligence', 'ethics of writing', 'play', 'schola', 'plagiarism']","Open AI가 개발한 GPT 시리즈의 등장은 글의 생산방식, 글의 위상, 글의 본질에 관한 사람들의 이해를 급격하게 변화시키고 있다. 글 쓰는 기계의 등장은 글쓰기라는 인간 활동의 성격이 무엇인가를 조명할 필요성을 부각시키고 있으며, 본 연구는 인공지능 시대에서 강조되어야 할 글쓰기 윤리의 핵심을 밝히는 데 그 목적을 둔다. 글쓰기는 산출물의 생산이라는 목적에 매인 ‘일’이 아니다. 그것은 언어 이면에 붙박여 있는 의미의 세계를 유희하는 ‘놀이’로서의 성격을 가진다. 표절이 글쓰기 윤리 위반인 것은 그것이 타인의 지식 재산을 도둑질한 행위이기 때문이기도 하지만, 보다 근본적으로 그것이 글쓰기 활동의 놀이적 성격을 인정하지 않는 인간적 자세를 나타내기 때문이다. 윤리적인 글은 놀이의 정신 속에서, 스콜라의 정신 속에서 쓰인 글이며, 글쓰기 윤리는 글 쓰는 사람에게 요구되는 인간적 자세와 태도를 밝히는 일을 그 자신의 과제로 삼을 필요가 있다.",다국어 초록 정보 없음
이커머스 고객 문의 AI 자동 응답 챗봇 개발,2022,"['자연어처리', 'Transformer', 'BERT', 'GPT', '자동응답챗봇']",국문 초록 정보 없음,다국어 초록 정보 없음
한국어 자연어생성에 적합한 사전훈련 언어모델 특성 연구,2022,"['Pretrain Language Model', 'Transformer', 'Abstractive text summarization', 'BART', 'GPT']",국문 초록 정보 없음,다국어 초록 정보 없음
스마트팜 제어 데이터 유무에 따른 딥러닝을 이용한 내부 온·습도 예측 알고리즘의 정확성 평가,2022,"['스마트팜', '딥러닝', '온도예측', '습도예측']","최근 국내에서 스마트팜과 빅데이터 및 인공지능 기술을 연계하려는 다양한 연구가 진행되고 있다. 빅데이터 및 AI기술을 스마트팜 내 환경제어와 연계하기 위해서는 정확성 높은 내부 환경 예측이 필요하다. 딥러닝을 활용하여 정확성이 높은 시설 내부 환경 예측을 위해서는 시설의 내ㆍ외부 환경데이터 뿐만 아니라 제어데이터가 수반되어야 한다. 하지만 현재 국내 시설농가에서 환경데이터와 제어데이터를 모두 확보할 수 있는 농가는 일부에 불과하다. 본 연구는 충청북도 진천에 위치한 약 2000평 규모의 파프리카 스마트팜에서 저장된 2021년 환경데이터 및 제어데이터와 GPT 신경망을 활용하여 온실 내부 온·습도를 예측하는 딥러닝 알고리즘을 개발하였다. 학습은 실제 농장 데이터와 출력값의 MSE(mean square errㆍr)를 최소화하는 것을 목표로하여 진행되었다. 제어변수에 따른 알고리즘의 정확성 평가를 위하여 외부습도, 창문 개폐율, 커튼 개폐율, 난방수 온도&난방펌프 작동유무를 각각의 하나의 제어변수로 설정하여 총 4가지의 제어데이터 유무에 따른 16가지 경우의 알고리즘을 구성 및 학습을 진행하였다. 학습을 통해 시설 내부 온ㆍ습도 예측값과 실제값 간의 결정계수(R-square)를 통해 결과를 나타내었고 이를 비교분석하였다. 제어데이터를 포함하지 않고 외기, 광량을 입력, 내부 온ㆍ습도를 출력으로 한 기본모델의 온습도 예측 결과는 각각 0.90, 0.68로 나타났다. 이는 모든 학습 결과의 R2 평균값인 온도 0.89, 습도 0.63과 유사하게 나타나는 것을 확인하였다. 이를 통해 스마트 팜의 제어데이터 없이 기본적인 환경데이터만을 활용하여도 정확성 있는 내부 온ㆍ습도 예측이 가능한 것을 확인하였다.",다국어 초록 정보 없음
액화산소와 산소용해기 설치에 따른 해상가두리 양식장의 고수온 피해저감 효과,2022,"['Korean rockfish', 'Damage reduction devices', 'Dissolved oxygen', 'Blood analysis']",국문 초록 정보 없음,"This study investigated the effect of damage reduction devices installed to reduce the mass mortality of farmed fish at high water temperatures period in marine net cage. The effect of rearing densities on fish mortalities in summer was also investigated. It was found that, dissolved oxygen and survival rate of Korean rockfish, Sebastes schlegeli in the experimental groups, i.e. the net cages equipped with liquefied oxygen and oxygen dissolver and with a lower rearing density were found to be higher than in the controls, i.e. net cages without the above devices and a higher density. According to the blood analysis results of the experimental fish, hemoglobin, hematocrit, glucose, glutamic oxalate transaminase (GOT) and glutamic pyruvate transaminase (GPT) in the fish of the control cage without damage reduction devices increased while the same blood parameters in the fish of the experimental group showed no significant difference from the initial values. Dissolved oxygen concentrations were found to be higher in the group with the combined use of both liquefied oxygen and oxygen dissolver than in the group with the single use of either device. It was concluded that the damage reduction devices and lower rearing densities were effective to raise dissolved oxygen concentrations in the high water temperature period."
"HBFT-아쿠아포닉 시스템 내 독립 및 종속 미생물의 적용에 따른 시베리아 철갑상어 (Acipenser baerii), 바질 (Ocimum basilicum) 및 엽채류 4종류의 성장과 수질변화",2022,"['Aquaponics', 'Basil', 'Lettuce cultivars', 'Microorganism', 'Siberian sturgeon']",국문 초록 정보 없음,"Aquaponics is a cultivation system that combines aquaculture and agricultural hydroponics. This study investigated water quality changes and growth of fish (siberian sturgeon) and leafy vegetables (basil and four leafy vegetables) through the use of nitrifying autotrophic bacteria alone (Auto+CO2)and the mixed use of heterotrophic and autotrophic microorganism (Hetero+HBFT) in operation of aquaponic system. After basil stocking (5-8 weeks) in EXP1, Auto+CO2 experimental groups showed rapid water quality stabilization for about 2 weeks compared to Hetero+HBFT groups, but the lattergroups showed significantly higher growth of both fish and basil (P<0.05).. In EXP2 (9-12 weeks), the growth factors of fish and 4 kinds of european lettuce cultivars in Hetero+HBFT experimental groups were higher than Auto+CO₂ groups. In hematological ananlysis [hematocrit (PCV, %), hemoglobin (g/dL), GOT (U/L), GPT (U/L), ALB (g/dL), GLU (mg/dL), plasma Na, K, Cl (mEq/L) and inorganic phosphorus (mg/dL)] between two experimental groups, all items were not significantly different at the end of the experimental period (P>0.05). This study shows that for aquaponics water quality management, it is more effective to use it in combination with heterotrophic bacteria rather than autotrophic bacteria alone to stabilize water quality."
염분 변화에 따른 명태(Gadus Chalcogrammus)의 스트레스 반응; 혈액생리학적 변화 및 유전자 발현,2022,"['Blood physiology', 'Gasus chalcogrammus', 'Salinity', 'Stress', 'Stress-related gene', 'Walleye pollock']",국문 초록 정보 없음,"The study evaluated the change of stress indicators in plasma and mRNA expression levels of stress-related genes in head-kidney and liver of walleye pollock, Gadus chalcogrammus according to acute salinity fluctuation. Walleye pollocks were exposed for 24 hours in five groups, as salinity are 34, 30, 25, 20, and 15 psu. The levels of hematocrit (Ht), glutamic oxaloacetate transaminase (GOT), cortisol, glucose, and superoxide dismutase (SOD) in plasma were increased as the salinity of the breeding water decreased. Ht increased with 37.5 ± 1% at 20 psu. GOT with 41.0 ± 5.03 U/L, cortisol with 882.73 ± 32.73 ng/mL, and glucose with 67.67±11.6 ng/dL increased a significant difference at 25 psu. The SOD activity levels in plasma were highest at 15 psu with 3.51 ± 0.13 U/mL. Plasma hemoglobin (Hb) and glutamic pyruvic transaminase (GPT) did not show a statistically significant difference according to salinity, but showed a tendency to increase as the salinity decreased. Total protein (TP) levels in plasma were the highest with 5.9 ± 0.61 g/dL at 25 psu. The relative mRNA expression of stress-related genes was investigated in the head-kidney and liver of walleye pollocks, and the mRNA expression of steroidogenic acute regulatory protein (StAR) and heat shock protein 70 (HSP70) increased, as the salinity decreased from 25 psu, whereas glucocorticoid receptor (GR) mRNA expression decreased as the salinity decreased. These results suggest that walleye pollocks become stressed-condition to salinity below 25 psu, but homeostasis can be maintained up to 15 psu in short-term."
HCl/ethanol로 유발한 급성 위염 마우스에서 PI3K/Akt/NF-κB 신호전달경로를 통한 진피 열수 추출물의 보호 효과,2022,"['Gastritis', 'anti-inflammatory', 'PI3K/Akt/NF-κB signaling', 'Citrus unshiu Peel', 'HCl/ethanol']","본 연구에서는 우선 진피 열수 추출물의 in vitro 항산화능을 평가하기 위해 total polyphenol, total flavonoid 함량, DPPH 및 ABTS radical 소거능을 분석한 후, 150 mM/60% ethanol로 유발한 급성 위염 동물 실험을 진행하여 급성 위염 완화 효과를 검증하였다. 약물투여군의 혈청 내 ROS와 MPO 수준, 조직 내 MDA 수준의 유의성 있는 감소를 확인하였으며, western blot을 통해 NOX2와 p22<SUP>phox</SUP>를 포함한 산화적 스트레스 관련 단백질을 억제하였고, PI3K/Akt/NF-κB 신호 전달 경로를 통한 염증성 단백질의 현저한 감소를 확인하였다. 따라서 이러한 결과는 진피열수 추출물이 급성 위염에 대한 완화 효과를 나타냈으며, 위염 및 천연 치료제의 후보 소재로서 가능성이 있다고 판단된다. 또한, 향후 만성 위염, 위암과 같은 위장 질환에 관한 추가 연구에서 진피 열수 추출물의 활용 가능성을 시사한다.","This study aimed to verify the effect of Citrus unshiu peel water extract (CUP) on a mouse model of acute gastritis (AG) induced by HCl/ethanol. Several studies have found that CUP has anti-inflammatory effects. The AG model was induced by oral administration of 150 mM HCl/60% ethanol (550 μL) to all groups except the control group. Also, for drug treatment, sucralfate (10 mg/kg) and CUP (100 or 200 mg/kg) were orally administered for 90 minutes before induction. The effect of CUP treatment was confirmed by gross gastric mucosal damage measurement, and the levels of Glutamic Oxaloacetic Transaminase (GOT), Glutamic Pyruvic Transaminase (GPT), and myeloperoxidase were reduced as well as the levels of oxidative stress biomarkers and their related proteins. In addition, the levels of inflammatory proteins, mediators, and cytokines were significantly downregulated byPI3K/Akt signaling. Taken together, these results show that CUP treatment alleviates AG by regulating PI3K/Akt signaling."
