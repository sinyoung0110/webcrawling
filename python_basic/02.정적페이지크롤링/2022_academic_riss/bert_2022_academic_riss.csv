title,date,keywords,abstract,multilingual_abstract
BERT 기반 2단계 분류 모델과 Co-Attention 메커니즘을 이용한 치매와 조현병 관련 질병 진단,2022,"['치매', '조현병', 'BERT', '페어 언어모델', 'Co-Attention 메커니즘', 'dementia', 'schizophrenia', 'BERT', 'pair-language model', 'co-attention mechanism']","최근 환자가 많이 증가함에 따라 사회적 문제를 야기하는 치매와 조현병 진단을 위한 모델을 제안한다. 의사와 내담자의 대화 음성 시료를 전사 작업한 스크립트를 이용해 치매와 조현병의 세부적인 분류를 시도하였다. 두 단계 과정으로 분류를 진행하는 2단계 분류 모델과 Co-Attention 메커니즘을 이용한 분류 모델을 제안하였다. 2단계 분류 모델은 정상군과 환자군 각각의 발화에서 계산되는 perplexity 차이에 기반한 분류와 미세 조정한 BERT 모델을 이용한 분류의 통합을 시도한 모델이다. Co-Attention 메커니즘을 이용한 분류 모델은 의사와 내담자 발화를 분리해 각 발화에 대해 표상을 구하고, 이를 바탕으로 표상 간의 어텐션 가중치 공유를 통해 분류하는 모델이다. BERT 모델을 미세 조정하여 분류를 시도한 Baseline 모델과의 F1 점수 비교를 통해 2단계 분류 모델은 7개 분류 태스크 중 4개의 태스크에서 성능 향상을 확인 하였고, Co-Attention 메커니즘 모델은 8개 분류 태스크 중 4개의 태스크에서 가장 높은 F1 점수를 보인 것을 확인하였다.","Noting the recently increasing number of patients, we present deep learning methods for automatically diagnosing dementia and schizophrenia by exploring the use of the novel two-stage classification and the co-attention mechanism. First, the two-stage classification consists of two steps - the perplexity-based classification and the standard BERT-based classification. 1) the perplexity-based classification first prepares two types of BERTs, i.e., control-specific and patients-specific BERTs, pretrained from transcripts for controls and patients as the additional pretraining datasets, respectively, and then performs a simple threshold-based classification based on the difference between perplexity values of two BERTs for an input test transcript; then, for ambiguous cases where the perplexity difference only does not provide sufficient evidence for the classification, the standard BERT-based classification is performed based on a fine-tuned BERT. Second, the co-attention mechanism enriches the BERT-based representations from a doctor’s transcript and a client’s one by applying the cross-attention over them using the shared affinity matrix, and performs the classification based on the enriched co-attentive representations. Experiment results on a large-scale dataset of Korean transcripts show that the proposed two-stage classification outperforms the baseline BERT model on 4 out of 7 subtasks and the use of the co-attention mechanism achieves the best F1 score for 4 out of 8 subtasks."
BERT 학습에서 GEMM 연산의 낮은 GPU 활용도 분석,2022,"['BERT 학습', '낮은 GPU 활용도', '메모리 계층 구조', 'DRAM 용량', 'BERT training', 'GPU under-utilization', 'memory hierarchy', 'DRAM capacity']","GPU는 효율적인 병렬화 연산을 바탕으로 딥 뉴럴 네트워크(Deep Neural Network) 학습에 주로 사용된다. 하지만, BERT 학습 간 나타나는 GEMM의 연산 특성으로 인해 GPU는 최대 성능을 제공하지 못한다. 본 논문에서 우리는 V100, A100 GPU를 이용하여 BERT 학습의 가장 중요한 연산인 GEMM을 수행했을 때 GPU가 연산기들을 효율적으로 활용하지 못하는 원인들을 분석하였다. 이를 통해 DRAM 용량의 제한과 BERT의 구조적인 특성으로 인해 GPU가 일을 균등하게 할당받지 못하는 문제를 확인하였다. 추가적으로, 일의 양을 작은 단위로 나누어 GPU의 병렬성을 높이는 방법과 메모리 계층의 대역폭의 트레이드-오프에 대해서 분석하였으며 병렬성을 높이더라도 메모리 대역폭 병목에 의해서 실제 GPU의 성능은 낮아지는 것을 확인하였다. 이러한 분석 결과들을 바탕으로 GPU의 DRAM 용량과 메모리 계층 구조에서 대역폭의 중요성을 확인한다.","Graphics processing units (GPUs) are mainly used for deep neural network training based on efficient parallel computation. However, due to the computational characteristics of GEMM when executing BERT training, GPUs do not provide maximum performance. In this paper, we analyze the reasons behind why GPUs cannot be utilized efficiently when GPUs perform the GEMM operation, which is the most important task in BERT training. We identify challenges that the GPU does not allocate tasks evenly to parallel computing units due to the limitation of DRAM capacity and the structural characteristics of BERT. In addition, we analyze the trade-off between increasing the parallelism of the GPU by dividing the number of tasks into smaller units and the memory bandwidth. We confirm that even if the parallelism increases, the performance of the actual GPU is reduced due to the memory bandwidth bottleneck. Based on our results, we explain the importance of the DRAM capacity and bandwidth of the memory hierarchy in the GPU."
CBCA 준거 분류에서의 BERT 기반 모델 성능 비교,2022,"['CBCA', 'criteria classification', 'BERT', 'RoBERTa', 'CBCA', '준거 분류', 'BERT', 'RoBERTa']","아동 성범죄의 경우 피해자의 진술은 사건의 유, 무죄를 판별함에 있어서 매우 중요하게 작용하기 때문에, 대검찰청에서는 피해자 진술 분석 기법인 Criteria-Based Content Analysis (CBCA)에 따라, 진술 내용을 총 19개의 준거로 분류하여 진술 전체의 신빙성을 판단한다. 그러나 이는 진술분석관의 주관적 의견에 따라 준거 분류가 상이할 수 있다. 따라서 본 논문에서는 BERT와 RoBERTa를 사용하여 객관적 분류 모델을 제시하기 위하여 크게 두가지 분류 방식을 적용하여 비교 분석하였다. 두 가지 방법은 전체 준거를 동시에 분류하는 방식과 4개의 그룹으로 나누어 1차 분류 후 해당 그룹 내에서 어떠한 준거인지 2차 분류하는 방식으로 구성하였다. 진술 문장을 CBCA의 중복 분류되는 준거를 제외한 16개 준거로 분류하고, 여러 사전 학습 모델을 사용한 비교 분석을 수행하였다. 분류 결과, 전자의 분류 방식이 총 16개의 준거 중 13개의 준거에서 후자의 분류 방식보다 성능이 높았으며, 학습 데이터의 수가 상대적으로 부족한 3개의 준거에서 후자의 방식이 효과적임을 확인하였다. 또한 RoBERTa 기반 모델이 16개의 준거 중 15개의 준거에서 BERT 기반 모델보다 성능이 높았으며, 나머지 1개의 준거에서는 한국어 대화형 구어체만으로 사전학습한 BERT 모델만이 유일하게 분류하였다. 이는 대화형 구어체 데이터로 사전 학습된 모델이 아동의 진술 문장을 분류함에 있어서 효과적임을 알 수 있다.","In the case of child sex crimes, the victim's statement plays a critical role in determining the existence or innocence of the case, so the Supreme Prosecutors' Office classifies the statement into a total of 19 criteria according to Criteria-Based Content Analysis (CBCA), a victim's statement analysis technique. However, this may differ in criteria classification according to the subjective opinion of the statement analyst. Thus, in this paper, two major classification methods were applied and analyzed to present an criteria classification model using BERT and RoBERTa. The two methods comprise of a method of classifying the entire criterion at the same time, as well as method of dividing it into four groups, and then classifying the criteria within the group secondarily. The experiment classified statements into 16 criteria of CBCA and performed comparative analysis using several pre-trained models. As a result of the classification, the former classification method performed better than the latter classification method in 13 of the total 16 criteria, and the latter method was effective in three criteria with a relatively insufficient number of training data. Additionally, the RoBERTa-based model performed better than the BERT-based model in 15 of the 16 criteria, and the BERT model, which was pre-trained using only Korean conversational colloquial language, classified the remaining one criterion uniquely. This paper shows that the proposed model, which was pre-trained using interactive colloquial data is effective in classifying children's statement sentences."
사용자의 구매만족도 향상을 위한 BERT 기반의 리뷰 분석 방법,2022,"['BERT', '딥러닝', '텍스트 분석', 'BERT', 'Deep Learning', 'Text Analysis']","본 논문은 BERT(Bidirectional Encoder Representations from Transformers) 기반의 선호도 분석 모델을 통한 쇼핑몰 상품 분석 플랫폼으로 사용자의 구매만족도를 향상하기 위한 리뷰 분석 시스템을 제안한다. 온라인 쇼핑몰의 발달로 다양한 제품들을 온라인으로 판매함에 따라 구매자는 자신의 선호도에 맞는 상품을 구매하기 점차 어려워지고, 원하는 상품을 고르는 것에 많은 피로도를 느끼게 되었다. 본 논문은 이러한 불편함을 해결하기 위해 사용자가 배송, 사이즈, 품질 3가지 기준에 대한 자신의 선호도를 입력하면 해당 선호도에 맞는 상품들을 점수로 정렬하여 보여주는 방법을 제안한다. 본 논문에서 제안하는 방법은 사전에 스케줄된 크롤링 시스템을 통해 수집된 상품들과 각 상품의 리뷰들을 데이터베이스에 저장한다. 상품 검색 시, 저장된 상품들 중 사용자가 원하는 카테고리에 속하는 각 상품의 리뷰 텍스트들을 BERT 기반의 선호도 분석 AI 모델을 통해 분석하여 사용자 개인별 선호도에 따라 점수화해서 표현해준다. 점수화된 선호도를 통해 사용자는 상품 구매에 도움을 받을 수 있다.","In this paper, we propose a review analysis system to improve user's purchase satisfaction with a shopping mall product analysis platform through a BERT (Bidirectional Encoder Representations from Transformers)-based preference analysis model. The growth of online shopping malls made it more challenging for customers to find products that matched their preferences, and they experienced significant tiredness when making their selections. To address this inconvenience, this paper proposes a method to display products that meet the preference by score when a user enters his or her preferences for three criteria: delivery, size, and quality. The BERT-based preference analysis AI model is used to analyze the review texts of each product in the user-desired category among the stored items, and the results are conveyed by scoring in accordance with the user's own preferences. Users can assist in making purchases by using their scored preferences."
HTML 태그 깊이 임베딩: 웹 문서 기계 독해 성능 개선을 위한 BERT 모델의 입력 임베딩 기법,2022,[],,"Recently the massive amount of data has been generated because of the number of edge devices increases. And especially, the number of raw unstructured HTML documents has been increased. Therefore, MRC(Machine Reading Comprehension) in which a natural language processing model finds the important information within an HTML document is becoming more important. In this paper, we propose HTDE(HTML Tag Depth Embedding Method), which allows the BERT to train the depth of the HTML document structure. HTDE makes a tag stack from the HTML document for each input token in the BERT and then extracts the depth information. After that, we add a HTML embedding layer that takes the depth of the token as input to the step of input embedding of BERT. Since tokenization using HTDE identifies the HTML document structures through the relationship of surrounding tokens, HTDE improves the accuracy of BERT for HTML documents. Finally, we demonstrated that the proposed idea showing the higher accuracy compared than the accuracy using the conventional embedding of BERT."
BERT-Fused Transformer 모델에 기반한한국어 형태소 분석 기법,2022,"['Natural Language Processing', 'Morphological Analysis', 'Transfer Learning', 'Transformer', 'BERT-fused Model', '자연어처리', '형태소분석', '전이학습', 'Transformer', 'BERT-fused 모델']",,"Morphemes are most primitive units in a language that lose their original meaning when segmented into smaller parts. In Korean,a sentence is a sequence of eojeols (words) separated by spaces. Each eojeol comprises one or more morphemes. Korean morphologicalanalysis (KMA) is to divide eojeols in a given Korean sentence into morpheme units. It also includes assigning appropriatepart-of-speech(POS) tags to the resulting morphemes. KMA is one of the most important tasks in Korean natural language processing(NLP). Improving the performance of KMA is closely related to increasing performance of Korean NLP tasks. Recent research on KMAhas begun to adopt the approach of machine translation (MT) models. MT is to convert a sequence (sentence) of units of one domaininto a sequence (sentence) of units of another domain. Neural machine translation (NMT) stands for the approaches of MT that exploitneural network models. From a perspective of MT, KMA is to transform an input sequence of units belonging to the eojeol domain intoa sequence of units in the morpheme domain. In this paper, we propose a deep learning model for KMA. The backbone of our modelis based on the BERT-fused model which was shown to achieve high performance on NMT. The BERT-fused model utilizes Transformer,a representative model employed by NMT, and BERT which is a language representation model that has enabled a significant advancein NLP. The experimental results show that our model achieves 98.24 F1-Score."
BERT 기반 리뷰의 감성정보를 활용한 제품정보 시각화 시스템의 구현,2022,"['Web crawling', 'Deep-learning', 'Emotional', 'Review', 'Visualization', 'BERT']",,"According to a recent survey by Embrain Trend Monitor, 86.9% of 1,200 men and women between the ages of 19-49 needed a product review. Among them, 78.6% of all respondents said that they always check consumer reviews when purchasing products. As such, in recent years, as the importance of product reviews increases, various attempts are being introduced to improve reviews. Representative domestic examples include Daily Hotel's True Review, CGV's Golden Egg Index, Play Store's popular function, and Mango Plate's Dining Code. The positive and negative intentions of consumers implied in these reviews can become factors that influence other consumers' purchasing decisions in the future. Therefore, in this paper, a system for visualizing emotional information for product reviews was proposed using the BERT model applied with deep learning techniques. The proposed system consists of three modules: a web crawling module that collects basic information about food products from the web, a module that creates an emotional information model of reviews collected by using the BERT model, and a module that visualizes product information based on the created emotional information. It is composed of subsystems. The system proposed in this paper obtained an excellent measurement value of 93.8 in accuracy measurement, which is one of the performance evaluation indicators of the BERT model. It is expected that this will be applied to various fields that utilize positive/negative evaluation of the food product review."
딥러닝 기반의 BERT 모델을 활용한 학술 문헌 자동분류,2022,['BERT'],,"In this study, we analyzed the performance of the BERT-based document classification model by automatically classifying documents in the field of library and information science based on the KoBERT. For this purpose, abstract data of 5,357 papers in 7 journals in the field of library and information science were analyzed and evaluated for any difference in the performance of automatic classification according to the size of the learned data. As performance evaluation scales, precision, recall, and F scale were used. As a result of the evaluation, subject areas with large amounts of data and high quality showed a high level of performance with an F scale of 90% or more. On the other hand, if the data quality was low, the similarity with other subject areas was high, and there were few features that were clearly distinguished thematically, a meaningful high-level performance evaluation could not be derived. This study is expected to be used as basic data to suggest the possibility of using a pre-trained learning model to automatically classify the academic documents."
인공지능 BERT 모델 기반 텍스트 임베딩과 클러스터링을  활용한 국내 경제교육 연구 동향 분석,2022,"['경제교육', '연구 동향', '논문 분석', '인공지능', 'BERT 기반 텍스트 임베딩', '클러스터링', 'economic education', 'research trends', 'analysis of research paper', 'AI', 'BERT-based text embedding', 'clustering']","목적 본 연구는 경제교육 분야에서 최근 10년간 수행된 연구 논문을 분석하여 경제교육의 연구 동향을 살펴보기 위한 목적으로 실시되었다.방법 경제교육 관련 249편의 논문을 수집하여 논문의 제목과 초록을 대상으로 텍스트 분석을 수행하여 유사한 주제의 논문 군집을형성하고, 이 결과를 바탕으로 경제교육 연구의 동향을 파악하였다. 텍스트 분석에서는 BERT 기반 텍스트 임베딩(Text Embedding) 과 클러스터링(Clustering)을 순차적으로 수행하였다.결과 경제교육 관련 연구들은 ‘학생 경제교육 성취 실증 분석’, ‘학교 경제교육 교수⋅학습의 실제’, ‘경제교육 내용 요소의 이해와적용’, ‘금융교육의 필요성과 금융이해력’, ‘경제교육 교육과정 및 교과서 구성’, ‘경제교육 담론과 향후 경제교육의 과제’의 6개 클러스터로 분류되었다. 이후 각 클러스터별로 나타난 주요한 경제교육 논의의 내용을 확인하였다.결론 분석 결과를 바탕으로 경제교육 분야 연구들의 주요 주제와 특징을 파악하고, 향후 경제교육 연구의 발전을 위한 시사점을 제시하였다.","Objectives The purpose of this study is to examine the trends in economic education by analyzing research papers conducted over the past 10 years.Methods After collecting 249 research papers related to economic education, text analysis was performed to form a group of papers on similar subjects, and based on this, research trends of economic education were identified.Results Studies related to school economic education were classified into six clusters, respectively, ‘empirical analysis of student economic education achievement’, ‘teaching-learning in school economic education’, ‘understanding and application of content elements of economic education’, and ‘necessity of financial education and financial literacy’, ‘economic education curriculum and textbook’, and ‘discourse and future tasks of economic education’. After that, the contents of the major economic education discourses that appeared for each cluster were discussed.Conclusions Based on the analysis results, implications for the development of economic education research were discussed."
BERT 기반의 사전 학습 언어 모형을 이용한 한국어 문서 추출 요약 베이스라인 설계,2022,"['deep learning', 'natural language processing', 'Korean document summarization', 'extractive summarization', 'automatic evaluation metric', '.']","디지털 문서가 기하급수적으로 증가한 현대 사회에서 문서 내 중요한 정보를 효율적으로 획득하는 것은 중요한 요구사항이 되었다. 그러나 방대한 디지털 문서의 양은 개별 문서의 중요 정보를 식별하고 축약하는 데 어려움을 야기하였다. 문서 요약은 자연어 처리의 한 분야로서 원본 문서의 핵심적인 정보를 유지하는 동시에 중요 문장을 추출 또는 생성하는 작업이다. 하지만 벤치마크로 사용하기에 적절한 한국어 문서 데이터의 부재와 베이스라인 없이 문서 요약 연구가 진행되어 발전이 미진한 상황이다. 본 논문에서는 데이터에 대한 검증과 접근성을 충족하고 글의 특성이 다른 두 개의 문서 집합을 선정하였다. BERT 기반의 다국어 및 한국어 사전 학습 언어 모형들을 선정하여 비교 및 실험하였다. 주요 결과로는 한국어 사전 학습 언어 모형이 ROUGE 점수에서 다국어 사전 학습 언어 모형을 능가하였으며, 이에 대한 원인을 추출된 요약 문장의 비율을 통해 분석하였다.","In modern society, where digital documents have increased exponentially, it is essential to efficiently obtain important information within documents. However, due to the vast amount of digital documents, it has become difficult for humans to abbreviate important information on individual documents. Document summarization is a Natural Language Processing field that extracts or generates meaningful sentences shorter than the original document while maintaining key information on the original document. However, since there is no appropriate Korean summarization data for benchmark, research has been conducted without a baseline, and development in this field is insufficient. In this paper, two document datasets that satisfy the accessibility and verification of summarization data and different text characteristics were selected. In addition, BERT-based multilingual and Korean pre-trained language models were selected, compared, and tested. For Korean documents, the Korean pre-trained language models outperformed the multilingual pre-trained language models in ROUGE scores. The cause was analyzed through the extraction ratio of selected summary sentences."
BERT를 활용한 미국 기업 공시에 대한 감성 분석 및 시각화,2022,"['Sentiment Analysis', 'Visualization', 'BERT', 'SEC', 'Disclosure', 'Form 10-K', 'Form 10-Q', 'COVID-19']",,"Purpose This study quantified companies’ views on the COVID-19 pandemic with sentiment analysis of U.S. public companies’ disclosures. It aims to provide timely insights to shareholders, investors, and consumers by analyzing and visualizing sentiment changes over time as well as similarities and differences by industry.Design/methodology/approach From more than fifty thousand Form 10-K and Form 10-Q published between 2020 and 2021, we extracted over one million texts related to the COVID-19 pandemic. Using the FinBERT language model fine-tuned in the finance domain, we conducted sentiment analysis of the texts, and we quantified and classified the data into positive, negative, and neutral. In addition, we illustrated the analysis results using various visualization techniques for easy understanding of information.Findings The analysis results indicated that U.S. public companies’ overall sentiment changed over time as the COVID-19 pandemic progressed. Positive sentiment gradually increased, and negative sentiment tended to decrease over time, but there was no trend in neutral sentiment. When comparing sentiment by industry, the pattern of changes in the amount of positive and negative sentiment and time-series changes were similar in all industries, but differences among industries were shown in neutral sentiment."
경제학 지식 분석을 위한 BERT 기반 관계추출 모델과 지식그래프,2022,"['관계 정보 추출', '키워드 개체 정보 추출', '지식 그래프', '경제학 논문', 'relation extraction', 'keyword (entity) information extraction', 'knowledge graph', 'economics paper']","경제학 지식은 조직이 좀 더 데이터 기반의 의사 결정하도록 지원하는 분석, 데이터 마이닝, 데이터 시각화, 데이터 도구, 모델링, 인프라를 모두 포함한다. 빠르게 변해가는 사회에서 좀 더 과학적 방법으로 해답을 제시해줄 수 있다.  본 논문에서는 경제학 논문 텍스트에서 키워드 개체 추출 시스템과 관계 정보를 학습하고 예측할 수 있는 관계 기반 BERT 모델을 제안한다. 관계 기반 BERT 모델을 실험을 통해 검증한 후에 지식 그래프 추출 및 지식 그래프 기반 의미론적 추론을 제시한다. 그래프의 연결성은 직접 연결된 관계 정보뿐 아니라 간접적으로 관련된 개체들을 통해 개념이 연결되어 문서 간의 잠재적인 숨겨진 관계를 드러낼 수 있다. 본 논문은 간접적인 관계 정보의 추출을 통해 새로운 의미 정보, 관계 정보를 추론하는 데 나아가는 연구의 방향성을 제시한다.","Economic knowledge includes all analytics, data mining, data visualization, data tools, modeling, and infrastructure that help organizations make more data-driven decisions. In a rapidly changing society, solutions can be presented in a more scientific way.  In this paper, we propose a relationship-based BERT model that can learn and predict keyword (entity) extraction systems and relationship information from the text of an economics paper. We present knowledge graph extraction and knowledge graph-based semantic inference after experimental validation of the relationship-based BERT model. The connectivity of graphs can reveal potential hidden relationships between documents by linking concepts through indirectly related objects as well as directly linked relationship information. This paper presents the direction of research moving forward to infer new semantic information and relational information through the extraction of indirect relational information."
KB-BERT: 금융 특화 한국어 사전학습 언어모델과 그 응용,2022,"['Natural language processing', 'Finance', 'Deep learning', 'BERT', 'PLM', '자연어 처리', '금융', '딥러닝', 'BERT', '사전학습언어모델']","대량의 말뭉치를 비지도 방식으로 학습하여 자연어 지식을 획득할 수 있는 사전학습 언어모델(Pre-trained Language Model)은 최근 자연어 처리 모델 개발에 있어 매우 일반적인 요소이다. 하지만, 여타 기계학습 방식의 성격과 동일하게 사전학습 언어모델 또한 학습 단계에 사용된 자연어 말뭉치의 특성으로부터 영향을 받으며, 이후 사전학습 언어모델이 실 제 활용되는 응용단계 태스크(Downstream task)가 적용되는 도메인에 따라 최종 모델 성능에서 큰 차이를 보인다. 이와 같은 이유로, 법률, 의료 등 다양한 분야에서 사전학습 언어모델을 최적화된 방식으로 활용하기 위해 각 도메인에 특화된 사전학습 언어모델을 학습시킬 수 있는 방법론에 관한 연구가 매우 중요한 방향으로 대두되고 있다. 본 연구에서는 금융 (Finance) 도메인에서 다양한 자연어 처리 기반 서비스 개발에 활용될 수 있는 금융 특화 사전학습 언어모델의 학습 과정 및그응용방식에대해논한다. 금융도메인지식을보유한언어모델의사전학습을위해경제뉴스, 금융상품설명서 등으로 구성된 금융 특화 말뭉치가 사용되었으며, 학습된 언어 모델의 금융 지식을 정량적으로 평가하기 위해 토픽 분류, 감성 분류, 질의 응답의 세 종류 자연어 처리 데이터셋에서의 모델 성능을 측정하였다. 금융 도메인 말뭉치를 기반으로 사전 학습된 KB-BERT는 KoELECTRA, KLUE-RoBERTa 등 State-of-the-art 한국어 사전학습 언어 모델과 비교하여 일 반적인 언어 지식을 요구하는 범용 벤치마크 데이터셋에서 견줄 만한 성능을 보였으며, 문제 해결에 있어 금융 관련 지식 을 요구하는 금융 특화 데이터셋에서는 비교대상 모델을 뛰어넘는 성능을 보였다.",
버트(BERT)를 활용한 인간번역의 자동평가: 여러 모델의 성능 비교 및 활용 가능성,2022,"['번역', '품질', '기계', '평가', '버트', 'translation', 'quality', 'machine', 'evaluation', 'BERT']",,"The most recent language model in NLP, known as BERT, has numerous advantages over other language models. It ‘understands’ human language in a subword unit and can recognize rare words and out-of-vocabulary words. Furthermore, it considers syntax and context during the text-understanding process. We applied the evaluation metric “BERTscore” using this BERT model to evaluate human translation (HT). 120 translated texts were evaluated with five evaluation metrics: BLEU, METEOR, emBLEU, emMETEOR, as well as BERTscore and the result was compared with professional translators’ evaluation. The comparison examines the validity and reliability of these metrics, particularly the BERTscore, for future application for HT evaluation. BERTscore demonstrated a stable performance, taking first place in scores, and third in ranks. The validity of metrics of word2vec models, especially that of emBLEU, was somewhat disappointing, probably owing to the domain difference between the training corpus and test corpus."
보수 언론과 진보 언론의 북한 전문가 활용 방식의 차이 탐색 : 인용문에 대한 KPF-BERT 기반 딥러닝 분석을 중심으로,2022,"['북한 보도', '전문가 인용', '정파적 지표화', 'KPF-BERT', '딥러닝', 'North Korean news', 'experts', 'partisan indexing', 'KPF-BERT', 'deep-learning']","본 연구는 언론이 정치성향에 따라 북한 전문가 인용 보도에서 편향성을 나타내는 실태를 정파적 지표화(partisan indexing) 현상과 딥러닝 모형의 적용을 통해 살펴보았다. 언론의 지표화 가설(press indexing hypothesis)에 따르면, 언론은 공적 정보원의 영향을 받아 보도 범위를 설정한다. 반면, 보수와 진보 언론이 자신의 정치적 성향에 맞춰 정보원과 인용 내용을 선택적으로 활용하는 현상은 정파적 지표화라고 할 수 있다. 본 연구진은 북한 보도에서 이런 현상을 탐색하고자 북한 관련 메시지를 보수와 진보 성향으로 분류하는 딥러닝 모형을 KPF-BERT를 기반으로 개발했다. 이어서 모형을 활용해 빅카인즈에서 제공하는 북한 전문가 80인의 인용문 42,375건(2011년~2021년)에 대해 정치성향 지수를 측정했다. 분석 결과, 북한 전문가들은 보수 성향에서 진보 성향에 이르기까지 다양하게 분포했으며, 극단적 정치성향에서는 진보 전문가보다 보수 전문가가 많은 것으로 나타났다. 언론사 분포에서는 보수 언론사 3곳(조선일보, 중앙일보, 동아일보)이 분석 대상 52곳 가운데 보수 성향 순서로 2, 4, 5위를 기록했다. 한편, 진보 언론사 한겨레신문과 경향신문은 중도에 가까운 위치를 나타냈고, 내일신문만 뚜렷하게 진보 성향을 보였다. 핵심 분석 결과는 보수와 진보 언론이 자신의 성향과 유사한 전문가를 더 자주 인용하고, 동일 전문가에 대해 자신의 성향과 유사한 내용을 더 많이 선택해 보도했다는 점이다. 북한 보도에서 ‘정파적 지표화(partisan indexing)’ 현상이 유의미하게 검증된 것이다.",
BERT 마스크 언어 모델을 활용한 내부자 위협행위 탐지,2022,"['Bert', 'CERT dataset', 'Insider threat detection', 'Masked language model', 'Pre-trained model', 'Threshold-based detection']",,"The risk of insider threat is increasing due to the increase in telecommuting because of pandemics. Companies have various security solutions to reduce insider threats, but there is a limit to capturing all human behavior. To solve this problem, studies on the detection of insider threat behavior using deep learning are being actively conducted. However, two problems that attack data is very scarce due to the nature of insider threat behavior, and time and action data must be learned at the same time remain challenges. In this paper, we propose an insider threat behavior detection model using the mask language model, which is a pre-learning method of Bi-directional Encoder Representation of Transformer(BERT). We used a CERT dataset for learning and used a custom tokenizer to map the data to actions and times at 30-minute intervals. The characteristics were learned by inputting only normal data into the mask language model, and the sum of the loss values derived from normal data was calculated as a threat score, and the maximum value was set as a detection boundary for each user. After that, if the threat score that occurs when new data is put into the model exceeds the detection boundary, it was detected as an abnormal behavior. The experimental results showed that compared to existing deep learning models, the model reacted sensitively when the behavior occurred at an unusual time, and in the case of scenarios where preparation and initiation of threat behavior were separated, appropriate detection performance was considered. As a result, the unbalanced dataset which was a challenge of detecting insider threat behavior could use the large amount of normal data as an advantage. And unlike the existing LSTM derivatives, sufficiently detailed time data can be mapped to behavior and used for learning."
사전 학습된 BERT 기반 모델을 이용한 구어체 텍스트 요약,2022,"['텍스트마이닝', 'BERT', '문서요약', '생성요약', '구어체데이터', 'Text Mining', 'BERT', 'Text Summarization', 'Abstractive Summarization', 'Dialogue Data']","본 연구는 일목요연하게 정리되지 않은 구어체(대화)에 대한 텍스트 자동 요약 모델을 적용시키는 방법을 제안한다. 본 연구에서는 구어체 데이터인 SAMSum 데이터를 활용하였고, 선행연구에서 문어체 데이터 텍스트 자동 요약 모델 연구에서 제안한 BERTSumExtAbs 모델을 적용하였다.SAMSum 데이터셋은 70% 이상은 두 사람 간 대화, 나머지 약 30%는 세 사람 이상 간 대화로 구성되어 있다. 본 논문에서는 텍스트 자동 요약 모델을 구어체 데이터에 적용하여, ROUGE Score R-1 부문에서 42.43 이상의 결과를 도출해내었다. 또한, 텍스트 요약 모델로 기존에 제안된 모델인 BERTSum 모델을 fine-tuning하여, 45.81의 높은 점수를 도출했다. 본 연구를 통하여 구어체 데이터에 대한 텍스트 생성 요약의 성능을 입증하였으며, 앞으로 사람의 자연어를 있는 그대로 컴퓨터가 이해하여 다양한 task를 해결하는 데 기초 자료로 활용되길 바란다.","In this paper, we propose how to implement text summaries for colloquial data that are not clearly organized. For this study, SAMSum data, which is colloquial data, was used, and the BERTSumExtAbs model proposed in the previous study of the automatic summary model was applied. More than 70% of the SAMSum dataset consists of conversations between two people, and the remaining 30% consists of conversations between three or more people. As a result, by applying the automatic text summarization model to colloquial data, a result of 42.43 or higher was derived in the ROUGE Score R-1. In addition, a high score of 45.81 was derived by fine-tuning the BERTSum model, which was previously proposed as a text summarization model. Through this study, the performance of colloquial generation summary has been proven, and it is hoped that the computer will understand human natural language as it is and be used as basic data to solve various tasks."
BERT Learns More than Word Frequency Information: A Case Study of Do-Be Constructions,2022,"['Do-Be construction', 'neural language model', 'web corpora', 'agreement attraction', 'synonym substitution']",,"This study aims to understand BERT’s linguistic ability using naturally occurring data. In particular, the study collected marginal language data, such as what we do is create Frankenstein, which is referred to as a Do-Be construction (DBC) (Flickinger & Wasow, 2013). Using web corpora, the study first collected 17,737 instances of the DBC across text genres and English dialects. The corpus analysis supports the idea that DBC is a computationally challenging phenomenon for data-driven language systems due to its statistical sparsity and linguistic complexity. With manual annotations of DBCs, the study designed two computational prediction tasks: subject―verb agreement and synonym substitution tasks, based on the introspective judgment of linguists. The study found that BERT is hugely sensitive to linguistic acceptability of grammatical forms and felicitous words in the prediction tasks, even though the target phenomenon is rarely observed in corpus data. These results show that the neural language model, BERT, can learn abstract linguistic properties beyond surface frequency information."
Multicriteria Movie Recommendation Model Combining Aspect-based Sentiment Classification Using BERT,2022,"['Recommender Systems', 'Collaborative Filtering', 'BERT', 'Multicriteria Recommendation', 'Aspect-Based Sentiment Analysis', '추천시스템', '협업필터링', '다기준 추천', '속성기반 감성분석']","본 논문에서는 영화 추천 시 평점뿐 아니라 사용자 리뷰도 함께 사용하는 영화 추천 모형을 제안한다. 제안 모형은 고객의 선호도를 다기준 관점에서 이해하기 위해, 사용자 리뷰에 속성기반 감성분석을 적용하도록 설계되었다. 이를 위해, 제안 모형은 고객이 남긴 리뷰를 다기준 속성별로 나누어 암시적 속성을 파악하고, BERT를 통해 이를 감성 분석함으로써 각 사용자가 중요시 생각하는 속성을 선별적으로 협업필터링에 결합하여 추천 결과를 생성한다. 본 연구에서는 유용성을 검증하기 위해 제안모형을 실제 영화 추천 사례에 적용해 보았다. 실험결과 전통적인 협업필터링보다 제안 모형의 추천 정확도가 향상되는 것을 확인할 수 있었다. 본 연구는 개인의 특성을 고려하여 모형을 선별하여 사용하는 새로운 접근법을 제시하였고, 속성 각각에 대한 평가 없이 리뷰로부터 여러 속성을 파악할 수 있는 방법을 제시했다는 측면에서 학술적, 실무적 의의가 있다.","In this paper, we propose a movie recommendation model that uses the users’ ratings as well as their reviews. To understand the user’s preference from multicriteria perspectives, the proposed model is designed to apply attribute-based sentiment analysis to the reviews. For doing this, it divides the reviews left by customers into multicriteria components according to its implicit attributes, and applies BERT-based sentiment analysis to each of them. After that, our model selectively combines the attributes that each user considers important to CF to generate recommendation results. To validate usefulness of the proposed model, we applied it to the real-world movie recommendation case. Experimental results showed that the accuracy of the proposed model was improved compared to the traditional CF. This study has academic and practical significance since it presents a new approach to select and use models in consideration of individual characteristics, and to derive various attributes from a review instead of evaluating each of them."
Towards Improving Causality Mining using BERT with Multi-level Feature Networks,2022,"['Causality Mining', 'Relation Network', 'Multi-level Relation Network', 'Relation Classification', 'Cause-effect Relation Classification']",,"Causality mining in NLP is a significant area of interest, which benefits in many daily life applications, including decision making, business risk management, question answering, future event prediction, scenario generation, and information retrieval. Mining those causalities was a challenging and open problem for the prior non-statistical and statistical techniques using web sources that required hand-crafted linguistics patterns for feature engineering, which were subject to domain knowledge and required much human effort. Those studies overlooked implicit, ambiguous, and heterogeneous causality and focused on explicit causality mining. In contrast to statistical and non-statistical approaches, we present Bidirectional Encoder Representations from Transformers (BERT) integrated with Multi-level Feature Networks (MFN) for causality recognition, called BERT+MFN for causality recognition in noisy and informal web datasets without human-designed features. In our model, MFN consists of a three-column knowledge-oriented network (TC-KN), bi-LSTM, and Relation Network (RN) that mine causality information at the segment level. BERT captures semantic features at the word level. We perform experiments on Alternative Lexicalization (AltLexes) datasets. The experimental outcomes show that our model outperforms baseline causality and text mining techniques."
Creating Knowledge Graph of Electric Power Equipment Faults Based on BERT–BiLSTM–CRF Model,2022,"['Knowledge graph', 'Electric power equipment', 'Fault diagnosis', 'Entity recognition', 'Relation extraction']",,"Creating a large-scale knowledge graph of electric power equipment faults will facilitate the development of automatic fault diagnosis and intelligent question answering (QA) in the electric power industry. However, most existing methods have lower accuracy in Chinese entity recognition, thus it is hard to build such a high-quality knowledge graph by extracting knowledge from Chinese technical literature. To solve the problem, a novel model called BERT–BiLSTM–CRF is proposed. It blends Bi-directional Encoder Representation from Transformers (BERT), Bi-directional Long Short-Term Memory (BiLSTM), and Conditional Random Field (CRF). The model fi rstly identifi es and extracts electric power equipment entities from preprocessed Chinese technical literature. Then, the semantic relations between the entities are extracted based on the relation classifi cation method based on dependency parsing. Finally, the extracted knowledge is stored in the Neo4j database in the form of the triplet and visualized in the form of a graph. Through the above steps, a Chinese knowledge graph of electric power equipment faults can be built. The novelty of the model just lies in its subtle blend: the BERT module can not only learn phrase-level information representation, but also learn rich semantic information features; the CRF module realizes the constraint on the label prediction value and reduces the irregular recognition rate, so the accuracy rate of entity recognition is improved. Taking the Chinese technological literature, which is about fault diagnosis of electric power equipment as the experimental object, the experimental results show that the model identifi es and extracts Chinese entities more accurately than traditional methods. Thus, a comprehensive and accurate Chinese knowledge graph of electric power equipment faults could be constructed more easily."
BERT로 측정한 경영자 과신 이례현상과 장기 투자성과,2022,"['CEO Overconfidence', 'Anomaly', 'Machine-Learning', 'BERT', 'Textual Analysis', '경영자 과잉확신', '과신', '이례현상', '행태재무', '텍스트분석']",본 연구는 CEO의 과신 성향이 높은 포트폴리오의 투자성과가 과신 성향이 낮은 포트폴리오보다높을 것이라는 경영자 과신(over-confidence) 이례현상을 제시한다. 이를 위하여 본 연구에서는 CEO의과신 성향을 토대로 포트폴리오를 구성하여 수익률의 차이를 검증하였다. 경영자 과신은 구글 BERT의자연어 처리 기술을 이용하여 기업의 사업보고서의 “이사의 경영진단 및 분석의견”을 분석하였다.그 결과 경영자 과신 포트폴리오를 매수하고 비과신 포트폴리오를 매도하는 투자전략을 통하여 0.26%의비정상수익률을 얻을 수 있다는 것을 확인하였다.최근 강조되는 ESG 경영 환경하에서 기업지배구조(G)는 경영자의 과신 성향을 제어하는 역할을할 것이라 예측할 수 있다. 이러한 예측 아래 실증분석한 결과 기업지배구조 점수가 높은 기업일수록경영자 과신의 문제가 약화되어 과신 이례현상도 줄어드는 것으로 나타났다. 반대로 경영자 과신투자 전략이 기업지배구조 점수가 낮은 경우에 더욱 유효하다는 결과를 제시한다. 추가적으로 경제상황이전반적으로 하락하는 시기에 시장에서 경영자 과신을 판단하는 오류가 커져 경영자 과신 이례현상이유의하게 나타나는 것을 발견하였다.본 연구는 경영자 과신을 머신러닝 방법론을 통해 측정하였다는데 높은 공헌을 가진다. 이러한방법론을 통해 구성한 포트폴리오를 통해 양의 비정상 수익률을 얻을 수 있음을 보고하였다. 이를통해 주식시장 투자자들과 다양한 연기금에서 실질적으로 활용할 수 있는 투자전략을 제시하였다.,
BERT를 활용한 AR-KNU+ 감성사전 기반의 대학 평판도 평가,2022,"['university reputation evaluation', 'sentiment vocabulary classification', 'BERT']",,"Various studies have been conducted to evaluate university reputation by applying surveys, Delphi, and data mining techniques. However, there are limitations to the accuracy and reliability of the reputation evaluation because university rankings can change depending on the evaluation items, evaluation methods, and the period and scale of the data used. Therefore, in this paper, to solve this limitation, we propose a method for evaluating university reputation based on sentiment vocabulary included in university news articles by using the BERT model and the AK-KNU+ sentiment dictionary. Through the proposed method, it is possible to accurately classify the sentiment words expressing the university reputation. It is shown that the university reputation evaluation can be performed more objectively based on the classified sentiment vocabulary."
BERT와 FastText를 활용한 온라인 진로상담 문서 분류,2022,"['온라인 진로상담', '텍스트 자료 분석', 'BERT', 'FastText', 'Online career counseling', 'text data analysis']",,
BERT를 이용한 딥러닝 기반 소스코드 취약점 탐지 방법 연구,2022,"['Deep Learning', 'Vulnerability Detection', 'Source Code', 'BERT', 'Program Slicing']",,
A BERT-Based Automatic Scoring Model of Korean Language Learners' Essay,2022,"['Automatic Writing Scoring', 'Bidirectional Encoder Representations from Transformers', 'Korean as a Foreign Language', 'Natural Language Processing']",,"This research applies a pre-trained bidirectional encoder representations from transformers (BERT) handwritingrecognition model to predict foreign Korean-language learners’ writing scores. A corpus of 586 answersto midterm and final exams written by foreign learners at the Intermediate 1 level was acquired and used forpre-training, resulting in consistent performance, even with small datasets. The test data were pre-processedand fine-tuned, and the results were calculated in the form of a score prediction. The difference between theprediction and actual score was then calculated. An accuracy of 95.8% was demonstrated, indicating that theprediction results were strong overall; hence, the tool is suitable for the automatic scoring of Korean writtentest answers, including grammatical errors, written by foreigners. These results are particularly meaningful inthat the data included written language text produced by foreign learners, not native speakers."
KoBERTSEG: 한국어 BERT를 이용한 Local Context 기반 주제 분리 방법론,2022,"['Natural Language Processing', 'Topic Segmentation', 'Text Segmentation', 'KoBERT', 'BERTSUM']",,"Topic segmentation refers to the work of separating a document consisting of several topics into unit documents, such as paragraphs, with one single topic. Topic segmentation has been considered as one of main preprocessing step prior to performing natural language processing tasks, such as document summary or document classification. This paper proposes a Korean BERT-based news article segmentation method aiming at separating a single news article, in which multiple subjects exist, into news segments, each of which contains a single subject. The proposed model has the advantage of being able to capture a wider range of semantic relationships compared to existing topic segmentation studies by borrowing a structure proposed for document summarization. Experimental results on a Korean news article dataset show that the proposed method outperform the benchmark models for topic segmentation. In addition, we also show that the proposed method can be used for practical news clip summarization task, supporting the possibility of implementing the application service based on Korean topic segmentation model."
What Can BERT Do for English Linguistics?: A Case Study of Examining It-cleft Constructions,2022,"['deep-learning', 'language model', 'it-cleft', 'pronominal case', 'platform']",,"This study used the machine learning technique known as Bidirectional Encoder Representations from Transformers (BERT) to understand the pronominal distribution in it-cleft constructions. While language models (LMs) have successfully been used to examine and verify linguistic representations, most studies have focused on the general phenomenon of subject-verb agreement. As such, this study focused on the particular construction known as it-cleft, in order to consolidate the LM’s role as a linguistic platform. In the experiment using BERT, the surprisal of each case (i.e., nominative and accusative) is compared. The investigation successfully replicated the findings of previous studies, specifically by capturing the use of the accusative form, inhibition of the first-person pronominal, and collocate feature of the first-person pronominal with the complementizer who.The results from the LM also deliver further contribution to the study of it-cleft, involving the between number and the relationship between the pronominal and syntactic role. Thus, the usage of BERT both revealed new research questions while verifying evidence from previous studies on the it-cleft pronominal. This demonstrates that BERT can correctly represent particular constructions and broadens the possibilities of LMs as a linguistic platform."
Layer-wise Semantic Role Labeling with the KR-BERT Language Model,2022,"['semantic role labeling', 'Korean neural language model', 'performance assessment', 'layer-wise analysis', 'heatmap analysis']",,"The purpose of this study is to assess the performance of semantic role labeling (SRL) predicted by the neural language models (NLMs, or Transformer-based pre-trained models) of Korean. First, the study built two models: the KR-BERT-BiLSTM-CRF model and the KR-BERT-Verb Position Feature (VPF)-BiLSTM-CRF model. The results from testing these two models show that the KR-BERT-VPF-BiLSTM-CRF model (67.3%) outperformed the KR-BERT-BiLSTM-CRF model (66.4%). In addition, this study examined which hidden layer improved the performance of NLMs during training. As expected, the NLM that was trained on the last hidden layer performed better than other alternative options such as the second-to-last-hidden layer and the concatenated last four layers. Thus, this study renders support to the general observation that an NLM should be trained on the last hidden layer to reach the highest performance. This study is meaningful since it is the first attempt to investigate which hidden layer is useful to train NLMs in SRL tasks of Korean."
Hot Keyword Extraction of Sci-tech Periodicals Based on the Improved BERT Model,2022,"['Bidirectional encoder', 'hot keyword', 'representations from transformers (bert)', 'sci-tech periodicals', 'similarity measurement']",,"With the development of the economy and the improvement of living standards, the hot issues in the subject area have become the main research direction, and the mining of the hot issues in the subject currently has problems such as a large amount of data and a complex algorithm structure. Therefore, in response to this problem, this study proposes a method for extracting hot keywords in scientific journals based on the improved BERT model.It can also provide reference for researchers,and the research method improves the overall similarity measure of the ensemble,introducing compound keyword word density, combining word segmentation, word sense set distance, and density clustering to construct an improved BERT framework, establish a composite keyword heat analysis model based on I-BERT framework.Taking the 14420 articles published in 21 kinds of social science management periodicals collected by CNKI(China National Knowledge Infrastructure) in 2017-2019 as the experimental data, the superiority of the proposed method is verified by the data of word spacing, class spacing, extraction accuracy and recall of hot keywords. In the experimental process of this research, it can be found that the method proposed in this paper has a higher accuracy than other methods in extracting hot keywords, which can ensure the timeliness and accuracy of scientific journals in capturing hot topics in the discipline, and finally pass Use information technology to master popular key words."
(AL)BERT Down the Garden Path: Psycholinguistic Experiments for Pre-trained Language Models,2022,"['targeted evaluation approach', 'transformers', 'garden-path structure', 'natural language processing', 'psycholinguistics']",,"This study compared the syntactic capabilities of several neural language models (LMs) including Transformers (BERT / ALBERT) and LSTM and investigated whether they exhibit human-like syntactic representations through a targeted evaluation approach, a method to evaluate the syntactic processing ability of LMs using sentences designed for psycholinguistic experiments. By employing garden-path structures with several linguistic manipulations, whether LMs detect temporary ungrammaticality and use a linguistic cue such as plausibility, transitivity, and morphology is assessed. The results showed that both Transformers and LSTM exploited several linguistic cues for incremental syntactic processing, comparable to human syntactic processing. They differed, however, in terms of whether and how they use each linguistic cue. Overall, Transformers had a more human-like syntactic representation than LSTM, given their higher sensitivity to plausibility and ability to retain information from previous words. Meanwhile, the number of parameters does not seem to undermine the performance of LMs, contrary to what was predicted in previous studies. Through these findings, this research sought to contribute to a greater understanding of the syntactic processing of neural language models as well as human language processing."
Legal search method using S-BERT,2022,[],,"In this paper, we propose a legal document search method that uses the Sentence-BERT model. The general public who wants to use the legal search service has difficulty searching for relevant precedents due to a lack of understanding of legal terms and structures. In addition, the existing keyword and text mining-based legal search methods have their limits in yielding quality search results for two reasons: they lack information on the context of the judgment, and they fail to discern homonyms and polysemies. As a result, the accuracy of the legal document search results is often unsatisfactory or skeptical. To this end, This paper aims to improve the efficacy of the general public's legal search in the Supreme Court precedent and Legal Aid Counseling case database. The Sentence-BERT model embeds contextual information on precedents and counseling data, which better preserves the integrity of relevant meaning in phrases or sentences. Our initial research has shown that the Sentence-BERT search method yields higher accuracy than the Doc2Vec or TF-IDF search methods."
인공지능 모델의 추론을 이용한 법적 논증,2022,"['artificial intelligence', 'natural language processing', 'BERT', 'legal argumentation', 'training data', '인공지능', '자연어처리모델', 'BERT', '법적논증', '학습데이터']","인공지능이란 컴퓨터가 학습할 수 있음을 전제로 사용되는 개념이다. 인공지능의 학습 대상은 데이터이고 학습의 결과는 일정한 규칙(rule)이다.특정(specific)한 데이터를 학습해서 컴퓨터가 일반적(general)인 규칙을찾아낸다. 학습을 마친 인공지능은, 즉 인공지능 모델은 일반적 규칙을 적용하여 개별 사건에 대해서 추론(inference)한다. 법적판단에 해당하는 인공지능 모델의 추론이 법적논증으로 성립될 수 있다.또는 인공지능 모델이 법적논증의 근거로서 기능할 수도 있다, 즉 법규범과 경험적 명제에서 연역적 귀결의 근거로서 일정한 규칙이나 절차 또는 명제가 찾아지면 법적논증은 성립한다. 머신러닝 알고리즘은 문언을 읽고 법리에 따라 해석하여 연역적으로 사고하는 것이 아니라, 피처를 선별한 데이터, 즉 가공데이터를 정수(integer)로 연산해서 추론의 규칙을 도출한다. 따라서 연역적 사고에 따른 논리적 타당성은 측정될 수 없고, 추론규칙의 정확도가 계산된다.딥러닝의 BERT 모델은 함수식을 도출하고 그에 따라서 추론을 한다.따라서 주어진 법리가 논리적으로 타당하게 적용되는가의 여부가 아니라추론 규칙의 정확도, 즉 함수식의 정확도가 계산된다. 머신러닝 피처 엔지니어링과는 달리, BERT 모델은 언어의 이해를 전제로 하기에 수식의 정확도를 둘러싸고 다양한 쟁점이 제기된다. BERT 모델의 정확도가 신뢰되지 않는다면 법리적용의 타당성 요건이 충족되지 않는다.또한 법리의 이론적 정당성에 관해서는, 약관 라벨링 모델은 300개의 데이터셋을 학습하여 약관규제법에 따라 무효 라벨링과 유효 라벨링에 각각해당하는 함수식을 도출한다. 300개의 데이터셋 각각은 약관규제법 조항에 따라 무효 또는 유효로 판정되는 조문을 담고 있다. 달리 말하면 데이터셋 각각은 약관규제법에 따라서 유효 또는 무효로 판정되는 사실관계에해당한다. 그래서 그러한 사실관계와 라벨링간의 관계를 BERT를 이용한 라벨링 모델이 학습해서 도출하는 함수식은 약관규제법 조항의 해석 규칙내지는 적용 원리를 담고 있다. 그래서 약관 라벨링 모델의 추론이 법적논증이기 위해서는 그 함수식이 법리의 정당성 요건을 충족하야야 한다.정당성 요건중의 하나는 보편성이다. 사람이 통제할 수 있는 과정이 매우 제한적이라는 점에서 BERT 모델은 사람에 의한 편향성 문제를 겪지않을 것이고, 그래서 보편성을 충족할 수도 있다. 그러나 편향성 문제가 없는 점과는 대비되는 쟁점으로서, 사람의 통제가 없다면 법리를 적용하는 것이 정당한가에 관한 가치체계가 모델의 연산에 주입될 수 없다는 쟁점은제기된다. 따라서 약관 모델링은 법적논증으로서 성립된다고 보기 어렵다.","Artificial intelligence as a concept is based upon the presupposition that computer is able to learn. The object of the learning is the data, and the result of the learning is a sort of rule. AI learns and finds training data and general rules. AI may perform the inference regarding the specific case according to the general rule. Such inference as a sort of legal decision may be equal to a valid legal argumentation.AI model may also operate as a logical basis for the legal argumentation. Legal argumentation is valid as legal norms and empirical propositions is providing the deductive basis for a certain rulem or propositions. The output of machine learning feature engineering model is not the result of deductive reasoning over the text but the inference based upon the rules resulting from integer calculation of the feature. Therefore, validity of the logical reasoning is not to be witnessed. The precision will be measured.BERT model has learned the 300 datasets with labelled data and made the inference function corresponding each labelling such as valid article or invalid article of end user agreement. 300 datasets are made of 300 article of end user agreement. In other words, each datasets equal to the specific relevant factual statetement. Therefore, the BERT classification model returns the inference function including the rule and rationale of interpretation of article of the agreement.One element of the justification of the rule and rationale consists in an universality. As objective calculation is free from the human bias, BERT model may fulfill such condition. However, a calculation may not entail value propositions such that BERT model inference is not a valid legal argumentation."
KoBERT를 이용한 기업관련 신문기사 감성 분류 연구7),2022,"['NLP', 'BERT', 'KoBERT', 'sentiment  analysis', 'corporate  news', '자연어처리', 'BERT', 'KoBERT', '감성분석', '기업뉴스']","이 연구에서는 Google에서 개발한 BERT에 기반한 KoBERT 모형을 사용하여 한국 신문기사의 감성분석 정확도를 테스트하였다. 비교를 위해, Google에서 다국어용으로 제시한 MBERT, Google에서 API를 통해 제공하는 Google Sentiment Analysis, 그리고 사전적 접근법 을 통한 감성분석 결과를 사용하였다. 감성분석 학습 결과, KoBERT를 사용한 경우가 85.7%의 정확도를 보여, 여타 모형의 정확도에 비해 상당히 높은 수준의 정확도를 달성하는 것을 확인하였다. 다른 모형의 경우, MBERT가 77.3%의 정확도로 KoBERT에 비해 상당히 낮은 결과를 보였으며 Google Sentiment Analysis와 사전적 접근법은 더욱 낮은 정확도를 보였다. 감성분석 결과가 실질적으로 의미있는 유용한 정보를 제공하는지 확인하기 위하여 뉴스가 나온 날짜를 기준으로 3일 후, 그리고 5일 후까지 주가수익률을 종속변수로 하여 회귀분석한 결과, KoBERT를 사용한 결과가 다른 결과에 비해 미래 수익률을 더욱 잘 예측하는 것을 발견하였다. 이와 같은 결과는 추후 회계⋅재무 분야의 텍스트에 대한 감성분석을 이용한 다양한 연구를 수행하는 데 참고가 될 것으로 기대한다.","This study explores the accuracy level of the sentiment analysis of news article sentences from Korean newspaper, using KoBERT which is a modified version of BERT developed by Google. For comparison, we use MBERT which is the multilingual version of BERT, Google Sentiment Analysis provided through Google API, and dictionary based approach. This paper finds that the accuracy level of the sentiment classification based on  KoBERT  is  the  highest  at 85.7%,  achieving  a  significantly higher  level  of  accuracy  compared  to  the  other  three  models.  MBERT  shows  the next  highest  accuracy  level  at  77.5%  and  the  other  two  models  provide  even  lower  accuracy  rate.  We  further  investigate  whether  the  sentiment classification results obtained from these four models could predict future stock return. Using cumulative future stock returns for 3 or 5 days after the news on corporation publishes, we find that the sentiment score based on the sentiment classification from the KoBERT model predicts future return better than the other three models. Overall, these findings would serve as a reference for conducting further studies related to sentiment analysis on  accounting  and  financial  text."
한국 대중 서사 기반 감정 데이터 구축과 활용 - 감정 딥러닝 모델 구현을 통한 문학 연구의 활용 가능성 탐색을 중심으로,2022,"['emotion', 'data', 'Natural Language Processing', 'BERT', 'LSTM', 'Korean novels', 'Korean dramas', 'digital humanities', 'artificial intelligence humanities', '감정', '데이터(data)', '자연어처리', 'BERT', 'LSTM', '한국소설', '한국드라마', '디지털인문학', '인공지능인문학']","본 연구는 한국 대중 서사 기반의 감정 데이터를 활용하여 한국 문학의 감정을 연구할 가능성을 모색해보고자 하였다. 이에 중앙대학교 인문콘텐츠연구소에서 구축한 문학과 콘텐츠 감정 데이터를 소개하고, 이를 활용하여 ‘문학’, ‘콘텐츠’, ‘문학+콘텐츠’의 데이터별 LSTM 및 BERT 2분류(긍정·부정) 모델 총 6개, LSTM 3분류(분노·슬픔·즐거움) 모델 1개를 구축하였다.LSTM 및 BERT 2분류 모델은 80~87%의 감정 예측 정확도를 보여주었고, 긍정보다는 부정을 더욱 정확하게 예측하였다. 학습 방법별로 LSTM보다 BERT가 예측 정확도가 높았다. 감정 데이터별로는 ‘문학+콘텐츠’ 모델이 예측 정확도가 높았다. 이러한 결과는 정보공학의 측면에서 학습한 데이터 총량의 문제와 연관되는 것으로 파악되지만, 인문학적으로 볼 때 토대 데이터의 성격 즉 소설과 드라마의 성격 차이에 따른 감정 양상의 차이에서 비롯되는 것으로 파악된다. 이후 <구운몽>을 대상으로 한 ‘감정 딥러닝 모델의 감정 판단 데이터’와 ‘주석자 감정 판단 데이터’의 비교 검증을 진행하였는데, 감정 수치가 높은 긍정, 부정 범주의 판단 사례가 대체로 일치하는 결과를 보여주었다. 즉 감정 딥러닝 모델이 문학 연구에 활용될 가능성을 보여준 것이다.",
Protected Health Information Recognition by Fine-Tuning a Pre-training Transformer Model,2022,"['Artificial Intelligence', 'Big Data', 'Medical Informatics', 'Data Anonymization', 'Deep Learning']",,"Objectives: De-identifying protected health information (PHI) in medical documents is important, and a prerequisite to deidentificationis the identification of PHI entity names in clinical documents. This study aimed to compare the performanceof three pre-training models that have recently attracted significant attention and to determine which model is more suitablefor PHI recognition. Methods: We compared the PHI recognition performance of deep learning models using the i2b2 2014dataset. We used the three pre-training models—namely, bidirectional encoder representations from transformers (BERT),robustly optimized BERT pre-training approach (RoBERTa), and XLNet (model built based on Transformer-XL)—to detectPHI. After the dataset was tokenized, it was processed using an inside-outside-beginning tagging scheme and WordPiecetokenizedto place it into these models. Further, the PHI recognition performance was investigated using BERT, RoBERTa,and XLNet. Results: Comparing the PHI recognition performance of the three models, it was confirmed that XLNet had asuperior F1-score of 96.29%. In addition, when checking PHI entity performance evaluation, RoBERTa and XLNet showeda 30% improvement in performance compared to BERT. Conclusions: Among the pre-training models used in this study,XLNet exhibited superior performance because word embedding was well constructed using the two-stream self-attentionmethod. In addition, compared to BERT, RoBERTa and XLNet showed superior performance, indicating that they were moreeffective in grasping the context."
Affinity Prober를 이용한 언어 모델의 문장 수용성 판단 결정의 경계 요인 분석,2022,"['Transformer', 'BERT', 'Self Attention', 'Minimal pair', 'Linguistic acceptability']",,"Recently, many studies endeavor to reveal the intrinsic nature of BERT, since transformer-based language models have achieved the state-of-the-art in many natural language understanding tasks. However, it is still hard to probe BERT’s universal linguistic properties since different probing methods lead to fluctuating results between tasks and models. Thus, this paper suggests Affinity Prober, which is a flexible task- and model-agnostic probing method to investigate transformer-based language model’s decision boundaries when processing linguistic phenomena. Affinity Prober is designed to investigate potential linguistic knowledge from the Self Attention Mechanism. Using Affinity Prober, the study examines whether a bert-base-cased model has any explainable decision boundaries on sentence acceptability in terms of a lexical category in English. The results in syntactic phenomena show that the Affinity Relationship between function words is reinforced in the upper layers, while the Affinity Relationship between content words plays a key role in semantic phenomena. The study concludes that Affinity Prober is helpful to analyze the model’s decision boundary with lexical categories on specific linguistic phenomena."
"LUKE를 이용한 한국어 자연어 처리: 개체명 인식, 개체 연결",2022,"['knowledge-enhanced language model', 'LUKE', 'named entity recognition', 'entity linking', 'BERT', 'RoBERTa', 'entity-aware self-attention', '지식 증강 언어 모델', 'LUKE', '개체명 인식', '개체 연결', 'BERT', 'RoBERTa', 'entity-aware 셀프어텐션']","BERT와 같은 트랜스포머 기반의 언어 모델은 대용량의 레이블이 없는 말뭉치를 자가 학습방법을 통해 학습한 후 다양한 자연어 처리 응용 태스크에 적용하여 놀라운 성능 향상을 보였다. 이와 같은 언어 모델은 실세계 지식 정보를 표현할 수 없는 단점이 존재하고 이러한 문제를 해결하기 위해 언어 모델에 지식 베이스를 반영하려는 다양한 연구들이 수행되었다. 본 연구에서는 단어 시퀀스 이외에 엔티티 시퀀스와 임베딩을 정의하고 단어와 엔티티의 모든 시퀀스 쌍에 따라 별도의 쿼리 파라미터를 두고 셀프 어텐션을 수행하는 LUKE 모델을 한국어 위키피디아 상에서 학습한 후 엔티티 관련 태스크인 개체명 인식, 개체 연결에 적용하여 기존의 RoBERTa 기반 모델 대비 각각 0.5%p, 1.05%p의 성능 향상을 가져왔다.","Transformer-based language models (LM) such as BERT trained from a large amount of unlabeled corpus using self-supervised learning methods have shown remarkable performance improvement on various natural language processing (NLP) application tasks. Despite the marked improvements, the classical pretrained language model has not directly incorporate external real-world knowledge bases such as a Wikipedia knowledge graph or triples. To inject the real-world knowledge bases to a pretrained language model, many studies towards “knowledge enhanced” pretrained language models have been conducted. Among them, LUKE attaches a sequence of entities to a sequence of original input tokens and performs entity-aware self-attention using entity embeddings, leading to noticeable improved results on entity-related tasks and the state-of-the-art performance in SQuAD dataset. In this paper, we present a Korean version of LUKE pretrained from a large amount of Korean Wikipedia corpus and show its application results on entity-related tasks of Korean. In particular, we newly propose a way of applying LUKE to the entity linking task which has not been explored in the previous works of using LUKE. Experiment results on both Korean named entity recognition and entity linking tasks show improvements over the RoBERTa-based models."
뉴스 제목 머신러닝을 통한 코로나 팬데믹 기간 동안의 제약회사 주가 예측,2022,"['Machine learning', 'Stock price prediction', 'Stock market liquidity', 'COVID-19', 'Stock investment strategy', 'Natural language processing', '머신러닝', '주가예측', '주가 변동성', 'COVID-19', '주식투자전략', '자연어처리']","코로나 팬데믹 동안 진단키트, 백신, 치료제 등의 이슈로 바이오, 제약회사들의 주가는 극심한 유동성을가지게 되었다. 투자자들은 기업들의 코로나 백신이나 치료제에 대한 개발성과에 관련된 주요 정보 를 대부분뉴스를 통해서 취득할 수 있었다. 본 연구에서는 정보원천으로서의 뉴스를 BERT(bidirectional encoder representations from transforms)로 학습한 뒤 S제약에 대한 주가를 예측하고자 한다. 이를 통해 코로나 팬데믹과같은 외부적인 요인들에 의해 극심한 주가 변동성이 발생하였을 때, 투자자들의 투자 방법론을 모색하고자한다.본 연구의 연구방법은 첫째, 코로나 팬데믹 기간 동안 S제약 주가 변동시기를 유동성에 따라 나누었다.둘째, 모델의 학습을 위한 트레이닝 데이터 셋이 될 S제약과 관련된 뉴스를 시기별로 수집하였다. 셋째, 수집한 뉴스에서 헤드라인을 추출하여 그 뉴스가 S제약의 주가 상승이나 주가 하락과 관련된 뉴스인지를태깅하였으며 넷째, 주가의 예측을 위해 수집된 뉴스 헤드라인을 BERT에 단일문장 분류(single sentence classification)로 학습하도록 하였다. 다섯째, 지난 2년 동안의 S제약과 관련된 모든 뉴스를 수집하고 이 뉴스의제목을 전이학습시켜 주가상승과 하락을 예측하였다.전체적인 예측정확도는 0.78, 주가하락과 주가상승을 예측했을 경우 f1-score는 각각 0.80, 0.78의 값으로나타났다. 주가 상승일 발행 뉴스에 대한 상승 예측 정확도는 72.0%, 주가 하락일 발행 뉴스에 대한 주가하락 예측 정확도는 65.0%이었다.","Research Purpose: Developing an investment methodology with BERT for individual investors when extreme stock price liquidity occurs due to external factors like the COVID-19 pandemic.Research Methods: The learning direction of BERT was determined by single sentence classification and transfer learning was performed with news headlines related to a pharmaceutical company during the COVID-19 pandemic.Results in Research: The overall accuracy of the stock price prediction was 0.78. Specifically, accuracy of stock price rise prediction for news published on the day of stock price rise was 72.0%, and the accuracy of stock price fall prediction for news published on a fall day was 65.0%.Research Conclusion: This model, which learned the news, predicted the direction of stock prices well. If several clues, such as drug development, relationships with other companies, and comments from management or business owners, are reflected in the news, it will be easier to predict how the company's stock price will change.Suggestion in Research: Individual investors have limited access to corporate information. In particular, when extreme external factors occur such as COVID-19, they are forced to make investment decisions through the news related to companies interested. This study presents a stock price prediction methodology using a machine learning with the news headlines."
DG-based SPO tuple recognition using self-attention M-Bi-LSTM,2022,"['dependency grammar', 'information extraction', 'long short-term memory', 'SPO tuple']",,"This study proposes a dependency grammar-based self-attention multilayered bidirectional long short-term memory (DG-M-Bi-LSTM) model for subject–predicate–object (SPO) tuple recognition from natural language (NL) sentences. To add recent knowledge to the knowledge base autonomously, it is essential to extract knowledge from numerous NL data. Therefore, this study proposes a high-accuracy SPO tuple recognition model that requires a small amount of learning data to extract knowledge from NL sentences. The accuracy of SPO tuple recognition using DG-M-Bi-LSTM is compared with that using NL-based self-attention multilayered bidirectional LSTM, DG-based bidirectional encoder representations from transformers (BERT), and NL-based BERT to evaluate its effectiveness. The DG-M-Bi-LSTM model achieves the best results in terms of recognition accuracy for extracting SPO tuples from NL sentences even if it has fewer deep neural network (DNN) parameters than BERT. In particular, its accuracy is better than that of BERT when the learning data are limited. Additionally, its pretrained DNN parameters can be applied to other domains because it learns the structural relations in NL sentences."
나라장터 입찰공고 인공지능 검색모델 개발에 관한 연구,2022,"['BERT', 'CEDR', 'BM25', '입찰정보', '정보검색', '약지도학습', 'BERT', 'CEDR', 'BM25', 'Tender notice', 'Information retrieval', 'Weak-supervision']","나라장터(g2b.go.kr)의 검색 기능은 매우 부정확하여 많은 사용자들이 불편을 느끼고 있다. 입찰에 참여하려는 조달 기업은 입찰공고의 검색에 많은 시간을 소비하여 업무시간의 낭비를 초래하고있다. 본 논문에서는 나라장터 입찰공고의 불편하고 부정확한 검색기능을 대체할 신경망 기반의 정보검색 모델을 도출하는 방안을 연구하였다. 자연어 검색에 획기적인 발전을 가져온 BERT를 기반으로검색결과를 리랭킹함으로써 사용자의 만족도를 향상시키고자 하였다. 신경망 기반 IR모델 학습에 필요한 데이터를 전처리하기 위하여 기존의 연구를 토대로 문서를 구절로 분할하고, 분할된 구절에서 키워드를 추출하여 의사-질의를 생성하였다. 생성된 의사-질의를 입력으로 엘라스틱 서치의 BM25모델을 사용하여 의사-레이블인 질의-문서쌍(q-d pair)을 생성하였다. 학습용 데이터셋의 양과 유형, 배치크기, 러닝 레이트, positive 라벨링 비율 등의 하이퍼파라미터 값 변화에 따른 다양한 실험을 통하여입찰공고 데이터에 적합한 IR모델을 개발하였다. 개발된 IR모델로 실제 입찰공고 조회시 현행 나라장터보다는 훨씬 정확한 검색결과를 보여주었다.",
Weibo Disaster Rumor Recognition Method Based on Adversarial Training and Stacked Structure,2022,"['Weibo disaster', 'Rumor recognition', 'Adversarial training', 'BERT', 'Stacked LSTM']",,"To solve the problems existing in the process of Weibo disaster rumor recognition, such as lack of corpus, poor text standardization, difficult to learn semantic information, and simple semantic features of disaster rumor text, this paper takes Sina Weibo as the data source, constructs a dataset for Weibo disaster rumor recognition, and proposes a deep learning model BERT_AT_Stacked LSTM for Weibo disaster rumor recognition. First, add adversarial disturbance to the embedding vector of each word to generate adversarial samples to enhance the features of rumor text, and carry out adversarial training to solve the problem that the text features of disaster rumors are relatively single. Second, the BERT part obtains the word-level semantic information of each Weibo text and generates a hidden vector containing sentence-level feature information. Finally, the hidden complex semantic information of poorly-regulated Weibo texts is learned using a Stacked Long Short-Term Memory (Stacked LSTM) structure. The experimental results show that, compared with other comparative models, the model in this paper has more advantages in recognizing disaster rumors on Weibo, with an F1_Socre of 97.48%, and has been tested on an open general domain dataset, with an F1_Score of 94.59%, indicating that the model has better generalization."
토픽모델링을 활용한 국내 메타버스 연구동향 분석: 2007~2022년 학술연구를 중심으로,2022,"['Metaverse', 'Topic Modeling', 'Bidirectional Encoder Representations from Transformers(BERT)', 'Research Trend', 'Big Data', '메타버스', '토픽모델링', 'Bidirectional Encoder Representations from Transformers 모델', '연구동향', '빅데이터']","본 연구는 토픽모델링을 활용하여 메타버스 연구동향을 분석함으로써 향후 성공적인 메타버스 환경의 구현과 연구 방향을 모색하고자 하였다. 메타버스 키워드의 KCI 학술논문 총 582편에 대해 정량적 빈도분석과 BERT모델을 활용한 토픽모델링을 수행하였다. 그 결과, 2021년 이후 연구가 급증한 점과 사회과학, 복합학 분야의 비중이 크다는 점을 확인하였다. 연구토픽의 주요 키워드는 ‘교육’, ‘학습’, ‘온라인’, ‘현실’, ‘가상’, ‘의도’, ‘사회적’, ‘음악’, ‘패션’, ‘플랫폼’, ‘빅데이터’ 등의 순으로 나타났다. 토픽 주제어는 ‘교육적 활용’과 ‘가상현실과 세계’로 약 40%로 큰 비중을 차지하였으며, ‘사용의도’, ‘엔터테인먼트’, ‘패션산업’, ‘플랫폼디자인’, ‘빅데이터분석’ 등이 그 뒤를 따랐다. 세 개 시사점은 첫째, 메타버스 기반기술이나 비즈니스 모델 개발이 더욱 활발히 이루어져야 하고 둘째, 새로운 법리와 규율 연구가 선행되어야 하며 셋째, 메타버스 환경의 새로운 문제를 예측, 규명하기 위한 연구가 더욱 중요하다는 것이다. 연구결과가 디지털전환 시대에 국내 연구의 초석이 되고 디지털사회 구현에 이바지할 것을 기대한다.","This study analyzed research trends using topic modeling and intended to explore research direction of a successful metaverse environment. Quantitative frequency analysis and topic modeling using the BERT were performed on a 582 articles. As a result, it was confirmed that research has increased rapidly after 2021 and that the proportion of social science is large. The topic’s subject were 'educational use' and 'virtual reality world', accounting for about 40%. Three implications were that the development of the underlying technology or business model should be more active, research on new discipline should be preceded and, research to predict new problems is more important. It is expected that the results will become a cornerstone of research and contribute to the realization of a digital society."
딥러닝을 이용한 한국어 Head-Tail 토큰화 기법과 품사 태깅,2022,"['Morphological Analysis', 'Tokenizer', 'POS Tagging', 'Head-Tail', 'Deep Learning', 'BERT']",,"Korean is an agglutinative language, and one or more morphemes are combined to form a single word.Part-of-speech tagging method separates each morpheme from a word and attaches a part-of-speech tag. In this study, we propose a new Korean part-of-speech tagging method based on the Head-Tail tokenization technique that divides a word into a lexical morpheme part and a grammatical morpheme part without decomposing compound words.In this method, the Head-Tail is divided by the syllable boundary without restoring irregular deformation or abbreviated syllables. Korean part-of-speech tagger was implemented using the Head-Tail tokenization and deep learning technique. In order to solve the problem that a large number of complex tags are generated due to the segmented tags and the tagging accuracy is low, we reduced the number of tags to a complex tag composed of large classification tags, and as a result, we improved the tagging accuracy. The performance of the Head-Tail part-of-speech tagger was experimented by using BERT, syllable bigram, and subword bigram embedding, and both syllable bigram and subword bigram embedding showed improvement in performance compared to general BERT.Part-of-speech tagging was performed by integrating the Head-Tail tokenization model and the simplified part-of-speech tagging model, achieving 98.99% word unit accuracy and 99.08% token unit accuracy. As a result of the experiment, it was found that the performance of part-of-speech tagging improved when the maximum token length was limited to twice the number of words."
반려동물 질병 질의응답 시스템을 위한 개체명 인식,2022,"['개체명 인식', '동물 질병 개체명', '인간 질병 개체명', '자연어처리', '기계학습', 'Named Entity Recognition', 'Animal Disease Name', 'Human Disease Name', 'Natural Language Processing', 'Machine Learning']",,"While the number of companion animals is rapidly increasing due to growing interest in pets, it is still not easy to find disease-related information when a companion animal is sick. In this paper, we propose a pet disease Q&A system that answers disease names related to the companion animal symptoms input by users. In our system, we create a BERT-DIS-NER model that adds a CRF layer to BERT for the disease named entity recognition and use syllable unit-based named entity recognition that can reflect the characteristics of disease names. In order to solve the problem of lack of animal disease data, a base model was trained using human disease data expected to have a similar context to pet disease, and data on the animal disease were used for fine-tuning. In experiments the F1-score of BERT-DIS-NER showed 0.74 trained with only human data, 0.77 trained with only animals, and 0.81 trained with human data and fine-tuned with animal data, so it was confirmed that the proposed method improves performance."
Complex NP Island Constraints in English: Experimental vs. Deep Learning Approach,2022,"['complex NP island constraints', 'appositives', 'relatives', 'experimental approach', 'deep learning approach']",,"This paper examined two types of complex NP island constraints (Appositives and Relatives) in English, with an experimental approach and a deep learning approach. In the experimental approach, this paper followed the design in Author (2018). A total of 120 sentences were employed in the experiment: 40 sentences for the target and 80 sentences for the fillers. For the deep learning approach, this paper utilized the BERT-LARGE model. The dataset was composed of 240 sentences: 40 sentences for the target and 200 sentences for the fillers. These 240 sentences were used as an input dataset to the BERT-LARGE model, and the acceptability scores were calculated for each sentence. After the acceptability scores were obtained for all the target sentences in two different types of approaches, they were normalized into the z-scores and statistical analyses were applied to them. Through the analysis, the followings were observed: (i) both the experimental approach and the BERT-LARGE model correctly identified two complex NP island constraints in English, (ii) two factors (Island and Location) and their interaction (Island:Location) affected the acceptability scores of island sentences, and (iii) two approaches made different predictions on the DD scores of the two complex NP island constraints."
경량화 데이터와 딥러닝 모델을 적용한 효율적인 네트워크 트래픽 분류 방법,2022,"['Encrypted Network Traffic Classification', 'BERT', 'Knowledge Distillation', 'Efficiency']","트래픽 분류는 컴퓨터 네트워크 영역에서 서비스관리 및 보안 등의 분야에서 그 역할이 점점 더 중요해지고 있다. 초기에는 포트넘버, DPI, 통계정보 등을 활용해 트래픽 분류가 가능했다. 그러나 정보보호 측면에서 트래픽의 페이로드가 암호화되면서 분류가 제한되었지만 머신러닝기법이 추가 활용되면서 문제점이 해결됐다. 이후 딥러닝 모델들이 활용되고 성능은 향상되었으나, 많은 변수를 입력으로 넣어도 트래픽 분류가 가능해 짐에 따라 모델과 데이터가 점점 무거워져서 자원과 시간이 많이 소모되었다. 부담스럽고 성가시게 된 모델과 데이터 경량화 활용에 본 연구의 목적을 두고, KD(Knowledge distillation) 기법을 바탕으로 BERT가 경량화된 DistilBERT를 선정했고, 경량화한 데이터를 적용했다. 첫 번째 패킷 1개 중 앞 100bytes 크기의 입력데이터(패킷 단위)와 이러한 5개의 패킷이 연결된 입력데이터(플로우 단위)로 정확도 / F1-score가 각각 0.9707 / 0.9731과 0.9703 / 0.9706로 매우 우수한 성능을 보였다.",
텍스트 유사도를 활용한 경로 매칭 배송 추천 시스템,2022,"['경로추천', '경로 매칭', '텍스트유사도', '단거리 매칭', '지하철배송', '플랫폼 서비스', 'route recommendation', 'route matching', 'text similarity', 'short-distance delivery', 'subway delivery', 'platform service']","본 연구에서는 급증하는 배송 서비스 수요에 맞춰 더 신속하고 최저 비용으로 근거리 배송을가능하게 하는 알고리즘을 제안하고자 한다. 본 연구에서 제안하는 알고리즘에서는 배송원으로지하철 승객을 물류 이동에 참여시킨다. 이때 승객은 이동 경로와 일치하는 배송 물류를 선택할수 있다. 그리고 서비스 이용자의 입장에서는 현재 근처에 경로가 일치하는 배송원을 선택할 수있다. 이때 배송원 추천은 TF-IDF&N-gram과 BERT를 결합한 텍스트 유사도 측정 방식으로 진행된다. 따라서 기존 택배 시스템과 달리 소비자-배송원 간의 man-to-man 방식으로 양방향 선택을지원한다. 탑승 중인 승객을 물류 이동에 참여시킨다는 점에서 비용 최소화와 배송 기간 단축을모두 보장할 수 있다. 더하여 운송 측면에서도 특별한 기술을 요하지 않으므로, 일자리 입지가 축소된 노동자들에게 경제 참여 기회를 제공할 수 있다는 점에서도 의의가 있다.","In this paper, we propose an algorithm that enables near-field delivery at a faster and lowest cost to meet the growing demand for delivery services. The algorithm proposed in this study involves subway passengers (shipper) in logistics movement as delivery sources. At this time, the passenger may select a delivery logistics matching subway route. And from the perspective of the service user, it is possible to select a delivery man whose route matches . At this time, the delivery source recommendation is carried out in a text similarity measurement method that combines TF-IDF&N-gram and BERT. Therefore, unlike the existing delivery system, two-way selection is supported in a man-to-man method between consumers and delivery man. Both cost minimization and delivery period reduction can be guaranteed in that passengers on board are involved in logistics movement. In addition, since special skills are not required in terms of transportation, it is also meaningful in that it can provide opportunities for economic participation to workers whose job positions have been reduced."
InferSent를 활용한 오픈 도메인 기계독해,2022,"['오픈 도메인 기계독해', '문서 검색', '문서 독해', 'ALBERT', 'InferSent', 'Open Domain Machine Reading Comprehension', 'Document Search', 'Document Reader', 'ALBERT', 'InferSent']","오픈 도메인 기계독해는 질문과 연관된 단락이 존재하지 않아 단락을 검색하는 검색 기능을 추가한 모델이다. 문서 검색은 단어 빈도 기반인 TF-IDF로 많은 연구가 진행됐으나 문서의 양이 많아지면 낮은 성능을 보이는 문제가 있다. 아울러 단락 선별은 단어 기반 임베딩으로 많은 연구가 진행됐으나 문장의 특징을 가지는 단락의 문맥을 정확히 추출하지 못하는 문제가 있다. 그리고 문서 독해는 BERT로 많은 연구가 진행됐으나 방대한 파라미터로 느린 학습 문제를 보였다. 본 논문에서는 언급한 3가지 문제를 해결하기 위해 문서의 길이까지 고려한 BM25를 이용하며 문장 문맥을 얻기 위해 InferSent를 사용하고, 파라미터 수를 줄이기 위해 ALBERT를 이용한 오픈 도메인 기계독해를 제안한다. SQuAD1.1 데이터셋으로 실험을 진행했다. 문서 검색은 BM25의 성능이 TF-IDF보다 3.2% 높았다. 단락 선별은 InferSent가 Transformer보다 0.9% 높았다. 마지막으로 문서 독해에서 단락의 수가 증가하면 ALBERT가 EM에서 0.4%, F1에서 0.2% 더 높았다.","An open domain machine reading comprehension is a model that adds a function to search paragraphs as there are no paragraphs related to a given question. Document searches have an issue of lower performance with a lot of documents despite abundant research with word frequency based TF-IDF. Paragraph selections also have an issue of not extracting paragraph contexts, including sentence characteristics accurately despite a lot of research with word-based embedding. Document reading comprehension has an issue of slow learning due to the growing number of parameters despite a lot of research on BERT. Trying to solve these three issues, this study used BM25 which considered even sentence length and InferSent to get sentence contexts, and proposed an open domain machine reading comprehension with ALBERT to reduce the number of parameters. An experiment was conducted with SQuAD1.1 datasets. BM25 recorded a higher performance of document research than TF-IDF by 3.2%. InferSent showed a higher performance in paragraph selection than Transformer by 0.9%. Finally, as the number of paragraphs increased in document comprehension, ALBERT was 0.4% higher in EM and 0.2% higher in F1."
"‘멀리서 읽기’를 통한『한비자(韓非子)』「초현진(初見秦)」, 「존한(存韓)」진위 논쟁 재검토-주성분분석을 중심으로-",2022,"['Distant Reading', 'The Han Feizi', 'Authorship Attribution', 'The Zhan Guo Ce', 'CTEXT', 'PCA', '멀리서 읽기', '한비자', '진위 논쟁', '전국책', 'CTEXT', '주성분분석']","본 연구의 배경은 한문 고전 텍스트를 대상으로 ‘멀리서 읽기’ 가능성을 탐색하고자 하는 데 있다. 멀리서 읽기 개념에 대한 재검토를 바탕으로, 본 연구는『한비자(韓非子)』「초현진(初見秦)」, 「존한(存韓)」 편의 진위 논쟁을 대상으로 한 멀리서 읽기 사례 연구를 제시한다. 자료 수집 및 분석을 위해서는 CTEXT 및 유기문고(維基文庫) 등 온라인 데이터베이스와 주성분분석·BERT 등 군집화·분류 알고리즘을 활용한다. 분석 결과, 『한비자』「초현진」, 「존한」 상호 간의 높은 유사도 및 두 편과 『전국책』 간의 높은 유사도를 확인하였다. 이는 위 두 편의 위작 가능성과 『전국책』이 출처일 가능성을 제시한다. 본 연구는 디지털화된 한문 고전 텍스트를 대상으로 한 멀리서 읽기의 유효성을 보여준다.","This study is aimed at reexamining the authorship attribution of “Chujianqin” and “Cunhan” in the Han Feizi. Drawing on recent discussions regarding distant reading on literary history, this study presents a case study example of distant reading to build and test a hypothesis on Chinese classics. In the present study, both an unsupervised clustering and a supervised classification algorithm are used to explore patterns among pre-Qin and Han texts and to verify a hypothesis on the authorship attribution of the Han Feizi. More specifically, this study first applies Principal Component Analysis and then applies a BERT-based classification model to pre-Qin and Han texts with much focus on the Han Feizi.  Results from this study reveal the existence of a remarkable similarity between the first two chapters of the Han Feizi and the Zhan Guo Ce (Annals of the Warring States). It not only provides a supplement to existing doubts over the authenticity of these two chapters but further suggests strong evidence of their Zhan Guo Ce origins."
비정형 데이터를 이용한 ICO(Initial Coin Offering) 정량적 평가 방법에 대한 연구,2022,"['블록체인', '암호화폐', '텍스트마이닝', '암호화폐백서', '사기탐지', 'ICO', 'Cryptocurrency', 'Text Mining', 'Black Chain', 'Fraud detection']","기업공개(IPO)는 투자자 보호를 위한 법적 테두리가 마련되어 있으며, 다양한 정량적 평가 요소가 존재하기 때문에 객관적인 분석이 가능하며 다양한 연구가 수행되어 왔다. 또한, 크라우드펀딩 역시 투자자 보호를 위한 법적 제도와 무분별한 펀딩을 방지하기 위한 여러 장치가 마련되어 있다. 반면에 최근 각광받는 블록체인 기반의 암호화폐 백서(ICO)는 투자자를 보호할 법적 수단과 기준이 모호하며 ICO를 객관적으로 평가하기 위한 정량적 평가 방법이 미흡한 상황이다. 따라서 본 연구는 ICO의 사기 여부를 탐지하기 위해 온라인상 공개된 ICO 백서를 수집하고 텍스트 임베딩 기법인 BERT에 기반한 ICO 사기 예측을 수행하였고 기존의 Random Forest 머신러닝 기법과 비교하여 정량적 방법으로 사기 탐지가 가능함을 보였다. 최종적으로 본 연구는 비정형 데이터에 기반하여 ICO의 사기 여부를 판단할 수 있는 정량적 접근 방법론의 활용 가능성을 제시함으로써 정량적 방법에 기초한 ICO 사기 탐지 연구에 기여할 수 있을 것으로 기대된다","Initial public offering (IPO) has a legal framework for investor protection, and because there are various quantitative evaluation factors, objective analysis is possible, and various studies have been conducted. In addition, crowdfunding also has several devices to prevent indiscriminate funding as the legal system for investor protection. On the other hand, the blockchain-based cryptocurrency white paper (ICO), which has recently been in the spotlight, has ambiguous legal means and standards to protect investors and lacks quantitative evaluation methods to evaluate ICOs objectively. Therefore, this study collects online-published ICO white papers to detect fraud in ICOs, performs ICO fraud predictions based on BERT, a text embedding technique, and compares them with existing Random Forest machine learning techniques, and shows the possibility on fraud detection. Finally, this study is expected to contribute to the study of ICO fraud detection based on quantitative methods by presenting the possibility of using a quantitative approach using unstructured data to identify frauds in ICOs."
머신러닝을 활용한 영재 학생들의 졸업논문 주제 경향 분석,2022,"['과학 영재', '연구 주제', '졸업 논문', '자연어 처리', '군집 분석', '머신러닝', 'science gifted student', 'research topic', 'graduation thesis', 'natural language processing', 'clustering analysis', 'machine learning']","본 연구는 영재학교에 재학 중인 학생들의 졸업 연구 논문을 분석하여 연구주제의 경향을 파악하고자 하였다. 이에 S 영재학교에서 최근 5년간 제출한 292건의 졸업 연구 논문의 제목 및 초록을 수집하여 BERT를 기반으로 한 문서 요약 및 키워드 추출 기능을 활용하였고 그 결과를 K-Means 군집 분석을 통해 범주화하여 그 특징을 분석하였다. 연구 결과, 학생들의 연구 주제는 문제 해결을 위한 시스템 개발, 알고리즘 및 모형⋅이론의 제안, 구성 요소 또는 성분의 분석, 선행연구 기반 문제 해결, 센서 및 도구를 활용한 문제 해결, 데이터 기반 분석 및 활용, 문제 해결을 위한 새로운 방법 제안 등 7개의 범주로 나타났다. 본 연구에서 적용한 자연어 처리를 활용한 머신러닝과 딥러닝의 방법은 과학영재 학생들의 과제연구 지도 및 평가 등에 효과적으로 활용될 수 있을 것으로 기대하며 관련된 시사점을 제시하였다.",
의료 산업에 있어 현대인의 비대면 의학 상담에 대한 관심도 분석 기법,2022,[],,"This study aims to analyze the interest of modern people in non-face-to-face medical counseling in the medical industrys. Big data was collected on two social platforms, 지식인, a platform that allows experts to receive medical counseling, and YouTube. In addition to the top five keywords of telephone counseling, ""internal medicine"", ""general medicine"", ""department of neurology"", ""department of mental health"", and ""pediatrics"", a data set was built from each platform with a total of eight search terms: ""specialist"", ""medical counseling"", and ""health information"". Afterwards, pre-processing processes such as morpheme classification, disease extraction, and normalization were performed based on the crawled data. Data was visualized with word clouds, broken line graphs, quarterly graphs, and bar graphs by disease frequency based on word frequency. An emotional classification model was constructed only for YouTube data, and the performance of GRU and BERT-based models was compared."
키워드와 문장 임베딩을 활용한 조항별 분류모델 기반 계약서 적격성 검증,2022,"['법률 문서', '계약서', '계약서 적격성', 'BERT', '키워드 임베딩', 'legal text', 'contract', 'contract eligibility', 'BERT', 'keyword embeddings']","최근에는 계약서를 포함한 법률 문서들을 대량으로, 빠르고 정확하게 처리하기 위하여 인공지능을 활용한 자동화된 분석 방법이 요구된다. 계약서는 그 안에 필수적인 조항들이 모두 포함되었는지, 어느 한 쪽에 불리한 조항은 없는지 등을 확인하여 적격성을 검증할 수 있다. 이때 계약서를 이루는 조항들은 계약서의 종류와 관계없이 매우 정형적이고 반복적인 경우가 많다. 본 연구에서는 이러한 성격을 이용하여 계약서 내 조항별 분류 모델을 구축하였으며, 계약서의 관습적인 요구사항에 기반하여 구성한 키워드 임베딩을 구축하고 이를 BERT 임베딩과 결합하여 사용한다. 이때 BERT 모델은 한국어 사전학습모델을 법률 도메인 문서를 이용하여 미세 조정한 것이다. 각 조항의 분류 결과는 정확도 90.57과 90.64, F1 점수 93.27과 93.26으로 우수한 수준이며, 이렇게 계약서를 이루는 각 조항이 어떤 필수조항에 해당되는지의 예측 결과를 통해 계약서의 적격성을 검증할 수 있다.","Contracts need to be reviewed to be verified if they include all the essential clauses for them to be valid. Such clauses are highly formal and repetitive regardless of the kinds of contracts, and automated legal technologies are required for legal text comprehension. In this paper, we have constructed a simple item-by-item classification model for clauses in contracts to estimate contract eligibility by addressing formal and repetitive properties of contract clauses. We have used keyword embeddings based on conventional requirements of contracts and concatenate them to sentence embeddings of clauses, extracted from a BERT model fine-tuned with legal documents. The contract eligibility can be verified by the predicted labels. Based on our methods, we report reasonable performances with the accuracy of 90.57 and 90.64, and an F1-score of 93.27 and 93.26, using additional keyword embeddings with BERT embeddings."
다중 인코더 구조를 활용한 기계번역 품질 예측,2022,"['quality estimation', 'english-german translation', 'cross-lingual', 'pre-trained model', 'BERT', 'dual-encode', '기계번역 품질 예측', '영어-독일어 기계번역', '교차 언어', '사전학습 모델', 'BERT', '이중 인코더']","기계번역 품질 예측은 기계가 번역 문장의 품질을 주어진 정답 번역 문장이 없이 예측하는 것을 말한다. 최근에 품질 예측 분야의 연구 동향은 다량의 병렬 말뭉치로 학습된 트랜스포머 인코더 기반의 사전학습 모델을 이용하여 전이 학습을 적용한다. 본 논문에서는 품질 예측과 같은 교차 언어 태스크에서 단일 인코더 구조가 가지는 한계를 극복하기 위해 인코더에서 각 언어에 대한 단일 언어 표현을 개별적으로 학습하고 상호 참조망에서 교차 언어 표현을 학습하는 이중 인코더 구조를 제시한다. 이중 인코더 구조가 단일 인코더 구조보다 품질 예측 태스크에서 구조적으로 유리함을 입증하고, 나아가 이중 인코더 모델에 사전학습된 언어 모델을 적용하여 품질 예측 태스크에서 이중 인코더 모델의 성능과 안정성을 높인다. WMT20 품질 예측 영어-독일어 쌍에 대해서 실험을 진행했다. 사전학습 모델로서 영어 Bidirectional Encoder Representations from Transformers (BERT) 언어 모델과 독일어 BERT 언어 모델이 각각의 인코더에 적용되었고 가장 뛰어난 성능을 보여주었다.","Quality estimation (QE) is the task of estimating the quality of given machine translations (MTs) without their reference translations. A recent research trend is to apply transfer learning to a pre-training model based on Transformer encoder with a parallel corpus in QE. In this paper, we proposed a dual-encoder architecture that learns a monolingual representation of each respective language in encoders. Thereafter, it learns a cross-lingual representation of each language in cross-attention networks. Thus, it overcomes the limitations of a single-encoder architecture in cross-lingual tasks, such as QE. We proved that the dual-encoder architecture is structurally more advantageous over the single-encoder architecture and furthermore, improved the performance and stability of the dual-encoder model in QE by applying the pre-trained language model to the dual-encoder model. Experiments were conducted on WMT20 QE data for En-De pair. As pre-trained models, our model employs English BERT (Bidirectional Encoder Representations from Transformers) and German BERT to each encoder and achieves the best performance."
A Multi-task Self-attention Model Using Pre-trained Language Models on Universal Dependency Annotations,2022,"['KR-', 'Ko']",,"In this paper, we propose a multi-task model that can simultaneously predict general-purpose tasks such as part-of-speech tagging, lemmatization, and dependency parsing using the UD Korean Kaist v2.3 corpus. The proposed model thus applies the self-attention technique of the BERT model and the graph-based Biaffine attention technique by fine-tuning the multilingual BERT and the two Korean-specific BERTs such as KR-BERT and KoBERT. The performances of the proposed model are compared and analyzed using the multilingual version of BERT and the two Korean-specific BERT language models."
언어 정보가 반영된 문장 점수를 활용하는삭제 기반 문장 압축,2022,"['Sentence Compression', 'Linguistic Information', 'Language Model', 'Perplexity', '문장 압축', '언어 정보', '언어 모델', '펄플렉시티']",,"Sentence compression is a natural language processing task that generates concise sentences that preserves the important meaning ofthe original sentence. For grammatically appropriate sentence compression, early studies utilized human-defined linguistic rules.Furthermore, while the sequence-to-sequence models perform well on various natural language processing tasks, such as machine translation,there have been studies that utilize it for sentence compression. However, for the linguistic rule-based studies, all rules have to be definedby human, and for the sequence-to-sequence model based studies require a large amount of parallel data for model training. In orderto address these challenges, Deleter, a sentence compression model that leverages a pre-trained language model BERT, is proposed. Becausethe Deleter utilizes perplexity based score computed over BERT to compress sentences, any linguistic rules and parallel dataset is not requiredfor sentence compression. However, because Deleter compresses sentences only considering perplexity, it does not compress sentencesby reflecting the linguistic information of the words in the sentences. Furthermore, since the dataset used for pre-learning BERT are farfrom compressed sentences, there is a problem that this can lad to incorrect sentence compression. In order to address these problems,this paper proposes a method to quantify the importance of linguistic information and reflect it in perplexity-based sentence scoring.Furthermore, by fine-tuning BERT with a corpus of news articles that often contain proper nouns and often omit the unnecessary modifiers,we allow BERT to measure the perplexity appropriate for sentence compression. The evaluations on the English and Korean dataset confirmthat the sentence compression performance of sentence-scoring based models can be improved by utilizing the proposed method."
KorSciQA 2.0: 과학기술 분야 한국어 논문 기계독해를 위한 질의응답 데이터셋,2022,"['기계독해', '질의응답', '언어모델', 'BERT', 'machine reading comprehension', 'question answering', 'language model', 'BERT']","최근 다양한 질의응답 공개 과제를 통해 기계독해 시스템의 성능은 향상되고 있으며, 더욱 지능화된 기계독해 시스템의 학습을 위해 여러 텍스트 단락과 지문을 포괄적으로 이해하고 이산적인 추론을 해야 하는 도전적인 과제가 공개되고 있다. 그러나 한국어 학술정보를 이해하기 위한 복합추론 목적 질의 응답 데이터셋의 부재로 인해 학술 논문에 대한 기계독해 연구는 활발히 이루어지지 않고 있다. 본 논문에서는 한국어 학술 논문의 전문을 대상으로 난이도를 일반, 하, 상으로 나누어 기계독해 시스템의 변별력을 확인할 수 있는 질의응답 데이터인 KorSciQA 2.0을 구축하였으며, KorSciQA 2.0을 구축하기 위한 방법론과 프로세스, 그리고 시스템을 제안하였다. KorSciQA 2.0에 대한 기계독해 성능 평가 실험 결과, 과학기술분야 도메인에 대한 한국어 기반 BERT 모델인 KorSciBERT 모델을 기반으로 미세 조정(Fine-tuning)하였을 때, F1 성능이 80.76%로 가장 높은 성능을 보였다.","Recently, the performance of the Machine Reading Comprehension(MRC) system has been increased through various open-ended Question Answering(QA) task, and challenging QA task which has to comprehensively understand multiple text paragraphs and make discrete inferences is being released to train more intelligent MRC systems. However, due to the absence of a QA dataset for complex reasoning to understand academic information in Korean, MRC research on academic papers has been limited. In this paper, we constructed a QA dataset, KorSciQA 2.0, for the full text including abstracts of Korean academic papers and divided the difficulty level into general, easy, and hard for discriminative MRC systems. A methodology, process, and system for constructing KorSciQA 2.0 were proposed. We conducted MRC performance evaluation experiments and when fine-tuning based on the KorSciBERT model, which is a Korean-based BERT model for science and technology domains, the F1 score was 80.76%, showing the highest performance."
한국어 학습 모델별 한국어 쓰기 답안지점수 구간 예측 성능 비교,2022,"['한국어 심층학습 언어모델', 'KoBERT', 'KcBERT', 'KR-BERT', '문서 분류', 'Deep Learning-Based Korean Language Model', 'KoBERT', 'KcBERT', 'KR-BERT', 'Document Classification']",,"We investigate the performance of deep learning-based Korean language models on a task of predicting the score range of Korean essayswritten by foreign students. We construct a data set containing a total of 304 essays, which include essays discussing the criteria for choosinga job (‘job’), conditions of a happy life (‘happ’), relationship between money and happiness (‘econ’), and definition of success (‘succ’). Theseessays were labeled according to four letter grades (A, B, C, and D), and a total of eleven essay score range prediction experiments wereconducted (i.e., five for predicting the score range of ‘job’ essays, five for predicting the score range of ‘happiness’ essays, and one forpredicting the score range of mixed topic essays). Three deep learning-based Korean language models, KoBERT, KcBERT, and KR-BERT,were fine-tuned using various training data. Moreover, two traditional probabilistic machine learning classifiers, naive Bayes and logisticregression, were also evaluated. Experiment results show that deep learning-based Korean language models performed better than the twotraditional classifiers, with KR-BERT performing the best with 55.83% overall average prediction accuracy. A close second was KcBERT (55.77%)followed by KoBERT (54.91%). The performances of naïve Bayes and logistic regression classifiers were 52.52% and 50.28% respectively. Dueto the scarcity of training data and the imbalance in class distribution, the overall prediction performance was not high for all classifiers.Moreover, the classifiers’ vocabulary did not explicitly capture the error features that were helpful in correctly grading the Korean essay.By overcoming these two limitations, we expect the score range prediction performance to improve."
Honorific agreement and plural copying revisited: Experimental and deep learning approach,2022,"['honorific agreement', 'plural copying', 'deep learning', 'KR-BERT', 'K-CoLA']",,"The study investigated two types of subjecthood diagnostics in Korean using two different kinds of approaches. Based on the previous studies on subjecthood diagnostics in Korean, this paper examined the two subjecthood diagnostics: Honorific Agreement (HA) and Plural Copying (PC). For the experimental analysis, this study adopted the analysis results of Kim et al. (2017). For the deep learning analysis, this paper employed the KR-BERT for the deep learning model and three sources of data sets: the Korean version of the Corpus of Linguistic Acceptability (K-CoLA), the Sejong Morphologically-Analyzed Corpus, and the extended target sentences of Kim et al. (2017). Two separate experiments were conducted in the deep learning analysis. In the first experiment, the KR-BERT was trained only with the K-CoLA, and the target sentences were analyzed. In the second experiment, the KR-BERT was trained with the K-CoLA and the sentences from the Sejong corpus, and the acceptability scores of the target sentences were measured. The acceptability scores were measured with the numeric scores using the algorithms in Lee (2021). After the experiments with the deep learning models, the scores were normalized and were statistically analyzed with Generalized Linear Models (GLMs). Through the two experiments, the following fact was observed: both HA and PC did not show similar tendencies of experimental results with human participants in the first experiment, but they did in the second experiment. The analysis results demonstrated that both HA and PC could be used as subjecthood diagnostics but that they played significant roles only when native speakers were exposed to enough examples."
한국어 수사를 위한 기계 독해 데이터 세트,2022,"['machine reading comprehension', 'question answering', 'BERT', 'natural language processing', 'Korean numerals', 'arabic numerals', '기계 독해', '질의응답', 'BERT', '자연어 처리', '한국어 수사', '아라비아 숫자']","본 논문에서는 한국어 수사를 위한 기계 독해 데이터 세트 구축 방법을 제안한다. 구축한 데이터 세트를 BERT 기반 한국어 질의응답 언어모형에 추가로 훈련하여, 수사를 포함한 질의응답의 성능 개선을 가져왔다. 데이터 세트 구축을 위해 대한민국 법령문서를 이용하였고, 해당 문서의 구조적 정보를 반영하기 위한 가공 절차를 거쳤다. 질의응답 생성 절차 마련을 위해 한국어 물음말, 한국어 분류사, 국제단위계 및 범용 단위를 수사와 결합하는 방식을 활용했다. 생성한 데이터 세트의 일반화를 알기 위해 전자신문 기사를 포함한 평가 데이터 세트로 실험을 수행했다. 실험 결과, 해당 데이터 세트를 훈련한 시스템의 성능은 EM 83.4, F1 91.0으로, 기존 시스템보다 EM 14.0, F1 9.6 향상되었다. 데이터 세트 평가를 위해 KorQuAD 검증 데이터 세트 중 수사를 포함한 데이터를 무작위로 추출하여 실험했고, 이 또한 기존시스템보다 EM 13.0, F1 7.2 향상된 성능을 보였다.","In this study, a method of constructing the Machine Reading Comprehension Data sets for Korean Numerals is presented. Data sets were trained in addition to the Korean QA Language Model which was based on BERT, leading to improved performance of QA including Korean Numerals. In this work, Korean statute documents were used for constructing data sets which were then processed to apply a structural information of documents. In order to prepare a QA generation procedure, a method combining Korean question word, Korean classifier, SI and other units was used.To as to verify the generalization of these data sets, an experiment was conducted using a test data set including e-newspaper articles. Performance of this system showed an EM of 83.4 and an F1 of 91.0, which showed improvement compared to the previous system (EM of 14.0 and F1 of 9.6). To evaluate data sets, the experiment was carried out with randomly extracted KorQuAD dev data sets including numerals. It had an EM of 13.0 and an F1 of 7.2, also showing improvement than the previous system"
Improving Abstractive Summarization by Training Masked Out-of-Vocabulary Words,2022,"['BERT', 'Deep Learning', 'Generative Summarization', 'Selective OOV Copy Model', 'Unknown Words']",,"Text summarization is the task of producing a shorter version of a long document while accurately preservingthe main contents of the original text. Abstractive summarization generates novel words and phrases using alanguage generation method through text transformation and prior-embedded word information. However,newly coined words or out-of-vocabulary words decrease the performance of automatic summarization becausethey are not pre-trained in the machine learning process. In this study, we demonstrated an improvement insummarization quality through the contextualized embedding of BERT with out-of-vocabulary masking. Inaddition, explicitly providing precise pointing and an optional copy instruction along with BERT embedding,we achieved an increased accuracy than the baseline model. The recall-based word-generation metric ROUGE-1 score was 55.11 and the word-order-based ROUGE-L score was 39.65."
딥러닝을 이용한 법률 분야 한국어 의미 유사판단에 관한 연구,2022,"['NLP', 'LegalTech', 'Semantic Similarity', 'BERT', 'Legal', '자연어처리', '리걸테크', 'Semantic Similarity', 'BERT', '법률']",,"Keyword-oriented search methods are mainly used as data search methods, but this is not suitable as a search method in the legalfield where professional terms are widely used. In response, this paper proposes an effective data search method in the legal field. Wedescribe embedding methods optimized for determining similarities between sentences in the field of natural language processing of legaldomains. After embedding legal sentences based on keywords using TF-IDF or semantic embedding using Universal Sentence Encoder,we propose an optimal way to search for data by combining BERT models to check similarities between sentences in the legal field."
건설현장 정형·비정형데이터를 활용한 기계학습 기반의 건설재해 예측 모델 개발,2022,"['Construction accident', 'Prediction model', 'Machine learning', 'BERT', 'Decision forest', '건설재해', '예측모델', '기계학습', 'BERT', 'Decision Forest']","현재 국내 건설업에서는 꾸준히 증가하는 건설재해를 예방하기 위해 다양한 정책적 노력과 연구가 활발하게 진행되고 있다. 기존 연구에서 건설재해 예방을 위해 개발한 예측 모델의 경우, 주로 정형데이터만을 활용하였기에 건설현장의 다양한 특성을 충분히 고려하지 못한 예측 결과가 도출되었다. 따라서, 본 연구에서는 정형데이터와 텍스트 형식의 비정형데이터를 동시에 활용하여 건설현장의 특성을 충분히 고려할 수 있는 기계학습 기반 건설재해 사전 예측 모델을 개발하였다. 본 연구는 기계학습을 위해 건설공사 안전관리 종합정보망(CSI)의 최근 3년간 건설재해 데이터 6,826건을 수집하였다. 수집된 데이터 중 정형데이터의 학습은 5가지 알고리즘의 성능 분석을 통해 Decision forest 알고리즘을 사용하였고비정형데이터의 학습은 BERT 언어모델을 사용하였다. 정형 및 비정형데이터를 동시에 활용한 건설재해 예측 모델의 성능 비교 결과, 정형데이터만을 활용한 경우보다 약 20 % 향상된 95.41 %의 예측정확도가 도출되었다. 본 연구 결과, 비정형데이터를 동시에 활용함으로써 예측 모델의효과적인 성능 향상을 확인하였으며, 보다 정확한 예측을 통한 건설재해 저감을 기대할 수 있다.",
Transformer 언어 모델을 활용한 초중등 학습자 작문 연령 예측 모델 구현,2022,"['Deep-learning', 'Transformer Language Model', 'BERT', 'BART', 'Learners’ Writings', 'Writing Development', '딥러닝', 'Transformer 언어 모델', 'BERT', 'BART', '학습자 작문', '작문 발달']","본 연구에서는 Transformer 언어 모델, 그중에서도 한국어 BERT 및 BART 모델을 활용하여 초중등 학습자 작문의 연령대를 예측할 수 있는 연령 예측 모델을 구현하였다. 대규모의 코퍼스에 대하여 사전 학습한 KoBERT, KcBERT, KoBART 모델에 기반하여 국립국어원 ‘모두의 말뭉치’ 중 ‘국립국어원 비출판물 말뭉치(버전 1.1)’를 대상으로 미세 조정(fine-tuning)을 진행함으로써 학습자 작문 연령 예측 모델을 구현하였다. 결과적으로 예측 모델은 약 61.1%에서 68.1%의 정확도를 가지는 것으로 나타났으며, 그중에서도 KoBART에 기반을 둔 예측 모델이 가장 우수한 성능을 보였다. 특히, 작문의 장르를 고려하여 미세 조정을 진행한 경우 최대 70% 이상의 정확도를 보여 초중등 학습자 작문에 대한 연령 예측 모델 정교화의 가능성을 확인할 수 있었다. 본고는 학습자 작문 연령 예측 모델을 활용하여 연령대별로 전형적이라고 판단되는 학습자 작문들을 추출하였다. 이를 통해 학습자 작문의 발달 양상을 분석하는 것이 가능하며, 더 나아가서는 전형성과 대표성을 가지는 학습자 작문을 바탕으로 학습자 작문 평가 또는 작문 교수·학습 내용의 단서를 탐색할 수 있는 기반을 마련할 수 있을 것으로 기대된다.",
Negative Polarity Items in English: A Deep Learning Model and Statistical Analysis,2022,"['NPI', 'BERT&lt', 'SUB&gt', 'LARGE&lt', '/SUB&gt', 'licensor', 'scope', 'monotonicity']",,"This paper examined English negative polarity items (NPIs) with a deep learning model and statistical analysis. Warstadt et al. (2019a) developed a deep learning model with the Bidirectional Encoder Representations from Transformers (BERT), and the study analyzed English NPIs. Their dataset included a total of 400,000 sentences, and they encoded a few linguistic variables per each sentence. This paper took the same dataset in Warstadt et al. (2019a), but one more factor (Monotonicity) was included. As for the deep learning model, this paper took the BERTLARGE model in Lee (2021), where the syntactic acceptability was calculated with numeric scores (0~100), rather than measured by the binary classification. After the acceptability scores were calculated per each sentence, the scores were converted with z -scores, and statistical analysis was conducted with six linguistic factors and interactions of three factors. Through the analysis, the followings were observed: (i) an NPI had to exist within the scope of the licensor, (ii) downward entailment also played a significant role in the NPI licensing, and (iii) the interaction between NPI licensor and scope played more crucial roles in the NPI licensing."
딥러닝 언어모형의 평가와 언어학,2022,"['deep learning', 'language models', 'BERT', 'evaluation', 'GLUE', 'linguistics', '딥러닝', '언어모형', 'BERT', '평가', 'GLUE', '언어학']",,"This article addresses how the deep learning-based language models can be evaluated with respect to linguistic knowledge. Building upon the overlook, this article discusses how linguistics can make a substantial contribution to the development of the artificial intelligence systems. As many transformer-based models have been competitively implemented for the last few years, it is required to evaluate the multiple models in a common and reliable way. For this purpose, a wide range of linguistic evaluation metrics have been designed and constructed. The evaluation datasets involve the concepts used in theoretical linguistics, such as syntax, semantics, and pragmatics. The evaluation process follows the guideline used in psycholinguistic experiments. As such, the linguistic knowledge enhances interpretability of the deep leaning-based natural language processing techniques. It is contended that linguistics will play a pivotal role in evaluating and improving the language models in further research."
Rethinking ROUGE Scores for Video Game Review Summarization,2022,"['summarization', 'ROUGE', 'game review', 'BERT']",,"Recall-oriented understudy for gisting evaluation (ROUGE) is a prevalent evaluation measure in the field of natural language processing, especially for text summarization. However, ROUGE's reliability has been continuously debated because a high ROUGE score does not guarantee a high-quality summary and vice versa. As an empirical study in the video game review summary, we address that existing state-of-the-art summarization techniques fail in generating high-quality game review summaries, and ROUGE scores for those summaries are quite problematic. To this end, game review data are newly collected, and BERT based automatic review summarizations are performed on the dataset to reconsider ROUGE use in video game review summarizations. Especially, we provide an in-depth discussion between the ROUGE score and the scores of human annotators in terms of game review factors."
딥러닝 기반 사전학습 언어모델에 대한 이해와 현황,2022,"['NLP', 'deep learning', 'language model', 'Transformer', 'BERT', 'GPT', '자연어 처리', '딥러닝', '언어모델', '트랜스포머', 'BERT', 'GPT']","사전학습 언어모델은 자연어 처리 작업에서 가장 중요하고 많이 활용되는 도구로, 대량의 말뭉치를 대상으로 사전학습이 되어있어 적은 수의 데이터를 이용한 미세조정학습으로도 높은 성능을 기대할 수 있으며, 사전학습된 토크나이저과 딥러닝 모형 등 구현에 필요한 요소들이 함께 배포되기 때문에 자연어 처리 작업에 소요되는 비용과 기간을 크게 단축시켰다. 트랜스포머 변형 모형은 이와 같은 장점을 제공하는 사전학습 언어모델 중에서 최근 가장 많이 사용되고 있는 모형으로, 번역을 비롯하여 문서 요약, 챗봇과 같은 질의 응답, 자연스러운 문장의 생성 및 문서의 분류 등 다양한 자연어 처리 작업에 활용되고 있으며 컴퓨터 비전 분야와 오디오 관련 분야 등 다른 분야에서도 활발하게 활용되고 있다. 본 논문은 연구자들이 보다 쉽게 사전학습 언어모델에 대해 이해하고 자연어 처리 작업에 활용할 수 있도록 하기 위해, 언어모델과 사전학습 언어모델의 정의로부터 시작하여 사전학습 언어모델의 발전과정과 다양한 트랜스포머 변형 모형에 대해 조사하고 정리하였다.",
딥러닝 한국어 모델은 관형사절 속 공백을 포착하는가?,2022,"['한국어 딥러닝 모델', '공백', '관계절', 'BERT', '어텐션', 'Korean deep learning language model', 'gap', 'relative clause', 'BERT', 'Attention']","본고는 딥러닝 한국어 모델이 공백이 있는 관계 관형사절(공백 관계절)과 그 밖의 관형사절(비공백 관계절)을 구별하는지 살펴본 연구이다. 궁극적으로는 딥러닝 한국어 모델이 관형사절 속 공백을 포착하는지 살펴보고자 했다. 연구 결과는 다음과 같다. 첫째, 딥러닝 한국어 모델이 공백 관계절과 비공백 관계절을 최소 89%에서 최대 100%의 정확도로 분류하였다. 둘째, 히트맵 분석 결과 공백 관계절의 경우 머리명사보다 관계절 속 서술어에 더 많은 어텐션(Attention)이 놓였다. 이러한 결과들은 딥러닝 한국어 모델이 관계절 속 공백을 포착한다는 직접적인 단서는 되지 못하므로 후속 연구가 필요하다.","This paper examines whether the Korean deep learning language model can identify the gap associated with the relativized noun in a relative clause. The results of the current experimental study are as follows. First, the deep learning model used in the present study can classify the gapped relative clauses and the gapless relative clauses (by the minimum successful rate of 89% and maximum successful rate of 100%). Second, the heatmap analysis indicates that a bigger attention tends to be assigned to the embedded predicate rather than the head noun when it comes to the gapped relative clauses. These findings, however, do not necessarily mean that deep learning language model can capture the gap in the relative clauses."
한국어 텍스트 질의를 활용한 주식 정보 검색 및 분석 기법,2022,"['stock market', 'search system', 'text interface', 'BERT model', 'random forest model']",,"With the explosively increasing interest in US stock, entering the market is on vogue. However, beginners who have little knowledge are having trouble searching related information. Therefore, a easy system for searching US stock market information is indeed necessary, but there are few researches dealing with these trouble. This paper proposes a new text interface that can conveniently look up US stock information. In addition, the proposed interface includes a BERT model and a Random Forest ensemble model to process natural language queries and shows stock information in an easy way written in Korean. We believe that this interface can help beginners with no knowledge in US stock market and struggling to search for related information."
An Experimental Investigation of Discourse Expectations in Neural Language Models,2022,"['discourse expectation', 'implicit causality bias', 'neural language model', 'BERT', 'GPT-2', 'LSTM', 'next sentence prediction', 'coreference resolution', 'surprisal']",,"The present study reports on three language processing experiments with most up-to-date neural language models from a psycholinguistic perspective. We investigated whether and how discourse expectations demonstrated in the psycholinguistics literature are manifested in neural language models, using the language models whose architectures and assumptions are considered most appropriate for the given language processing tasks. We first attempted to perform a general assessment of a neural model’s discourse expectations about story continuity or coherence (Experiment 1), based on the next sentence prediction module of the bidirectional transformer-based model BERT (Devlin et al. 2019). We also studied language models’ expectations about reference continuity in discursive contexts in both comprehension (Experiment 2) and production (Experiment 3) settings, based on so-called Implicit Causality biases. We used the unidirectional (or left-to-right) RNN-based model LSTM (Hochreiter and Schmidhuber 1997) and the transformer-based generation model GPT-2 (Radford et al. 2019), respectively. The results of the three experiments showed, first, that neural language models are highly successful in distinguishing between reasonably expected and unexpected story continuations in human communication and also that they exhibit human-like bias patterns in reference expectations in both comprehension and production contexts. The results of the present study suggest language models can closely simulate the discourse processing features observed in psycholinguistic experiments with human speakers. The results also suggest language models can, beyond simply functioning as a technology for practical purposes, serve as a useful research tool and/or object for the study of human discourse processing."
신경망 언어 모델과 인간 언어 사용자의 주어와 목적어 관계절 처리 비교 연구,2022,"['subject/object relative clause', 'PP modification', 'reading time', 'surprisal', 'BERT', 'GPT-2', '주어/목적어 관계절', '전치사구 수식', '읽기 시간', '서프라이절']",,"This paper is to investigate the distinct aspects of processing subject and object relative clauses (SRC and ORC) in neural language models (LMs) and humans using the materials, which are constructed by manipulating two RC types (SRC vs ORC) and two intervening PP types (locative vs temporal). Surprise values are collected from the transformer GPT-2 and BERT language models. Reading time data for humans are taken from Lowder and Gordon’s (2021) eye-tracking experiment. According to Lowder and Gordon (ibid.) that take as a critical region the matrix verb after the RC in the subject position, for humans the locative PP contained in the RC register longer reading times ORCs than SRCs, while the temporal PP does so for SRCs than ORCs. By contrast, for the two neural LMs, surprisals are higher for ORCs than SRCs regardless of whether the PP in question is locative or temporal. There was no statistically significant linear fit between human and the LMs’ responses. The result shows that to the extent that the distinction between locative and temporal PPs in RCs is syntactico-semantic, neither of the two neural language models is able to acquire human-like sensitivity to such a distinction."
자연어 질의 기반 부동산 환경 검색 시스템 구현,2022,"['text mining', 'natural language', 'real estate', 'text interface', 'BERT model']",,"The existing real estate search system, in which the desired area to be searched must be specified at least inward units, has a limitation in that areas with appropriate conditions can be excluded from consideration depending on the individuals background knowledge. Therefore, in this study, we propose a natural language query system that can search for information about the actual rental price of Jeonse(long-term rent with lump-sum deposit) in Seoul through a Korean query. The system processes natural language queries with the BERT model and the Random Forest model, sorts them in the order of real estate that meets the conditions, and visualizes the location of the real estate on the map. The performance of the system was evaluated and implemented so that users can easily find suitable properties for each user."
한국어 특허 문장 기반 CPC 자동분류 연구 ―인공지능 언어모델 KorPatBERT를 활용한 딥러닝 기법 접근―,2022,"['자동 특허 분류', '멀티 라벨', '특허', '버트', '인공지능', 'KorPatBERT', 'CPC', 'Automatic Patent Classification', 'Multi Label', 'IPC', 'Patent', 'BERT', 'AI']",,"With the advent of various new technologies in the 4th industrial revolution, securing intellectual property rights has become increasingly important to countries or companies for maintaining technological competitiveness and building growth engines. In particular, a patent is a technical document that contains the core technology and is widely used for measuring corporate value and analyzing competitive technologies. To make this support, the CPC that including latest and detailed technical fields has been developed and more than 62 million documents worldwide have been classified as CPC. And five advanced patent offices which account for more than 80% of the world’s patent applications invest big budget for CPC of new patent applications every year.In this study, we had generated the KorPatBERT that was pre-trained and outperformed in patent field using the BERT language model which understands the meaning of sentences beyond the limits of keywords. And we proposed the methods and constructed the dataset that relieved imbalanced distribution for each CPC code. And finally, we had generated the AI CPC model that can classify into main group level and verified through reliable evaluation indicators. Through this, we want to contribute the sustainable development of Korean patent based classification and NLP field."
인공지능 윤리 인식에 대한 데이터 분석 및 시각화 연구 - 대화형 인공지능 서비스 ‘이루다’를 중심으로 -,2022,"['인공지능', '인공지능 윤리', '이루다', '감성 분석', '빈도분석', 'CONCOR분석', 'AI', 'Artificial Intelligence', 'AI ethics', 'BERT', 'CONCOR analysis']",,
The Ability of an English Language Model to Understand of World Knowledge with Backshift,2022,"['World knowledge', 'backshift', 'deep-learning', 'language model', 'evaluation']",,"This study evaluated whether an English language models using artificial intelligence represent world knowledge, focusing on the backshift phenomenon. Backshift refers to the past tense being used in indirect speech, with direct speech being in the present tense. This study argued that language models capture grammatical phenomena that interact with world knowledge and can represent knowledge beyond mere grammatical knowledge. This study used BERT and mBERT models to measure the surprisals of verbs in indirect speech. Surprisal is a measurement that increase in proportion to the difficulty of language processing. Experimental results demonstrated that BERT and mBERT models were sensitive to the backshift phenomenon, revealing that the artificial intelligence language model captured backshift in indirect speech. As backshift requires an understanding of truth, knowledge, and commonsense, these results indicated that the language model understood world knowledge."
사전 학습 언어 모델의 미세 튜닝을 활용한 버그 담당자 추천 기법,2022,"['bug management', 'bug fixer assignment', 'machine learning', 'natural language processing', 'pre trained language mode', '버그 관리', '버그 담당자 배정', '머신러닝', '자연어 처리', '사전 학습 언어 모델']","최근 소프트웨어의 규모와 복잡성이 지속해서 증가하고 있으며, 이는 다양한 버그를 유발하고 있다. 이에 따라 체계적 버그 관리의 필요성이 지속해서 제기되고 있다. 산업계에서는 다수의 연구들은 단어 기반의 학습 모델을 이용하여 버그 담당자 배정을 자동화하는 방법을 제시하였다. 하지만 이들은 대체로 단어 맥락 미고려, 클래스 개수 과다 등의 요인으로 정확도가 만족스럽지 못한 성능을 보인다. 본 논문에서는 BERT 및 이를 기반으로 한 RoBERTa, DeBERTa, CodeBERT등을 기반으로 사전 학습 언어 모델을 파인 튜닝하여 Top-10 정확도 기준 약 27%p의 정확도 향상을 이루어 냈으며, 결과적으로 약 70%의 정확도를 보이는 것을 실험을 통해 확인하였다. 이를 통해 파인 튜닝된 사전 학습 언어 모델 기반의 접근 방법이 버그 담당자 배정 자동화 문제에 효과적으로 적용될 수 있음을 보였다.","The scale and complexity of software continue to increase; hence they contribute to the occurrence of diverse bugs. Therefore, the necessity of systematic bug management has been raised.A few studies have proposed automating the assignment of bug fixers using word-based deep learning models. However, their accuracy is not satisfactory due to context of the word is ignored, and there is an excessive number of classes. In this paper, the accuracy was improved by about 27%p over the top-10 accuracies by using a fine-tuned pre-trained language model based on BERT, RoBERTa, DeBERTa, and CodeBERT. Experiments confirmed that the accuracy was about 70%. Through this, we showed that the fine-tuned pretrained language model could be effectively applied to automated bug-fixer assignments."
KorPatELECTRA : 자연어처리 분야에서의 성능 향상을 위한 한국어 특허 문헌 사전학습 언어모델(KorPatELECTRA),2022,"['Patent', 'ELECTRA', 'pre-training', 'NLP', 'tokenizer', 'Language model ∙', '특허', '일렉트라', '사전학습', '자연어처리', '토크나이저', '언어모델']","특허 분야에서 자연어처리(Natural Language Processing) 태스크는 특허문헌의 언어적 특이성으로문제 해결의 난이도가 높은 과제임에 따라 한국 특허문헌에 최적화된 언어모델의 연구가 시급한실정이다. 최근 자연어처리 분야에서는 특정 도메인에 특화되게 사전 학습(Pre-trained)한 언어모델을 구축하여 관련 분야의 다양한 태스크에서 성능을 향상시키려는 시도가 지속적으로 이루어지고있다. 그 중, ELECTRA는 Google이 BERT 이후에 RTD(Replaced Token Detection)라는 새로운 방식을 제안하며 학습 효율성을 높인 사전학습 언어모델이다. 본 연구에서는 대량의 한국 특허문헌데이터를 사전 학습한 KorPatELECTRA를 제안한다. 또한, 특허 문헌의 특성에 맞게 학습 코퍼스를 정제하고 특허 사용자 사전 및 전용 토크나이저를 적용하여 최적화된 사전 학습을 진행하였다. KorPatELECTRA의 성능 확인을 위해 실제 특허데이터를 활용한 NER(Named Entity Recognition), MRC(Machine Reading Comprehension), 특허문서 분류 태스크를 실험하였고 비교 대상인 범용 모델에 비해 3가지 태스크 모두에서 가장 우수한 성능을 확인하였다.","In the field of patents, as NLP(Natural Language Processing) is a challenging task due to the linguistic specificity of patent literature, there is an urgent need to research a language model optimized for Korean patent literature. Recently, in the field of NLP, there have been continuous attempts to establish a pre-trained language model for specific domains to improve performance in various tasks of related fields.Among them, ELECTRA is a pre-trained language model by Google using a new method called RTD(Replaced Token Detection), after BERT, for increasing training efficiency. The purpose of this paper is to propose KorPatELECTRA pre-trained on a large amount of Korean patent literature data. In addition, optimal pre-training was conducted by preprocessing the training corpus according to the characteristics of the patent literature and applying patent vocabulary and tokenizer. In order to confirm the performance, KorPatELECTRA was tested for NER(Named Entity Recognition), MRC(Machine Reading Comprehension), and patent classification tasks using actual patent data, and the most excellent performance was verified in all the three tasks compared to comparative general-purpose language models."
리뷰 데이터와 제품 정보를 이용한 멀티모달 감성분석,2022,"['감성분석', '멀티모달', '특성 추출', '인공 신경망', 'Sentiment Analysis', 'Multi-Modal', 'Feature Selection', 'Neural Network']","최근 의류 등의 특정 쇼핑몰의 온라인 시장이 크게 확대되면서, 사용자의 리뷰를 활용하는 것이 주요한 마케팅 방안이 되었다. 이를 이용한 감성분석에 대한 연구들도 많이 진행되고 있다. 감성분석은 사용자의 리뷰를 긍정과 부정 그리고 필요에 따라서 중립으로 분류하는 방법이다. 이 방법은 크게 머신러닝 기반의 감성분석과 사전기반의 감성분석으로 나눌 수 있다. 머신러닝 기반의 감성분석은 사용자의 리뷰 데이터와 그에 대응하는 감성 라벨을 이용해서 분류 모델을 학습하는 방법이다. 감성분석 분야의 연구가 발전하면서 리뷰와 함께 제공되는 이미지나 영상 데이터 등을 함께 고려하여 학습하는 멀티모달 방식의 모델들이 연구되고 있다.리뷰 데이터에서 제품의 카테고리와 사용자별로 사용되는 단어 등의 특징이 다르다. 따라서 본 논문에서는 리뷰데이터와 제품 정보를 동시에 고려하여 감성분석을 진행한다. 리뷰를 분류하는 모델로는 기본 순환신경망 구조에서 Gate 방식을 도입한 Gated Recurrent Unit(GRU), Long Short-Term Memory(LSTM) 그리고 Self Attention 기반의 Multi-head Attention 모델, Bidirectional Encoder Representation from Transformer(BERT)를 사용해서 각각 성능을 비교하였다. 제품 정보는 모두 동일한 Multi-Layer Perceptron(MLP) 모델을 이용하였다. 본 논문에서는 사용자 리뷰를 활용한 Baseline Classifier의 정보와 제품 정보를 활용한 MLP모델의 결과를 결합하는 방법을 제안하며 실제 데이터를 통해 성능의 우수함을 보인다.","Due to recent expansion of online market such as clothing, utilizing customer review has become a major marketing measure. User review has been used as a tool of analyzing sentiment of customers. Sentiment analysis can be largely classified with machine learning-based and lexicon-based method. Machine learning-based method is a learning classification model referring review and labels. As research of sentiment analysis has been developed, multi-modal models learned by images and video data in reviews has been studied. Characteristics of words in reviews are differentiated depending on products' and customers’ categories. In this paper, sentiment is analyzed via considering review data and metadata of products and users. Gated Recurrent Unit (GRU), Long Short-Term Memory(LSTM), Self Attention-based Multi-head Attention models and Bidirectional Encoder Representation from Transformer (BERT) are used in this study. Same Multi-Layer Perceptron (MLP) model is used upon every products information. This paper suggests a multi-modal sentiment analysis model that simultaneously considers user reviews and product meta-information."
제주 예멘난민 사태 전후 국내 여론변화 분석: 심층 인공신경망을 활용한 감성분석을 중심으로,2022,"['refugees', 'sentiment analysis', 'interrupted time series analysis', '난민', '감성분석', '단절적 시계열 분석']","본 연구는 난민에 대한 국내의 관심이 증가하게 된 2018년 제주 예멘 난민 사건을 중심으로 사건 전후의 난민에 대한 대중의 태도 변화를 분석하였다. 분석의 단계는 두 단계로 먼저, 언론 보도의 양과 국민의 관심 및 태도를 시기별로 나누어 확인한 후 난민에 대한 태도를 귀납적으로 유형화하고, 이후, 제주 예멘난민 사태가 난민에 대한 태도에 미친 영향을 분석하였다. 분석의 대상은2015년부터 2021년까지 주요 일간지와 뉴스방송사의 13,481개 기사에 대한 359,984개의 댓글이다. 난민에 대한 태도는 긍정과 부정의 비율을 볼 수 있도록 분류하고, 부정의 경우 그 근거에 따라‘종교/문화/안전’, ‘자원부족과 우선순위’, ‘자격과 책임’으로 분류하였다. 개별 댓글의 분석은 자연어처리기술(NLP: Natural Language Processing)을 활용한 감성분석(Sentiment Analysis)을 통해 이루어 졌으며, 한국어 BERT(Bidirectional Encoder Representations for Transformers) 모델중 하나인 KcBERT를 활용하였다. 영향분석에는 단절적 시계열 분석을 사용하였다. 분석기간 동안 전반적으로 부정적 여론의 비중이 높았으며, 제주 예멘난민 사건 이후 부정적 여론이 급격히증가하였고 부정의견의 유형이 다양화되는 것을 확인할 수 있었다. 또한 영향분석 결과, 긍정･부정 비(ratio)에 대해 부정여론이 즉각적으로 증가하는 영향을 주었지만, 전체적인 추세에는 영향을 주지 않았다. 부정 유형의 경우 ‘종교/문화/안전’ 유형의 비율은 감소하였고, ‘자원부족과 우선순위’ 유형의 비율은 증가하였으나 이들의 장기적 추세에는 유의미한 변화가 나타나지 않았다. 반면, ‘자격과 책임’ 유형은 비율의 수준과 추세 모두에서 유의미한 변화가 나타나지 않았다","As the number of refugees increases worldwide, the Republic of Korea is required as a member of the international community to fulfill its responsibilities and obligations to accept refugees. Sufficient discussion, public support, and social consensus are needed to accommodate and support refugees. This study analyzed how public attitudes toward refugees shifted over time, focusing on the 2018 Jeju Yemeni refugee problems (about 550 Yemenis applied for refugee status on Jeju Island), where there was domestic controversy over the increase in the number of refugees. The subject of analysis is 359,984 comments on 13,481 articles from major daily newspapers and news broadcasters in 2015–2021. Analysis of individual comments was conducted through sentiment analysis using natural language processing (NLP). For this, KcBERT, a Korean Bidirectional Encoder Representations from Transformers (BERT) model, was used. In addition, an interrupted time series analysis was performed to understand the influence relationship. The analysis showed that the proportion of negative public opinion on refugees was high both before and after the Jeju Yemen refugee crisis; negative public opinion increased overwhelmingly; and the reasons for this were more diverse. In addition, negative public opinion on the positive and negative ratios immediately increased as a result of the impact analysis but this did not affect the overall trend. We draw theoretical and practical implications based on these results."
딥러닝 기반의 문서요약기법을 활용한 뉴스 추천,2022,"['BART', 'BERT', 'Document-Summarization', 'Recommendation', 'RNN', 'Seq2Seq']",최근 스마트폰 또는 타블렛 PC와 같은 스마트기기가 정보의 창구 역할을 하게 되면서 다수의 사용자가 웹포털을통해 웹 뉴스를 소비하는 것이 더욱 중요해졌다. 하지만 인터넷 상에 생성되는 뉴스의 양을 사용자들이 따라가기 힘들며중복되고 반복되는 폭발하는 뉴스 기사에 오히려 혼란을 야기 시킬 수도 있다. 본 논문에서는 뉴스 포털에서 사용자의질의로부터 검색된 뉴스후보들 중 KoBART 기반의 문서요약 기술을 활용한 뉴스 추천 시스템을 제안한다. 실험을 통해서 새롭게 수집된 뉴스 데이터를 기반으로 학습한 KoBART의 성능이 사전훈련보다 더욱 우수한 결과를 보여주었으며KoBART로부터 생성된 요약문을 환용하여 사용자에게 효과적으로 뉴스를 추천하였다.,"Recently smart device(such as smart phone and tablet PC) become a role as an information gateway, using of the web news by multiple users from the web portal has been more important things.However, the quantity of creating web news on the web makes hard to catch the information which the user wants and confuse the users cause of the similar and repeated contents. In this paper, we propose the news recommend system using the document summarization based on KoBART which gives the selected news to users from the candidate news on the news portal. As a result, our proposed system shows higher performance and recommending the news efficiently by pre-training and fine-tuning the KoBART using collected news data."
딥러닝을 활용한 고객 경험 기반 상품 평가 변화 예측 방법론,2022,"['Deep learning', 'BERT', 'Review Analysis', 'Future Prediction', 'Text Classification', 'Customer Experience']",,"From the past to the present, reviews have had much influence on consumers' purchasing decisions. Companies are making various efforts, such as introducing a review incentive system to increase the number of reviews. Recently, as various types of reviews can be left, reviews have begun to be recognized as interesting new content. This way, reviews have become essential in creating loyal customers. Therefore, research and utilization of reviews are being actively conducted. Some studies analyze reviews to discover customers' needs, studies that upgrade recommendation systems using reviews, and studies that analyze consumers' emotions and attitudes through reviews. However, research that predicts the future using reviews is insufficient. This study used a dataset consisting of two reviews written in pairs with differences in usage periods. In this study, the direction of consumer product evaluation is predicted using KoBERT, which shows excellent performance in Text Deep Learning. We used 7,233 reviews collected to demonstrate the excellence of the proposed model. As a result, the proposed model using the review text and the star rating showed excellent performance compared to the baseline that follows the majority voting."
Prolonged QT Interval in Cirrhosis: Twisting Time?,2022,"['Acquired long QT syndrome', 'Torsade de pointes', 'Cirrhosis', 'Drug interaction', 'Ven\x1f-tricular repolarization']",,"Approximately 30% to 70% of patients with cirrhosis have QT interval prolongation. In patients without cirrhosis, QT prolongation is associated with an increased risk of ventricular arrhythmias, such as torsade de pointes (TdP). In cirrhotic patients, there is likely a significant association between the corrected QT (QTc) interval and the severity of liver disease, and possibly with increased mortality. We present a stepwise overview of the pathophysiology and management of acquired long QT syndrome in cirrhosis. The QT interval is mainly determined by ventricular repolarization. To compare the QT interval in time it should be corrected for heart rate (QTc), preferably by the Fridericia method. A QTc interval >450 ms in males and >470 ms in females is considered prolonged. The pathophysiological mechanism remains incompletely understood, but may include metabolic, autonomic or hormonal imbalances, cirrhotic heart failure and/or genetic predisposition. Additional external risk factors for QTc prolongation include medication (IKr blockade and altered cytochrome P450 activity), bradycardia, electrolyte abnormalities, underlying cardiomyopathy and acute illness. In patients with cirrhosis, multiple hits and cardiac-hepatic interactions are often required to sufficiently erode the repolarization reserve before long QT syndrome and TdP can occur. While some risk factors are unavoidable, overall risk can be mitigated by electrocardiogram monitoring and avoiding drug interactions and electrolyte and acid-base disturbances. In cirrhotic patients with prolonged QTc interval, a joint effort by cardiologists and hepatologists may be useful and significantly improve the clinical course and outcome. (Gut Liver 2022;16:849-860)"
Elite Polarization in South Korea: Evidence from a Natural Language Processing Model,2022,"['political polarization', 'elite polarization', 'natural language processing', 'BERT', 'text classification', 'South Korea']",,This study analyzes political polarization among the South Korean elite by examining 17 years’ worth of subcommittee meeting minutes from the South Korean National Assembly's standing committees. Its analysis applies various natural language processing techniques and the bidirectional encoder representations from the transformers model to measure and analyze polarization in the language used during these meetings. Its findings indicate that the degree of political polarization increased and decreased at various times over the study period but has risen sharply since the second half of 2016 and remained high throughout 2020. This result suggests that partisan political gaps between members of the South Korean National Assembly increase substantially.
Analysis of International Research Trends on Metaverse,2022,"['Metaverse', 'Topic Modeling', 'Bidirectional Encoder Representations from Transformers(BERT)', 'Research Trend', 'Big Data']",,"This study attempted to explore the realization and research direction of a successful metaverse environment in the future by analyzing international research trends of the metaverse using topic modeling. A total of 208 papers among WoS and ScienceDirect papers using metaverse as keywords were selected, and quantitative frequency analysis and topic modeling were performed. As a result, it was confirmed that research has rapidly increased after 2022. The main keywords of the research topics were ‘second’, ‘life’, ‘learning’, ‘reality’, ‘metaverse’, ‘virtual’, ‘blockchain’, ‘nft’, ‘medical’, ‘avatar’, etc. The topic keywords ‘Second life & Education’ and ‘Virtual Reality & Medical’ accounted for a large proportion of 57%, followed by ‘Blockchain & Cryptocurrency’, ‘Avatar & Interaction’, and ‘Sensing and Device’. As a result of semantic analysis, current metaverse research is focused on application and utilization, and research on underlying technologies and devices is also active. Therefore, it is necessary to identify the commonalities and differences between domestic and foreign studies, and to study the application method considering the domestic environment. In addition, new jurisprudence research is more necessary along with predicting new problems. It is expected that the results of study will provide the right research direction for domestic researchers in the era of digital transformation and contribute to the realization of a digital society."
반려동물 질병 진단 보조를 위한 딥러닝 프레임워크,2022,"['Animal Healthcare', 'Image Classification', 'Swin-Transformer', 'Chatbot', 'Natural Language Processing', 'BERT']",,"To date, the number of people who have companion animals has gradually increased and the need for advancement in veterinary care and pet health care has been increased. Deep learning models are taking their places in healthcare and can be used for detecting diseases. We aimed to build and validate a framework for auxiliary diagnosis of pet diseases in everyday life before hospital visits. Our framework utilizes disease image classification and natural language models with Swin-Transformer and Bidirectional Encoder Representations from Transformers as the backbone, respectively, and both presented the accuracy of 84.5% and 84%, respectively. This proposed framework can be useful in understanding animals’ symptoms for pet owners as well as assisting a veterinarian for diagnosis."
딥러닝기반 텍스트 분석을 통한 직업분류시스템 구축에 관한 연구,2022,"['기업가정신', '기업가정신교육', '창업의지', '성별 조절효과', 'Deep Learning', 'Machine Learning', 'LSTM', 'BERT', 'Online Job Posting', 'job classification']","본 연구의 목적은 온라인구인공고 텍스트 데이터를 활용하여 해당 일자리의 직종을 판별하는 분류모델을 생성해 평가하는 것이다. 워크넷 온라인구인공고(OJPs) 텍스트 자료에 딥러닝 기계학습 기법을 적용하여 자동으로 직업을 판별하는 것이다. 텍스트 자동 분류를 위한 기계학습 기법이 규칙기반 모델에서 인공신경망 모델로 전환하는 연구 흐름을 반영하고, 대규모의 온라인구인공고 자료와 텍스트의 문맥적 의미를 잘 다룰 수 있다는 점을 고려하여, 인공신경망의 최신 모델인 Bi-LSTM과 KoBERT 모델을 적용하였다. 1999-2021년 간의 워크넷 구인공고 데이터 800만 개에 모델을 적용한 결과, 0.62-0.82 정도의 매칭 정확도를 달성했다. 특히, 직무 기술(job description)이 특수하고 정확한 전문직에서 높은 정확도를 달성했다.","The purpose of this study is to create a classification model that can identify the type of job by using online job posting text data and evaluate the performance of the model. By applying the latest deep learning machine learning method to Work-Net online job postings(OJPs) text data, it is to automatically determine the occupat ional code of the OJPs. Considering the research trends shifting from a rule-based model to an artificial neural network model. and the merit of handling large-scale online job posting materials and the contextual meaning of text well. the latest models of artificial neural networks. Bi-LSTM and KoBERT models, were applied. As a result of applying the model to 8 million text data of employment insurance Work-Net job posting data from 1999 to 2001 . matching accuracy of 0.62 to 0.82 was achieved. The result is not very high performance. but it is generally judged to be a model that can determine the occupation. In particular. high accuracy was achieved in professions where job descriptions were specific and precise. Although it is not yet perfect for practical use, it is expected that the performance of the automatic occupational classification system will improve in the future when recruitment practices into the job-type labor market change and more precise data pre-processing and model applications are made."
Establishment of the Heart and Brain Team for Patent Foramen Ovale Closure in Stroke Patients: An Expert Opinion,2022,"['Stroke', 'Patent foramen ovale', 'Clinical decision-making', 'Secondary prevention']",,"The online 2021 Asian-Pacific Heart and Brain Summit was organized to present and discuss experiences within leading Asian-Pacific centers with regard to institutional heart and brain teams managing the diagnosis, treatment, and follow-up of cryptogenic stroke (CS) patients with patent foramen ovale (PFO). This manuscript presents a narrative review of presentations and discussions during the summit meeting. Percutaneous PFO closure is an established therapy for CS patients in whom PFO is considered to be causal. Guidelines and consensus statements emphasize the importance of multidisciplinary clinical decision-making regarding PFO closure with the involvement of several clinical specialties, including neurology, cardiology, and hematology. It is also recommended that the patient be closely involved in this process. The heart and brain team is a collaborative platform that facilitates such a multidisciplinary decision-making process and patient involvement. It also creates opportunities for education and evaluation of the healthcare provided to patients with CS. This review provides insights into the implementation, composition, organization, and operation of a heart and brain team. Methods and metrics are suggested to evaluate the team’s role. We suggest that an efficient heart and brain team can implement guideline-recommended multidisciplinary clinical decision-making with regard to PFO closure in CS patients and play an important role in the management of these patients."
Self-Attention 시각화를 사용한 기계번역 서비스의 번역 오류 요인 설명,2022,"['Self-Attention', 'Machine translation', 'XAI', 'exBERT', 'Visualization']",,"This study analyzed the translation error factors of machine translation services such as Naver Papago and Google Translate through Self-Attention path visualization. Self-Attention is a key method of the Transformer and BERT NLP models and recently widely used in machine translation. We propose a method to explain translation error factors of machine translation algorithms by comparison the Self-Attention paths between ST(source text) and ST'(transformed ST) of which meaning is not changed, but the translation output is more accurate. Through this method, it is possible to gain explainability to analyze a machine translation algorithm's inside process, which is invisible like a black box. In our experiment, it was possible to explore the factors that caused translation errors by analyzing the difference in key word's attention path. The study used the XLM-RoBERTa multilingual NLP model provided by exBERT for Self-Attention visualization, and it was applied to two examples of Korean-Chinese and Korean-English translations."
Attention-CNN을 이용한 유도탄 비행데이터의 이상성 검출 기법 연구,2022,"['Monte-Carlo simulation', 'anomaly detection', 'transformer', 'attention score', 'convolutional neural networks', 'data analytics', '.']",,.
재난 사고 데이터 기반 위험 예측 기술 연구,2022,"['사회재난', '사고 사례', '인공지능', '하인리히 법칙', '위험 예측', 'Social disaster', 'Incident cases', 'AI', 'Heinrich’s law', 'Risk prediction']",,"Although we can not control the occurrences of Natural Disaster, we can reduce the occurrences of the social disaster by the safety control and preventions activities. Using Heinrich’s law, we can predict the possible disasters from the news data. In this study, we construct the incident and disaster case DB. AI models such as BERT and MLP, learn the characteristics of the DB. and then predict the risk occurrence possibilities and damage scale from the extracted news data of the news data. We design the operation platform with the DB, AI model and real-time news monitoring from the web."
Deep Learning Models Based on Subword Tokenization for Vulnerability Detection of Source Code,2022,"['취약점 탐지', '소스코드', '서브워드 토큰화', '딥러닝 모델', '1차원 컨볼루션 모델', 'Vulnerability detection', 'Source code', 'Subword tokenization', 'Deep learning model', 'One-dimensional convolution model']",,"The study of vulnerability detection in source code has been attracting attention in practice and academia because web applications can be vulnerable to attacks from the outside due to the open access characteristics. This study aims to build deep learning models and evaluate their performances for the field of source code vulnerability detection. The proposed deep learning models tackle class imbalance problem, long-term dependency problem, and out-of-vocabulary problem which are challenging problems in detecting source code vulnerabilities. As an experiment result, the precision of the subword tokenization-based one-dimensional convolution model showed 39%, which is about 20 times higher than the expected precision of 1.92% of the model predicted by chance. Although Conv1d+BT model using the BERT tokenizer showed the highest AUC value of 0.9116, the precision and recall of this model were 0.39 and 0.35, so it is judged that further improvement is needed for practical application."
BERTopic을 활용한 불면증 소셜 데이터 토픽 모델링 및 불면증 경향 문헌 딥러닝 자동분류 모델 구축,2022,[],,"Insomnia is a chronic disease in modern society, with the number of new patients increasing by more than 20% in the last 5 years. Insomnia is a serious disease that requires diagnosis and treatment because the individual and social problems that occur when there is a lack of sleep are serious and the triggers of insomnia are complex. This study collected 5,699 data from 'insomnia', a community on 'Reddit', a social media that freely expresses opinions. Based on the International Classification of Sleep Disorders ICSD-3 standard and the guidelines with the help of experts, the insomnia corpus was constructed by tagging them as insomnia tendency documents and non-insomnia tendency documents. Five deep learning language models (BERT, RoBERTa, ALBERT, ELECTRA, XLNet) were trained using the constructed insomnia corpus as training data. As a result of performance evaluation, RoBERTa showed the highest performance with an accuracy of 81.33%. In order to in-depth analysis of insomnia social data, topic modeling was performed using the newly emerged BERTopic method by supplementing the weaknesses of LDA, which is widely used in the past. As a result of the analysis, 8 subject groups ('Negative emotions', 'Advice and help and gratitude', 'Insomnia-related diseases', 'Sleeping pills', 'Exercise and eating habits', 'Physical characteristics', 'Activity characteristics', 'Environmental characteristics') could be confirmed. Users expressed negative emotions and sought help and advice from the Reddit insomnia community. In addition, they mentioned diseases related to insomnia, shared discourse on the use of sleeping pills, and expressed interest in exercise and eating habits. As insomnia-related characteristics, we found physical characteristics such as breathing, pregnancy, and heart, active characteristics such as zombies, hypnic jerk, and groggy, and environmental characteristics such as sunlight, blankets, temperature, and naps."
Biaffine Average Attention 모델을 이용한 의미역 결정,2022,[],,"Semantic role labeling task(SRL) is to extract predicate and arguments such as agent, patient, place, time. In the previously SRL task studies, a pipeline method extracting linguistic features of sentence has been proposed, but in this method, errors of each extraction work in the pipeline affect semantic role labeling performance. Therefore, methods using End-to-End neural network model have recently been proposed. In this paper, we propose a neural network model using the Biaffine Average Attention model for SRL task. The proposed model consists of a structure that can focus on the entire sentence information regardless of the distance between the predicate in the sentence and the arguments, instead of LSTM model that uses the surrounding information for prediction of a specific token proposed in the previous studies. For evaluation, we used F1 scores to compare two models based BERT model that proposed in existing studies using F1 scores, and found that 76.21% performance was higher than comparison models."
Aspect-based Sentiment Analysis of Product Reviews using Multi-agent Deep Reinforcement Learning,2022,"['Sentiment Analysis', 'Deep Reinforcement Learning', 'Product Review', 'Artificial Intelligence', 'Machine Learning']",,"The existing model for sentiment analysis of product reviews learned from past data and new data was labeled based on training. But new data was never used by the existing system for making a decision. The proposed Aspect-based multi-agent Deep Reinforcement learning Sentiment Analysis (ADRSA) model learned from its very first data without the help of any training dataset and labeled a sentence with aspect category and sentiment polarity. It keeps on learning from the new data and updates its knowledge for improving its intelligence. The decision of the proposed system changed over time based on the new data. So, the accuracy of the sentiment analysis using deep reinforcement learning was improved over supervised learning and unsupervised learning methods. Hence, the sentiments of premium customers on a particular site can be explored to other customers effectively. A dynamic environment with a strong knowledge base can help the system to remember the sentences and usage State Action Reward State Action (SARSA) algorithm with Bidirectional Encoder Representations from Transformers (BERT) model improved the performance of the proposed system in terms of accuracy when compared to the state of art methods."
Various Techniques of Sentence Embedding using Question-Answering in Game Play,2022,"['Question-Answering', 'Sentence Embedding', 'Game Play']",,"Automatic question-answering is a classical problem in natural language processing, which aims at designing systems that can automatically answer a question, in the same way as human does. The need to query information content available in various formats including structured and unstructured data has become increasingly important. Thus, Question Answering Systems (QAS) are essential to satisfy this need. QAS aim at satisfying users who are looking to answer a specific question in natural language.Moreover, it is a representative of open domain QA systems, where the answer selection process leans on syntactic and semantic similarities between the question and the answering text snippets. Such approach is specifically oriented to languages with fine grained syntactic and morphologic features that help to guide the correct QA match. Furthermore, word and sentence embedding have become an essential part of any Deep-Learning-based natural language processing systems as they encode words and sentences in fixed-length dense vectors to drastically improve the processing of textual data. The paper will concentrate on incorporating the sentence embedding with its various techniques like Infersent, ElMo and BERT in the construction of Question Answering systems, and also it can be used in game play."
독일어 대화 자동요약을 위한 핵심 발화쌍 추출 연구,2022,"['대화 요약', '핵심 발화쌍 추출', '대화 구조', '인접쌍의 확장', '기계학습', 'Gesprächszusammenfassung', 'Extraktion von Schlüsseläußerungspaaren', 'Gesprächsstrukur', 'Adjacent pair expansion', 'maschinelles Lernen']","본 논문은 독일어 대화 요약을 위하여 대화에서 등장하는 핵심 표현을 대화 구조를 바탕으로 추출하는기법을 다룬다. 기존 연구와 달리 본 연구에서 제안하는 방법론은 화자들이 각자 하나씩 발화하였고 인접화행쌍에 속하는 두 발화로 이루어진 발화쌍을 대화 요약을 위한 핵심 표현의 후보로 삼는다. 이러한 조건을만족하는 후보 발화쌍 중에서 인접쌍의 확장 유형에의 포함 여부, 발화의 등장 위치, BERT 임베딩 기반의의미적 대표성 등의 언어학적 자질을 사용하여 핵심 발화쌍을 추출한다. 본 연구에서 제안한 추출 기법을검증하기 위해 독일어 대화 코퍼스를 대상으로 핵심 발화쌍을 자동 추출하는 기계학습 실험을 수행하였다.나이브 베이즈, 랜덤 포레스트 등의 다양한 기계학습 알고리즘을 적용하였으며, 핵심 발화쌍 자동 추출 정확도는 평균 91.83%로 나타났다. 제안하는 언어학적 자질을 평가하기 위해 문장의 의미만을 고려한 딥러닝기반의 Textrank 모델과 비교한 결과 본 논문에서 제안하는 모델이 약 20%p 더 높은 정확도를 보였다.","In der vorliegenden Arbeit wird eine Methode zur Extraktion von Schlüsselinformationen und linguistischen Merkmalen für die automatische Zusammenfassung eines deutschen Gesprächs vorgestellt. Um ein Gespräch zusammenzufassen, ist es wichtig herauszufinden, welche der im Gespräch vorkommenden Äußerungen die wichtigsten sind. Meistens stützen sich die vorherigen Studien dafür auf lexikalische Informationen des Gesprächs. Für die Gesprächszusammenfassungen ist es aber auch wichtig, die Interaktionen zwischen Sprechern im Gespräch zu verstehen.Unsere automatische Extraktionsmethode unterscheidet sich von vorherigen Studien in zwei Punkten. Erstens, wir extrahieren die Schlüsselausdrücke nicht direkt aus dem Gespräch, sondern aus den Interaktionseinheiten, in denen ein Gespräch segmentiert ist. Zweitens, die extrahierten Schlüsselausdrücke sind immer Äußerungspaare, die aus zwei Äußerungen, die von zwei Sprechern gemacht werden, bestehen. Nach der Auswahl von Kandidaten-Äußerungspaaren, die diese Bedingungen erfüllen, werden Schlüsseläußerungspaare mithilfe linguistischer Merkmale extrahiert.Ein Experiment mittels maschinellem Lernens wurde durchgeführt, um die vorgeschlagene Extraktionsmethode zu überprüfen. Verschiedene maschinelle Lernalgorithmen wie Naive Bayes oder Random Forest wurden angewendet. Unser Modell zeigte eine durchschnittliche Korrektheit von 91,83% bei der automatischen Extraktion der Schlüsseläußerungspaare. Das Modell unserer vorgeschlagenen Methode hat eine um etwa 20%p höhere Korrektheit als das vorhandene Textrank-basierte Modell."
전화금융사기 수사 정보 자동 추출 연구: 자연어처리 딥러닝 모델을 중심으로,2022,"['전화금융사기', '보이스피싱', '수사기법', '범죄 예방', '자연어처리', '딥러닝', 'voice-phising', 'phone scam', 'investigative technique', 'natural language processing', 'deep learning']","전화금융사기(보이스피싱)는 개인의 금전적 손실뿐만 아니라, 금융 인프라 전반에 대한 신뢰를 손상시켜 불필요한 사회적 비용을 낳는 심각한 범죄다. 최근 한국 정보통신기술의 성장과 코로나 19 예방을 위한 비대면 활동 정책이 맞물려 보이스피싱 범죄의 피해 규모와 범위가 급격히 증가하 고 있어, 혁신적인 수사기법을 활용한 효율적인 범죄 예방⋅수사 활동이 그 어느 때 보다 시급한 시점이라 하겠다. 본 연구는 과거 보이스피싱 사건 수사관들이 직접 기록을 읽고 수기로 작성했던 14종의 사건정보(범행 수법, 피해 금액, 범행 시 사용 전화번호, 사칭 기관, 사칭 인물, 특이사항, SNS유형, 사건접수 번호, 사건접수 관서, 피해자 성명, 피해자 전화번호, 피해자 주민번호, 피해자 직업, 피해자 주거지)를 자연어처리 딥러닝 모델 개발을 통해 자동 인식⋅추출하고자 했다. 광학 문자 인식을 포함한 전처리(Pre-processing) 작업 후 자연어처리 기술인 정규표현식(Regular Expression)을 사용해 12종의 사건정보를 선-추출하였으며, 한국해양대학교의 개체명 인식 데이 터(n=23,962)를 활용하여 학습한 딥러닝 모델을 통해 2종의 사건정보(사칭기관, 사칭인물)를 후-추출하였다. 서울경찰청이 제공한 실제 사건 수사자료(n=100)에 완성된 자연어처리 모델을 적용하여 테스트한 결과 평균 85∼90%에 달하는 추출 정확도를 확인할 수 있었다. 본 연구에서 제 안된 자연어처리 딥러닝 모델과 연구방법 모형이 전화금융사기 범죄자의 신속한 검거와 범부처적 대응방안 수립에 폭넓게 활용되기를 기대한다.","A telephone scam is a serious crime that not only causes financial damage to an individual but also raises a question about the credibility of financial infrastructure. This study attempts to extract 14 types of case information (i.e., pretended person, pretended institution, amount of financial damage, phone number used in crime, police station in charge of the investigation, case reception date, SNS type, victim’s name, victim’s phone number, victim’s occupation, and victim’s residence, etc) through deep learning method in the context of Natural Language Processing(NLP). Specifically, 12 types of information are pre-extracted using regular expression and customized dictionary method, and the other two types of information are extracted through the KO-BERT deep learning model. As a result of applying the NLP model to victim testimony records(n=100), which is provided by the Seoul Metropolitan Police Agency, the extraction accuracy of an average of 80-90% is confirmed. Although there is still room for improvement in terms of the accuracy of the NLP model, the outcome of this study is expected to facilitate an effective crime prevention policy and  further  promote  rapid  information  sharing  among  criminal  justice  agencies."
적대적 사례에 기반한 언어 모형의 한국어 격 교체 이해 능력 평가,2022,"['adversarial examples', 'case alternation', 'deep learning', 'intended noise', 'robustness', 'language model', 'evaluation', '적대적 사례', '격 교체', '딥러닝', '고의적 잡음', '견고성', '언어 모형', '평가']",,"In the field of deep learning-based language understanding, adversarial examples refer to deliberately constructed examples of data, slightly different from original examples. The contrasts between the original and adversarial examples are less perceivable to human readers, but the disruption has a notorious effect on the performance of machines. Thus, adversarial examples facilitate assessing whether and how a specific deep learning architecture (e.g., a language model) robustly works. Out of the multiple layers of linguistic structures, this study lays focus on a morphosyntactic phenomenon in Korean, namely, case alternation. We created a set of adversarial examples regarding case alternation, and then tested the morpho-syntactic ability of neural language models. We extracted the instances of case alternation from the Sejong Electronic Dictionary, and made use of mBERT and KR-BERT as the language models. The results (measured by means of surprisal) indicate that the language models are unexpectedly good at discerning case alternation in Korean. In addition, it turns out that the Korean-specific language model performs better than the multilingual model. These imply that an in-depth knowledge of linguistics is essential for creating adversarial examples in Korean."
