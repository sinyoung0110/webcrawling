title,date,keywords,abstract,multilingual_abstract
음향 이벤트 탐지를 위한 멀티-스케일 특징 기반 트랜스포머 모델,2022,"['sound event detection', 'transformer encoder', 'feature-pyramid', 'convolutional neural network', 'attention model', '.']","본 연구에서는 소리 탐지를 위해 멀티-스케일 특징을 활용하는 방법을 제안하였다. 이를 위해 소리 신호의 시계열 상관관계 모델링에 있어서 기존의 RNN(Recurrent Neural Network)에 비해서 우수한 성능을 보인 트랜스포머-인코더 기반의 심층신경망 구조에 특징-피라미드 기법을 적용하였다. 제안된 방법인 멀티-스케일 특징을 사용함으로써, 기존의 심층신경망 모델보다 클래스별 다양한 소리 신호의 길이 변화에 더욱 강인해질 수 있다. 본 연구에서 제안된 방법을 DCASE 2019 Task 4 데이터셋에 대해 실험하고 평가하였으며, 멀티-스케일 특징을 사용하지 않은 기존의 심층신경망 모델에 비해 상대적 개선도가 5.4% 더 우수함을 확인할 수 있었다.",We propose a method that utilizes multi-scale features for sound event detection. We employed a feature-pyramid component in a deep neural network architecture based on the transformer encoder that is used to model the time correlation of sound signals owing to its superiority over conventional recurrent neural networks. The proposed method is motivated by the idea that the multi-scale features will make the network more robust against the dynamic duration of the sound signals depending on their classes. We conducted experiments using the DCASE 2019 Task 4 dataset to evaluate the performance of the proposed method. The experimental results show that the proposed method outperforms the baseline neural network without multi-scale features.
Sequence dicriminative training 기법을 사용한 트랜스포머 기반 음향 모델 성능 향상,2022,[],국문 초록 정보 없음,"In this paper, we adopt a transformer that shows remarkable performance in natural language processing as an acoustic model of hybrid speech recognition. The transformer acoustic model uses attention structures to process sequential data and shows high performance with low computational cost. This paper proposes a method to improve the performance of transformer AM by applying each of the four algorithms of sequence discriminative training, a weighted finite-state transducer (wFST)-based learning used in the existing DNN-HMM model. In addition, compared to the Cross Entropy (CE) learning method, sequence discriminative method shows 5 % of the relative Word Error Rate (WER)."
임상 피부영상의 초해상화를 위한 트랜스포머 기반 심층학습 모델,2022,"['Deep Learning', 'Super resolution', 'Transformer', 'Skin image', 'Medical AI', '딥러닝', '초해상화', '트랜스포머', '피부 영상', '의료 인공지능']","피부질환 진단을 위한 딥러닝 모델은 주로 고해상도 더모스코프 영상에 기반하여 발전되어 온 반면, 저해상도 임상피부영상도 저가형 인공지능 기반 의료기기에 활용될만한 가치가 있다. 트랜스포머는 영역간 유사한 패턴을 보이는 피부 텍스처의 복원에 적합한 반면, 최근 제안된 트랜스포머 기반 초해상화 모델인 ESRT는 임상피부영상의 초해상화에 만족할만한 지각적 성능을 보이지 않는다. 본 논문에서는 저해상도 임상피부영상에서 더모스코프 수준의 고해상도 피부영상을 복원하기 위한 ESRT의 확장 모델을 제안한다. 제안된 모델은 ESRT를 백본으로 하되 트랜스포머 백본과 합성곱 신경망 백본을 기존의 직렬 대신 병렬 구조로 재구성하여 피부 병변 텍스쳐의 특징을 효율적으로 학습한다. 또한 지각적 손실함수를 적용하여 미세한 텍스쳐의 질감도와 지각적 품질을 향상시킬 수 있다. 다른 대표적인 초해상화 모델과의 성능비교 실험결과, 제안된 모델이 초해상화 피부영상의 지각적 품질을 향상시키는데 효과적임을 보여준다.","Deep learning models for skin cancer detection have made a great progress based on high-resolution dermoscopic images, while low-resolution clinical skin images are also valuable to be exploited for low-cost AI-based medical devices. Transformer may be appropriate for reconstructing skin textures where a similar pattern can be easily found even between distant regions, but the efficient super resolution transformer (ESRT) model does not exhibit acceptable perceptual quality in super resolution of clinical skin images. In this paper, we propose an extension of ESRT as a super resolution approach to reconstructing dermoscopy-level high-resolution skin images from low-resolution clinical skin images. In the propose model, the transformer backbone and the convolutional neural network backbone are connected in parallel rather than in series by changing the existing architecture to learn the features of skin lesion texture efficiently. Moreover, the perceptual loss is employed to enhance the perceptual quality of fine textures. We show that the proposed model effectively improves the perceptual quality of super resolution skin images compared to other existing models."
멀티모달 패션 추천 대화 시스템을 위한 개선된 트랜스포머 모델,2022,"['대화시스템', '트랜스포머', '멀티모달', '자연어처리', '인공지능', 'Dialogue System', 'Transformer', 'MultiModal', 'NLP', 'AI']","최근 챗봇이 다양한 분야에 적용되어 좋은 성과를 보이면서 쇼핑몰 상품 추천 서비스에도 챗봇을 활용하려는 시도가 많은 이커머스 플랫폼에서 진행되고 있다. 본 논문에서는 사용자와 시스템간의 대화와 패션 이미지 정보에 기반해 사용자가 원하는 패션을 추천하는 챗봇 대화시스템을 위해, 최근 자연어처리, 음성인식, 이미지 인식 등의 다양한 AI 분야에서 좋은 성능을 내고 있는 트랜스포머 모델에 대화 (텍스트) 와 패션 (이미지) 정보를 같이 사용하여 추천의 정확도를 높일 수 있도록 개선한 멀티모달 기반 개선된 트랜스포머 모델을 제안하며, 데이터 전처리(Data preprocessing) 및 학습 데이터 표현(Data Representation)에 대한 분석을 진행하여 데이터 개선을 통한 정확도 향상 방법도 제안한다. 제안 시스템은 추천 정확도는 0.6563 WKT(Weighted Kendall’s tau)으로 기존 시스템의 0.3372 WKT를 0.3191 WKT 이상 크게 향상시켰다.","Recently, chatbots have been applied in various fields and have shown good results, and many attempts to use chatbots in shopping mall product recommendation services are being conducted on e-commerce platforms. In this paper, for a conversation system that recommends a fashion that a user wants based on conversation between the user and the system and fashion image information, a transformer model that is currently performing well in various AI fields such as natural language processing, voice recognition, and image recognition. We propose a multimodal-based improved transformer model that is improved to increase the accuracy of recommendation by using dialogue (text) and fashion (image) information together for data preprocessing and data representation. We also propose a method to improve accuracy through data improvement by analyzing the data. The proposed system has a recommendation accuracy score of 0.6563 WKT (Weighted Kendall’s tau), which significantly improved the existing system’s 0.3372 WKT by 0.3191 WKT or more."
객체 탐지 과업에서의 트랜스포머 기반 모델의 특장점 분석 연구,2022,"['Object Detection', 'Transformer', 'Inductive Bias', 'Computer Vision', 'Deep Learning']",국문 초록 정보 없음,"Transformers are the most famous deep learning models that has achieved great success in natural language processing and also showed good performance on computer vision. In this survey, we categorized transformer-based models for computer vision, particularly object detection tasks and perform comprehensive comparative experiments to understand the characteristics of each model. Next, we evaluated the models subdivided into standard transformer, with key point attention, and adding attention with coordinates by performance comparison in terms of object detection accuracy and real-time performance. For performance comparison, we used two metrics: frame per second (FPS) and mean average precision (mAP). Finally, we confirmed the trends and relationships related to the detection and real-time performance of objects in several transformer models using various experiments."
ECG 잡음에 강건한 트랜스포머 기반 부정맥 감지 모델,2022,"['심전도', '부정맥', '트랜스포머', '데이터 잡음', '강건', 'Electrocardiogram', 'Arrhythmia', 'Transformer', 'Data noise', 'Robustness']","머신러닝 기반의 심전도 신호를 기반으로 한 심장 부정맥의 진단은 정제된 데이터에서는 효과적이지만 잡음이 섞인 실제 심전도 신호에서 부정맥을 판별하는 데에 일관된 성능을 보이지 못하는 한계가 있다. 본 논문에서는 시계열 신호 패턴 추출에서 우수한 성능을 보이는 트랜스포머를 기반으로 잡음에 강건한 심전도 신호 부정맥 검출 모델을 제안한다. 제안 방법을 검증하기 위하여 3가지 다른 종류의 잡음을 포함한 MIT-BIH 부정맥 데이터셋에서 실험을 진행하였고, 기존 모델에 비하여 정제된 데이터와 잡음 데이터 모두에서 높은 성능을 보이는 것을 확인하였다.","Diagnosis of cardiac arrhythmia based on machine learning-based ECG signals is effective in refined data, but there are limitations in which it does not show consistent performance in discriminating arrhythmia from actual ECG signals mixed with noise. In this paper, we propose an ECG signal arrhythmia detection model robust to noise based on transformers showing excellent performance in time series signal pattern extraction. To verify the proposed method, experiments were conducted on MIT-BIH arrhythmia datasets including three different types of noise, and it was confirmed that both refined and noise data showed higher performance than existing models."
미세먼지 예측 성능 개선을 위한 시공간 트랜스포머 모델의 적용,2022,"['시계열 데이터 분석', '미세먼지 농도 예측', '어텐션 기법', '시공간 트랜스포머', '타임투벡터 임베딩', 'Time series data analysis', 'PM concentration prediction', 'Attention mechanism', 'Spatiotemporal transformer', 'Time to Vector']",국문 초록 정보 없음,"It is reported that particulate matter(PM) penetrates the lungs and blood vessels and causes various heart diseases and respiratory diseases such as lung cancer. The subway is a means of transportation used by an average of 10 million people a day, and although it is important to create a clean and comfortable environment, the level of particulate matter pollution is shown to be high. It is because the subways run through an underground tunnel and the particulate matter trapped in the tunnel moves to the underground station due to the train wind. The Ministry of Environment and the Seoul Metropolitan Government are making various efforts to reduce PM concentration by establishing measures to improve air quality at underground stations. The smart air quality management system is a system that manages air quality in advance by collecting air quality data, analyzing and predicting the PM concentration. The prediction model of the PM concentration is an important component of this system. Various studies on time series data prediction are being conducted, but in relation to the PM prediction in subway stations, it is limited to statistical or recurrent neural network-based deep learning model researches. Therefore, in this study, we propose four transformer-based models including spatiotemporal transformers. As a result of performing PM concentration prediction experiments in the waiting rooms of subway stations in Seoul, it was confirmed that the performance of the transformer-based models was superior to that of the existing ARIMA, LSTM, and Seq2Seq models. Among the transformer-based models, the performance of the spatiotemporal transformers was the best. The smart air quality management system operated through data-based prediction becomes more effective and energy efficient as the accuracy of PM prediction improves. The results of this study are expected to contribute to the efficient operation of the smart air quality management system."
트랜스포머 모델을 활용한 질의 유형 분류와 질의응답 시스템,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
트랜스포머 기반 한국어 모델의 학습과 응용,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
복원오류를 활용한 시공간 트랜스포머 모델의 미세먼지 예측 성능 향상,2022,"['미세먼지 농도 예측', '시계열 데이터 분석', '복원오류', '시공간 트랜스포머']",국문 초록 정보 없음,다국어 초록 정보 없음
음성-텍스트 결합 감정인식을 위한 자가 지도학습 모델로부터의 특징 추출과 트랜스포머 기반 멀티모달 표현 학습,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
비전 트랜스포머 기반 디지털 병리 대장암 진단 모델,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
CNN과 비전 트랜스포머 기반 차선 검출 모델,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
자동차 신뢰성 예측을 위한 상호보완적 신뢰성 관점의 트랜스포머 기반 예측 모델 개발,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
상품 리뷰의 홍보성 유무 판단을 위한 트랜스포머 기반 인공지능 모델 구현,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
자동차 신뢰성 예측을 위한 상호보완적 신뢰성 관점의 트랜스포머 기반 예측 모델 개발,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
연속혈당 값 및 클래스 분류를 위한 트랜스포머 모델 개발,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
복숭아 나무 영상에서 과실 분할과 생장 모니터링을 위한 트랜스포머 모델,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
Transformer 언어 모델을 활용한 초중등 학습자 작문 연령 예측 모델 구현,2022,"['Deep-learning', 'Transformer Language Model', 'BERT', 'BART', 'Learners’ Writings', 'Writing Development', '딥러닝', 'Transformer 언어 모델', 'BERT', 'BART', '학습자 작문', '작문 발달']","본 연구에서는 Transformer 언어 모델, 그중에서도 한국어 BERT 및 BART 모델을 활용하여 초중등 학습자 작문의 연령대를 예측할 수 있는 연령 예측 모델을 구현하였다. 대규모의 코퍼스에 대하여 사전 학습한 KoBERT, KcBERT, KoBART 모델에 기반하여 국립국어원 ‘모두의 말뭉치’ 중 ‘국립국어원 비출판물 말뭉치(버전 1.1)’를 대상으로 미세 조정(fine-tuning)을 진행함으로써 학습자 작문 연령 예측 모델을 구현하였다. 결과적으로 예측 모델은 약 61.1%에서 68.1%의 정확도를 가지는 것으로 나타났으며, 그중에서도 KoBART에 기반을 둔 예측 모델이 가장 우수한 성능을 보였다. 특히, 작문의 장르를 고려하여 미세 조정을 진행한 경우 최대 70% 이상의 정확도를 보여 초중등 학습자 작문에 대한 연령 예측 모델 정교화의 가능성을 확인할 수 있었다. 본고는 학습자 작문 연령 예측 모델을 활용하여 연령대별로 전형적이라고 판단되는 학습자 작문들을 추출하였다. 이를 통해 학습자 작문의 발달 양상을 분석하는 것이 가능하며, 더 나아가서는 전형성과 대표성을 가지는 학습자 작문을 바탕으로 학습자 작문 평가 또는 작문 교수·학습 내용의 단서를 탐색할 수 있는 기반을 마련할 수 있을 것으로 기대된다.",다국어 초록 정보 없음
Transformer 기반의 Clustering CoaT 모델 설계,2022,[],최근 컴퓨터 비전 분야에서 Transformer를 도입한 연구가 활발히 연구되고 있다. 이 모델들은 Transformer의 구조를 거의 그대로 사용하기 때문에 확장성이 좋으며 large 스케일 학습에서 매우 우수한 성능을 보여주었다. 하지만 Transformer를 적용한 비전 모델은 inductive bias의 부족으로 학습 시 많은 데이터와 시간을 필요로 하였다. 그로 인하여 현재 많은 Vision Transformer 개선 모델들이 연구되고 있다. 본 논문에서도 Vision Transformer의 문제점을 개선한 Clustering CoaT 모델을 제안한다.,다국어 초록 정보 없음
BERT-Fused Transformer 모델에 기반한한국어 형태소 분석 기법,2022,"['Natural Language Processing', 'Morphological Analysis', 'Transfer Learning', 'Transformer', 'BERT-fused Model', '자연어처리', '형태소분석', '전이학습', 'Transformer', 'BERT-fused 모델']",국문 초록 정보 없음,"Morphemes are most primitive units in a language that lose their original meaning when segmented into smaller parts. In Korean,a sentence is a sequence of eojeols (words) separated by spaces. Each eojeol comprises one or more morphemes. Korean morphologicalanalysis (KMA) is to divide eojeols in a given Korean sentence into morpheme units. It also includes assigning appropriatepart-of-speech(POS) tags to the resulting morphemes. KMA is one of the most important tasks in Korean natural language processing(NLP). Improving the performance of KMA is closely related to increasing performance of Korean NLP tasks. Recent research on KMAhas begun to adopt the approach of machine translation (MT) models. MT is to convert a sequence (sentence) of units of one domaininto a sequence (sentence) of units of another domain. Neural machine translation (NMT) stands for the approaches of MT that exploitneural network models. From a perspective of MT, KMA is to transform an input sequence of units belonging to the eojeol domain intoa sequence of units in the morpheme domain. In this paper, we propose a deep learning model for KMA. The backbone of our modelis based on the BERT-fused model which was shown to achieve high performance on NMT. The BERT-fused model utilizes Transformer,a representative model employed by NMT, and BERT which is a language representation model that has enabled a significant advancein NLP. The experimental results show that our model achieves 98.24 F1-Score."
DeepLabV3+와 Swin Transformer 모델을 이용한 Sentinel-2 영상의 구름탐지,2022,"['Cloud detection', 'Deep learning', 'Sentinel-2']",국문 초록 정보 없음,"Sentinel-2 can be used as proxy data for the Korean Compact Advanced Satellite 500-4 (CAS500-4), also known as Agriculture and Forestry Satellite, in terms of spectral wavelengths and spatial resolution. This letter examined cloud detection for later use in the CAS500-4 based on deep learning technologies. DeepLabV3+, a traditional Convolutional Neural Network (CNN) model, and Shifted Windows (Swin) Transformer, a state-of-the-art (SOTA) Transformer model, were compared using 22,728 images provided by Radiant Earth Foundation (REF). Swin Transformer showed a better performance with a precision of 0.886 and a recall of 0.875, which is a balanced result, unbiased between over- and under-estimation. Deep learning-based cloud detection is expected to be a future operational module for CAS500-4 through optimization for the Korean Peninsula."
Transformer기반의 언어모델 Bert와 GPT-2 성능 비교 연구,2022,[],"최근 자연어처리 분야에서는 Bert, GPT 등 Transformer기반의 언어모델 연구가 활발히 이뤄지고 있다. 이러한 언어모델은 대용량의 말뭉치 데이터와 많은 파라미터를 이용하여 사전학습을 진행하여 다양한 자연어처리 테스트에서 높은 성능을 보여주고 있다. 이에 본 논문에서는 Transformer기반의 언어모델인 Bert와 GPT-2의 성능평가를 진행한다. 성능평가는 ‘네이버 영화 리뷰’ 데이터 셋을 통해 긍정 부정의 정확도와 학습시간을 측정한다. 측정결과 정확도에서는 GPT-2가 Bert보다 최소 4.16%에서 최대 5.32% 높은 정확도를 나타내었지만 학습시간에서는 Bert가 GPT-2보다 최소 104초에서 116초 빠르게 나타났다. 향후 성능 비교는 더 많은 데이터와 다양한 조건을 통해 구체적인 성능 비교가 필요하다.",다국어 초록 정보 없음
Transformer 기반의 한국어 언어 모델의 사회적 편향 분석,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
Transformer 기반의 한국어 언어 모델의 사회적 편향 분석,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
ImageNet Dataset 벤치마크에서 Transformer 기반 모델들의 비교와 분석,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
효과적인 사람 재식별을 위한 transformer 모델 글로벌-로컬 특징 강화,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
잠재 공간을 활용한 transformer 기반 문서 이미지 강화 모델,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
효과적인 사람 재식별을 위한 transformer 모델 글로벌-로컬 특징 강화,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
잠재 공간을 활용한 transformer 기반 문서 이미지 강화 모델,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
사람 자세 추정을 위한 CNN-Vision Transformer 기반 결합 모델,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
피부영상 털 제거를 위한 트랜스포머 기반 인페인팅 기법:기존 방법의 적용성 평가,2022,"['hair removal', 'deep learning', 'image inpainting', 'transformer', 'generative adversarial networks', '털 제거', '딥러닝', '영상 인페인팅', '트랜스포머', '생성적 적대 신경망']","피부영상에서 털 제거는 자동화된 피부진단을 위한 필수적인 전처리 과정이다. 기존의 털 제거 기법은 대부분 영상처리에 기반한 방식이었지만, 털이 많은 피부에는 성능이 크게 저하되는 문제점이 있다. 다른 접근방식으로서 심층학습 모델에 기반한 인페인팅 기법을 고려할 수 있는데, 털이 많은 피부영상 학습 데이터의 부족으로 인하여 심층학습 기반 털 제거 모델은 거의 보고된 바 없다. 본 논문에서는 피부영상에서 털 제거를 위한 영상 인페인팅 기법으로서 트랜스포머 기반 심층학습 모델인 BAT-Fill의 적용성을 평가한다. BAT-Fill은 양방향 자기회귀 트랜스포머(BAT) 기반의 거친 네트워크와 생성적 적대 신경망(GAN) 기반의 정제 네트워크로 구성되어 있다. 인페인팅은 일반적으로 이웃 픽셀의 특징으로부터 점차적으로 채워가는 방식으로 이루어지는 반면, 가느다란 털이 많이 분포되어 있는 피부에서는 이웃 피부의 특성을 정확하게 추출하기 어렵다. 트랜스포머는 서로 다른 영역 사이의 유사성을 학습하여 피부를 복원하는 방식으로서, 전역적인 피부 특징으로부터 지역적인 피부 특성을 정확하게 복원하는 것을 가능하게 한다. 모델에 대한 성능 평가 결과, BAT-Fill이 기존 방법에 비해서 PSNR 및 SSIM에서는 유의미한 차이가 없었으나 FID에서는 피부영상 털 제거에서 실감성과 인지적 품질이 향상되었음을 보여준다.","Hair removal in skin images is the essential preprocessing task for automatized skin cancer detection. Most existing methods for hair removal are based on image processing, but their performance tends to be significantly degraded in hairy skin images. As an alternative, an image inpainting approach based on deep learning can be taken into account, while few deep learning models for hair removal have been reported due to the lack of datasets. In this paper, we evaluate the applicability of BAT-Fill which is a transformer-based deep learning model as an image inpainting method for hair removal in skin images. BAT-Fill consists of not only the coarse network based on bidirectional autoregressive transformers but also the refinement network based on generative adversarial network (GAN). While a typical process of image inpainting is to progressively fill the inner hidden block using the properties of neighbor pixels, it is challenging to accurately extract the skin properties of neighbor pixels when hairs are largely distributed. The transformer is a solution for reconstructing the skin texture by learning the similarity of pixel properties between two distant areas, where local skin textures can be accurately predicted using global skin properties. The model assessment shows that BAT-Fill is effective to enhance the perceptual quality of the hair-removed image as it exhibits significant improvement in FID although it does not in PSNR and SSIM."
트랜스포머를 활용한 한국 수어 생성 아바타 개발,2022,"['한국 수어', '딥러닝', '트랜스포머', '자연어']","농아인들이 일반인들과 수어 통역사 없이 의사소통을 하기 위한 인공지능 수어 처리 시스템은 수어를 인식하여 자연어로 변환하는 모델과 자연어를 수어로 변환하는 모델로 구성된다. 본 논문에서는 한국어를 번역하여 한국 수어를 생성하는 트랜스포머 기반의 End To End 심층 신경망 모델을 구현한다. 자연어 텍스트 입력으로부터 한국 수어 동작 자세에 대한 좌표에 대한 시계열 데이터를 생성하는 트랜스포머 모델을 구현하였고, 시계열 좌표 데이터를 실제 아바타 동작으로 변환하는 Unity 기반의 아바타 모델을 개발하였다. 수어 생성 트랜스포머 모델을 학습시키기 위해 AI Hub 수어 데이터를 활용하였으며, 생성모델의 정확도를 기존 연구들과 비교하였다. 또한 생성된 아바타의 동작이 농아인이 이해할 수 있는 동작인지 수어 통역사를 통해 검증하였다. 학습 데이터에 따른 정확도 차이는 추가적인 연구를 통해 확인해야할 부분이지만 기존 연구대비 더 나은 성능을 보였다.",다국어 초록 정보 없음
Graph2Tree 모델을 이용한 한국어 수학 문장제 문제 풀이,2022,"['math   word   problem', 'mathematical   relationship   extraction', 'graph-to-tree    learning', '수학 문장제 문제', '수학적 관계 추출', '그래프 투 트리 학습']","본 논문은 8개 유형으로 이루어진 한국어 수학 문장제 문제 데이터셋을 자체적으로 구축하여 이를 기반으로 기존에 제시되지 않았던 Graph2Tree 모델 기반 한국어 수학 문장제 문제 자동 풀이 모델인 Ko-Graph2Tree 모델을 제시한다. 최근 공개된 Graph2Tree 모델은 영어 수학 문장제 문제 자동 풀이에 기존의 자연어 처리 모델들보다 뛰어난 성능을 보인 모델이다. 해당 모델은 문제 텍스트 내의 숫자 간 관계성 및 순서, 즉 수학적 관계를 반영한 두 가지의 그래프를 풀이 생성에 사용함으로써 기존의 트리 기반 모델들보다 향상된 성능을 보인다. 자체 제작한 한국어 수학 문장제 문제 데이터셋으로 학습시킨 후 성능을 측정한 결과, 시퀀스 투 시퀀스 구조의 트랜스포머 모델은 정확도가 42.3%, 본 논문이 제시한 Ko-Graph2Tree 모델은 정확도가 68.3%로 26.0%p 더 높은 성능을 보였다.","This paper builds its own data set of eight types of Korean math word problems and presents a Ko-Graph2Tree model, an automatic solution model for Korean math word problems based on Graph2Tree model not previously presented. The recently released Graph2Tree model is a graph-to-tree learning based model that shows better performance than existing natural language processing models for automatic solving English math word problems. Using two types of graphs reflecting the relationship and order between numbers in the problem text, that is, mathematical relations, in solution generation, the model showed improved performance compared to existing tree-based models. As a result of measuring performance after learning with a self-produced Korean math word problem dataset, the transformer model with sequence-to-sequence structure showed an accuracy of 42.3%, whereas the Ko-Graph2Tree model showed an accuracy of 68.3%, resulting in 26.0%p higher performance."
CAN과 Dense Synthesizer를 이용한 트랜스포머 디코딩 속도 개선,2022,"['신경망 기계 번역', '기계 번역', '트랜스포머', '속도 개선', 'machine translation', 'neural machine translation', 'transformer', 'speed improvement']","최근 신경망을 이용한 기계 번역 연구들이 다양하게 진행되고 있으며, 그중에서 트랜스포머 모델이 가장 우수한 번역 성능을 보이나, 번역 속도가 느리다는 단점이 있어 번역 속도를 증가시키기 위한 많은 연구가 진행되고 있다. 본 논문에서는 번역 속도를 증가시키는 것을 목표로 하며 선행 연구 중 트랜스포머 디코더의 Self-Attention, Cross-Attention, FFN(Feed Forward Network)을 하나의 Attention으로 통합한 CAN(Compressed Attention Network)과 Self-Attention을 대체할 수 있는 Synthetic Attention 중 하나인 Dense Synthesizer를 결합하여 트랜스포머 모델의 디코딩 속도를 개선하였다. 실험 결과, Baseline과 비교할 때 CAN과 Dense Synthesizer를 결합한 모델은 번역 성능이 BLEU 2.8 감소하였지만, 번역 속도를 153.9% 증가시켰다.","Recently, various studies have been conducted on machine translation using neural networks, and among them, the transformer model has shown the best translation performance, but it has a disadvantage as the translation speed is slow. Thus, many studies are being conducted to increase the translation speed. In this paper, with the aim of increasing the translation speed, among previous studies, CAN(Compressed Attention Network) that integrates Self-Attention, CrossAttention, and FFN(Feed Forward Network) of the transformer decoder into one attention and Dense Synthesizer, one of the Synthetic Attentions that can replace Self-Attention, are combined. The decoding speed of the transformer model is improved. As a result of the experiment, compared with the baseline, the model combining CAN and Dense Synthesizer decreased the translation performance by 2.8 BLEU, but it increased the translation speed by 153.9%."
시계열 예측을 위한 3D 컨볼루션 트랜스포머,2022,"['딥러닝', '트랜스포머', '시계열 예측', '3D 컨볼루션', '시퀀스 투 시퀀스', 'Deep learning', 'Transformer', 'Time-series forecasting', '3D convolution', 'Sequence-to-Sequence']","다양한 분야에서 시계열 데이터의 중요성이 증가하면서 미래의 시계열 데이터를 정확하게 예측하는 것이 중요한 문제로 대두되고 있다. 시계열 데이터 예측 분야에서는 오랫동안 LSTM 기반 모델을 사용했지만, 최근에는 트랜스포머 모델이 주목받기 시작했다. 하지만 트랜스포머 모델 구조는 시계열의 순차적인 특성을 보존하는 능력이 약하다는 한계를 가진다. 본 논문에서는 이것을 해결하기 위해 3D 컨볼루션 네트워크와 Transformer를 결합한 구조의 3DCformer를 제안한다. 3DCformer의 인코더는 dilated 컨볼루션을 사용하여 입력 데이터의 순차적인 특성을 강화한 뒤 2D 입력 데이터를 3D로 확장한다. 그 후 3D 컨볼루션 네트워크를 통해 확장된 특성에서 표현을 추출한다. 3DCformer의 디코더는 인코더에서 추출한 짧은 기간과 긴 기간의 시계열 데이터의 표현을 효율적으로 결합 및 학습하는 구조로 설계한다. 광범위한 실험을 통해 3DCformer가 기존 State-Of-The-Art 모델에 비해 16.18% 낮은 MAE와 24.53% 낮은 MSE를 달성함을 보인다.",다국어 초록 정보 없음
부가 정보를 활용한 비전 트랜스포머 기반의 추천시스템,2022,"['추천시스템', '딥러닝', '트랜스포머', '부가 정보', '보조 분류기', 'Recommender Systems', 'Deep Learning', 'Transformer', 'Side Information', 'Auxiliary Classifier']","최근 추천 시스템 연구에서는 사용자와 아이템 간 상호 작용을 보다 잘 표현하고자 다양한 딥 러닝 모델을 적용하고 있다. ONCF(Outer product-based Neural Collaborative Filtering)는 사용자와 아이템의 행렬을 외적하고 합성곱 신경망을 거치는 구조로 2차원 상호작용 맵을 제작해 사용자와 아이템 간의 상호 작용을 더욱 잘 포착하고자 한 대표적인 딥러닝 기반 추천시스템이다. 하지만 합성곱 신경망을 이용하는 ONCF는 학습 데이터에 나타나지 않은 분포를 갖는 데이터의 경우 예측성능이 떨어지는 귀납적 편향을 가지는 한계가 있다. 본 연구에서는 먼저 NCF구조에 Transformer에 기반한 ViT(Vision Transformer)를 도입한 방법론을 제안한다. ViT는 NLP분야에서 주로 사용되던 트랜스포머를 이미지 분류에 적용하여 좋은 성과를 거둔 방법으로 귀납적 편향이 합성곱 신경망보다 약해 처음 보는 분포에도 robust한 특징이 있다.다음으로, ONCF는 사용자와 아이템에 대한 단일 잠재 벡터를 사용하였지만 본 연구에서는 모델이 더욱 다채로운 표현을 학습하고 앙상블 효과도 얻기 위해 잠재 벡터를 여러 개 사용하여 채널을 구성한다. 마지막으로 ONCF와 달리 부가 정보 (side information)를 추천에 반영할 수 있는 아키텍처를 제시한다. 단순한 입력 결합 방식을 활용하여 신경망에 부가 정보를 반영하는 기존 연구와 달리 본 연구에서는 독립적인 보조 분류기(auxiliary classifier)를 도입하여 추천 시스템에 부가 정보를 보다 효율적으로 반영할 수 있도록 하였다. 결론적으로 본 논문에서는 ViT 의 적용, 임베딩 벡터의 채널화, 부가정보 분류기의 도입을 적용한 새로운 딥러닝 모델을 제안하였으며 실험 결과 ONCF보다 높은 성능을 보였다.",다국어 초록 정보 없음
합성곱 신경망과 비전 트랜스포머를 활용한 포트홀과 맨홀 감지 성능 비교,2022,[],"본 논문은 도로 위의 포트홀(Pothole)과 맨홀(Manhole)을 감지하는 과업을 위해 합성곱 신경망을 이용한 모델과 최근 많은 연구가 이뤄지고 있는 트랜스포머(Transformer) 구조를 활용한 모델의 성능 비교에 관하여 서술하였다. 학습과 검증에 사용된 데이터로는 과학기술정보통신부와 한국지능정보사회진흥원의「지능정보산업 인프라 조성」사업의 목적으로 구축한 도로 장애물 및 표면 인지(Detection) 영상 데이터 집합을 사용하였고 그 중 포트홀과 맨홀 데이터만을 선별하여 사용하였다. 합성곱 신경망을 이용한 모델로는 Yolo-v3를 사용하였고 트랜스포머 구조를 활용한 모델로는 Meta Research 에서 발표한 DEtection TRansformer(DE:TR)을 사용하였다. 실험 결과 DE:TR은 bbox_mAP가 0.306, Yolo-v3는 bbox_mAP가 0.231로 DE:TR이 0.075 더 높은 성능을 보여주었다.",다국어 초록 정보 없음
인포머 기반의 음성 음원분리 모델의 경량화에 관한 연구,2022,[],"본 논문에서는 딥러닝 기반의 음성 음원분리 모델의 경량화를 위해 기존의 트랜스포머(Transformer) 대신 인포머(Informer)를 적용한 새로운 음성 음원분리 모델을 제안한다. 즉, 부호기-분리모듈-복호기 구조로 구성된 음원분리 모델의 분리모듈에 인포머를 적용한다. 두 명의 발화자 음원이 혼합된 Librimix 데이터베이스를 활용하여 성능을 평가한 결과, 트랜스포머를 적용한 모델 대비 객관적 성능 지표인 source-to-distortion ratio (SDR)과 short-time objective intelligibility (STOI)는 유사한 것으로 평가되었고, 모델 파라미터 수는 17% 감소된 것을 확인하였다.",다국어 초록 정보 없음
디리클레 분포 기반 모델 기여도 예측을 이용한 앙상블 트레이딩 알고리즘,2022,"['앙상블', '트레이딩 알고리즘', '디리클레 분포', 'Transformer', 'ensemble', 'trading algorithm', 'Dirichlet distribution', 'Transformer']","알고리즘을 이용하여 금융 상품을 거래하는 알고리즘 트레이딩은 시장의 많은 요인들로 인해 그 결과가 안정적이지 못한 문제가 있다. 이 문제를 완화시키기 위해 트레이딩 알고리즘들을 조합한 앙상블 기법들이 제안되었다. 하지만 이 앙상블 방법에도 여러 문제가 존재한다. 첫째, 앙상블의 필요 요건인 앙상블에 포함된 알고리즘의 최소 성능 요건(랜덤 이상)을 만족시키도록, 트레이딩 알고리즘을 선택하지 못할 수 있다는 점이다. 둘째, 과거에 우수한 성능을 보인 앙상블 모델이 미래에도 우수한 성능을 보일 것이라는 보장이 없다는 점이다. 이 문제점들을 해결하기 위해 앙상블 모델에 포함되는 트레이딩 알고리즘들을 선택하는 방법을 다음과 같이 제안한다. 과거의 데이터를 기반으로 상위 성능의 앙상블 모델들에 포함된 트레이딩 알고리즘들의 기여도를 측정한다. 그러나 이 과거 데이터에만 기반 된 기여도들은 과거의 데이터가 충분히 많지 않고 과거 데이터의 불확실성이 반영되어 있지 않기 때문에 디리클레 분포를 사용하여 기여도 분포를 근사시키고, 기여도 분포에서 기여도 값들을 샘플하여 불확실성을 반영한다. 과거 데이터로부터 구한 트레이딩 알고리즘의 기여도 분포를 기반으로 Transformer을 훈련하여 미래의 기여도를 예측한다. 예측된 미래 기여도가 높은 트레이딩 알고리즘들을 앙상블 모델에 선택하여 포함시킨다. 실험을 통하여 제안된 앙상블 방법이 기존 앙상블 방법들과 비교하여 우수한 성능을 보임을 입증하였다.","Algorithmic trading, which uses algorithms to trade financial products, has a problem in that the results are not stable due to many factors in the market. To alleviate this problem, ensemble techniques that combine trading algorithms have been proposed. However, there are several problems with this ensemble method. First, the trading algorithm may not be selected so as to satisfy the minimum performance requirement (more than random) of the algorithm included in the ensemble, which is a necessary requirement of the ensemble. Second, there is no guarantee that an ensemble model that performed well in the past will perform well in the future. In order to solve these problems, a method for selecting trading algorithms included in the ensemble model is proposed as follows. Based on past data, we measure the contribution of the trading algorithms included in the ensemble models with high performance. However, for contributions based only on this historical data, since there are not enough past data and the uncertainty of the past data is not reflected, the contribution distribution is approximated using the Dirichlet distribution, and the contribution values are sampled from the contribution distribution to reflect the uncertainty. Based on the contribution distribution of the trading algorithm obtained from the past data, the Transformer is trained to predict the future contribution. Trading algorithms with high predicted future contribution are selected and included in the ensemble model. Through experiments, it was proved that the proposed ensemble method showed superior performance compared to the existing ensemble methods."
미술 심리 치료 분야의 의사결정 지원을 위한 비지도학습 기반의 AI 모델 적용에 관한 연구,2022,"['미술치료', '인간과 컴퓨터 상호작용', '비지도학습', '딥러닝', '이미지 분류', '트랜스포머']","본 논문에서는 미술 심리 치료의 그림검사(Drawing Test) 과정에서 심리 치료사의 의사 결정 도움을 위해 소규모 데이터셋을 활용한 비지도 학습 기반의 모델 개발 가능성에 대해 살펴본다. 비지도학습 기반의 VQViT(Vector Quantized-Vision Transformer)와 BEiT(Bidirectional Encoder representation from Image Transformers) 모델을 지도학습 기반의 ViT(Vision Transformer)와 Resnet-101 모델과 성능 비교 실험을 진행하였다. 그 결과 비지도학습 기반의 BEiT 가 가장 높은 정확도를 보였다. 비지도학습 기반 모델을 활용해 웹 페이지를 제작하였으며, 미술 심리 치료에서의 이미지 분류 모델 활용을 통한 전문가의 의사결정 지원 가능성을 확인하였다. 마지막으로, 본 논문에서는 미술 심리치료 분야에서 딥러닝 기반 이미지 분류 모델 의사결정 지원 방향성을 논의한다.",다국어 초록 정보 없음
KoBigBird를 활용한 수능 국어 문제풀이 모델,2022,[],"최근 자연어 처리 분야에서 기계학습 독해 관련 연구가 활발하게 이루어지고 있다. 그러나 그 중에서 한국어 기계독해 학습을 통해 문제풀이에 적용한 사례를 찾아보기 힘들었다. 기존 연구에서도 수능 영어와 수능 수학 문제를 인공지능(AI) 모델을 활용하여 문제풀이에 적용했던 사례는 있었지만, 수능 국어에 이를 적용하였던 사례는 존재하지 않았다. 또한, 수능 영어와 수능 수학 문제를 AI 문제풀이를 통해 도출한 결괏값이 각각 12점, 16점으로 객관식이라는 수능의 특수성을 고려했을 때 기대에 못 미치는 결과를 나타냈다. 이에 본 논문은 한국어 기계독해 데이터셋을 트랜스포머(Transformer) 기반 모델에 학습하여 수능 국어 문제 풀이에 적용하였다. 이를 위해 객관식으로 이루어진 수능 문항의 각각의 선택지들을 질문 형태로 변형하여 모델이 답을 도출해낼 수 있도록 데이터셋을 변형하였다. 또한 BERT(Bidirectional Encoder Representations from Transformer)가 가진 입력값 개수의 한계를 극복하기 위해 더 큰 입력값을 처리할 수 있는 트랜스포머 기반 모델 중에서 한국어 기계독해 학습에 적합한 KoBigBird를 사전학습모델로 설정하여 성능을 높였다.",다국어 초록 정보 없음
트랜스포머 기반의 판소리 소리꾼 검출과 얼굴 표정 인식,2022,"['Pansori', 'Transformer', 'Self-supervised training', 'Facial expression classification']","인공지능 기술을 이용하는 음악 연구가 활발하게 진행되고 있다. 본 연구에서는 한국 전통음악인 판소리의 발림동작과 얼굴 표정의 인식을 목표로 한다. 이를 위해 동영상을 구성하는 매 프래임에서 소리꾼 객체를 검출하고, 소리꾼을 포함하는 프래임에서 동작 및 얼굴 표정 인식을 시도한다. 단 동작 인식은 이전 논문[1]에서 언급하였기 때문에 본 논문에서는 소리꾼 얼굴 표정 인식에 관련된 과정만을 강조하여 언급한다. 본 논문에서 부족한 주석 데이터 때문에 소리꾼 검출에서는 MS COCO 데이터를 혼합하여 활용하였으며, 얼굴 표정 분류에는 ImageNet 데이터셋과 RAF-DB 데이터를 활용하여 MAE 자기 지도 학습 모델로 구성한 ViT 트랜스포머를 백본으로 활용하였다.소리꾼 검출의 경우 IoU 문턱치 75%에서 약 91.2%의 평균 정밀도를 얻었으며, 4개 범주의 얼굴 표정의 분류에서78.44%의 정확도를 달성했다. 본 연구의 결과는 문화 콘텐츠 보존이나 교육에 의미 있는 활용이 예상된다.","Our work aims to recognize the singer’s motions and facial expression of Pansori, a traditional Korean music. In order to achieve our goal, the region of a singer is at first detected from every video frame, then the motion and facial expression are classified based on still image analysis in the detected region. In order to overcome the difficulty due to insufficient labelled data, the person class of MS COCO data is mixed up with the collected Pansori dataset for detecting Pansori singer, and ViT(Visual Transformer) pre-trained backbone is adopted to construct a singer’s facial expression classifier after self-supervised training in MAE model with ImageNet and RAF-DB dataset. For singer detection, 91.2% of AP has been achieved under 75% of IoU threshold. Also, 78.44% of accuracy has been obtained in Pansori singer’s facial expression classified into 4 categories. The results of this study are expected to be meaningful for preservation or education of cultural content."
트랜스포머를 이용한 1×1 초광대역 무선 신호 기반 사람의 자세 추정,2022,"['ultra wide band', '1×1', 'transformer', 'human pose estimation', 'body segmentation', '초광대역 무선 신호', '1×1', '트랜스포머', '사람의 자세 추정', '신체 세그멘테이션']","특정 공간에서 사람의 자세를 추정하는 문제는 컴퓨터 비전의 주요 분야 중 하나로 게임, 의료, 재난, 소방 보안, 군사 등 다양한 분야에 활용될 수 있는 중요한 기술이다. 최근 기계 학습과 접목하여 자세 추정의 정확도를 크게 높일 수 있었다. 하지만 이미지 기반의 방식은 신체의 일부 또는 전체가 장애물로 가려지거나 조명이 어두운 경우 자세 추정이 어렵다는 한계가 있다. 최근에는 무선 신호를 사용하여 사람의 자세를 추정하는 연구가 등장하였으며 이는 조명의 밝기에 영향을 받지 않고 장애물을 투과할 수 있다는 장점을 가지고 있다. 무선 신호를 기반으로 특정 위치를 추정하기 위해서는 두 쌍 이상의 송수신기가 필요하다는 것이 기존의 인식이었다. 본 논문에서는 한 쌍의 송수신기로 수집한 1×1 초광대역 무선 신호만으로 딥 러닝을 적용하여 사람의 자세 추정 및 신체 세그멘테이션이 가능함을 보인다. 또한 트랜스포머 기반 모델을 통해 합성곱 신경망을 대체하고 더 나은 성능을 보이는 방법을 제안한다.","The problem of estimating a human’s pose in specific space from an image is one of the main area of computer vision and is an important technology that can be used in various fields such as games, medical care, disaster, fire fighting, and the military. By combining with machine learning, the accuracy of pose estimation has been greatly improved. However, the image-based approach has a limitation in that it is difficult to estimate pose when part or whole of the body is occluded by obstacles or when the lighting is dark. Recently, studies have emerged to estimate a human pose using wireless signals, which have the advantage of penetrating obstacles without being affected by brightness. The previous stereotype was that two or more pairs of transceivers are required to estimate a specific location based on wireless signals. This paper shows that it is possible to estimate the human pose and to perform body segmentation by applying deep learning only with 1x1 ultra wide band signals collected by 1×1 transceiver. We also propose a method of replacing convolution neural networks and showing better performance through transformer models."
Affinity Prober를 이용한 언어 모델의 문장 수용성 판단 결정의 경계 요인 분석,2022,"['Transformer', 'BERT', 'Self Attention', 'Minimal pair', 'Linguistic acceptability']",국문 초록 정보 없음,"Recently, many studies endeavor to reveal the intrinsic nature of BERT, since transformer-based language models have achieved the state-of-the-art in many natural language understanding tasks. However, it is still hard to probe BERT’s universal linguistic properties since different probing methods lead to fluctuating results between tasks and models. Thus, this paper suggests Affinity Prober, which is a flexible task- and model-agnostic probing method to investigate transformer-based language model’s decision boundaries when processing linguistic phenomena. Affinity Prober is designed to investigate potential linguistic knowledge from the Self Attention Mechanism. Using Affinity Prober, the study examines whether a bert-base-cased model has any explainable decision boundaries on sentence acceptability in terms of a lexical category in English. The results in syntactic phenomena show that the Affinity Relationship between function words is reinforced in the upper layers, while the Affinity Relationship between content words plays a key role in semantic phenomena. The study concludes that Affinity Prober is helpful to analyze the model’s decision boundary with lexical categories on specific linguistic phenomena."
딥러닝 언어모델과 Surprisal을 활용한 언어분석,2022,"['딥러닝', '언어모델', 'surprisal', '트랜스포머', '수용성', 'deep learning', 'language model', 'surprisal', 'transformer', 'acceptability']","본고는 영어 및 한국어의 딥러닝 모델을 활용하여 언어 연구를 하는 방법론에 대해서 소개한다. 딥러닝 언어모델은 언어 표현의 연쇄가 가지는 확률적 자연스러움을 학습하므로, 그 자연스러움에 반하는 이상 분포에 대해서는 민감하게 반응한다. 이러한 이상치를 계산하는 심리언어학적 방식이 surprisal이다. 이 산술식을 이용한 언어 연구는 사실상 언어의 전 층위에 적용 가능하다. 형태론, 통사론, 의미론 등의 문장 단위 구성은 물론이며 담화 및 정보구조 등의 연구에도 사용할 수 있다. 나아가 언어 데이터에 함축되어 있는 인간의 세계 지식 및 상식 판단에 대해서도 준용할 수 있다. 본고는 surprisal 기반 실험을 실시할 때 주요한 고려 사항에 대해서도 개괄한다. 물론, 딥러닝 기반 방법이 자연언어에 대한 모든 것에 해법을 줄 수 있는 만능열쇠는 아니다. 그러나 인간 언어를 분석하기 위한 새로운 도구로서 실효성을 가진다는 점에서 앞으로 그 활용 여지가 크다. 관심있는 연구자의 편의를 위해 라이브러리를 함께 공개한다.",다국어 초록 정보 없음
Perceiver 모델 기반의 음성 녹음 파일 분류 방법,2022,"['음성 녹음 분류', 'Perceiver', 'MFCC', '딥러닝', '디지털 포렌식', 'Voice Recording Classification', 'Perceiver', 'MFCC', 'Deep Learning', 'Digital Forensics']","최근에 대부분의 디지털 범죄는 개인용 컴퓨터와 스마트폰을 이용한 데이터를 통해 발생하고 있다. 오디오도 범죄 수사의 대상이며 오디오에 대한 디지털 포렌식 기술의 수요가 증가하고 있다. 본 논문에서는 딥러닝을 이용하여 다양한 오디오 파일에서 일반 오디오 및 공개 음성 녹음, 사용자 음성 녹음의 3종에 대한 분류를 위한 방법을 제안한다. 제안한 방법의 학습과 평가를 위해 일반 오디오 및 공개 음성 녹음, 사용자 음성 녹음의 3종 클래스를 정의하여 데이터셋을 구축하였고, 전체 오디오에 대하여 세그먼트 단위로 분류를 수행한 후에 최종적으로 클래스를 결정하였다. 세그먼트 단위로 음성 녹음 분류에 있어서 컨볼루션 신경망 기반의 딥러닝 모델은 정확도가 낮아서, 자연어 처리에 사용되는 트랜스포머 신경망 기반의 Perceiver 모델을 도입하였다. MFCC 특징 추출을 통하여 입력 데이터를 생성하고, 음성 녹음 분류에 적합하도록 어텐션과 트랜스포머 계층을 수정하고 파라미터를 최적화하였다. 실험에서는 제안한 방법을 구축한 데이터셋으로 학습하고, 오디오 세그먼트 단위와 전체 오디오 파일 단위의 분류 성능을 분석하였다. 그 결과 세그먼트 단위로 83.47% 정확도를 달성하였고, 전체 오디오 파일에 대하여 95.19% 정확도를 달성하였다.","Most recent digital crimes are committed through data using devices such as PC and smartphones. Audio is also a subject of criminal investigations, and the demand for audio digital forensics technology is increasing. In this paper, we propose a method to classify general audio, public voice recording, and user voice recording from various audio files using deep learning. For training and evaluation of the proposed method, a dataset was constructed by defining the 3 classes of general audio, public voice recording, and user voice recording. After classifying the entire audio in segment units, the class was finally determined. Since the deep learning model based on the convolutional neural network has low accuracy for classifying voice recording in segment units, the Perceiver model, which is a variation of the transformer neural network used in natural language processing, was adapted. The input data was generated through MFCC feature extraction, attention and transform layers were modified to be suitable for voice recording classification. Also, parameters were optimized. In the experiment, the proposed method was trained as a constructed dataset and the classification performance was analyzed in the audio segment units and the entire audio file units. As a result, 83.47% accuracy was achieved for each segment unit and 95.19% accuracy was achieved for the entire audio file."
강화학습과 표적 이미지 교환 모델을 이용한 객체 추적,2022,[],"최근 객체 추적 분야는 여러 분야에서 다양하게 응용되며 발전하고 있다. 또한 강화학습, Transformer, GAN 등 새로운 모델을 적용하면서 높은 성능을 보이며 현재 연구가 활발하게 진행되고 있다. 본 논문에서는 기존 객체 추적 모델의 성능 향상을 위한 강화학습 모델을 제안한다. 또한 관심 객체에 대한 기준이 되는 표적 이미지를 프레임의 흐름에 따라 더욱 효과적인 표적 이미지로 바꿔주는 표적 이미지 교환 분점을 추가로 설계하였다. 제안 모델은 VOT-2018 데이터 세트를 이용하여 정확도 0.613, 견고도 0.267을 달성하여 기존 모델의 성능을 높였다.",다국어 초록 정보 없음
KoBERT 기반의 연구개발 직무 추천 모델 연구,2022,[],"최근 취업 시장은 청년 취업률이 낮음에도 불구하고 조기 퇴사하는 비율이 높은 아이러니한 현상이 나타나고 있다. 이러한 현상의 주요 사유 중 하나는 전공이 맞지 않기 때문인데 이공계 인력에게는 더 두드러지고 있다. 이에 본 논문에서는 이공계 구직자에게 적합한 직무를 추천하는 모델을 연구한다. Attention 메커니즘과 Transformer 아키텍처를 기반으로 하는 BERT 언어모델을 한국어로 Pre training한 KoBERT를 이용한다. 실제 약 1,000여명의 연구 이력서를 자연어처리하고 자동차 산업의 3가지 연구개발 직무인 전자제어, PE시스템, 연료전지/배터리로 분류하도록 Fine tuning하였다. 결과적으로 약 평균 80% 정확도를 갖는 분류 모델과 더불어 희망하는 분야에 얼마나 적합한지, 이력서 중 가장 높은 적합성을 가지는 문장은 어떤 것인지 출력하는 Explainable AI를 구현하였다. 본 모델은 구직자 외에 특정 기술 분야의 인재를 확보해야 하는 채용담당자에게 인재를 추천함으로써 맨아워 개선도 기대할 수 있다. 결론적으로 구직자와 채용담당자 모두에게 기회비용을 개선할 뿐만 아니라 정보의 갭을 해소함으로써 연구개발직군의 취업 시장에 긍정적인 영향을 줄 것으로 기대된다.",다국어 초록 정보 없음
루간다어-영어 병렬코퍼스 구축과 번역 모델 학습,2022,"['루간다어', '신경 기계 번역', '트랜스포머', '하이퍼-파라미터', 'luganda', 'neural machine translation', 'transformer', 'hyper-parameter']","최근 번역 성능을 혁신하고 있는 신경망 기계 번역(NMT)은 대규모 데이터 세트를 필요로 하기 때문에 리소스가 많은 언어를 대상으로 한다. 따라서 Luganda 언어와 같은 병렬 말뭉치 자원이 부족한 언어에는 적용하기 어렵고, 'Google 번역'도 이 글을 쓰는 시점에서 Luganda를 지원하지 않고 있다. 이 논문에서 3개의 다른 오픈 소스 말뭉치를 기반으로 Luganda와 영어에 대한 41,070 쌍의 병렬 말뭉치를 구축한다. 그런 다음 하이퍼 변수 검색을 사용하여 NMT 모델을 훈련하고 최고의 모델을 찾는다. 실험 결과 Luganda에서 영어로 번역할 때 21.28의 BLEU 점수를 얻었고, 영어에서 Luganda로는 17.47의 BLEU 점수를 얻었다. 또한 일부 번역 예를 보여줌으로써 번역 품질을 확인할 수 있다. 이것은 최초의 Luganda-English NMT 모델이며, 우리가 구축한 Luganda-English 병렬 언어 데이터 세트는 공개할 것이다.","Recently, neural machine translation (NMT) which has achieved great successes needs large datasets, so NMT is more premised on high-resource languages. This continuously underpins the low resource languages such as Luganda due to the lack of high-quality parallel corpora, so even ‘Google translate’ does not serve Luganda at the time of this writing. In this paper, we build a parallel corpus with 41,070 pairwise sentences for Luganda and English which is based on three different open-sourced corpora. Then, we train NMT models with hyper-parameter search on the dataset.Experiments gave us a BLEU score of 21.28 from Luganda to English and 17.47 from English to Luganda. Some translation examples show high quality of the translation. We believe that our model is the first Luganda-English NMT model. The bilingual dataset we built will be available to the public."
BERT 마스크 언어 모델을 활용한 내부자 위협행위 탐지,2022,"['Bert', 'CERT dataset', 'Insider threat detection', 'Masked language model', 'Pre-trained model', 'Threshold-based detection']",국문 초록 정보 없음,"The risk of insider threat is increasing due to the increase in telecommuting because of pandemics. Companies have various security solutions to reduce insider threats, but there is a limit to capturing all human behavior. To solve this problem, studies on the detection of insider threat behavior using deep learning are being actively conducted. However, two problems that attack data is very scarce due to the nature of insider threat behavior, and time and action data must be learned at the same time remain challenges. In this paper, we propose an insider threat behavior detection model using the mask language model, which is a pre-learning method of Bi-directional Encoder Representation of Transformer(BERT). We used a CERT dataset for learning and used a custom tokenizer to map the data to actions and times at 30-minute intervals. The characteristics were learned by inputting only normal data into the mask language model, and the sum of the loss values derived from normal data was calculated as a threat score, and the maximum value was set as a detection boundary for each user. After that, if the threat score that occurs when new data is put into the model exceeds the detection boundary, it was detected as an abnormal behavior. The experimental results showed that compared to existing deep learning models, the model reacted sensitively when the behavior occurred at an unusual time, and in the case of scenarios where preparation and initiation of threat behavior were separated, appropriate detection performance was considered. As a result, the unbalanced dataset which was a challenge of detecting insider threat behavior could use the large amount of normal data as an advantage. And unlike the existing LSTM derivatives, sufficiently detailed time data can be mapped to behavior and used for learning."
토픽 기반의 지식그래프를 이용한 BERT 모델,2022,[],"최근 딥러닝의 기술발전으로 자연어 처리 분야에서 Q&A, 문장추천, 개체명 인식 등 다양한 연구가 진행 되고 있다. 딥러닝 기반 자연어 처리에서 좋은 성능을 보이는 트랜스포머 기반 BERT 모델의 성능향상에 대한 다양한 연구도 함께 진행되고 있다. 본 논문에서는 토픽모델인 잠재 디리클레 할당을 이용한 토픽별 지식그래프 분류와 입력문장의 토픽을 추론하는 방법으로 K-BERT 모델을 학습한다. 분류된 토픽 지식그래프와 추론된 토픽을 이용해 K-BERT 모델에서 대용량 지식그래프 사용의 효율적 방법을 제안한다.",다국어 초록 정보 없음
어텐션과 어텐션 흐름 그래프를 활용한 의료 인공지능 모델의 설명가능성 연구,2022,[],"의료 인공지능은 특정 진단에서 높은 정확도를 보이지만 모델의 신뢰성 문제로 인해 활발하게 쓰이지 못하고 있다. 이에 따라 인공지능 모델의 진단에 대한 원인 설명의 필요성이 대두되었고 설명가능한 의료 인공지능에 관한 연구가 활발히 진행되고 있다. 하지만 MRI 등 의료 영상 인공지능 분야에서 주로 진행되고 있으며, 이미지 형태가 아닌 전자의무기록 데이터 (Electronic Health Record, EHR) 를 기반으로 한 모델의 설명가능성 연구는 EHR 데이터 자체의 복잡성 때문에 활발하게 진행되지 않고 있다. 본 논문에서는 전자의무기록 데이터인 MIMIC-III (Medical Information Mart for Intensive Care) 를 전처리 및 그래프로 표현하고, GCT (Graph Convolutional Transformer) 모델을 학습시켰다. 학습 후, 어텐션 흐름 그래프를 시각화해서 모델의 예측에 대한 직관적인 설명을 제공한다.",다국어 초록 정보 없음
다중 스케일 특징 융합을 통한 트랜스포머 기반 장기 시계열 예측 정확도 향상 기법,2022,[],"본 논문에서는 정확한 장기 시계열 예측을 위해 시계열 데이터의 다양한 스케일 (시간 규모)에서 표현을 학습하는 트랜스포머 모델을 제안한다. 제안하는 모델은 시계열의 다중 스케일 특징을 추출하고, 이를 트랜스포머에 반영하여 예측 시계열을 생성하는 구조로 되어 있다. 스케일 정규화 과정을 통해 시계열의 전역적 및 지역적인 시간 정보를 효율적으로 융합하여 종속성을 학습한다. 3 가지의 다변량 시계열 데이터를 이용한 실험을 통해 제안하는 방법의 우수성을 보인다.",다국어 초록 정보 없음
토마토 과실의 검출 및 생육단계 추정을 위한 RGB-D 영상과 딥러닝 객체분할 모델의 적용,2022,"['토마토', 'RGB-D', '영상', '딥러닝', '객체분할']","토마토는 대표적인 원예작물 중 하나로, 맛을 비롯하여 항산화, 항염증, 항암 효과 등의 다양한 건강적 이점을 갖는다. 최근 각광받는 스마트팜 분야에서는, 원예작물의 과실의 생장단계 모니터링 및 수확을 위한 영상인식기술에 대한 연구들이 활발히 수행되고 있다. 이러한 기술은 농가의 생산량 예측 뿐 아니라 수확의 자동화를 위해서도 필수적이다. 최근 영상 기반의 연구들에서는, 딥러닝 기반의 모델들이 활발히 사용되고 있다. 특히 객체의 영역을 찾아내는 객체 분할 영역 또한 합성곱신경망이나 트랜스포머 등의 딥러닝 알고리즘이 기존 방식들 대비 높은 성능을 보인다. 본 연구에서는 토마토 과실의 객체 분할 모델을 개발하기 위하여 토마토 농가의 RGB-D 영상을 촬영하여 수집하였다. 수집된 데이터는 객체 영역이 폴리곤 형태로 라벨링되었으며, 학습 및 평가를 거쳐 여러 객체 분할 모델들의 성능이 비교 평가되었다.",다국어 초록 정보 없음
트랜스포머의 일반화 성능에 영향을 주는 로스 랜드스케이프 연구,2022,[],"뉴럴 네트워크는 학습에 사용하는 파라미터를 문제에 맞게 최적화하여 일반화 성능을 향상시키는 것이 목적이다. 선행 연구들은 다차원의 로스 랜드스케이프(loss landscape)를 시각화하는 방법을 탐구하며, 모델의 일반화 측면에서 어떤 영향을 주는지 탐구한다. 하지만 아직까지 로스 랜드스케이프가 근본적으로 일반화 성능에 어떠한 영향을 주는지 잘 알려져 있지 않으며, 평평하거나 경사진 로스 랜드스케이프 중 어떤 형태가 일반화 성능에 더 효과적인지 여러 의견이 나뉜다. 따라서 우리는 로스 랜드스케이프가 일반화 성능과 연관 있음을 실험을 통해 파악한다. 나아가 비전문제에서 MSA(multi-head self-attention) 레이어를 기반으로 구성된 트랜스포머 구조를 사용해 작은 유도 편향(inductive bias)을 가지며 소규모 데이터 셋 체제에서의 단점을 보완한다. 결론적으로 평평한 로스 랜드스케이프가 일반화 성능에 긍정적인 영향을 끼친다는 것을 관찰한다.",다국어 초록 정보 없음
RF 트랜스포머를 사용한 광대역 전력증폭기 설계,2022,"['Amplifiers', 'RF transformer', 'Broadband', 'inter-stage matching network', 'Fractional bandwidth']",국문 초록 정보 없음,"In this paper, a two-stage single-ended power amplifier (PA) with broadband gain characteristics was presented by utilizing a radio frequency (RF) transformer (TF), which is essential for a differential amplifier. The bandwidth of a PA can be improved by designing TF to have broadband characteristics and then applying it to the inter-stage matching network (IMN) of a PA. For broadband gain characteristics while maintaining the performance and area of the existing PA, an IMN was implemented on an monolithic microwave integrated circuit (MMIC) and a multi-layer printed circuit board (PCB), and the simulation results were compared. As a result of simulating the PA module designed using InGaP/GaAs HBT model, it has been confirmed that the PA employing the proposed design method has an improved fractional bandwidth of 19.8% at a center frequency of 3.3GHz, while the conventional PA showed that of 11.2%."
HRNet-OCR과 Swin-L 모델을 이용한 조식동물 서식지 수중영상의 의미론적 분할,2022,"['Undersea image', 'Semantic segmentation', 'HRNet-OCR', 'Swin transformer']",국문 초록 정보 없음,"In this paper, we presented a database construction of undersea images for the Habitats of Ecklonia cava and Sargassum and conducted an experiment for semantic segmentation using state-of-the-art (SOTA) models such as High Resolution Network-Object Contextual Representation (HRNet-OCR) and Shifted Windows-L (Swin-L). The result showed that our segmentation models were superior to the existing experiments in terms of the 29% increased mean intersection over union (mIOU). Swin-L model produced better performance for every class. In particular, the information of the Ecklonia cava class that had small data were also appropriately extracted by Swin-L model. Target objects and the backgrounds were well distinguished owing to the Transformer backbone better than the legacy models. A bigger database under construction will ensure more accuracy improvement and can be utilized as deep learning database for undersea images."
ViT 기반 모델의 강건성 연구동향,2022,[],컴퓨터 비전 분야에서 오랫동안 사용되었던 CNN(Convolution Neural Network)은 오분류를 일으키기 위해 악의적으로 추가된 섭동에 매우 취약하다. ViT(Vision Transformer)는 입력 이미지의 전체적인 특징을 탐색하는 어텐션 구조를 적용함으로 CNN의 국소적 특징 탐색보다 특성 픽셀에 섭동을 추가하는 적대적 공격에 강건한 특성을 보이지만 최근 어텐션 구조에 대한 강건성 분석과 다양한 공격 기법의 발달로 보안 취약성 문제가 제기되고 있다. 본 논문은 ViT가 CNN 대비 강건성을 가지는 구조적인 특징을 분석하는 연구와 어텐션 구조에 대한 최신 공격기법을 소개함으로 향후 등장할 ViT 파생 모델의 강건성을 유지하기 위해 중점적으로 다루어야 할 부분이 무엇인지 소개한다.,다국어 초록 정보 없음
신경망 언어 모델과 인간 언어 사용자의 주어와 목적어 관계절 처리 비교 연구,2022,"['subject/object relative clause', 'PP modification', 'reading time', 'surprisal', 'BERT', 'GPT-2', '주어/목적어 관계절', '전치사구 수식', '읽기 시간', '서프라이절']",국문 초록 정보 없음,"This paper is to investigate the distinct aspects of processing subject and object relative clauses (SRC and ORC) in neural language models (LMs) and humans using the materials, which are constructed by manipulating two RC types (SRC vs ORC) and two intervening PP types (locative vs temporal). Surprise values are collected from the transformer GPT-2 and BERT language models. Reading time data for humans are taken from Lowder and Gordon’s (2021) eye-tracking experiment. According to Lowder and Gordon (ibid.) that take as a critical region the matrix verb after the RC in the subject position, for humans the locative PP contained in the RC register longer reading times ORCs than SRCs, while the temporal PP does so for SRCs than ORCs. By contrast, for the two neural LMs, surprisals are higher for ORCs than SRCs regardless of whether the PP in question is locative or temporal. There was no statistically significant linear fit between human and the LMs’ responses. The result shows that to the extent that the distinction between locative and temporal PPs in RCs is syntactico-semantic, neither of the two neural language models is able to acquire human-like sensitivity to such a distinction."
L2 영어 교과서를 ‘학습’한 L2-신경망 언어 모델의 문법 일반화 양상,2022,"['언어학적 일반화', '신경망 언어 모델', 'LSTM', 'GPT-2', 'L2-신경망 언어 모델', 'linguistic generalization', 'neural language model', 'LSTM', 'GPT-2', 'L2-language models']",국문 초록 정보 없음,.
지도 학습 기반의 에세이 점수 예측 모델 성능 비교 연구,2022,"['Automated Essay Scoring', 'Random Forest', 'Gradient Boosting Machine', 'BERT', 'Bi-LSTM']","최근 여러 벤치마크 데이터 세트가 공개되면서 텍스트 마이닝에서 에세이 점수를 정확하게 예측하는 자동 에세이 채점방식이 개발되었다. 본 연구에서는 다양한 텍스트에서 더 나은 결과를 보여준 랜덤 포레스트(RF), 그래디언트 부스팅 머신(GBM), 양방향 장단기 메모리(Bi-LSTM), BERT와 같은 주요 기계 학습 및 딥러닝 모델을 기반으로 한 자동 에세이 채점방식의 정확도를 비교한다. 집중적인 실험을 통해, BERT와 그래디언트 부스팅 머신(GBM)은 랜덤 포레스트(RF)와 Bi-LSTM에 비해 상대적으로 더 나은 결과를 보여주는 것으로 나타났다. 본 논문에서는 실험 결과에 대한 자세한 내용을 논의한다.","As several benchmark datasets have recently been released in public, the automated essay scoring method that accurately predicts essay scores in text mining has been developed. In this work, we compare the accuracy of the automated essay scoring methods based on main machine learning and deep learning models such as Random Forest (RF), Gradient Boosting Machine (GBM), Bidirectional Long Short-Term Memory (Bi-LSTM), and Bi-directional Encoder Representations form Transformers (BERT), which have shown better results in various text mining problems. Through our intensive experiments, it turns out that BERT and GBM show relatively better results compared to RF and Bi-LSTM. In this paper, we discuss the details of the experimental results."
단독운전 모드 동작에서의 Triple-Active-Bridge 컨버터 제어 기법 및 소신호 모델을 기반으로 한 제어기 설계,2022,"['TAB (Triple-Active-Bridge) converter', 'DC distribution', 'Grid connected mode', 'Islanding mode', 'Small-signal model']",국문 초록 정보 없음,"In DC distribution systems, a TAB converter employing multiple transformers is one of the most widely used topologies due to its high power density, modularizability, and cost-effectiveness. However, the conventional control technique for a grid-connected mode in the TAB converter cannot maintain its reliability for an islanding mode under a blackout situation. In this paper, the islanding mode control technique is proposed to solve this issue. To verify the relative stability and dynamic characteristics of the control technique, small-signal models of both the grid connected and the islanding mode are derived. Based on the small-signal models, PI controllers are designed to provide suitable power control. The proposed control technique, the accuracy of small-signal models, and the performance of the controllers are verified by simulations and experiments with a 1-kW prototype TAB converter."
딥러닝 기반 사전학습 언어모델에 대한 이해와 현황,2022,"['NLP', 'deep learning', 'language model', 'Transformer', 'BERT', 'GPT', '자연어 처리', '딥러닝', '언어모델', '트랜스포머', 'BERT', 'GPT']","사전학습 언어모델은 자연어 처리 작업에서 가장 중요하고 많이 활용되는 도구로, 대량의 말뭉치를 대상으로 사전학습이 되어있어 적은 수의 데이터를 이용한 미세조정학습으로도 높은 성능을 기대할 수 있으며, 사전학습된 토크나이저과 딥러닝 모형 등 구현에 필요한 요소들이 함께 배포되기 때문에 자연어 처리 작업에 소요되는 비용과 기간을 크게 단축시켰다. 트랜스포머 변형 모형은 이와 같은 장점을 제공하는 사전학습 언어모델 중에서 최근 가장 많이 사용되고 있는 모형으로, 번역을 비롯하여 문서 요약, 챗봇과 같은 질의 응답, 자연스러운 문장의 생성 및 문서의 분류 등 다양한 자연어 처리 작업에 활용되고 있으며 컴퓨터 비전 분야와 오디오 관련 분야 등 다른 분야에서도 활발하게 활용되고 있다. 본 논문은 연구자들이 보다 쉽게 사전학습 언어모델에 대해 이해하고 자연어 처리 작업에 활용할 수 있도록 하기 위해, 언어모델과 사전학습 언어모델의 정의로부터 시작하여 사전학습 언어모델의 발전과정과 다양한 트랜스포머 변형 모형에 대해 조사하고 정리하였다.",다국어 초록 정보 없음
다중 패치를 이용한 트랜스포머 기반 장면 텍스트 인식,2022,"['Deep learning', 'Scene text recognition', 'Transformer', '.']",국문 초록 정보 없음,.
An Improved Mathematical Model for Estimating Polarization Parameters of Transformer Oil-Paper Insulation,2022,"['Transformer', 'Oil-paper insulation', 'Recovery voltage method', 'Improved mathematical model', 'Particle swarm optimization algorithm']",국문 초록 정보 없음,"The parameters of dielectric response based on extended Debye circuit model can refl ect oil-paper insulation state. The model using the initial slop of recovery voltage to estimate circuit parameters, serve as good quality consistency with the measured data. However, the model can be applicable for large power transformers but not for distribution transformers. This paper proposed an improved mathematical model using recovery voltage peak, peak time and initial slope characteristics to solve dielectric response equivalent circuit parameters. The identifi cation of equivalent circuit parameters is converted into a mathematical optimization problem. And then particle swarm optimization algorithm is used for solving the problem.The improved mathematical model can decrease sampling data of recovery voltage. To check the validity of the estimated parameters, the on-site measured data of RVM experiments on actual transformers in various capacities is applied. The calculated result shows that the recovery voltage curve calculated and measured recovery voltage curve have good consistency, which can be advantageous to diagnose the oil-paper insulation status of transformer. It illustrates that the improved method proposed in this paper is feasible and eff ective."
팬옵틱 분할을 위한 고속 트랜스포머,2022,"['Panoptic segmentation', 'Transformer']",국문 초록 정보 없음,"Recent high-performance panoptic segmentation models are based on transformer architectures. However, transformer-based panoptic segmentation methods are basically slower than convolution-based methods, since the attention mechanism in the transformer requires quadratic complexity w.r.t. image resolution. Also, sine and cosine computation for positional embedding in the transformer also yields a bottleneck for computation time. To address these problems, we adopt three modules to speed up the inference runtime of the transformer-based panoptic segmentation. First, we perform channel-level reduction using depth-wise separable convolution for inputs of the transformer decoder. Second, we replace sine and cosine-based positional encoding with convolution operations, called conv-embedding. We also apply a separable self-attention to the transformer encoder to lower quadratic complexity to linear one for numbers of image pixels. As result, the proposed model achieves 44% faster frame per second than baseline on ADE20K panoptic validation dataset, when we use all three modules."
Infrared Pedestrian Dataset Training using Swin Transformer model,2022,"['Swin Transformer', 'Infrared Image', 'Object Detection', 'Transformer', 'Ir image', 'Backbone training']",국문 초록 정보 없음,"Recently, studies to replace CNN-based models with Transformer models are being actively conducted. In this paper, the Swin Transformer model, which has recently been attracting attention for its excellent performance, was trained using infrared images and its performance was examined. A Swin Transformer designed for RGB images was tuned for infrared image training to train a separate infrared pedestrian dataset and a simple experiment was performed to further improve the infrared image training performance. In addition, in order to properly tune the Swin Transformer Backbone model to the infrared image data and to improve the training performance, we trained separately configured RGB and infrared datasets and analyzed the results. As a result, it was concluded that the Swin Transformer would be suitable for infrared data training if it was slightly tuned for infrared data and lightweight to avoid overfitting. Based on this experiment, the author plans to create a model suitable for infrared datasets in the future and apply it to various practical applications."
ANN Based Model for Current Transformers’ Saturation Error Compensation in Medium Voltage Switchgears,2022,"['Artificial neural network (ANN)', 'Current transformer (CT)', 'CT saturation', 'Medium voltage (MV)']",국문 초록 정보 없음,"Current Transformers’ saturation has been a major challenge facing protection engineers’ to-date even with the advent of diff erential protection system. This saturation causes erroneous measurement of the service currents which can lead to malfunctioning of protection relays and consequently cause false or delayed system trips with severe consequences to the power plant. This research presents an artifi cial neural networks (ANN) based approach for the compensation of these saturation errors caused by secondary current waveform distortions under transient or fault state. In this research, an ANN algorithm to be implemented in the numerical protection device systems to compensate for these errors was developed. This ANN model applies the multi-regression technique to map a point-to-point compensated waveform referred from the saturated waveform data and ideal/calculated current waveform from the current transformer. The model is developed and trained on python platform with validation and tests to effi ciently work with accuracy under the various simulated extreme circuit conditions that the MV system could experience. Electrical transient analyzer program and electromagnetic transient’s program-restructured version software were used to model and simulate current transformer’s transient scenarios generating suffi cient waveform data to train, validate and test the ANN algorithm. The model’s processing speed and accuracy was found to be satisfactory for real time application in digital protection devices."
머신러닝 기법을 이용한 변압기 주파수 응답 모델 파라미터 추정,2022,"['High-frequency model', 'Machine learning', 'Power transformer', 'Parameter estimation', 'Random forest', 'Sweep frequency response analysis']",국문 초록 정보 없음,"Examining power transformer faults is crucial for maintaining the reliability of the power system. The most popular methods for detecting power transformer fault include thermal analysis, vibration analysis, partial discharge analysis, dissolved gas analysis(DGA), and sweep frequency response analysis(SFRA). Especially, the SFRA test is examined to detect transformer internal fault such as winding fault. Simulation-level frequency response analysis enables inspection of the power transformer before connecting to the grid. This paper proposes a parameter estimation method using machine learning for the power transformer frequency response equivalent model."
딥러닝 의류 가상 합성 모델 연구: 가중치 공유 & 학습 최적화 기반 HR-VITON 기법 활용,2022,"['Virtual Try-on Service', 'Deep learning', 'HR-VITON', 'Weight Sharing', 'Mixed Precision', 'Gradient Accumulation']",국문 초록 정보 없음,"Purpose The purpose of this study is to develop a virtual try-on deep learning model that can efficiently learn front and back clothes images. It is expected that the application of virtual try-on clothing service in the fashion and textile industry field will be vitalization.Design/methodology/approach The data used in this study used 232,355 clothes and product images. The image data input to the model is divided into 5 categories: original clothing image and wearer image, clothing segmentation, wearer's body Densepose heatmap, wearer's clothing-agnosting. We advanced the HR-VITON model in the way of Mixed-Precison, Gradient Accumulation, and sharing model weights.Findings As a result of this study, we demonstrated that the weight-shared MP-GA HR-VITON model can efficiently learn front and back fashion images. As a result, this proposed model quantitatively improves the quality of the generated image compared to the existing technique, and natural fitting is possible in both front and back images. SSIM was 0.8385 and 0.9204 in CP-VTON and the proposed model, LPIPS 0.2133 and 0.0642, FID 74.5421 and 11.8463, and KID 0.064 and 0.006. Using the deep learning model of this study, it is possible to naturally fit one color clothes, but when there are complex pictures and logos as shown in <Figure 6>, an unnatural pattern occurred in the generated image. If it is advanced based on the transformer, this problem may also be improved."
로봇시스템에서 작은 마커 인식을 하기 위한 사물 감지 어텐션 모델,2022,"['Unloading Robot', 'Object Detection', 'Deep learning']",국문 초록 정보 없음,"As robots are considered one of the mainstream digital transformations, robots with machine vision becomes a main area of study providing the ability to check what robots watch and make decisions based on it. However, it is difficult to find a small object in the image mainly due to the flaw of the most of visual recognition networks. Because visual recognition networks are mostly convolution neural network which usually consider local features. So, we make a model considering not only local feature, but also global feature. In this paper, we propose a detection method of a small marker on the object using deep learning and an algorithm that considers global features by combining Transformer’s self-attention technique with a convolutional neural network. We suggest a self-attention model with new definition of Query, Key and Value for model to learn global feature and simplified equation by getting rid of position vector and classification token which cause the model to be heavy and slow. Finally, we show that our model achieves higher mAP than state of the art model YOLOr."
RGB 이미지에서 트랜스포머 기반 고밀도 3D 재구성,2022,[],국문 초록 정보 없음,"Multiview stereo (MVS) 3D reconstruction of a scene from images is a fundamental computer vision problem that has been thoroughly researched in recent times. Traditionally, MVS approaches create dense correspondences by constructing regularizations and hand-crafted similarity metrics. Although these techniques have achieved excellent results in the best Lambertian conditions, traditional MVS algorithms still contain a lot of artifacts. Therefore, in this study, we suggest using a transformer network to accelerate the MVS reconstruction. The network is based on a transformer model and can extract dense features with 3D consistency and global context, which are necessary to provide accurate matching for MVS."
특징치 플립을 이용한 이미지 분류 모델,2022,"['.', 'Deep learning', 'Feature map', 'Flip', 'Backbone', 'Classification']",국문 초록 정보 없음,.
한국어 자연어생성에 적합한 사전훈련 언어모델 특성 연구,2022,"['Pretrain Language Model', 'Transformer', 'Abstractive text summarization', 'BART', 'GPT']",국문 초록 정보 없음,다국어 초록 정보 없음
생성적 사전학습 언어모델 기반의 판결문 문장 생성에 관한 파일럿 연구,2022,[],"인공지능 기술이 발전함에 따라 경찰의 범죄수사 분야에서도 인공지능 기술을 적용하고자 하는 연구가 활발하다. 범죄수사의 결과물인 수사결과 보고서 작성에 있어 판결문은 중요한 데이터가 될 수 있다. 그러나 판결문은 공개된 데이터의 이미지화로 인해 정형화된 데이터의 확보가 까다롭고, 소수의 법조계 전문가가 아닌 일반인이 생성해내기 어려워 데이터 확보가 쉽지 않은 현실이다. 이에 본 연구에서는 생성적 사전학습 언어모델을 이용한 판결문 문장 데이터 생성을 제안하였다. 카카오의 KoGPT를 활용하여 실제 판결문장 일부를 제시한 결과 판결문과 유사한 형태의 문장을 생성한 것을 확인하였다. 향후 판결문 데이터를 활용하기 위한 인공지능 기술 기반 범죄수사 연구에 있어, 생성된 판결문 데이터를 활용할 수 있을 것으로 기대된다.",다국어 초록 정보 없음
Solid State Transformer의 Averaged Model 개발 및 검증,2022,[],국문 초록 정보 없음,다국어 초록 정보 없음
Research on Stress Reduction Model Based on Transformer,2022,"['EEG', 'mental stress', 'music', 'self-attention', 'Transformer']",국문 초록 정보 없음,"People are constantly exposed to stress and anxiety environment, which could contribute to a variety of psychological and physical health problems. Therefore, it is particularly important to identify psychological stress in time and to find a feasible and universal method of stress reduction. This research investigated the influence of different music, such as relaxation music and natural rhythm music, on stress relief based on Electroencephalogram signals. Mental arithmetic test was implemented to create a stressful environment. 23 participants performed the mental arithmetic test with and without music respectively, while their Electroencephalogram signal was recorded. The effect of music on stress relief was verified through stress test questionnaires, including Trait Anxiety Inventory (STAI-6) and Self-Stress Assessment. There was a significant change in the stress test questionnaire values with and without music according to paired t-test (p<0.01). Furthermore, a model based on Transformer for stress level classification from Electroencephalogram signal was proposed. Experimental results showed that the method of listening to relaxation music and natural rhythm music achieved the effect of reducing psychological stress and the proposed model yielded a promising accuracy in classifying the Electroencephalogram signal of mental stress."
A Novel Power Transformer Fault Diagnosis Model Based on Harris-Hawks-Optimization Algorithm Optimized Kernel Extreme Learning Machine,2022,"['Dissolved gas analysis (DGA)', 'Kernel extreme learning machine (KELM)', 'Harris-Hawks optimization (HHO)', 'Transformer fault diagnosis']",국문 초록 정보 없음,"Dissolved gas analysis (DGA) method is widely used to detect the incipient fault of power transformers. This paper presents a novel DGA method for power transformer fault diagnosis based on Harris-Hawks-optimization (HHO) algorithm optimized kernel extreme learning machine (KELM). The non-code ratios of the gases are used as the characterizing vector for the KELM model, and the Harris-Hawks-optimization (HHO) algorithm is introduced to optimize the KELM parameters, which promotes the fault diagnostic performance of KELM. Based on dataset collected from IEC TC 10, the fault diagnosis capability of the proposed method is validated by different characterizing vectors and is compared with conventional KELM and other optimized KELM. Moreover, the generalization ability of the proposed method is confirmed by China DGA data. The results demonstrate that the proposed method is superior to other methods and is more effective and stable for power transformer fault diagnosis with high accuracy."
한국어 자연어생성에 적합한 사전훈련 언어모델 특성 연구,2022,"['Pre-train Language Model', 'Transformer', 'Abstractive text summarization', 'BART', 'GPT', '사전훈련 언어모델', '트랜스포머', '문서 생성요약', 'BART', 'GPT']",국문 초록 정보 없음,"This study empirically analyzed a Korean pre-trained language models (PLMs) designed for natural language generation. The performance of two PLMs – BART and GPT – at the task of abstractive text summarization was compared. To investigate how performance depends on the characteristics of the inference data, ten different document types, containing six types of informational content and creation content, were considered. It was found that BART (which can both generate and understand natural language) performed better than GPT (which can only generate). Upon more detailed examination of the effect of inference data characteristics, the performance of GPT was found to be proportional to the length of the input text. However, even for the longest documents (with optimal GPT performance), BART still out-performed GPT, suggesting that the greatest influence on downstream performance is not the size of the training data or PLMs parameters but the structural suitability of the PLMs for the applied downstream task. The performance of different PLMs was also compared through analyzing parts of speech (POS) shares. BART’s performance was inversely related to the proportion of prefixes, adjectives, adverbs and verbs but positively related to that of nouns. This result emphasizes the importance of taking the inference data’s characteristics into account when fine-tuning a PLMs for its intended downstream task."
여객 수하물 내 위험화물 검색을 위한 비전 트랜스포머 기반 One-stage 디텍터 객체 검출 연구,2022,"['Object detection', 'Vision transformer', 'One stage detector', 'Security inspection']",국문 초록 정보 없음,다국어 초록 정보 없음
작성자 언어적 특성 기반 가짜 리뷰 탐지 딥러닝 모델 개발,2022,"['Fake Review', 'Predictive Modeling', 'Deep Learning', 'Linguistic Feature', 'Semantic Feature']",국문 초록 정보 없음,"Purpose This study aims to propose a deep learning-based fake review detection model by combining authors’ linguistic features and semantic information of reviews.Design/methodology/approach This study used 358,071 review data of Yelp to develop fake review detection model. We employed linguistic inquiry and word count (LIWC) to extract 24 linguistic features of authors. Then we used deep learning architectures such as multilayer perceptron(MLP), long short-term memory(LSTM) and transformer to learn linguistic features and semantic features for fake review detection.Findings The results of our study show that detection models using both linguistic and semantic features outperformed other models using single type of features. In addition, this study confirmed that differences in linguistic features between fake reviewer and authentic reviewer are significant. That is, we found that linguistic features complement semantic information of reviews and further enhance predictive power of fake detection model."
생애이력 기반 특고압 전기설비 안전관리 시스템 구축을 위한 모델 연구,2022,"['Electrical Safety Management System', 'Electrical Facility', 'Electrical Safety Level', 'Electrical Life Assessment', 'Life Cycle']",국문 초록 정보 없음,"To ensure the electrical safety of electrical facilities, their performance should be maintained above a certain threshold value. To do this, it is necessary to manage electrical facilities similarly to inertial performance through periodic inspection and maintenance based on historical data. Because the information on the installation, maintenance, and disposal of an electrical facility is distributed among various management entities, difficulties arise in supporting the information for determining maintenance. This paper proposes an establishment model for a safety management system based on the life cycle of extra-high voltage electrical facilities. The life cycle of electrical facilities presents the perspective of management for electrical safety evaluation services such as electrical safety level, life assessment, life prediction, and an electrical safety management. In addition, the proposed model includes the concepts of system configuration and the design of system data. The performance of the proposed model was demonstrated by applying the periodic inspection and diagnosis data of transformers and cables."
효율적인 S/W 유지관리를 위한 Git의 커밋메시지 복합 분류모델 제안,2022,"['커밋 메시지(Commit Message)', '다중분류(Multi-Label Classification)', 'BERT(Bidirectional Encoder Representations from Transformers)', '소스 변경(Source Change)']",국문 초록 정보 없음,다국어 초록 정보 없음
차량용 블랙박스 영상 내 균열 인식용 의미적 분할 모델 성능 비교,2022,"['도로 상태 모니터링', '블랙박스 카메라', '의미적 분할', 'Transformer']",국문 초록 정보 없음,다국어 초록 정보 없음
Protected Health Information Recognition by Fine-Tuning a Pre-training Transformer Model,2022,"['Artificial Intelligence', 'Big Data', 'Medical Informatics', 'Data Anonymization', 'Deep Learning']",국문 초록 정보 없음,"Objectives: De-identifying protected health information (PHI) in medical documents is important, and a prerequisite to deidentificationis the identification of PHI entity names in clinical documents. This study aimed to compare the performanceof three pre-training models that have recently attracted significant attention and to determine which model is more suitablefor PHI recognition. Methods: We compared the PHI recognition performance of deep learning models using the i2b2 2014dataset. We used the three pre-training models—namely, bidirectional encoder representations from transformers (BERT),robustly optimized BERT pre-training approach (RoBERTa), and XLNet (model built based on Transformer-XL)—to detectPHI. After the dataset was tokenized, it was processed using an inside-outside-beginning tagging scheme and WordPiecetokenizedto place it into these models. Further, the PHI recognition performance was investigated using BERT, RoBERTa,and XLNet. Results: Comparing the PHI recognition performance of the three models, it was confirmed that XLNet had asuperior F1-score of 96.29%. In addition, when checking PHI entity performance evaluation, RoBERTa and XLNet showeda 30% improvement in performance compared to BERT. Conclusions: Among the pre-training models used in this study,XLNet exhibited superior performance because word embedding was well constructed using the two-stream self-attentionmethod. In addition, compared to BERT, RoBERTa and XLNet showed superior performance, indicating that they were moreeffective in grasping the context."
Embedded High Accuracy Compact Transformer Vision System for Real-Time Health Detection During Seedling Transplanting,2022,"['Computer vision', 'Seedling transplanting', 'Transformers']",국문 초록 정보 없음,"Seedling transplanting is an important step in plant production to transplant seedlings into low-density trays at specific growth stages while discarding unhealthy seedlings. Several health detection methods based on machine learning or deep learning have been proposed. However there are still gaps in accuracy and efficiency compared to manual work. In this study, we proposed an embedded compact transformer vision system for real-time health detection during seedling transplanting. The vision system consisted of a camera (Kinect Azure) capturing RGB images in real time and a detection algorithm in an embedded PC (Jetson AGX Xavier). We introduced compact Transformers in the detection algorithm by designing the appropriate size and convolutional tokeniztion, so that the model had fewer parameters (22.58M) while outperforming the current state-of-the-art CNN models. In the validation experiment, two groups of paprika seedlings were cultivated to distinguish healthy and unhealthy by treating water stress. The algorithm showed 94.4% accuracy in the random tests. The experimental results quantitatively demonstrate that the accuracy of our method is competitive for manual. The proposed simple and compact transformers algorithm performs well in embedded devices with limited computational resources. High accuracy detection of seedling health is achieved, which promotes the practical development of future automatic transplanting robots."
1d-SAX와 Transformer 딥러닝을 이용한 KOSPI200 지수 변동성 추정,2022,"['금융시계열', '변동성', '1d-SAX', '트랜스포머', '딥러닝']","금융시계열의 변동성은 현대 금융공학과 위험관리 분야에서 가장 중요하게 다루는 변수이다. 기존의 통계적 방법을 통한 정확한 수치적 예측보다는 실무적인 관점에서 유의미한 변동성 변화의 예측을 목표로 하여 실험을 하였다. 시계열의 차원축소를 위하여 1d-SAX를 통한 기호화를 하였으며 기호화된 시계열을 자연어처리 분야에서 두각을 나타낸 트랜스포머 딥러닝 모델을 이용하여 학습하였다. 시계열을 단어 형태의 순차적 기호열로 변환함으로써 내재된 반복 패턴을 예측하고자 하였으며 기존 자연어/시계열 처리 분야에 많이 사용되었던 LSTM 모델과 비교를 하였다. 실험 결과로 트랜스포머가 LSTM보다 더 높은 정확도로 예측을 하였으며 또한 금융시계열을 이산화, 기호화를 통해 근사함으로써 금융시장의 패턴 인식 가능성이 있음을 보였다.",다국어 초록 정보 없음
BERT model 기반 텍스트 감정 분석,2022,[],"본 논문은 최근 다양한 시장에서 고객 응대에 적극 활용중인 sentiment analysis task에 대해 다룬다. main model은 BERT(Bidirectional Encoder Representations from Transformers)와 GRU(Gated Recurrent Unit) model으로 구성되며, IMDB dataset을 사용한다. BERT를 통해 tokenized IMDB dataset의 representation을 추출하고, multi-layer bi-directional GRU를 통해 추출된 hidden state를 linear layer에 통과시켜서 text에 내제된 감정과 의도를 0(Negative)에서 1(Positive) 사이의 예측값으로 표현한다. 실험 과정과 모델의 구조를 상세히 서술하였으며, main model의 구조를 도식화하였다. 추가적으로, train 및 test 정확도를 측정하는 함수를 구현하여 이를 통해 실험 결과를 표로 정리하였다.",다국어 초록 정보 없음
강화학습을 이용하여 인공신경망에 대한 adversarial attack을 방어하는 affine transformer에 관한 연구,2022,[],본 논문은 사전훈련된 이미지 인식 모델의 adversarial attack을 방어하기 위해서 이미지가 모델의 입력되기 전에 강화학습을 통해 선정된 매개 변수를 이용하여 affine transform하여 적대적 공격에 대해 방어하는 이미지를 생성하는 알고리즘을 제안하였다. MNIST 데이터셋에 대해 정확도가 기존 98.5%에서 15.94%까지 떨어지게하는 FGSM 방식의 adversarial attack에 대해 본 논문의 알고리즘을 적용했을 때 68.12%까지 방어하는데 성공하였다.,다국어 초록 정보 없음
토픽모델링을 활용한 국내 메타버스 연구동향 분석: 2007~2022년 학술연구를 중심으로,2022,"['Metaverse', 'Topic Modeling', 'Bidirectional Encoder Representations from Transformers(BERT)', 'Research Trend', 'Big Data', '메타버스', '토픽모델링', 'Bidirectional Encoder Representations from Transformers 모델', '연구동향', '빅데이터']","본 연구는 토픽모델링을 활용하여 메타버스 연구동향을 분석함으로써 향후 성공적인 메타버스 환경의 구현과 연구 방향을 모색하고자 하였다. 메타버스 키워드의 KCI 학술논문 총 582편에 대해 정량적 빈도분석과 BERT모델을 활용한 토픽모델링을 수행하였다. 그 결과, 2021년 이후 연구가 급증한 점과 사회과학, 복합학 분야의 비중이 크다는 점을 확인하였다. 연구토픽의 주요 키워드는 ‘교육’, ‘학습’, ‘온라인’, ‘현실’, ‘가상’, ‘의도’, ‘사회적’, ‘음악’, ‘패션’, ‘플랫폼’, ‘빅데이터’ 등의 순으로 나타났다. 토픽 주제어는 ‘교육적 활용’과 ‘가상현실과 세계’로 약 40%로 큰 비중을 차지하였으며, ‘사용의도’, ‘엔터테인먼트’, ‘패션산업’, ‘플랫폼디자인’, ‘빅데이터분석’ 등이 그 뒤를 따랐다. 세 개 시사점은 첫째, 메타버스 기반기술이나 비즈니스 모델 개발이 더욱 활발히 이루어져야 하고 둘째, 새로운 법리와 규율 연구가 선행되어야 하며 셋째, 메타버스 환경의 새로운 문제를 예측, 규명하기 위한 연구가 더욱 중요하다는 것이다. 연구결과가 디지털전환 시대에 국내 연구의 초석이 되고 디지털사회 구현에 이바지할 것을 기대한다.","This study analyzed research trends using topic modeling and intended to explore research direction of a successful metaverse environment. Quantitative frequency analysis and topic modeling using the BERT were performed on a 582 articles. As a result, it was confirmed that research has increased rapidly after 2021 and that the proportion of social science is large. The topic’s subject were 'educational use' and 'virtual reality world', accounting for about 40%. Three implications were that the development of the underlying technology or business model should be more active, research on new discipline should be preceded and, research to predict new problems is more important. It is expected that the results will become a cornerstone of research and contribute to the realization of a digital society."
Improving Transformer with Dynamic Convolution and Shortcut for Video-Text Retrieval,2022,"['Video representation', 'cross-modal retrieval', 'Multi-modal', 'Local Descriptors', 'Transformer']",국문 초록 정보 없음,"Recently, Transformer has made great progress in video retrieval tasks due to its high representation capability. For the structure of a Transformer, the cascaded self-attention modules are capable of capturing long-distance feature dependencies. However, the local feature details are likely to have deteriorated. In addition, increasing the depth of the structure is likely to produce learning bias in the learned features. In this paper, an improved Transformer structure named TransDCS (Transformer with Dynamic Convolution and Shortcut) is proposed. A Multi-head Conv-Self-Attention module is introduced to model the local dependencies and improve the efficiency of local features extraction. Meanwhile, the augmented shortcuts module based on a dual identity matrix is applied to enhance the conduction of input features, and mitigate the learning bias. The proposed model is tested on MSRVTT, LSMDC and Activity-Net benchmarks, and it surpasses all previous solutions for the video-text retrieval task. For example, on the LSMDC benchmark, a gain of about 2.3% MdR and 6.1% MnR is obtained over recently proposed multimodal-based methods."
Disturbance observer-based transformer current estimation for bidirectional dual-active-bridge DC-DC converter using LMI-based optimization,2022,"['dual-active-bridge dc-dc converter', 'high-frequency transformer current', 'linear matrix inequality', 'disturbance observer', 'extended state observer']",국문 초록 정보 없음,"This paper proposes a high-frequency transformer current estimation for dual-active-bridge (DAB) dc-dc converters. The DAB converter model is formulated based on the first-order term of the Fourier series of transformer current and output voltage. Due to the high frequency of transformer current, the measurement of transferred current is not available for the digital control design. Therefore, the Fourier first-order coefficient of transformer current, one of the states in the converter model, will be estimated by the proposed observer. The partial unknown load is treated as the unknown disturbance, which is then considered as an extended state to form an augmented system. Then, the optimization problem for the observer gain is formulated using linear matrix inequalities (LMI) for the augmented system including the state and disturbance. In this optimization, the LMIs are established such that the observer gain is stable in the entire range of phase-shift ratio. The accuracy of transformer current estimation is verified via FFT analysis at various power loads in simulations."
Automatic Linkage Model of Classification Systems Based on a Pretraining Language Model for Interconnecting Science and Technology with Job Information,2022,"['linkage of classification systems', 'text analysis', 'pre-trained language model', 'Bidirectional Encoder Representations from Transformers']",국문 초록 정보 없음,"For national industrial development in the Fourth Industrial Revolution, it is necessary to provide researchers with appropriatejob information. This can be achieved by interconnecting the National Science and Technology Standard Classification Systemused for management of research activity with the Korean Employment Classification of Occupations used for job informationmanagement. In the present study, an automatic linkage model of classification systems is introduced based on a pre-trainedlanguage model for interconnecting science and technology information with job information. We propose for the first time anautomatic model for linkage of classification systems. Our model effectively maps similar classes between the National Science &Technology Standard Classification System and Korean Employment Classification of Occupations. Moreover, the model increasesinterconnection performance by considering hierarchical features of classification systems. Experimental results show thatprecision and recall of the proposed model are about 0.82 and 0.84, respectively."
English Translation Based on Neural Machine Translation Using Transformer,2022,"['machine translation', 'attention mechanism', 'transformer', 'neural machine  translation', 'parallel corpus', 'BLEU']",국문 초록 정보 없음,"Machine translation is one of the classic sub-fields of natural language processing, a field that has been studied for a long time. There are several methods related to machine translation, and neural machine translation is one of them. Neural machine translation is a method that translates the source sentence to the target sentence through a neural network model. Recently, the transformer model using the attention mechanism has become the SOTA technique in the field. In this research, the transformer model using the attention mechanism is trained through 2 parallel corpora: English-German and English-Korean. The results of translations are evaluated with a BLEU score for each. The transformer shows an 8.60 BLEU score when trained by the English-German parallel corpora, and a 0.43 BLEU score in the case of English-Korean. It seems like the reason why the performance of English-German translations is higher than the performance of English-Korean translations is that English and Germany are in the Germanic language family, which is the same. In contrast, unlike English, Korean belongs to Altaic or Isolated languages and has different linguistic characteristics. The performance of the transformer is also affected by the quality of the parallel corpus used for training. It can be interpreted that it is necessary to secure a higher quality English-Korean parallel corpus."
A study on handwritten parcel delivery invoice understanding model,2022,"['Optical character recognition', 'Handwritten parcel delivery invoice understanding', 'End-to-End framework', 'Document understanding transformer']",국문 초록 정보 없음,"Optical character recognition (OCR) technology is a field of continuous research in which text images are stored or utilized as data. However, OCR technology alone has limitations in classifying and recognizing attribute values such as address, name, and phone number expressed in text in a semi-structured form, such as a parcel delivery invoice (PDI) written by hand. Therefore, in this study, we propose a handwritten parcel delivery invoice understanding (HPDIU) model for automated parcel delivery reception. The proposed HPDIU model consists of two steps: region detection of the parcel delivery invoice (RD-PDI) and information extraction from the PDI (IE-PDI). The RD-PDI, which is the first step, minimizes the resolution adjustment by detecting only the necessary area, including the sender and recipient information in the image. The second step, IE-PDI, consists of an end-to-end framework without OCR technology using a document understanding transformer and integrates the process of character detection, recognition, and understanding.In other words, the proposed model can solve the limitations of the OCR technology because it can integrate the process of classifying and recognizing according to attributes. To prove the validity of the proposed model, we used 500 handwritten PDI datasets to evaluate the accuracy of the character units according to the attributes of address, name, and phone number. As a result of the evaluation, a total average of 91.67% in units of letters proved the superiority of the proposed HPDIU model."
Simplified model prediction current control strategy for permanent magnet synchronous motor,2022,"['PMSM', 'Model predictive current control', 'Dual voltage vector selection', 'Sector judgment', 'Cost function', 'Computation load']",국문 초록 정보 없음,"A simplified model predictive current control strategy based on mathematical auxiliary line method is proposed to increase the permanent magnet synchronous motor (PMSM) predictive current control system's steady-state performance while reducing the system's computational complexity. The mathematical models of current prediction, first-order compensation, and cost function are presented by analyzing the prediction current control strategy of the conventional models. Within a control cycle, the simplified model predictive current control algorithm applies two voltage vectors and omits the error calculation of cost function. The anticipated voltage of transformer obtained from the beat-free current control standard is considered the reference voltage, and the judgment of the area, where the reference voltage vector is found and the strategy for double voltage vector is selected, is put forward. The optimal voltage vector can be directly output without the error calculation of the cost function in the method, thereby effectively reducing the computation load and complexity of the system and improving the steady-state performance. Experimental results verify the validity and accuracy of the proposed simplified model of predictive current control strategy for PMSM control."
Fast offline transformer-based end-to-end automatic speech recognition for real-world applications,2022,"['connectionist temporal classification', 'end-to-end', 'speech recognition', 'transformer']",국문 초록 정보 없음,"With the recent advances in technology, automatic speech recognition (ASR) has been widely used in real-world applications. The efficiency of converting large amounts of speech into text accurately with limited resources has become more vital than ever. In this study, we propose a method to rapidly recognize a large speech database via a transformer-based end-to-end model. Transformers have improved the state-of-the-art performance in many fields. However, they are not easy to use for long sequences. In this study, various techniques to accelerate the recognition of real-world speeches are proposed and tested, including decoding via multiple-utterance-batched beam search, detecting end of speech based on a connectionist temporal classification (CTC), restricting the CTC-prefix score, and splitting long speeches into short segments. Experiments are conducted with the Librispeech dataset and the real-world Korean ASR tasks to verify the proposed methods. From the experiments, the proposed system can convert 8 h of speeches spoken at real-world meetings into text in less than 3 min with a 10.73% character error rate, which is 27.1% relatively lower than that of conventional systems."
TransNav: spatial sequential transformer network for visual navigation,2022,"['visual navigation', 'knowledge graph', 'reinforcement learning', 'spatial attention', 'transformer network']",국문 초록 정보 없음,"Visual navigation task is to steer an embodied agent finding the given target based on observation. The effective transformer from observation of the agent to visual representation determines the navigation actions and promotes more informed navigation policy. In this work, we propose a spatial sequential transformer network (SSTNet) for learning informative visual representation in deep reinforcement learning. SSTNet is composed by spatial attention probability fused model (SAF) and sequential transformer network (STNet). SAF enforces cross-modal state into visual clues in reinforcement learning. It encodes semantic information about observed objects, as well as spatial information about their location, which jointly exploiting image inter-relations. STNet generates (imagines) the next observations and makes action inference of the aspects most relevant to the target. It decodes the image intra-relations. This way, the agent learns to understand the causality between navigation actions and dynamic changes in observations. SSTNet is conditioned on an auto-regressive model on the desired reward, past states, actions, and knowledge graph. The whole navigation framework considers the local and global visual information, as well as time sequential information. Thus, it allows the agent to navigate towards the sought-after object effectively. We evaluate our model on the AI2THOR framework show that our method attains at least $10\%$ improvement of average success rate over most state-of-the-art models. Code and datasets can be found in https://github.com/zhoukang123/SDTNet_2022."
The Modelling and Design of a Linear Variable Differential Transformer,2022,"['Linear variable differential transformer', 'FEM model', 'Theoretical model', 'Sensitivity', 'Linear region']",국문 초록 정보 없음,"In the design and improvement of an LVDT, theoretical analysis or numerical analysis can facilitate optimal design for the sensor performance by effectively and quickly predicting the measurement range and the sensitivity with the changes of design and process variables. In this study, analysis models of the LVDT were proposed through theoretical analysis and the finite element method (FEM), and the effects of design and process variables on the sensitivity and linear region of the LVDT according to the core motion were analyzed by the proposed models. The theoretical model for the relation between the output voltage and the change in core position, including the position before entering and after passing the secondary coil, was developed by deriving the change in the mutual inductance of the primary and secondary coils. Meanwhile, the core, coil, magnetic shell, electric circuit, and core movement of the LVDT were constructed as a three-dimensional model for the FEM to obtain the voltage output using a commercial analysis program. The results of the LVDT output characteristics analyzed by the theoretical and the finite element models were mutually verified. By the verified models, a series of the analyses of the LVDT were performed with changes in the supply voltage, core size, number of primary and secondary turns, distance between coils, coil length, initial core position, and permeabilities of core and magnetic shell. The effects of those variables on the sensitivity and linear region of the LVDT could then be revealed."
A BERT-Based Automatic Scoring Model of Korean Language Learners' Essay,2022,"['Automatic Writing Scoring', 'Bidirectional Encoder Representations from Transformers', 'Korean as a Foreign Language', 'Natural Language Processing']",국문 초록 정보 없음,"This research applies a pre-trained bidirectional encoder representations from transformers (BERT) handwritingrecognition model to predict foreign Korean-language learners’ writing scores. A corpus of 586 answersto midterm and final exams written by foreign learners at the Intermediate 1 level was acquired and used forpre-training, resulting in consistent performance, even with small datasets. The test data were pre-processedand fine-tuned, and the results were calculated in the form of a score prediction. The difference between theprediction and actual score was then calculated. An accuracy of 95.8% was demonstrated, indicating that theprediction results were strong overall; hence, the tool is suitable for the automatic scoring of Korean writtentest answers, including grammatical errors, written by foreigners. These results are particularly meaningful inthat the data included written language text produced by foreign learners, not native speakers."
Layer-wise Semantic Role Labeling with the KR-BERT Language Model,2022,"['semantic role labeling', 'Korean neural language model', 'performance assessment', 'layer-wise analysis', 'heatmap analysis']",국문 초록 정보 없음,"The purpose of this study is to assess the performance of semantic role labeling (SRL) predicted by the neural language models (NLMs, or Transformer-based pre-trained models) of Korean. First, the study built two models: the KR-BERT-BiLSTM-CRF model and the KR-BERT-Verb Position Feature (VPF)-BiLSTM-CRF model. The results from testing these two models show that the KR-BERT-VPF-BiLSTM-CRF model (67.3%) outperformed the KR-BERT-BiLSTM-CRF model (66.4%). In addition, this study examined which hidden layer improved the performance of NLMs during training. As expected, the NLM that was trained on the last hidden layer performed better than other alternative options such as the second-to-last-hidden layer and the concatenated last four layers. Thus, this study renders support to the general observation that an NLM should be trained on the last hidden layer to reach the highest performance. This study is meaningful since it is the first attempt to investigate which hidden layer is useful to train NLMs in SRL tasks of Korean."
Structural reliability analysis using temporal deep learning-based model and importance sampling,2022,"['deep learning algorithm', 'numerical simulation', 'reliability analysis', 'stochastic processes', 'structural engineering']",국문 초록 정보 없음,"The main idea of the framework is to seamlessly combine a reasonably accurate and fast surrogate model with the importance sampling strategy. Developing a surrogate model for predicting structures’ dynamic responses is challenging because it involves high-dimensional inputs and outputs. For this purpose, a novel surrogate model based on cutting-edge deep learning architectures specialized for capturing temporal relationships within time-series data, namely Long-Short term memory layer and Transformer layer, is designed. After being properly trained, the surrogate model could be utilized in place of the finite element method to evaluate structures’ responses without requiring any specialized software. On the other hand, the importance sampling is adopted to reduce the number of calculations required when computing the failure probability by drawing more relevant samples near critical areas. Thanks to the portability of the trained surrogate model, one can integrate the latter with the Importance sampling in a straightforward fashion, forming an efficient framework called TTIS, which represents double advantages: less number of calculations is needed, and the computational time of each calculation is significantly reduced. The proposed approach’s applicability and efficiency are demonstrated through three examples with increasing complexity, involving a 1D beam, a 2D frame, and a 3D building structure. The results show that compared to the conventional Monte Carlo simulation, the proposed method can provide highly similar reliability results with a reduction of up to four orders of magnitudes in time complexity."
A Fault Diagnosis Method of Oil-Immersed Transformer Based on Improved Harris Hawks Optimized Random Forest,2022,"['Improved Harris Hawks optimization algorithm', 'Dissolved gas analysis', 'Oil-immersed transformer', 'Fault diagnosis', 'Random forest']",국문 초록 정보 없음,"In order to improve the accuracy and reliability of fault diagnosis for oil-immersed transformers, a fault diagnosis method for oil-immersed transformers based on improved Harris Hawks optimized random forest is proposed in this paper. First, logistic chaotic mapping is used to adjust the key parameters of the algorithm; then a nonlinear energy factor adjustment strategy is used to control the algorithm to transition from global search to local search; fi nally, the method of Gaussian mutation is introduced to strengthen the local search ability, and when the algorithm is stagnant, fi refl y perturbation is performed on the optimal solution to make the algorithm jump out of local optimum. The number of n_trees and n_layers of the random forest are jointly optimized by the improved Harris Hawks optimization algorithm, and the fault diagnosis model of oil-immersed transformer is established. The noncoded ratios of dissolved characteristic gases in oil are used as the characteristic input of the diagnosis model to obtain the fi nal diagnosis results. Compared with other models and verifi ed by examples, the results show that the proposed method has the advantage of high diagnostic accuracy and has certain practical engineering application value."
TransformRec: User-Centric Recommender System for e-Commerce Using Transformer,2022,"['TransformRec', 'User-Centric', 'Recommender System', 'e-Commerce']",국문 초록 정보 없음,"We propose a new User-Centric recommender system using Transformer model called TransformRec, which uses receipt data without personal information and identity and considers only the relationships between tokenized product names. TransformRec recommends a product based on its most recent receipt, which includes product names. Although a receipt includes a product that the Transformer has not learned, TransformRec can recommend a real product that is considered as most relevant to the user’s last purchase. We used two commercial datasets, an e-commerce dataset and Instacart dataset, and compared the performances of TransformRec, TransformRec without tokenizing, and Word2Vec. The experimental results demonstrated that the performance of TransformRec is superior to that of the other two models. Thus, we conclude that it is possible to recommend a product without using user identity or demographic information with higher performances. In addition, we confirmed that reflecting the relationship among tokens can improve recommendation performance."
Structural health monitoring data anomaly detection by transformer enhanced densely connected neural networks,2022,"['martensitic transformations', 'microstructural modelling', 'plastic deformation', 'shape memory alloys', 'TiNi', 'two-way shape memory', 'anomalous data', 'anomaly detection attention mechanism', 'deep learning', 'structural health monitoring']",국문 초록 정보 없음,"Guaranteeing the quality and integrity of structural health monitoring (SHM) data is very important for an effective assessment of structural condition. However, sensory system may malfunction due to sensor fault or harsh operational environment, resulting in multiple types of data anomaly existing in the measured data. Efficiently and automatically identifying anomalies from the vast amounts of measured data is significant for assessing the structural conditions and early warning for structural failure in SHM. The major challenges of current automated data anomaly detection methods are the imbalance of dataset categories. In terms of the feature of actual anomalous data, this paper proposes a data anomaly detection method based on data-level and deep learning technique for SHM of civil engineering structures. The proposed method consists of a data balancing phase to prepare a comprehensive training dataset based on data-level technique, and an anomaly detection phase based on a sophisticatedly designed network. The advanced densely connected convolutional network (DenseNet) and Transformer encoder are embedded in the specific network to facilitate extraction of both detail and global features of response data, and to establish the mapping between the highest level of abstractive features and data anomaly class. Numerical studies on a steel frame model are conducted to evaluate the performance and noise immunity of using the proposed network for data anomaly detection. The applicability of the proposed method for data anomaly classification is validated with the measured data of a practical supertall structure. The proposed method presents a remarkable performance on data anomaly detection, which reaches a 95.7% overall accuracy with practical engineering structural monitoring data, which demonstrates the effectiveness of data balancing and the robust classification capability of the proposed network."
Convolutional Neural Network based Partial Discharge Detection for Power Transformer using HFCT sensor,2022,"['Fault detection', 'phase-resolved partial discharge', 'power transformer', 'convolutional neural network']",국문 초록 정보 없음,"Fault detection in power equipment is an essential part for the stability of the power grids. Partial discharge inpower equipment is used to classify the insulation status of the power equipments. In this paper, a deep neural network based fault classification algorithm is proposed using phase resolved partial discharges (PRPDs) in power transformer. Convolutional neural network (CNN) is used as a classification algorithm for PRPDs from high frequency current transformer (HFCT). In addition, we apply our CNN model to edge devices. Experimental results showed that the proposed CNN method outperforms other machine learning (ML) based classification algorithms."
Negative Polarity Items in English: A Deep Learning Model and Statistical Analysis,2022,"['NPI', 'BERT&lt', 'SUB&gt', 'LARGE&lt', '/SUB&gt', 'licensor', 'scope', 'monotonicity']",국문 초록 정보 없음,"This paper examined English negative polarity items (NPIs) with a deep learning model and statistical analysis. Warstadt et al. (2019a) developed a deep learning model with the Bidirectional Encoder Representations from Transformers (BERT), and the study analyzed English NPIs. Their dataset included a total of 400,000 sentences, and they encoded a few linguistic variables per each sentence. This paper took the same dataset in Warstadt et al. (2019a), but one more factor (Monotonicity) was included. As for the deep learning model, this paper took the BERTLARGE model in Lee (2021), where the syntactic acceptability was calculated with numeric scores (0~100), rather than measured by the binary classification. After the acceptability scores were calculated per each sentence, the scores were converted with z -scores, and statistical analysis was conducted with six linguistic factors and interactions of three factors. Through the analysis, the followings were observed: (i) an NPI had to exist within the scope of the licensor, (ii) downward entailment also played a significant role in the NPI licensing, and (iii) the interaction between NPI licensor and scope played more crucial roles in the NPI licensing."
Performance Evaluation of Traffic Object Detection Using DINO Model,2022,"['Object Detection', 'Detection Transform', 'End-to-End Detector']",국문 초록 정보 없음,"In this paper, by cross-applying the DINO (DETR with Improved deNoising anchOrboxes) model to various datasets, we examine what characteristics of dataset are effective for traffic object detection training. DINO model is best DETR (DEtection with TRansformer)-like model in object detection. For the experiment, a total of two datasets were used: COCO and BDD100K datasets. As a result of evaluation with BDD100K dataset which contains diverse driving images, dataset with the same texture as the evaluation dataset showed similar performance with less data than the high texture dataset focused on each object."
Creating Knowledge Graph of Electric Power Equipment Faults Based on BERT–BiLSTM–CRF Model,2022,"['Knowledge graph', 'Electric power equipment', 'Fault diagnosis', 'Entity recognition', 'Relation extraction']",국문 초록 정보 없음,"Creating a large-scale knowledge graph of electric power equipment faults will facilitate the development of automatic fault diagnosis and intelligent question answering (QA) in the electric power industry. However, most existing methods have lower accuracy in Chinese entity recognition, thus it is hard to build such a high-quality knowledge graph by extracting knowledge from Chinese technical literature. To solve the problem, a novel model called BERT–BiLSTM–CRF is proposed. It blends Bi-directional Encoder Representation from Transformers (BERT), Bi-directional Long Short-Term Memory (BiLSTM), and Conditional Random Field (CRF). The model fi rstly identifi es and extracts electric power equipment entities from preprocessed Chinese technical literature. Then, the semantic relations between the entities are extracted based on the relation classifi cation method based on dependency parsing. Finally, the extracted knowledge is stored in the Neo4j database in the form of the triplet and visualized in the form of a graph. Through the above steps, a Chinese knowledge graph of electric power equipment faults can be built. The novelty of the model just lies in its subtle blend: the BERT module can not only learn phrase-level information representation, but also learn rich semantic information features; the CRF module realizes the constraint on the label prediction value and reduces the irregular recognition rate, so the accuracy rate of entity recognition is improved. Taking the Chinese technological literature, which is about fault diagnosis of electric power equipment as the experimental object, the experimental results show that the model identifi es and extracts Chinese entities more accurately than traditional methods. Thus, a comprehensive and accurate Chinese knowledge graph of electric power equipment faults could be constructed more easily."
Elite Polarization in South Korea: Evidence from a Natural Language Processing Model,2022,"['political polarization', 'elite polarization', 'natural language processing', 'BERT', 'text classification', 'South Korea']",국문 초록 정보 없음,This study analyzes political polarization among the South Korean elite by examining 17 years’ worth of subcommittee meeting minutes from the South Korean National Assembly's standing committees. Its analysis applies various natural language processing techniques and the bidirectional encoder representations from the transformers model to measure and analyze polarization in the language used during these meetings. Its findings indicate that the degree of political polarization increased and decreased at various times over the study period but has risen sharply since the second half of 2016 and remained high throughout 2020. This result suggests that partisan political gaps between members of the South Korean National Assembly increase substantially.
Semantic Vector Learning and Visualization with Semantic Cluster Using Transformers in Natural Language Understanding,2022,"['Semantic vector', 'Semantic vector learning', 'Natural language understanding', 'Transformer', 'Cluster-aware', 'Visualization']",국문 초록 정보 없음,"Natural language understanding (NLU) is a fundamental technology for implementing natural interfaces. The embedding of sentences and correspondence between text and its extracted semantic knowledge, called semantic frame, has recently shown that a semantic vector representation is key in the implementation or support of robust NLU systems. Herein, we propose an extension of cluster-aware modeling with various types of pre-trained transformers for consideration of the many-to-1 relationships of text-to-semantic frames and semantic clusters. To attain this, we define the semantic cluster, and design the relationships between cluster members to learn semantically meaningful vector representations. In addition, we introduce novel ensemble methods to improve the semantic vector applications around NLU, i.e., similarity-based intent classification and a semantic search. Furthermore, novel semantic vector and corpus visualization techniques are presented. Using the proposed framework, we demonstrate that the proposed model can learn meaningful semantic vector representations in ATIS, SNIPS, SimM, and Weather datasets."
SwinE-Net: hybrid deep learning approach to novel polyp segmentation using convolutional neural network and Swin Transformer,2022,"['polyp segmentation', 'convolutional neural networks', 'multidilation convolutional block', 'multifeature aggregation block', 'Swin Transformer', 'Vision Transformer']",국문 초록 정보 없음,"Prevention of colorectal cancer (CRC) by inspecting and removing colorectal polyps has become a global health priority because CRC is one of the most frequent cancers in the world. Although recent U-Net-based convolutional neural networks (CNNs) with deep feature representation and skip connections have shown to segment polyps effectively, U-Net-based approaches still have limitations in modeling explicit global contexts, due to the intrinsic nature locality of convolutional operations. To overcome these problems, this study proposes a novel deep learning model, SwinE-Net, for polyp segmentation that effectively combines a CNN-based EfficientNet and Vision Transformer (ViT)-based Swin Ttransformer. The main challenge is to conduct accurate and robust medical segmentation in maintaining global semantics without sacrificing low-level features of CNNs through Swin Transformer. First, the multidilation convolutional block generates refined feature maps to enhance feature discriminability for multilevel feature maps extracted from CNN and ViT. Then, the multifeature aggregation block creates intermediate side outputs from the refined polyp features for efficient training. Finally, the attentive deconvolutional network-based decoder upsamples the refined and combined feature maps to accurately segment colorectal polyps. We compared the proposed approach with previous state-of-the-art methods by evaluating various metrics using five public datasets (Kvasir, ClinicDB, ColonDB, ETIS, and EndoScene). The comparative evaluation, in particular, proved that the proposed approach showed much better performance in the unseen dataset, which shows the generalization and scalability in conducting polyp segmentation. Furthermore, an ablation study was performed to prove the novelty and advantage of the proposed network. The proposed approach outperformed previous studies."
Hot Keyword Extraction of Sci-tech Periodicals Based on the Improved BERT Model,2022,"['Bidirectional encoder', 'hot keyword', 'representations from transformers (bert)', 'sci-tech periodicals', 'similarity measurement']",국문 초록 정보 없음,"With the development of the economy and the improvement of living standards, the hot issues in the subject area have become the main research direction, and the mining of the hot issues in the subject currently has problems such as a large amount of data and a complex algorithm structure. Therefore, in response to this problem, this study proposes a method for extracting hot keywords in scientific journals based on the improved BERT model.It can also provide reference for researchers,and the research method improves the overall similarity measure of the ensemble,introducing compound keyword word density, combining word segmentation, word sense set distance, and density clustering to construct an improved BERT framework, establish a composite keyword heat analysis model based on I-BERT framework.Taking the 14420 articles published in 21 kinds of social science management periodicals collected by CNKI(China National Knowledge Infrastructure) in 2017-2019 as the experimental data, the superiority of the proposed method is verified by the data of word spacing, class spacing, extraction accuracy and recall of hot keywords. In the experimental process of this research, it can be found that the method proposed in this paper has a higher accuracy than other methods in extracting hot keywords, which can ensure the timeliness and accuracy of scientific journals in capturing hot topics in the discipline, and finally pass Use information technology to master popular key words."
Baseline of SWOT Classification using Bidirectional Encoder Representations from Transformers for Business Intelligence Cloud Platform,2022,"['SWOT Analysis', 'Business Intelligence', 'Bidirectional Encoder Representations from Transformers(BERT)', 'Cloud Platform']",국문 초록 정보 없음,"Companies establish business strategies through environmental analysis to generate the best performance. Accurately grasping the external and internal environment of a company and achieving the long- and short-term goals leads to an increase in the companys performance. As is well known, SWOT (Strength, Weakness, Opportunity, Threat) analysis is an analysis tool that can examine the strategic situation of a company by evaluating strengths and weaknesses inside the company, opportunities and threats outside the company. This study presents a deep learning framework that classifies information such as news articles, posts, etc., into S/W/O/T as the first step of a cloud platform that can provide real-time SWOT analysis of companies. This study presents baseline indicators by introducing the BERT model, which is widely used in the natural language processing field, for the first time in SWOT analysis. Starting with this approach, the baseline indicators of this study are expected to be useful for business intelligence cloud platforms that can be easily accessed by all stakeholders through deep learning of SWOT analysis."
다중 인코더 구조를 활용한 기계번역 품질 예측,2022,"['quality estimation', 'english-german translation', 'cross-lingual', 'pre-trained model', 'BERT', 'dual-encode', '기계번역 품질 예측', '영어-독일어 기계번역', '교차 언어', '사전학습 모델', 'BERT', '이중 인코더']","기계번역 품질 예측은 기계가 번역 문장의 품질을 주어진 정답 번역 문장이 없이 예측하는 것을 말한다. 최근에 품질 예측 분야의 연구 동향은 다량의 병렬 말뭉치로 학습된 트랜스포머 인코더 기반의 사전학습 모델을 이용하여 전이 학습을 적용한다. 본 논문에서는 품질 예측과 같은 교차 언어 태스크에서 단일 인코더 구조가 가지는 한계를 극복하기 위해 인코더에서 각 언어에 대한 단일 언어 표현을 개별적으로 학습하고 상호 참조망에서 교차 언어 표현을 학습하는 이중 인코더 구조를 제시한다. 이중 인코더 구조가 단일 인코더 구조보다 품질 예측 태스크에서 구조적으로 유리함을 입증하고, 나아가 이중 인코더 모델에 사전학습된 언어 모델을 적용하여 품질 예측 태스크에서 이중 인코더 모델의 성능과 안정성을 높인다. WMT20 품질 예측 영어-독일어 쌍에 대해서 실험을 진행했다. 사전학습 모델로서 영어 Bidirectional Encoder Representations from Transformers (BERT) 언어 모델과 독일어 BERT 언어 모델이 각각의 인코더에 적용되었고 가장 뛰어난 성능을 보여주었다.","Quality estimation (QE) is the task of estimating the quality of given machine translations (MTs) without their reference translations. A recent research trend is to apply transfer learning to a pre-training model based on Transformer encoder with a parallel corpus in QE. In this paper, we proposed a dual-encoder architecture that learns a monolingual representation of each respective language in encoders. Thereafter, it learns a cross-lingual representation of each language in cross-attention networks. Thus, it overcomes the limitations of a single-encoder architecture in cross-lingual tasks, such as QE. We proved that the dual-encoder architecture is structurally more advantageous over the single-encoder architecture and furthermore, improved the performance and stability of the dual-encoder model in QE by applying the pre-trained language model to the dual-encoder model. Experiments were conducted on WMT20 QE data for En-De pair. As pre-trained models, our model employs English BERT (Bidirectional Encoder Representations from Transformers) and German BERT to each encoder and achieves the best performance."
"LUKE를 이용한 한국어 자연어 처리: 개체명 인식, 개체 연결",2022,"['knowledge-enhanced language model', 'LUKE', 'named entity recognition', 'entity linking', 'BERT', 'RoBERTa', 'entity-aware self-attention', '지식 증강 언어 모델', 'LUKE', '개체명 인식', '개체 연결', 'BERT', 'RoBERTa', 'entity-aware 셀프어텐션']","BERT와 같은 트랜스포머 기반의 언어 모델은 대용량의 레이블이 없는 말뭉치를 자가 학습방법을 통해 학습한 후 다양한 자연어 처리 응용 태스크에 적용하여 놀라운 성능 향상을 보였다. 이와 같은 언어 모델은 실세계 지식 정보를 표현할 수 없는 단점이 존재하고 이러한 문제를 해결하기 위해 언어 모델에 지식 베이스를 반영하려는 다양한 연구들이 수행되었다. 본 연구에서는 단어 시퀀스 이외에 엔티티 시퀀스와 임베딩을 정의하고 단어와 엔티티의 모든 시퀀스 쌍에 따라 별도의 쿼리 파라미터를 두고 셀프 어텐션을 수행하는 LUKE 모델을 한국어 위키피디아 상에서 학습한 후 엔티티 관련 태스크인 개체명 인식, 개체 연결에 적용하여 기존의 RoBERTa 기반 모델 대비 각각 0.5%p, 1.05%p의 성능 향상을 가져왔다.","Transformer-based language models (LM) such as BERT trained from a large amount of unlabeled corpus using self-supervised learning methods have shown remarkable performance improvement on various natural language processing (NLP) application tasks. Despite the marked improvements, the classical pretrained language model has not directly incorporate external real-world knowledge bases such as a Wikipedia knowledge graph or triples. To inject the real-world knowledge bases to a pretrained language model, many studies towards “knowledge enhanced” pretrained language models have been conducted. Among them, LUKE attaches a sequence of entities to a sequence of original input tokens and performs entity-aware self-attention using entity embeddings, leading to noticeable improved results on entity-related tasks and the state-of-the-art performance in SQuAD dataset. In this paper, we present a Korean version of LUKE pretrained from a large amount of Korean Wikipedia corpus and show its application results on entity-related tasks of Korean. In particular, we newly propose a way of applying LUKE to the entity linking task which has not been explored in the previous works of using LUKE. Experiment results on both Korean named entity recognition and entity linking tasks show improvements over the RoBERTa-based models."
PrefixLM에 기반한 한국어 텍스트 요약,2022,"['language model', 'text generation', 'document summarization', 'PrefixLM', '언어 모델', '텍스트 생성', '문서 요약', 'PrefixLM']","본 논문에서는 거대 언어 모델 중 하나인 T5의 인코더-디코더 구조 대비 절반의 크기를 가지는 PrefixLM 구조의 한국어 모델을 학습하여 성능을 확인한다. PrefixLM 모델은 입력과 출력 시퀀스가 단일 시퀀스로 연결되어 트랜스포머 블록에 함께 입력된다. 이때 어텐션 내부 연산 시 사용되는 어텐션 마스크의 변형을 통해 단일 트랜스포머 블록에서 입력 시퀀스 부분은 양방향 어텐션, 출력 시퀀스 부분은 단방향 어텐션이 이루어지도록 조정된다. 이를 통해 인코더와 디코더 역할을 한 레이어에서 수행할 수 있게 된다. 소규모 데이터로 한국어 모델을 여러 방식으로 학습한다. 자연어 처리에서 중요한 태스크 중 하나인 텍스트 생성 요약 태스크에서 기반 모델, 위치 인코딩 방식 등에 따른 성능 차이를 확인한다. BART, T5와 비교하여 각각 2.17, 2.78점의 성능 향상을 보여 PrefixLM 구조가 한국어에서도 충분히 유효함을 보인다.","In this paper, we examine the effectiveness of PrefixLM that consists of half of the parameters of the T5's encoder-decoder architecture for Korean text generation tasks. Different from T5 where input and output sequences are separately provided, the transformer block of PrefixLM takes a single sequence that concatenates both input and output sequences. By designing the attention mask, PrefixLM performs uni- and bi-directional attentions on input and output sequences, respectively, thereby enabling to perform two roles of encoder and decoder with a single transformer block.Experiment results on Korean abstractive document summarization task show that PrefixLM leads to performance increases of 2.17 and 2.78 more than 2 in Rouge-F1 score over BART and T5, respectively, implying that the PrefixLM is promising in Korean text generation tasks."
다중 출력 플라이백 전력변환기를 위한 다중 권선 변압기 모델링 및 시뮬레이션,2022,"['multiple output', 'multiple winding transformer', 'cross regulation', 'inductance', 'flyback converter']","본 논문에서는 다중 출력 플라이백 전력변환기 설계를 위한 새로운 다중 권선 변압기 모델을 제안하였다. 제안된 모델은 이상적인 단일 권선 변압기의 병렬연결을 통해 기존의 모델이 가지지 못했던 확장성을 가질 수 있으며, 자화 및 누설 인덕턴스에 의한 비이상적인 전류와 전압 특성을 독립적으로 분석할 수 있기 때문에 다중 권선 변압기의 해석을 용이하게 수행할 수 있다. 추가적으로, 모델의 정확도 및 장점을 확인하기 위해 불연속 전류모드에서 동작하는 이중 출력 플라이백 전력변환기를 설계하여 기존의 모델과 비교 분석을 수행함으로서 제안된 모델의 우수성을 확인하였다.","This paper proposes a novel multiple windings transformer model for multiple output flyback converters. The proposed model can have scalability through the parallel connection of an ideal single winding transformer and analyze independently actual current and voltage characteristics owing to magnetizing and leakage inductance, thus facilitating the detailed analysis of the multiple winding transformer. In addition, the proposed model was verified by designing a dual output flyback converter operating in discontinuous conduction mode and performing comparative analysis with existing model."
대학수학능력시험 독서 영역의 교육 목표를 위한 자연어처리 기법을 통한 검증,2022,"['대학수학능력시험', '딥러닝', '이진 분류 태스크', '언어 모델', '데이터 전처리', 'Korean SAT', 'Deep learning', 'Binary classification task', 'Language models']","대학수학능력시험 국어 과목에서 중요한 비중을 차지하는 독서 영역의 주된 교육 목표는 주어진 지문을 온전히 이해할 수 있는가를 평가하는 데에 있다. 따라서 해당 지문에 포함된 질의를 주어진 지문만으로 풀이할 수 있는지는 해당 영역의 교육 목표와 관련이 깊다. 본 연구에서는 처음으로, 교육학 분야와 딥러닝을 접목하여 이러한 교육 목표가 실제로도 타당하게 실현 가능한지를 입증하고자 한다. 대학수학능력시험의 독서 영역의 개별 지문과 그에 수반된 다수의 문장 쌍(sentence pair)을 정제하여 추출하고, 해당 문장 쌍을 주어진 지문에 비추어 적절하거나(T), 적절하지 않은지(F)를 판단하는 이진 분류 태스크(binary classification task)에 적용하여 평가 하고자 한다. 그 결과, F1 스코어 기준 59.2%의 human performance를 뛰어넘는 성능을 62.49%의 KoELECTRA를 비롯한 대부분의 언어 모델에서 확인할 수 있었으며, 또한 데이터 전처리 과정에 변화를 줌으로써 언어 모델의 구조적 한계를 극복할 수 있었다.","The major educational goal of reading part, which occupies important portion in Korean language in Korean SAT, is to evaluated whether a given text can be fully understood. Therefore given questions in the exam must be able to solely solvable by given text. In this paper we developed a datatset based on Korean SAT’s reading part in order to evaluate whether a deep learning language model can classify if the given question is true or false, which is a binary classification task in NLP. In result, by applying language model solely according to the passages in the dataset, we were able to acquire better performance than 59.2% in F1 score for human performance in most of language models, that KoELECTRA scored 62.49% in our experiment. Also we proved that structural limit of language models can be eased by adjusting data preprocess."
리뷰 데이터와 제품 정보를 이용한 멀티모달 감성분석,2022,"['감성분석', '멀티모달', '특성 추출', '인공 신경망', 'Sentiment Analysis', 'Multi-Modal', 'Feature Selection', 'Neural Network']","최근 의류 등의 특정 쇼핑몰의 온라인 시장이 크게 확대되면서, 사용자의 리뷰를 활용하는 것이 주요한 마케팅 방안이 되었다. 이를 이용한 감성분석에 대한 연구들도 많이 진행되고 있다. 감성분석은 사용자의 리뷰를 긍정과 부정 그리고 필요에 따라서 중립으로 분류하는 방법이다. 이 방법은 크게 머신러닝 기반의 감성분석과 사전기반의 감성분석으로 나눌 수 있다. 머신러닝 기반의 감성분석은 사용자의 리뷰 데이터와 그에 대응하는 감성 라벨을 이용해서 분류 모델을 학습하는 방법이다. 감성분석 분야의 연구가 발전하면서 리뷰와 함께 제공되는 이미지나 영상 데이터 등을 함께 고려하여 학습하는 멀티모달 방식의 모델들이 연구되고 있다.리뷰 데이터에서 제품의 카테고리와 사용자별로 사용되는 단어 등의 특징이 다르다. 따라서 본 논문에서는 리뷰데이터와 제품 정보를 동시에 고려하여 감성분석을 진행한다. 리뷰를 분류하는 모델로는 기본 순환신경망 구조에서 Gate 방식을 도입한 Gated Recurrent Unit(GRU), Long Short-Term Memory(LSTM) 그리고 Self Attention 기반의 Multi-head Attention 모델, Bidirectional Encoder Representation from Transformer(BERT)를 사용해서 각각 성능을 비교하였다. 제품 정보는 모두 동일한 Multi-Layer Perceptron(MLP) 모델을 이용하였다. 본 논문에서는 사용자 리뷰를 활용한 Baseline Classifier의 정보와 제품 정보를 활용한 MLP모델의 결과를 결합하는 방법을 제안하며 실제 데이터를 통해 성능의 우수함을 보인다.","Due to recent expansion of online market such as clothing, utilizing customer review has become a major marketing measure. User review has been used as a tool of analyzing sentiment of customers. Sentiment analysis can be largely classified with machine learning-based and lexicon-based method. Machine learning-based method is a learning classification model referring review and labels. As research of sentiment analysis has been developed, multi-modal models learned by images and video data in reviews has been studied. Characteristics of words in reviews are differentiated depending on products' and customers’ categories. In this paper, sentiment is analyzed via considering review data and metadata of products and users. Gated Recurrent Unit (GRU), Long Short-Term Memory(LSTM), Self Attention-based Multi-head Attention models and Bidirectional Encoder Representation from Transformer (BERT) are used in this study. Same Multi-Layer Perceptron (MLP) model is used upon every products information. This paper suggests a multi-modal sentiment analysis model that simultaneously considers user reviews and product meta-information."
언어적 특성과 서비스를 고려한 딥러닝 기반 한국어 방언 기계번역 연구,2022,"['Korean Dialect Translation', 'Machine Translation', 'Transformer', 'Multilingual Translation', 'Language Convergence', '한국어 방언 기계번역', '기계번역', '트랜스포머', '다언어 기계번역', '언어 융합']","본 논문은 방언 연구, 보존, 의사소통의 중요성을 바탕으로 소외될 수 있는 방언 사용자들을 위한 한국어 방언 기계번역 연구를 진행하였다. 사용한 방언 데이터는 최상위 행정구역을 기반으로 배포된 AIHUB 방언 데이터를 사용하였다. 방언 데이터를 바탕으로 Transformer 기반의 copy mechanism을 적용하여 방언 기계번역기의 성능 향상을 도모하는 모델링 연구와 모델 배포의 효율성을 도모하는 Many-to-one 기반의 방언 기계 번역기를 제안한다. 본 논문은 one-to-one 모델과 many-to-one 모델의 성능을 비교 분석하고 이를 다양한 언어학적 시각으로 분석하였다. 실험 결과 BLEU점수를 기준으로 본 논문이 제안하는 방법론을 적용한 one-to-one 기계번역기의 성능 향상과 many-to-one 기계번역기의 유의미한 성능을 도출하였다.","Based on the importance of dialect research, preservation, and communication, this paper conducted a study on machine translation of Korean dialects for dialect users who may be marginalized. For the dialect data used, AIHUB dialect data distributed based on the highest administrative district was used. We propose a many-to-one dialect machine translation that promotes the efficiency of model distribution and modeling research to improve the performance of the dialect machine translation by applying Copy mechanism. This paper evaluates the performance of the one-to-one model and the many-to-one model as a BLEU score, and analyzes the performance of the many-to-one model in the Korean dialect from a linguistic perspective. The performance improvement of the one-to-one machine translation by applying the methodology proposed in this paper and the significant high performance of the many-to-one machine translation were derived."
BERT 의 웹 문서 질의 응답 성능 향상을 위한 HTML 태그 스택 및 HTML 임베딩 기법 설계,2022,[],최근 기술의 발전으로 인해 자연어 처리 모델의 성능이 증가하고 있다. 그에 따라 평문 지문이 아닌 KorQuAD 2.0 과 같은 웹 문서를 지문으로 하는 기계 독해 과제를 해결하려는 연구가 증가하고 있다. 최근 기계 독해 과제의 대부분의 모델은 트랜스포머를 기반으로 하는 추세를 보인다. 그 중 대표적인 모델인 BERT 는 문자열의 순서에 대한 정보를 임베딩 과정에서 전달받는다. 한편 웹 문서는 태그 구조가 존재하므로 문서를 이해하는데 위치 정보 외에도 태그 정보도 유용하게 사용될 수 있다. 그러나 BERT 의 기존 임베딩은 웹 문서의 태그 정보를 추가적으로 모델에 전달하지 않는다는 문제가 있었다. 본 논문에서는 BERT 에 웹 문서 태그 정보를 효과적으로 전달할 수 있는 HTML 임베딩 기법 및 이를 위한 전처리 기법으로 HTML 태그 스택을 소개한다. HTML 태그 스택은 HTML 태그의 정보들을 추출할 수 있고 HTML 임베딩 기법은 이 정보들을 BERT 의 임베딩 과정에 입력으로 추가함으로써 웹 문서 질의 응답 과제의 성능 향상을 기대할 수 있다.,다국어 초록 정보 없음
UNETR 기반 3D MRI 뇌 영상에서 다발성 경화증 병변 검출,2022,"['multiple sclerosis lesion', 'brain MRI', 'UNETR', 'transformer', 'deep learning', '.']","다발성 경화증 진단은 매우 어렵고 병리사들의 노고가 많이 든다. 최근에 딥러닝 기술을 이용한 의료 영상 분석 방법의 발전으로 특정 질환 분류 및 진단 효율성과 정확도가 높아지고 있다. 본 논문에서는 트랜스포머를 인코더로 대체한 UNETR 딥러닝 모델을 기반으로 다발성 경화증 병변의 검출 방법을 제안한다. 특히, 의료 분야 특성상 영상 데이터 수가 적기 때문에 모델의 충분한 학습과 과적합을 방지하기 위해 데이터 증강 기법을 적용하였다. 제안하는 방법의 성능은 2008 MICCAI MS Lesion Segmentation Challenge 데이터셋을 사용하여 검증하였고, 그 결과 75.22% DSC와 56.40% mean IoU 정확도를 달성하여, Residual 3D U-Net 기반 방법에 대비하여 각각 4.54%, 3.61% 향상하였다.","Diagnosis of multiple sclerosis is difficult and requires a lot of effort by pathologists. Recently, with the development of medical image analysis methods using deep learning, the efficiency and accuracy of specific disease classification and diagnosis are increasing. In this paper, we propose a method for detecting multiple sclerosis lesions based on the UNETR deep learning model that replaces a transformer with encoders. In particular, since the number of image data is small due to the characteristics of the medical field, a data augmentation technique is applied to prevent sufficient training and overfitting of the model. The performance of the proposed method was verified using the 2008 MICCAI MS Lesion Segmentation Challenge dataset and achieved 75.22% DSC and 56.40% mean IoU accuracy, which improved 4.54% and 3.61% respectively compared to the Residual 3D U-Net-based model."
그룹 집중 기술로 개선된 Trans-Unet기반 단일 영상 연무제거 신경망,2022,"['Computer vision', 'Image dehazing', 'Vision transformer', 'Unet', 'Group attention block']",국문 초록 정보 없음,다국어 초록 정보 없음
Conformer-CTC 기반 한국어 음성인식,2022,[],"본 논문은 Conformer 와 CTC 를 결합한 모델을 기반으로 한국어 음성인식을 제안한다. Conformer 는 광역 정보를 잘 표현하는 Transformer 와 지역정보를 잘 표현하는 CNN 을 결합한 모델이며, CTC 는 사전 정렬되지 않은 음성과 문자열의 대응관계를 정렬하며 학습하는 loss 함수이다. Conformer 와 CTC 를 결합하면 음성과 문자열의 사전 정렬없이 음성의 광역정보와 지역정보를 활용하여 음성인식을 할 수 있는 장점이 있다. 제안한 모델은 로그 스펙트로그램을 입력 받으며 CNN 을 통해 로그 스펙트로그램의 주파수와 시간 해상도를 줄인다. 그 후 Conformer 를 통해 음향 특징을 추출하며 CTC 를 이용하여 학습된다. 제안한 모델의 성능 평가를 위해 한국어 통화 기반 음성 말뭉치 데이터 셋인 ClovaCall-Base를 사용하여 학습과 테스트를 하였으며 테스트 결과 13.1%의 음절 오류율로 기존의 연구보다 우수한 성능을 보였다.",다국어 초록 정보 없음
"BERT, LSTM과 GRU를 사용한 네트워크 이상 탐지 성능 비교",2022,[],"자연어 처리 분야 문제에서 state-of-the-art(SOTA)을 달성한 bi-directional encoder representations from transformers(BERT)는 미세 조정(fine-tuning) 기반 전이 학습법으로서, 위키피디아와 BooksCorpus 같이 레이블이 없는 텍스트 데이터를 트랜스포머(transformer)의 인코더로 사전 학습(pre-trained)한 모델이다. 본 논문에서는 HTTP 메시지 페이로드(payload) 기반 네트워크 이상 탐지 과업에 대해 자연어 처리 분야에서 뛰어난 성능을 보였던 long short term memory(LSTM), gate recurrent unit(GRU)와 BERT의 성능을 비교 분석하였다. 그 결과 HTTP 메시지 페이로드 데이터 집합에 대해 BERT가 순환 신경망 모델보다 더 높은 F1 점수와 정확도를 보였다. 이러한 실험 결과를 바탕으로 추후 네트워크 이상 탐지 분야에서 BERT의 활용 가능성에 관한 연구를 계획할 수 있을 것으로 기대한다.",다국어 초록 정보 없음
데이터 증강 기법을 통한 한국어-수어문 코퍼스(KoSLA) 구축,2022,"['Korean Sign Language', 'KSL', 'Machine Translation', 'Data Augmentation', 'Sign Language Corpus']","본 논문은 수어 번역을 위한 효율적 말뭉치 생성 프레임워크를 제시하였다. 단순하면서 효과적인 데이터 증강 기술을 적용하여 최소한의 정보 손실로 한국어 원문 텍스트를 수어의 최소 의미 단위로 변환하는 방식을 새롭게 제안하였다. 수지, 비수지 및 도상으로 구성된 수어의 언어적 특성을 고려하여 각 의미 상태를 모두 포함하는 멀티모달 수어 증강 코퍼스(KoSLA)를 구축하였다. 본 연구의 코퍼스로 병원 내 대화에서 의미 있는 기계번역 성과를 얻을 수 있었다. 특히, 수어의 문법 및 의미 구조를 유지하면서 번역 모델 및 가용 데이터의 효율성을 높이기 위해 동의어 대체와 같은 NLP 데이터 증강 기법을 일부 적용했으며, Transformer 모델에 실험을 진행한 결과, KoSLA 코퍼스가 BLEU 평가에서 대조군보다 높은 점수를 얻을 수 있었다.","We present an efficient corpus framework for sign language translation. Aided with a simple but dramatic data augmentation technique, our method converts text into annotated forms with minimum information loss. By considering the linguistic features of the sign language, which is composed of manual signal, non-manual signal and iconic parts, our proposed framework is the first attempt to build a multi-modal sign language augmentation corpus containing both manual and non-manual modalities. Our corpus demonstrates confident results in the hospital context, showing improved performance with augmented datasets. In the process, we resorted to some NLP data augmentation techniques such as synonym replacement to boost our models efficiency and available data, while maintaining grammatical and semantic structures of sign language. Several experiments were carried out on Transformer-based models. The results were convincing, showing that the BLEU score with corpus was significantly higher than the control group."
SimOn: A Simple Framework for Online Temporal Action Localization,2022,[],"실시간 행동 감지(Online Temporal Action Localization)는 편집되지 않은 실시간으로 재생되는 비디오에서 즉시 행동 인스턴스를 제공하는 것을 목표로 한다. 모델이 과거 예측을 수정하기 위해 미래의 프레임과 처리 기술을 사용할 수 없기 때문에 실시간 행동 감지를 훨씬 더 어려운 태스크로 만든다. 이 논문에서 우리는 최근 주목받는 트랜스포머(transformer) 구조를 사용하여 종단 간 학습방식으로 행동 인스턴스를 예측하는 방법을 배우는 SimOn이라는 간단하면서도 효과적인 프레임워크를 제안한다. 구제적으로, 모델은 현재 프레임의 특징을 쿼리 (query)로 사용하고 과거 맥락 정보 집합을 트랜스포머의 키 (key)와 값 (value)으로 사용한다. 모델의 과거 출력들을 과거의 맥락으로 사용하는 이전 연구들과는 달리 과거 프레임의 시각적 맥락과 현재 프레임 쿼리에 대한 학습 가능한 맥락 임베딩을 활용한다. 코드는 https://anonymous.4open.science/r/SimOn 에 공개되어 있다",다국어 초록 정보 없음
사용자의 구매만족도 향상을 위한 BERT 기반의 리뷰 분석 방법,2022,"['BERT', '딥러닝', '텍스트 분석', 'BERT', 'Deep Learning', 'Text Analysis']","본 논문은 BERT(Bidirectional Encoder Representations from Transformers) 기반의 선호도 분석 모델을 통한 쇼핑몰 상품 분석 플랫폼으로 사용자의 구매만족도를 향상하기 위한 리뷰 분석 시스템을 제안한다. 온라인 쇼핑몰의 발달로 다양한 제품들을 온라인으로 판매함에 따라 구매자는 자신의 선호도에 맞는 상품을 구매하기 점차 어려워지고, 원하는 상품을 고르는 것에 많은 피로도를 느끼게 되었다. 본 논문은 이러한 불편함을 해결하기 위해 사용자가 배송, 사이즈, 품질 3가지 기준에 대한 자신의 선호도를 입력하면 해당 선호도에 맞는 상품들을 점수로 정렬하여 보여주는 방법을 제안한다. 본 논문에서 제안하는 방법은 사전에 스케줄된 크롤링 시스템을 통해 수집된 상품들과 각 상품의 리뷰들을 데이터베이스에 저장한다. 상품 검색 시, 저장된 상품들 중 사용자가 원하는 카테고리에 속하는 각 상품의 리뷰 텍스트들을 BERT 기반의 선호도 분석 AI 모델을 통해 분석하여 사용자 개인별 선호도에 따라 점수화해서 표현해준다. 점수화된 선호도를 통해 사용자는 상품 구매에 도움을 받을 수 있다.","In this paper, we propose a review analysis system to improve user's purchase satisfaction with a shopping mall product analysis platform through a BERT (Bidirectional Encoder Representations from Transformers)-based preference analysis model. The growth of online shopping malls made it more challenging for customers to find products that matched their preferences, and they experienced significant tiredness when making their selections. To address this inconvenience, this paper proposes a method to display products that meet the preference by score when a user enters his or her preferences for three criteria: delivery, size, and quality. The BERT-based preference analysis AI model is used to analyze the review texts of each product in the user-desired category among the stored items, and the results are conveyed by scoring in accordance with the user's own preferences. Users can assist in making purchases by using their scored preferences."
딥러닝 기반 네트워크 공격 및 침입 탐지 방법 연구,2022,"['Network Attack&Intrusion Detection', 'Deep Learning', 'Network Management']","안정적인 네트워크 관리를 위해서는 DoS/DDoS와 같은 네트워크 공격이나 침입과 같은 비정상적인 트래픽을 초기에 탐지하고 예방하는 것이 중요하다. 본 논문에서는 Recurrent Neural Network (RNN) 및 Transformer와 같은 딥러닝 모델을 기반으로 네트워크 공격 및 침입 트래픽을 탐지하는 방법을 제안한다. 그리고 제안하는 방법을 실제 네트워크 환경에 적용하기 위한 시스템을 설계한다. 비정상 트래픽의 분류 (탐지) 성능을 향상시키기 위해, 제안하는 모델은 이전 시간의 예측 결과를 활용하여 이전 시간 인덱스의 출력을 새 입력에 제공한다. 제안하는 모델의 성능 검증은 서로 다른 features를 가지는 세 가지 네트워크 공격 관련 공개 데이터셋 (NSL-KDD, UNSW-NB15, CICIDS 2017)을 사용하여 이진 (binary) 및 다중 클래스 (multi-class) 분류를 통해 그 성능을 검증한다. 실험 결과, 제안하는 모델은 3개의 데이터 세트에 대해 이진 분류에서 각각 0.956, 0.938 및 0.997의 개선된 성능 (F1 score)를 보여준다.","For stable network management, it is important to detect and prevent network attacks (e.g., DoS/DDoS) or abnormal traffic at an early stage. In this paper, we propose a method for detecting network attacks and intrusion traffic based on a deep learning model which has a sequential structure such as Recurrent Neural Network (RNN) and Transformer. To improve the classification performance, our proposed model uses the previous prediction to feed the output of the previous time index to new input. And we propose a system design to apply the proposed method to the real-world network environment. We evaluate our method through binary and multi-class classifications by using three public datasets (NSL-KDD, UNSW-NB15, CICIDS 2017) which have different features. Our model shows the improved F1 score 0.956, 0.938, and 0.997 in binary classification for the three datasets, respectively."
저전류 측정 정밀도 개선형 새로운 혼합형 로고스키 코일 개발,2022,"['Rogowski Coil', 'Air Core', 'Current Measurement', 'Magnet Saturation', 'Finite Element Analysis']","본 논문은 기존 변류기의 철심이 포화가 되는 문제점과 기존 무철심형 로고스키 코일의 저 전류 시의 어려운 계측과 코일의 턴 수가 많아짐으로써 발생하는 문제점을 해결하기 위해 부피의 증가와 포화의 문제가 없는 무철심형 로고스키 코일의 장점은 유지하고 낮은 전류 범위에서도 충분한 측정 정밀도를 발휘할 수 있는 철심과 공심을 혼합한 새로운 형태의 혼합형 로고스키 코일의 개발을 제안한다. 즉 철심과 공심코어 원리를 사용하여 일반 변류기의 자기포화와 비선형 오차의 한계 영향이 적고 일반적인 로고스키 코일 방식의 저 전류부분에서 선형도가 악화되는 문제를 해결한 최적인 철심과 공심의 비율 및 코일의 형상 설계치를 도출 하고자 한다. 본 논문의 모델은 1,000A 급 전류가 흐를 수 있는 도체를 선정하였으며, 10~1,000A 전류를 인가하였을 때 보빈에 3개의 적층형태로 이루어진 측정코일 모델을 설계하여 유기되는 전압을 측정하여 측정 전류의 정밀도를 비교하고자 한다. 또한 제시한 모델은 유한요소법을 사용하여 코일에 유기되는 전압을 시뮬레이션하고, 시작품을 통해 유기되는 전압파형을 오실로스코프를 이용하여 파형을 측정 하는 실험을 수행하여 제시한 새로운 혼합형 로고스키 코일의 유용성과 전류측정의 정밀성을 검증하고자 한다.","This research proposes to develop a new Rogowski Coil to solve the problem of saturating iron-core of the existing transformer. In addition, the new coil can solve the measurement difficulties during the low-current of the existing air-core Rogowski Coil and the problem caused by the increase in the number of turns of the coil. In other words, we propose to develop a new mixed-type Rogowski Coil that mixes iron and air cores. This mixed coil has the advantages of air-core type Rogowski Coil without increasing volume and saturation. It also exhibits sufficient measurement precision even in a low-current range. For the development of the mixed coil, this research selected a conductor through which a class current of 1,000A may flow. When a 10~1,000A current is applied, a measurement coil model composed of three stack types measures an induced voltage to compare the accuracy of the measured current. In addition, the proposed model uses Finite Element Analysis to simulate the voltage induced in the coil and measures the waveform using an oscilloscope for the voltage waveform induced through the prototype. Hence, the proposed mixed-Rogowski Coil""s usefulness and the accuracy of the current measurement are verified by the Finite Element Analysis."
몰드변압기의 보이드 결함 크기 판별,2022,"['Transformer', 'partial discharge', 'back propagation algorithm', 'void diameter']","본 논문에서는 신경망 모델을 적용한 몰드변압기의 보이드 결함 크기 판별에 관한 연구를 수행하였다. PCB 기반의 로고우스키 코일형 부분방전 센서를 제작하여 부분방전 신호를 측정하였고, 보이드에 의한 부분방전 결함을 모의하기 위한 PD 전극계를 제작하였다. 또한 보이드는 원통형 모양의 알루미늄 틀을 제작하여 에폭시가 경화되는 과정에서 실린지를 삽입하고 공기를 주입하여 서로다른 직경을 가지는 4개의 시편을 제작하였다. 보이드 결함 크기 판별을 위해 부분방전 전하량, 방전 펄스 수, 위상 분포의 부분방전 특성 파라미터를 추출하여 Labview 기반의 VI (Virtual Instrument)로 역전파 알고리즘을 설계하였다. 실험 결과로부터 제작된 알고리즘은 90%이상의 판별률로 결함의 직경크기를 구분할 수 있었다. 본 연구의 결과는 현장에서 PD 측정 시 몰드변압기의 유지보수 및 절연물 교체의 근거 자료로 활용될 수 있을 것으로 판단된다.","This paper presents the identification of void diameters for a cast-resin transformer using an artificial neural network (ANN) model. A PD signal was measured by the Rogowski coil sensor which has the planar and thin structures fabricated on a printed circuit board (PCB), and the PD electrode system was fabricated to simulate a PD defect by a void. In addition, void samples with different diameters were fabricated by injecting air in a cylindrical aluminum frame using a syringe during the epoxy curing process. To identify the diameter of void defects, PD characteristics such as the discharge magnitude, pulse count, and phase angle were extracted and back propagation algorithm (BPA) was designed using virtual instrument (VI) based on the Labview program. From the experimental results, the BPA algorithm proposed in this paper has over 90% accurate rate to identify the diameter of void defects and is expected to use reference data of maintenance and replacement of insulation for cast-resin transformers in the on-site PD measurement."
다양한 심층학습 아키텍처의 부하예측 성능 비교,2022,"['Load prediction(부하예측)', 'Transformer(트랜스포머)', 'Recurrent neural network(순환신경망)', 'Deep learning(심층학습)', 'Time series forecasting(시계열 예측)']",국문 초록 정보 없음,다국어 초록 정보 없음
다양한 심층학습 아키텍처의 부하예측 성능 비교,2022,"['Load prediction(부하예측)', 'Transformer(트랜스포머)', 'Recurrent neural network(순환신경망)', 'Deep learning(심층학습)', 'Time series forecasting(시계열 예측)']",국문 초록 정보 없음,"This study benchmarks load prediction performance using various deep learning architectures. Data were generated using a publicly available EnergyPlus simulation model and weather data for reproducibility. The deep learning architectures used in the benchmark were multi-layer perceptron, dilated convolutional network, recurrent neural network, gated recurrent unit, long-short term memory, and Transformer. Gated recurrent unit and Transformer showed the best performance when the training data was small. However, when the amount of training data was large, the difference in prediction performance of different deep learning architectures was slight."
의미론적 관계 추론을 위한 문장 유사성 전이 학습 방법,2022,"['의미적 관계 추론', '문장 유사성', '트랜스포머', '전이 학습']","최근 텍스트 데이터 분석의 요구가 증가함에 따라 문장의 의미적 관계를 이해하고 사용자가 요구하는 분석 정보를 제공 하기 위한 추론 기법의 필요성이 증가하고 있다. 이를 위해 의미 관계 추론 학습은 필수적이며, 추론 결과를 통해 빠르 고 정확하게 문장을 구성하는 단어들 사이의 관계를 보다 정확하게 분석할 수 있다. 본 논문에서는 NLI를 위한 트랜스 포머 모델을 이용하여 텍스트의 유사성 학습을 수행하고 의미적 추론을 수행하고자 한다. 실험에서 전이 학습을 수행한 결과 기존 방법보다 좋은 성능을 보임을 확인하였다.",다국어 초록 정보 없음
InferSent를 활용한 오픈 도메인 기계독해,2022,"['오픈 도메인 기계독해', '문서 검색', '문서 독해', 'ALBERT', 'InferSent', 'Open Domain Machine Reading Comprehension', 'Document Search', 'Document Reader', 'ALBERT', 'InferSent']","오픈 도메인 기계독해는 질문과 연관된 단락이 존재하지 않아 단락을 검색하는 검색 기능을 추가한 모델이다. 문서 검색은 단어 빈도 기반인 TF-IDF로 많은 연구가 진행됐으나 문서의 양이 많아지면 낮은 성능을 보이는 문제가 있다. 아울러 단락 선별은 단어 기반 임베딩으로 많은 연구가 진행됐으나 문장의 특징을 가지는 단락의 문맥을 정확히 추출하지 못하는 문제가 있다. 그리고 문서 독해는 BERT로 많은 연구가 진행됐으나 방대한 파라미터로 느린 학습 문제를 보였다. 본 논문에서는 언급한 3가지 문제를 해결하기 위해 문서의 길이까지 고려한 BM25를 이용하며 문장 문맥을 얻기 위해 InferSent를 사용하고, 파라미터 수를 줄이기 위해 ALBERT를 이용한 오픈 도메인 기계독해를 제안한다. SQuAD1.1 데이터셋으로 실험을 진행했다. 문서 검색은 BM25의 성능이 TF-IDF보다 3.2% 높았다. 단락 선별은 InferSent가 Transformer보다 0.9% 높았다. 마지막으로 문서 독해에서 단락의 수가 증가하면 ALBERT가 EM에서 0.4%, F1에서 0.2% 더 높았다.","An open domain machine reading comprehension is a model that adds a function to search paragraphs as there are no paragraphs related to a given question. Document searches have an issue of lower performance with a lot of documents despite abundant research with word frequency based TF-IDF. Paragraph selections also have an issue of not extracting paragraph contexts, including sentence characteristics accurately despite a lot of research with word-based embedding. Document reading comprehension has an issue of slow learning due to the growing number of parameters despite a lot of research on BERT. Trying to solve these three issues, this study used BM25 which considered even sentence length and InferSent to get sentence contexts, and proposed an open domain machine reading comprehension with ALBERT to reduce the number of parameters. An experiment was conducted with SQuAD1.1 datasets. BM25 recorded a higher performance of document research than TF-IDF by 3.2%. InferSent showed a higher performance in paragraph selection than Transformer by 0.9%. Finally, as the number of paragraphs increased in document comprehension, ALBERT was 0.4% higher in EM and 0.2% higher in F1."
GaN/Si 기반 60nm 공정을 이용한 고출력 W대역 전력증폭기,2022,"['GaN', 'HEMT', 'Power', 'Amplifier', 'W-Band']","본 논문에서는 60 nm GaN/Si HEMT 공정을 사용하여 전력증폭기(Power Amplifier)의 설계를 제시하였다.고주파 설계를 위하여 맞춤형 트랜지스터 모델을 구성하였다. Output stage는 저손실 설계를 위해 마이크로스트립 라인을 사용하여 회로를 구성하였다. 또한 RC 네트워크로 구성된 Bias Feeding Line과 Input bypass 회로의 AC Ground(ACGND) 회로를 각각 적용하여 DC 소스에 연결된 노드의 최소임피던스가 RF회로에 영향을 미치지 않도록하였다. 이득과 출력을 고려하여 3단의 구조로 설계되었다. 설계된 전력증폭기의 최종 사이즈는 3900 μm x 2300 μm 이다. 중심 주파수에서 설계된 결과는 12 V의 공급 전압에서 15.9 dB의 소 신호 이득, 29.9 dBm의 포화 출력(Psat), 24.2 %의 PAE를 달성하였다.","This study presents the design of power amplifier (PA) in 60 nm GaN/Si HEMT technology. A customized transistor model enables the designing circuits operating at W-band. The all matching network of the PA was composed of equivalent transformer circuit to reduce matching loss. And then, equivalent transformer is several advantages without any additional inductive devices so that a wideband power characteristic can be achieved. The designed die area is 3900 μm x 2300 μm. The designed results at center frequency achieved the small signal gain of 15.9 dB, the saturated output power (Psat) of 29.9 dBm, and the power added efficiency (PAE) of 24.2% at the supply voltage of 12 V."
XR기반 소방안전관리자용 메타버스 교육훈련 시스템을 위한 AI 기술 개발안,2022,"['XR (eXtended Reality)', 'metaverse', 'digital twin', 'fire safety manager training system', 'motion Recognition', 'BERT (Bidirectional Encoder Representations from Transformers)']","소방안전관리자용 훈련 시스템에서는 대피자들의 화상 및 질식 위험도를 기반으로 대피자들의 피난경로를 예측하는 시스템이 필요하다. 피난경로 예측 시스템과 더불어 훈련자들의 대피 유도 행동 및 음성을 인식하여 평가 시스템을 고도화하는 기술을 개발하고자 한다. 본 연구에서는 인공지능 기술을 기반으로 소방안전관리자의 훈련 시 피난 유도 행동을 자동 인식하며 행동 유형, 발생 시각, 인식 정확도 등의 분석 평가 요소를 자동 추출하여 실시간 훈련 평가 시스템을 개발하는 것을 목표로 한다. 제안된 기술은 딥러닝 기반의 모션 인식 모듈을 통해 인식 결과 및 정보를 실시간으로 디지털 파일로 변환하여 분석 평가 요소를 추출한다. 모션 인식 시스템에 더불어 STT (Speech-To-Text) 기반 인공지능 모델을 이용하여 소방안전관리자 훈련 시 실시간으로 음성을 인식하고 텍스트를 추출하여 시나리오와의 비교 및 평가를 수행하는 시스템을 구축한다. 자연어 처리 알고리즘 BERT (Bidirectional Encoder Representations from Transformers) 기반의 평가 알고리즘 적용을 통해 문맥을 파악하고 소방안전관리자 훈련에 최적화된 평가가 가능하다. 디지털 트윈 기술 기반으로 화재 및 대피자 위치에 따라 실시간으로 대피자 위험도를 예측한다. 화상 및 질식 위험도를 Pennes’ Bioheat Equation 및 Coburn-Forster-Kane Equation을 통한 일산화탄소 농도와 온도에 따른 구간별 중증도 점수 책정 후 중증도 계수를 적용하여 위험도를 단계별로 판별할 수 있다. 이후 Dijkstra's Shortest Path 기법을 활용하여 매핑된 건물 내에서 최소 위험도로 산출되는 최적 경로를 탐색한다. 본 연구를 통해 개발되는 디지털 트윈 기반 화재 상황에서의 대피자 피난경로 알고리즘 최적화 시스템 및 훈련자 맞춤형 음성⋅모션 평가 시스템을 활용하여 소방안전관리자 메타버스 교육 콘텐츠를 고도화할 수 있다. 또한, 개발된 시스템은 여러 재난 상황 혹은 건축물 환경에서도 활용이 가능하여 폭넓은 확장성을 기대할 수 있다.",다국어 초록 정보 없음
감정 역학과 멀티모달 정보 기반의 영화 요약,2022,"['요약', '영화 요약', '멀티모달', '감정 역학', 'summarization', 'movie summarization', 'multimodal', 'emotion dynamics']","자동 영화 요약은 영화의 중요한 장면을 담은 짧은 동영상을 만드는 것을 목적으로 하는 연구 주제이다. 본 연구는 자동 영화 요약을 위해 영화의 3가지 주요 요소인 인물, 줄거리, 동영상 정보를 종합적으로 고려한 요약 모델을 제안한다. 영화 줄거리 상의 주요 사건을 정확하게 식별하기 위해, 각본의 대사 정보와 주인공의 감정 변화 정보를 학습 자질로 사용하고 영화 각본과 동영상 정보를 결합하는 트랜스포머 기반 아키텍처를 제안한다. 실험을 통해 제안 방법이 영화의 주요 사건을 식별하는 정확도를 높이는 데 유용하며 결과적으로 영화 요약의 품질이 향상되는 것을 보인다.","Movie summarization is the task of summarizing a full-length movie by creating a short video summary containing its most informative scenes. This paper proposes an automatic movie summarization model that comprehensively considers the three main elements of the movie: characters, plot, and video information for movie summary. To accurately identify major events on the movie plot, we propose a Transformer-based architecture that uses the movie script's dialogue information and the main characters' emotion dynamics information as model training features, and then combines the script and video information. Through experiments, the proposed method is shown to be helpful in increasing the accuracy of identifying major events in movies and consequently improves the quality of movie summaries."
레이더 신호와 머신러닝을 이용한 비가시 공간에서의 삼차원 자세 추정 연구,2022,[],"비가시 공간의 물체를 탐지하는 기술은 군사적, 인명구조, 자율 주행 등의 다양한 목적에서 주목받고 있다. 본 논문은 다중 송 · 수신 안테나와 UWB(Ultra-Wide Bandwidth) 레이더 칩을 통해 데이터 수집 실험 환경을 구성한다. 구성한 환경을 이용하여 수집한 신호를 데이터로 활용하고 해당 데이터셋을 DETR (DEtaction TRansformer) 모델을 통해 비가시 공간의 사람에 대한 키포인트를 탐지하고 탐지한 키포인트를 이용하여 사람의 포즈를 추정하는 알고리즘을 제시한다.",다국어 초록 정보 없음
텍스트 인식률 개선을 위한 한글 및 영어 텍스트 이미지 초해상화,2022,[],"야외 환경을 카메라로 촬영한 일반 영상에서 텍스트 이미지를 검출하고 인식하는 기술은 로봇 비전, 시각 보조 등의 기반이 되는 기술로 활용될 수 있어 매우 중요한 기술이다. 하지만 저해상도의 텍스트 이미지의 경우 텍스트 이미지에 포함된 노이즈나 블러 등이 더 두드러지기 때문에 텍스트 내용을 인식하는 것이 어렵다. 이에 본 논문은 일반 영상에서의 저해상도 한글 및 영어 텍스트에 대한 이미지 초해상화를 통해 텍스트 인식 정확도를 개선하였다. 트랜스포머에 기반한 모델로 한글 및 영어 텍스트에 대한 이미지 초해상화를 수행하였으며, 영어 및 한글 데이터셋에 대해 제안한 초해상화 방법을 적용했을 때 그렇지 않을 때보다 텍스트 인식 성능이 개선되는 것을 확인하였다.",다국어 초록 정보 없음
양방향 GPT 네트워크를 이용한 VMS 메시지 이상 탐지,2022,"['VMS', 'GPT', 'NLL', '딥러닝', '이상탐지', 'VMS', 'GPT', 'NLL', 'Deep Learning', 'Anomaly Detection']","VMS (variable message signs) 시스템이 악의적인 공격에 노출되어 교통안전과 관련된 거짓정보를 출력하게 된다면 운전자에게 심각한 위험을 초래할 수 있다. 이러한 경우를 방지하기위해 VMS 시스템에 사용되는 메시지들을 수집하여 평상시의 패턴을 학습한다면 VMS 시스템에 출력될 수 있는 이상 메시지를 빠르게 감지하고 이에 대한 대응을 할 수 있을 것이다. 본논문에서는 양방향 GPT (generative pre-trained transformer) 모델을 이용하여 VMS 메시지의 평상 시 패턴을 학습한 후 이상 메시지를 탐지하는 기법을 제안한다. 구체적으로, 제안된 기법에VMS 메시지 및 시스템 파라미터를 입력 하고 이에 대한 NLL (negative log likelihood) 값을 최소화하도록 학습한다. 학습이 완료되면 판정해야 할 대상의 NLL 값을 계산한 후, 문턱치 값이상일 경우 이를 이상으로 판정한다. 실험 결과를 통해, 공격에 의한 악의적인 메시지 탐지뿐만 아니라 시스템의 오류가 발생하는 상황에 대한 탐지도 가능함을 보였다.","When a variable message signs (VMS) system displays false information related to traffic safety caused by malicious attacks, it could pose a serious risk to drivers. If the normal message patterns displayed on the VMS system are learned, it would be possible to detect and respond to the anomalous messages quickly. This paper proposes a method for detecting anomalous messages by learning the normal patterns of messages using a bi-directional generative pre-trained transformer (GPT) network. In particular, the proposed method was trained using the normal messages and their system parameters to minimize the corresponding negative log-likelihood (NLL) values. After adequate training, the proposed method could detect an anomalous message when its NLL value was larger than a pre-specified threshold value. The experiment results showed that the proposed method could detect malicious messages and cases when the system error occurs."
새로운 금속막대 커패시터를 적용한 감쇄모드 도파관 대역통과 여파기,2022,"['감쇄 모드 도파관', '결합 서셉턴스', '막대 커패시터', '원형 홈', 'Evanescent Waveguide', 'Junction Susceptance', 'Post Capacitor', 'Circular Groove']","본 논문에서는 보다 편리한 튜닝을 위해 Evanescent-Mode Rectangle Waveguide(EMRWG)에 삽입된 새로운 작은 직경의 원통형 포스트 커패시터를 제안하였다. EMRWG급전을 위한 제안된 구조는 입력 및 출력 끝에서 도파관과 동일한 너비와 높이를 갖는 단일 리지 직사각형 도파관을 사용하였다. 삽입된 포스트 커패시터는 EMRWG의 넓은 벽체 하부 중앙에 형성된 원형 홈과 상부에 삽입된 동심원기둥 포스트로 구성된다. 먼저 제안된 구조에 대한 등가회로 모델을 제시하였고, EMRWG와 단일 리지 도파관이 결합될 때 이상적인 변압기의 접합 서셉턴스와 권선비는 각각 HFSS(3d fullwave 시뮬레이터, Ansoft Co.)를 사용하여 두 가지 경우에 대해 시뮬레이션하였다. 얻어진 매개변수와 EMRWG의 특성을 이용하여 삽입된 기둥의 서셉턴스 및 공진 특성을 분석하였다. 중심주파수 4.5GHz, 대역폭 170MHz의 2포스트 필터는 WR-90 도파관을 이용하여 설계하였으며, 등가회로 모델에 대한 계산과 HFSS와 CST를 이용한 시뮬레이션 결과가 서로 일치하였다.","In this paper, a novel small-diameter cylindrical post capacitor inserted into an evanescent-mode rectangular waveguide (EMRWG) is proposed for easier tuning. In order to feed the EMRWG, the proposed structure uses a single ridge rectangular waveguide with the same width and height as the waveguide at the input and output ends. The inserted post capacitor are made up a circular groove formed in the center of the lower part of the broad wall of the EMRWG, and a concentric cylindrical post inserted into the upper part. First, the equivalent circuit model for the proposed structure is presented. When the EMRWG and the single ridge waveguide are combined, the joint susceptance and the turns ratio of the ideal transformer are calculated by two simulations using HFSS (3d fullwave simulator, Ansoft Co.) respectively. The susceptance and resonance characteristics of the inserted post were analyzed by using the obtained parameters and the characteristics of the EMRWG. A 2-post filter with a center frequency of 4.5 GHz and a bandwidth of 170 MHz was designed using a WR-90 waveguide, and the simulation results by using the HFSS and CST, equivalent circuit model were in good agreement."
제주 예멘난민 사태 전후 국내 여론변화 분석: 심층 인공신경망을 활용한 감성분석을 중심으로,2022,"['refugees', 'sentiment analysis', 'interrupted time series analysis', '난민', '감성분석', '단절적 시계열 분석']","본 연구는 난민에 대한 국내의 관심이 증가하게 된 2018년 제주 예멘 난민 사건을 중심으로 사건 전후의 난민에 대한 대중의 태도 변화를 분석하였다. 분석의 단계는 두 단계로 먼저, 언론 보도의 양과 국민의 관심 및 태도를 시기별로 나누어 확인한 후 난민에 대한 태도를 귀납적으로 유형화하고, 이후, 제주 예멘난민 사태가 난민에 대한 태도에 미친 영향을 분석하였다. 분석의 대상은2015년부터 2021년까지 주요 일간지와 뉴스방송사의 13,481개 기사에 대한 359,984개의 댓글이다. 난민에 대한 태도는 긍정과 부정의 비율을 볼 수 있도록 분류하고, 부정의 경우 그 근거에 따라‘종교/문화/안전’, ‘자원부족과 우선순위’, ‘자격과 책임’으로 분류하였다. 개별 댓글의 분석은 자연어처리기술(NLP: Natural Language Processing)을 활용한 감성분석(Sentiment Analysis)을 통해 이루어 졌으며, 한국어 BERT(Bidirectional Encoder Representations for Transformers) 모델중 하나인 KcBERT를 활용하였다. 영향분석에는 단절적 시계열 분석을 사용하였다. 분석기간 동안 전반적으로 부정적 여론의 비중이 높았으며, 제주 예멘난민 사건 이후 부정적 여론이 급격히증가하였고 부정의견의 유형이 다양화되는 것을 확인할 수 있었다. 또한 영향분석 결과, 긍정･부정 비(ratio)에 대해 부정여론이 즉각적으로 증가하는 영향을 주었지만, 전체적인 추세에는 영향을 주지 않았다. 부정 유형의 경우 ‘종교/문화/안전’ 유형의 비율은 감소하였고, ‘자원부족과 우선순위’ 유형의 비율은 증가하였으나 이들의 장기적 추세에는 유의미한 변화가 나타나지 않았다. 반면, ‘자격과 책임’ 유형은 비율의 수준과 추세 모두에서 유의미한 변화가 나타나지 않았다","As the number of refugees increases worldwide, the Republic of Korea is required as a member of the international community to fulfill its responsibilities and obligations to accept refugees. Sufficient discussion, public support, and social consensus are needed to accommodate and support refugees. This study analyzed how public attitudes toward refugees shifted over time, focusing on the 2018 Jeju Yemeni refugee problems (about 550 Yemenis applied for refugee status on Jeju Island), where there was domestic controversy over the increase in the number of refugees. The subject of analysis is 359,984 comments on 13,481 articles from major daily newspapers and news broadcasters in 2015–2021. Analysis of individual comments was conducted through sentiment analysis using natural language processing (NLP). For this, KcBERT, a Korean Bidirectional Encoder Representations from Transformers (BERT) model, was used. In addition, an interrupted time series analysis was performed to understand the influence relationship. The analysis showed that the proportion of negative public opinion on refugees was high both before and after the Jeju Yemen refugee crisis; negative public opinion increased overwhelmingly; and the reasons for this were more diverse. In addition, negative public opinion on the positive and negative ratios immediately increased as a result of the impact analysis but this did not affect the overall trend. We draw theoretical and practical implications based on these results."
다차원 영상을 활용한 매개모기 자동탐지 장치 개발,2022,"['매개모기', '인공지능', '딥러닝', '다차원 이미지 영상분석', '합성곱 알고리즘']","세계보건기구(WHO)에 의하면 말라리아, 일본 뇌염 그리고 뎅기 바이러스 등 모기 매개 감염병으로 인해 매년 70만명 이상이 사망한다. 최근 이상기온 및 기상이변으로 모기 개체수가 증가하여 감염병에 대한 노출이 심화되는 추세이다. 그러나 말라리아 이외의 모기 매개 감염병은 치료제가 개발되어 있지 않으므로 사전 방역 및 방제를 통한 매개모기 감염병에 대한 노출을 최소화하는 것이 중요하다. 효과적인 매개모기 감염병 확대 방지를 위해서는 감염병을 매개하는 모기 종류에 대한 개체수 모니터링이 필수적이다. 하지만 현재 모기 포집 장치는 종이 아닌 개체수만 측정이 가능하여 종별 감염병에 대해 효과적인 방제 체계에 기여를 하지 못하고 있다. 따라서, 본 연구에서는 딥러닝 영상분석기술을 활용하여 국내 주요 매개 모기 5종에 대해 종 동정 및 개체수 파악이 가능한 매개 모기 자동 탐지 장치를 개발하였다. 모기 자동 탐지 장치는 전기 충격 방식과 배경판 슬라이딩 방식으로 포집된 모기의 컬러 및 형광 영상을 선명하게 촬영할 수 있도록 개발되었다. 실제 모기 포집 현장에서의 조건과 동일한 영상 데이터를 확보하기 위해 살아있는 1３종의 야생 모기로 컬러 및 형광 이미지 데이터 셋을 구축하여 모기의 색, 형태 및 형광 특징을 분석에 활용할 수 있도록 하였다. 딥러닝 영상분석 알고리즘으로는 최신 BackBone인 Swin transformer와 딥러닝 객체 탐지 모델의 detector인 Faster R-CNN(Region-Convolutional Neural Network)을 결합하여 최종 탐지 모델을 구축하였다. 모델 성능 향상을 위해 컬러 및 형광 이미지의 예측 결과를 통합 후, NMS(Non-maximum suppression) 기법을 적용하였다. 외부 환경에 설치된 장치에 야생 모기를 투입하여 Blind test를 실시한 결과, 평균 91.7%의 분류 정확도를 확인할 수 있었다. 본 연구에서 개발된 딥러닝 객체 탐지 기술을 활용한 모기 포집 장치는 실외 현장에서 모기 종류 및 개체수 모니터링에 활용될 수 있을 것으로 사료된다.",다국어 초록 정보 없음
다차원 영상을 활용한 매개모기 자동탐지 장치 개발,2022,"['매개모기', '인공지능', '딥러닝', '다차원 이미지 영상분석', '합성곱 알고리즘']","세계보건기구(WHO)에 의하면 말라리아, 일본 뇌염 그리고 뎅기 바이러스 등 모기 매개 감염병으로 인해 매년 70만명 이상이 사망한다. 최근 이상기온 및 기상이변으로 모기 개체수가 증가하여 감염병에 대한 노출이 심화되는 추세이다. 그러나 말라리아 이외의 모기 매개 감염병은 치료제가 개발되어 있지 않으므로 사전 방역 및 방제를 통한 매개모기 감염병에 대한 노출을 최소화하는 것이 중요하다. 효과적인 매개모기 감염병 확대 방지를 위해서는 감염병을 매개하는 모기 종류에 대한 개체수 모니터링이 필수적이다. 하지만 현재 모기 포집 장치는 종이 아닌 개체수만 측정이 가능하여 종별 감염병에 대해 효과적인 방제 체계에 기여를 하지 못하고 있다. 따라서, 본 연구에서는 딥러닝 영상분석기술을 활용하여 국내 주요 매개 모기 5종에 대해 종 동정 및 개체수 파악이 가능한 매개 모기 자동 탐지 장치를 개발하였다. 모기 자동 탐지 장치는 전기 충격 방식과 배경판 슬라이딩 방식으로 포집된 모기의 컬러 및 형광 영상을 선명하게 촬영할 수 있도록 개발되었다. 실제 모기 포집 현장에서의 조건과 동일한 영상 데이터를 확보하기 위해 살아있는 1３종의 야생 모기로 컬러 및 형광 이미지 데이터 셋을 구축하여 모기의 색, 형태 및 형광 특징을 분석에 활용할 수 있도록 하였다. 딥러닝 영상분석 알고리즘으로는 최신 BackBone인 Swin transformer와 딥러닝 객체 탐지 모델의 detector인 Faster R-CNN(Region-Convolutional Neural Network)을 결합하여 최종 탐지 모델을 구축하였다. 모델 성능 향상을 위해 컬러 및 형광 이미지의 예측 결과를 통합 후, NMS(Non-maximum suppression) 기법을 적용하였다. 외부 환경에 설치된 장치에 야생 모기를 투입하여 Blind test를 실시한 결과, 평균 91.7%의 분류 정확도를 확인할 수 있었다. 본 연구에서 개발된 딥러닝 객체 탐지 기술을 활용한 모기 포집 장치는 실외 현장에서 모기 종류 및 개체수 모니터링에 활용될 수 있을 것으로 사료된다.",다국어 초록 정보 없음
KoEPT 기반 한국어 수학 문장제 문제 데이터 분류 난도 분석,2022,"['Math Word Problems', 'Generation Model', 'Transformer', 'Pointer Network', 'Classification Difficulty', '수학 문장제 문제', '생성 모델', '트렌스포머', '포인터 네트워크', '분류 난도']",국문 초록 정보 없음,"In this paper, we propose KoEPT, a Transformer-based generative model for automatic math word problems solving. A math wordproblem written in human language which describes everyday situations in a mathematical form. Math word problem solving requiresan artificial intelligence model to understand the implied logic within the problem. Therefore, it is being studied variously across theworld to improve the language understanding ability of artificial intelligence. In the case of the Korean language, studies so far havemainly attempted to solve problems by classifying them into templates, but there is a limitation in that these techniques are difficultto apply to datasets with high classification difficulty. To solve this problem, this paper used the KoEPT model which uses ‘expression’tokens and pointer networks. To measure the performance of this model, the classification difficulty scores of IL, CC, and ALG514, whichare existing Korean mathematical sentence problem datasets, were measured, and then the performance of KoEPT was evaluated using5-fold cross-validation. For the Korean datasets used for evaluation, KoEPT obtained the state-of-the-art(SOTA) performance with 99.1%in CC, which is comparable to the existing SOTA performance, and 89.3% and 80.5% in IL and ALG514, respectively. In addition, asa result of evaluation, KoEPT showed a relatively improved performance for datasets with high classification difficulty. Through an ablationstudy, we uncovered that the use of the ‘expression’ tokens and pointer networks contributed to KoEPT’s state of being less affectedby classification difficulty while obtaining good performance."
정압설비의 효율적 관리를 위한 예측모형 활용방안 연구,2022,"['Static pressure facility', 'time series', 'machine learning', 'prediction', 'safety management']",국문 초록 정보 없음,"The demand for urban gas continues to increase and is widely used in homes and industries. However, if the demand for urban gas is not properly predicted, the management costs of inventory increase, and supply and demand management is inefficient. For a stable supply of city gas, it is necessary to predict the pressure of the city gas static pressure device which maintains the gas pressure suitable for the gas user device at a constant level. This paper aims to increase the pressure management efficiency of the static pressure system by using pressure prediction model based on the time series model and the machine learning model. The data for analysis is pressure data collected based on the SCADA system. We added temperature variables, which affect the pressure, for analysis. Temperature variables are collected at the same time when the pressure of the stator is collected. After data preprocessing, training data and validation data for prediction are generated. The prediction model is implemented using SARIMA model, SARIMAX model, LSTM model, and Transformer model. As a result of the prediction, it was confirmed that the RMSE of the SARIMAX model was the lowest at 0.0248."
Comparative study of text representation and learning for Persian named entity recognition,2022,"['contextualized representation', 'NER', 'Persian language processing']",국문 초록 정보 없음,"Transformer models have had a great impact on natural language processing (NLP) in recent years by realizing outstanding and efficient contextualized language models. Recent studies have used transformer-based language models for various NLP tasks, including Persian named entity recognition (NER). However, in complex tasks, for example, NER, it is difficult to determine which contextualized embedding will produce the best representation for the tasks. Considering the lack of comparative studies to investigate the use of different contextualized pretrained models with sequence modeling classifiers, we conducted a comparative study about using different classifiers and embedding models. In this paper, we use different transformer-based language models tuned with different classifiers, and we evaluate these models on the Persian NER task. We perform a comparative analysis to assess the impact of text representation and text classification methods on Persian NER performance. We train and evaluate the models on three different Persian NER datasets, that is, MoNa, Peyma, and Arman. Experimental results demonstrate that XLM-R with a linear layer and conditional random field (CRF) layer exhibited the best performance. This model achieved phrase-based F-measures of 70.04, 86.37, and 79.25 and word-based F scores of 78, 84.02, and 89.73 on the MoNa, Peyma, and Arman datasets, respectively. These results represent state-of-the-art performance on the Persian NER task."
고전압 변압기 자성재료 적층 형태를 이용한 에너지 효율 향상,2022,"['Tesla transformer', 'Magnetic core', 'Eddy current loss', 'Magnetizing inductance', 'Stray inductance', 'Explosives materials']",국문 초록 정보 없음,"We constructed the high voltage transformer for driving a S-band RF source that confirm explosives materials. For designing a fast rise time and high energy efficiency Tesla Transformer to insert a magnetic core for minimizing inductance loss between primary and secondary coil.Magnetizing inductance of Tesla transformer changed by stray inductance which is generated by eddy current on the surface of magnetic core. In this paper, we calculate magnetizing inductance and energy efficiency of Tesla transformer by CST(Computer Simulation Technology) simulation of four types of laminated magnetic core's eddy current loss. Energy efficiency of design model(radial laminating) is about 3.0 ~ 5.8% higher than the existing methods, and it is thought that it will be used as a performance improvement of S-band RF radiation device."
낙뢰 분포와 배전용 변압기 뇌해고장의 영향에 관한 지리가중회귀분석,2022,"['Distribution Transformer', 'Lightning Strike', 'GWR(Geographically Weighted Regression)']",국문 초록 정보 없음,"Distribution transformers are known to be affected by not only direct lightning strikes but also induced lightning induced overvoltage due to their relatively low insulation strength to withstand lightning strikes. In this regard, the purpose of this paper is to implement an explanatory model between the lightning strikes and the lightning failures of distribution transformers. Since spatial distributions are hard to be analyzed by the ordinary least square (OLS) method, geographically weighted regression (GWR) analysis was applied to find the explanatory properties between the spatial lightning strikes and the spatial lightning failure transformers. As a result of the regression analysis, GWR analysis shows improved explanatory power compared to the OLS method and enables additional analysis to be performed by specifying points showing high explanatory power and regression coefficients in the visual analysis."
복합 고장 유형을 가지는 변압기에 대한 수명추정,2022,"['Reliability Estimation', 'Lifetime Distribution', 'Wearout Failure', 'Mixture Failure', 'Change-Point Analysis']",국문 초록 정보 없음,"Purpose: Because transformers are the primary component for the delivery of electric power, failures but also small irregularities can lead to expensive damage. Wear-out failure, in particular, can be minimized using lifetime analysis. This study focuses on the lifetime estimation of transformers and considers multiple failure-modes. In this way, the reliability of aging equipment can be analyzed individually.Methods: We propose a change-point model for the Weibull distribution for multiple failure-modes. By defining the function for a generalized likelihood ratio, the change-point for wear-out failure as well as the parameters for multiple lifetime-distributions can be estimated.Results: The proposed approach is applied to real-life data of electric transformers. As a result of the lifetime analysis, the lifetime distribution, including multiple failure-modes and their cutoff points (in time) can be determined.Conclusion: Using the investigated change-point analysis, both the lifetime and maintenance schedule of transformers with respect to multiple failure-modes can be estimated, and the chance for expensive equipment failure is reduced significantly."
Aspect-based Sentiment Analysis of Product Reviews using Multi-agent Deep Reinforcement Learning,2022,"['Sentiment Analysis', 'Deep Reinforcement Learning', 'Product Review', 'Artificial Intelligence', 'Machine Learning']",국문 초록 정보 없음,"The existing model for sentiment analysis of product reviews learned from past data and new data was labeled based on training. But new data was never used by the existing system for making a decision. The proposed Aspect-based multi-agent Deep Reinforcement learning Sentiment Analysis (ADRSA) model learned from its very first data without the help of any training dataset and labeled a sentence with aspect category and sentiment polarity. It keeps on learning from the new data and updates its knowledge for improving its intelligence. The decision of the proposed system changed over time based on the new data. So, the accuracy of the sentiment analysis using deep reinforcement learning was improved over supervised learning and unsupervised learning methods. Hence, the sentiments of premium customers on a particular site can be explored to other customers effectively. A dynamic environment with a strong knowledge base can help the system to remember the sentences and usage State Action Reward State Action (SARSA) algorithm with Bidirectional Encoder Representations from Transformers (BERT) model improved the performance of the proposed system in terms of accuracy when compared to the state of art methods."
Scene Text Recognition with Multi-Encoders,2022,"['Scene text recognition', 'Transformer', 'Deep learning', 'Convolutional neural network']",국문 초록 정보 없음,"Although text recognition has significantly evolved over the years, the current models still have huge challenges, especially for irregular text images, such as complex backgrounds, curved text, diverse fonts, distortions, etc. Currently, CNN-based text recognition networks have shown good performance but still face the above challenges. Recently, feature extractor based on transformer has shown excellent advantages for global feature extraction on images. Especially in irregular text images, which can use self-attention to establish the information connection of each part of the image, which can also reduce the influence of the irregular distribution of characters. Therefore, this paper proposes MESTR(Multi-Encoders Scene Text Recognition) that combines a CNN-based[1][2][6] feature extractor and a transformer-based feature extractor. MESTR can extract local and global features of text images at the same time and then integrate global features into local features. During training, we used CTC[6] as guide training in the decoder part, as the compensation training strategy for attentional decoder. Experimental results demonstrate that the proposed MESTR shows competitive results on all seven benchmarks. At the same time, we provide ablation experiments to show the effectiveness of the improved part on the text recognition model."
레이더 신호와 머신러닝을 이용한 비가시 공간에서의 삼차원 객체 인식 연구,2022,"['Radar', 'MIMO', 'Attention', 'Transformer', 'Object Detection']",국문 초록 정보 없음,다국어 초록 정보 없음
해충 분류를 위한 양방향 멀티스케일 특징 퓨전 기반의 피라미드 비전 변환기,2022,"['pest classification', 'bidirectional fusion', 'multiscale features', 'attention model', 'vision transformer']",국문 초록 정보 없음,"Pests damage crops, thereby resulting in reduced production and quality. In order to reduce the damage of pests, it is important to timely implement pest control according to various types of pests. Existing pest classification is identified by the naked eye of an experienced farmer or expert. This requires considerable time and cost, so an unmanned pest classification technology is needed. In this paper, a new bidirectional multiscale feature fusion method for pest image classification is proposed. In particular, in order to supplement the scale vulnerability of the existing PVT model, the multi-scale features extracted from the PVT backbone are applied bidirectionally by four fusion methods. Through the experimental results, the fusion method with the highest performance is derived and it is shown that the proposed bidirectional multiscale feature fusion method can improve the pest image classification performance."
Semantic Segmentation with Perceiver IO,2022,"['Semantic segmentation', 'Perceiver IO', 'Deep learning']",국문 초록 정보 없음,"Recently, in deep learning, the transformer is replacing the convolutional neural network (CNN) due to its performance and simple design. In particular, in recent studies, constructing an encoder of the transformer that effectively extracts features on an image has been widely used. However, even in these cases, models utilizing existing deep neural network structures needed to use a form suitable for each data format according to input modality. Recently, the Perceiver IO [6] has been proposed to overcome this limitation. It can process various data formats through one structure to extract a characteristic value. Also, it uses an output query to output data as we want. In this paper, a semantic segmentation model using the characteristics of the Perceiver IO is presented. Two types of input configuration are suggested, and experimental results show the feasibility of the proposed method."
유효한 키포인트 증식 기반의 가려진 사람의 재식별,2022,"['Deep learning', 'Occluded Person Re-ID', 'Transformer', 'Consistency Loss', 'keypoint']",국문 초록 정보 없음,"Occluded person re-identification is a challenging task which aims to search for or distinguish the specific person as human body is occluded by obstacles or other persons or by oneself. Some recent State of the art works which adopt transformer and/or pose-guided methods improve the feature representation and performances, but there is a room to enhance them in both representation and heavy structure. In this paper, we suggest to efficiently improve the transformer-based Re-ID method for the occluded person as follows. First, in data augmentation to improve Re-identification performance, instead of deleting an arbitrary area, only the part containing the keypoint feature of a person is deleted for effective learning in occlusion. Second, a consistency loss between global and local features of a body part is proposed for improving the discrimination to recognize the identical person.We compare the mAP and Rank-1 performances of our approach and various existing methods on the Occluded-Duke dataset.Experimental results show that our proposed model outperforms the competitive methods."
An Experimental Investigation of Discourse Expectations in Neural Language Models,2022,"['discourse expectation', 'implicit causality bias', 'neural language model', 'BERT', 'GPT-2', 'LSTM', 'next sentence prediction', 'coreference resolution', 'surprisal']",국문 초록 정보 없음,"The present study reports on three language processing experiments with most up-to-date neural language models from a psycholinguistic perspective. We investigated whether and how discourse expectations demonstrated in the psycholinguistics literature are manifested in neural language models, using the language models whose architectures and assumptions are considered most appropriate for the given language processing tasks. We first attempted to perform a general assessment of a neural model’s discourse expectations about story continuity or coherence (Experiment 1), based on the next sentence prediction module of the bidirectional transformer-based model BERT (Devlin et al. 2019). We also studied language models’ expectations about reference continuity in discursive contexts in both comprehension (Experiment 2) and production (Experiment 3) settings, based on so-called Implicit Causality biases. We used the unidirectional (or left-to-right) RNN-based model LSTM (Hochreiter and Schmidhuber 1997) and the transformer-based generation model GPT-2 (Radford et al. 2019), respectively. The results of the three experiments showed, first, that neural language models are highly successful in distinguishing between reasonably expected and unexpected story continuations in human communication and also that they exhibit human-like bias patterns in reference expectations in both comprehension and production contexts. The results of the present study suggest language models can closely simulate the discourse processing features observed in psycholinguistic experiments with human speakers. The results also suggest language models can, beyond simply functioning as a technology for practical purposes, serve as a useful research tool and/or object for the study of human discourse processing."
KAB: Knowledge Augmented BERT2BERT Automated Questions-Answering system for Jurisprudential Legal Opinions,2022,"['Islamic Fatwa', 'Natural Language Processing', 'Question Answering', 'Transformers']",국문 초록 정보 없음,"The jurisprudential legal rules govern the way Muslims react and interact to daily life. This creates a huge stream of questions, that require highly qualified and well-educated individuals, called Muftis. With Muslims representing almost 25% of the planet population, and the scarcity of qualified Muftis, this creates a demand supply problem calling for Automation solutions. This motivates the application of Artificial Intelligence (AI) to solve this problem, which requires a well-designed Question-Answering (QA) system to solve it. In this work, we propose a QA system, based on retrieval augmented generative transformer model for jurisprudential legal question. The main idea in the proposed architecture is the leverage of both state-of-the art transformer models, and the existing knowledge base of legal sources and question-answers. With the sensitivity of the domain in mind, due to its importance in Muslims daily lives, our design balances between exploitation of knowledge bases, and exploration provided by the generative transformer models. We collect a custom data set of 850,000 entries, that includes the question, answer, and category of the question. Our evaluation methodology is based on both quantitative and qualitative methods. We use metrics like BERTScore and METEOR to evaluate the precision and recall of the system. We also provide many qualitative results that show the quality of the generated answers, and how relevant they are to the asked questions."
누설 인덕턴스를 포함한 DAB 컨버터용 고주파 변압기의 머신러닝 활용한 최적 설계,2022,"['Multi-objective optimization', 'High-frequency transformer', 'Machine learning', 'DAB (Dual-Active-Bridge) converter']",국문 초록 정보 없음,다국어 초록 정보 없음
ABCD 파라미터를 활용한 PT의 고주파 전달특성 측정 및 분석,2022,"['S-parameter', 'ABCD parameter', 'potential transformer', 'voltage gain', 'transfer characteristics']",국문 초록 정보 없음,"This paper presents a measurement setup and network parameter-based analysis methodology for high-frequency transmission characteristics analysis between the primary and secondary sides of a PT (potential transformer). The raw data was extracted by measuring a scattering parameter (S-parameter) using a vector network analyzer. ABCD parameters were obtained using the measurement data to analyze the transmission characteristics of the transformer, and the source-transformer-load model was introduced to define the voltage and current gains considering the terminating impedance and input impedance applied to the transformer. Representative sine wave and square wave was applied to analyze the high-frequency transmission characteristics. In order to analyze the surge transmitted to the secondary system when applied to the actual system, the waveform of the IEC standard was generated and applied. When the load resistance is 1 MΩ, the voltage gain converted from the ideal turns ratio is applied to the input signal was confirmed that the maximum value of surge corresponding to about 1.9 times was formed."
Unsupervised Transfer Learning for Plant Anomaly Recognition,2022,"['Plant Disease Recognition', 'Unsupervised Transfer Learning', 'Plant Village Dataset', 'Vision Transformer']",국문 초록 정보 없음,"Disease threatens plant growth and recognizing the type of disease is essential to making a remedy. In recent years, deep learning has witnessed a significant improvement for this task, however, a large volume of labeled images is one of the requirements to get decent performance. But annotated images are difficult and expensive to obtain in the agricultural field. Therefore, designing an efficient and effective strategy is one of the challenges in this area with few labeled data. Transfer learning, assuming taking knowledge from a source domain to a target domain, is borrowed to address this issue and observed comparable results. However, current transfer learning strategies can be regarded as a supervised method as it hypothesizes that there are many labeled images in a source domain. In contrast, unsupervised transfer learning, using only images in a source domain, gives more convenience as collecting images is much easier than annotating. In this paper, we leverage unsupervised transfer learning to perform plant disease recognition, by which we achieve a better performance than supervised transfer learning in many cases. Besides, a vision transformer with a bigger model capacity than convolution is utilized to have a better-pretrained feature space. With the vision transformer-based unsupervised transfer learning, we achieve better results than current works in two datasets. Especially, we obtain 97.3% accuracy with only 30 training images for each class in the Plant Village dataset. We hope that our work can encourage the community to pay attention to vision transformer-based unsupervised transfer learning in the agricultural field when with few labeled images."
콘포머 기반 FastSpeech2를 이용한 한국어 음식 주문 문장 음성합성기,2022,[],국문 초록 정보 없음,"In this paper, we present the Korean menu-ordering Sentence Text-to-Speech (TTS) system using conformer-based FastSpeech2. Conformer is the convolution-augmented transformer, which was originally proposed in Speech Recognition. Combining two different structures, the Conformer extracts better local and global features. It comprises two half Feed Forward module at the front and the end, sandwiching the Multi-Head Self-Attention module and Convolution module. We introduce the Conformer in Korean TTS, as we know it works well in Korean Speech Recognition. For comparison between transformer-based TTS model and Conformer-based one, we train FastSpeech2 and Conformer-based FastSpeech2. We collected a phoneme-balanced data set and used this for training our models. This corpus comprises not only general conversation, but also menu-ordering conversation consisting mainly of loanwords. This data set is the solution to the current Korean TTS model's degradation in loanwords. As a result of generating a synthesized sound using ParallelWave Gan, the Conformer-based FastSpeech2 achieved superior performance of MOS 4.04. We confirm that the model performance improved when the same structure was changed from transformer to Conformer in the Korean TTS."
DG-based SPO tuple recognition using self-attention M-Bi-LSTM,2022,"['dependency grammar', 'information extraction', 'long short-term memory', 'SPO tuple']",국문 초록 정보 없음,"This study proposes a dependency grammar-based self-attention multilayered bidirectional long short-term memory (DG-M-Bi-LSTM) model for subject–predicate–object (SPO) tuple recognition from natural language (NL) sentences. To add recent knowledge to the knowledge base autonomously, it is essential to extract knowledge from numerous NL data. Therefore, this study proposes a high-accuracy SPO tuple recognition model that requires a small amount of learning data to extract knowledge from NL sentences. The accuracy of SPO tuple recognition using DG-M-Bi-LSTM is compared with that using NL-based self-attention multilayered bidirectional LSTM, DG-based bidirectional encoder representations from transformers (BERT), and NL-based BERT to evaluate its effectiveness. The DG-M-Bi-LSTM model achieves the best results in terms of recognition accuracy for extracting SPO tuples from NL sentences even if it has fewer deep neural network (DNN) parameters than BERT. In particular, its accuracy is better than that of BERT when the learning data are limited. Additionally, its pretrained DNN parameters can be applied to other domains because it learns the structural relations in NL sentences."
AI-guided Story Generation Framework with Automatic Storyline Generator,2022,"['Story Generation', 'Storyline Generation', 'Human-guided AI', 'Deep  Learning', 'Natural Language Generation']",국문 초록 정보 없음,"A story generation task is to develop a system that can continuously generate natural, consistent, and coherent stories for consecutive scenes. Recently transformer-based language models have shown considerable results at the sentence-level generation for learning human-writing ability. However, it is very crucial to understand the way of developing the story using a combination of various contents. Recent works have mainly focused on human-guided AI story generation methods in which humans as guidance determine the next storyline, and the system creates a story that reflects the storyline well. This study focuses on the way of replacing the human role with the AI-based model. Based on this, this study deals with the methodology for creating a long story spanning multiple scenes rather than creating a story at the level of one scene. In this regard, we propose a novel AI-guided story generation framework with automatic storyline generator. It is a pipeline structure consisting of two modules such as a storyline generator and a story generator, which enables the continuous creation of coherent stories. Particularly, we transform the storyline generation problem into a multiple-choice QA problem to predict the next storyline. This study shows the possibility of generating continuous stories for multiple scenes without any human intervention."
Self-Attention 시각화를 사용한 기계번역 서비스의 번역 오류 요인 설명,2022,"['Self-Attention', 'Machine translation', 'XAI', 'exBERT', 'Visualization']",국문 초록 정보 없음,"This study analyzed the translation error factors of machine translation services such as Naver Papago and Google Translate through Self-Attention path visualization. Self-Attention is a key method of the Transformer and BERT NLP models and recently widely used in machine translation. We propose a method to explain translation error factors of machine translation algorithms by comparison the Self-Attention paths between ST(source text) and ST'(transformed ST) of which meaning is not changed, but the translation output is more accurate. Through this method, it is possible to gain explainability to analyze a machine translation algorithm's inside process, which is invisible like a black box. In our experiment, it was possible to explore the factors that caused translation errors by analyzing the difference in key word's attention path. The study used the XLM-RoBERTa multilingual NLP model provided by exBERT for Self-Attention visualization, and it was applied to two examples of Korean-Chinese and Korean-English translations."
방사선 투과 이미지에서의 용접 결함 검출을 위한 딥러닝 알고리즘 비교 연구,2022,"['Radiographic Testing', 'Welding Defect', 'Deep Learning']",국문 초록 정보 없음,"An automated system is needed for the effectiveness of non-destructive testing. In order to utilize the radiographic testing data accumulated in the film, the types of welding defects were classified into 9 and the shape of defects were analyzed. Data was preprocessed to use deep learning with high performance in image classification, and a combination of one-stage/two-stage method and convolutional neural networks/Transformer backbone was compared to confirm a model suitable for welding defect detection. The combination of two-stage, which can learn step-by-step, and deep-layered CNN backbone, showed the best performance with mean average precision 0.868."
AI기법과 고주파 전류 변환센서를 활용한 배전반 on board 진단시스템,2022,"['Partial discharge', 'Switchboard', 'Ai algorithm', 'Diagnosis system', 'Pre-processing']",국문 초록 정보 없음,"This paper presents an on-board switchboard diagnosis system based on the Ai algorithm for the purpose of establishing a real-time partial discharge monitoring diagnosis system. For classifying pattern of partial discharge, FCM-based RBFNN is applied to on-board diagnosis system and K-means, NN(Neural Networks) are used for comparative analysis. The data are obtained by HFCT(High Frequency Current Transformer) sensor from designed partial discharge simulation environment and its phase is divided into 128 degree. Commonly used partial discharge data analysis methods such as statistical analysis, PRPS(Phase Resolved Pulse Sequence) and PRPD(Phase Resolved Partial Discharge) are introduced. In this paper PRPDA(Phase Resolved Partial Discharge Analysis) and PCA(Principal Component Analysis) are used as a pre-processing. The Ai algorithm comparison is performed twice in total. Firstly, a comparison analysis of the test data validation of each model trained in the Python environment is conducted through a confusion matrix. Afterwards, an on-board diagnostic device is added to the partial discharge simulation circuit, and the judgment results for the actual operation are compared. Through comparative analysis in virtual and real environments, it is confirmed that the case in which FCM RBFNN is mounted shows excellent performance."
산업 환경의 객체 검출 성능 개선을 위한 GAN 기반 이미지 데이터 증강,2022,"['생성적 적대 신경망', '데이터 증강', '객체 검출', '딥러닝', '산업 환경', 'Generative Adversarial Networks', 'Data Augmentation', 'Object Detection', 'Deep Learning', 'Industrial Environment']",국문 초록 정보 없음,"Object detection is one of the important industrial safety technologies that can automatically provide a worker with alerts to avoid unexpected near misses. However, deep learning-based object detection models require large amounts of training data to achieve higher performance, and data collection and labeling work is laborious and requires human resources. To address these limitations, we propose a GAN-based data augmentation that can supplement the original dataset with more diverse examples. In addition, we present a transformer-based generator network to improve the fidelity of generated data and evaluate the existing object detection model(YOLOv5) trained under different augmentation settings for a comparison study. The evaluation results show that the classification ability of the model trained with 20% augmented data has improved by 0.9% without localization performance losses."
MLOps workflow language and platform for time series data anomaly detection,2022,[],국문 초록 정보 없음,"In this study, we propose a language and platform to describe and manage the MLOps(Machine Learning Operations) workflow for time series data anomaly detection. Time series data is collected in many fields, such as IoT sensors, system performance indicators, and user access. In addition, it is used in many applications such as system monitoring and anomaly detection. In order to perform prediction and anomaly detection of time series data, the MLOps platform that can quickly and flexibly apply the analyzed model to the production environment is required. Thus, we developed Python-based AI/ML Modeling Language (AMML) to easily configure and execute MLOps workflows. Python is widely used in data analysis. The proposed MLOps platform can extract and preprocess time series data from various data sources (R-DB, NoSql DB, Log File, etc.) using AMML and predict it through a deep learning model. To verify the applicability of AMML, the workflow for generating a transformer oil temperature prediction deep learning model was configured with AMML and it was confirmed that the training was performed normally."
캐릭터 MBTI 기반 GPT-2 활용 뮤지컬 가사 생성 시스템,2022,"['가사생성', '인공지능', '뮤지컬', '마이어스-브릭스 유형지표', '자연어처리', 'Lyrics generation', 'Artificial intelligence', 'Musical', 'MBTI', 'Natural language processing']",국문 초록 정보 없음,"Recently, various cultural and artistic content using AI (Artificial Intelligence) technology have been spreading. In the field of natural language processing that write lyrics for songs, a lot of research has been focused on how to create lyric-like sentences through using rhyme. However, if the lyrics are created by simply considering the rhyme, the song in the musical may be poor at telling the mood on the flow of the play because it can not reflect the characters or the situation in the play. In this paper, we propose a GPT-2 (Generative Pre-trained Transformer-2)-based musical lyrics generation system that fits the MBTI (Myers-Briggs Type Indicator) of characters in musical plays. Character tendency in the play is analyzed using MBTI, and song playlist that match the character MBTI are collected to generate learning data, and musical lyrics that match the character’s tendency are generated through the GPT-2 algorithm based on the learned model. In this paper, it was shown that it is possible to generate musical lyrics suitable for character tendency and atmosphere in the play by proposing the relevant song search keywords for each MBTI and hyper-parameter values of the GPT-2 model suitable for lyrics generation."
전장 상황 인지 보고서 생성을 위한 Text-to-Text 멀티 태스크 학습,2022,"['Battlefield Situation Awareness', 'Multi-Task Learning', 'Table-To-Text', 'Text-To-Text']",국문 초록 정보 없음,"Advances in new communication technologies enable commanders to collect various information in battlefield situations. However, it is difficult to make quick and accurate decisions on the battlefield because of vast amount of information. To address this problem, several studies attempt to change tabular data into an easy-to-understand text format. Existing table-to-text studies are not suitable for battlefield situations because they use specific domain data such as WIKIBIO and WIKITABLETEXT. In this study, we propose a table-to-text transfer transformer (TaT4) that uses special tokens to transform log table data into a single sequence to preserve table information. Moreover, the proposed TaT4 uses multi-task learning that can leverage cross-task data of types in a single model to improve generalization performance. We conduct experiments on eight datasets generated from three Korean defense modeling and simulations (M&S) of battlefield situations in the Army, Air Force, and Navy. The proposed TaT4 outperforms the existing table-to-text models."
Towards Improving Causality Mining using BERT with Multi-level Feature Networks,2022,"['Causality Mining', 'Relation Network', 'Multi-level Relation Network', 'Relation Classification', 'Cause-effect Relation Classification']",국문 초록 정보 없음,"Causality mining in NLP is a significant area of interest, which benefits in many daily life applications, including decision making, business risk management, question answering, future event prediction, scenario generation, and information retrieval. Mining those causalities was a challenging and open problem for the prior non-statistical and statistical techniques using web sources that required hand-crafted linguistics patterns for feature engineering, which were subject to domain knowledge and required much human effort. Those studies overlooked implicit, ambiguous, and heterogeneous causality and focused on explicit causality mining. In contrast to statistical and non-statistical approaches, we present Bidirectional Encoder Representations from Transformers (BERT) integrated with Multi-level Feature Networks (MFN) for causality recognition, called BERT+MFN for causality recognition in noisy and informal web datasets without human-designed features. In our model, MFN consists of a three-column knowledge-oriented network (TC-KN), bi-LSTM, and Relation Network (RN) that mine causality information at the segment level. BERT captures semantic features at the word level. We perform experiments on Alternative Lexicalization (AltLexes) datasets. The experimental outcomes show that our model outperforms baseline causality and text mining techniques."
무선전력전송 코일간의 전력전달 원리와 공진 토폴로지 선택 방법,2022,"['Wireless Power Transfer', 'Mutual Inductance', 'Magnetizing Inductance', 'Maximum Power Transfer', 'Resonant Topology']",국문 초록 정보 없음,"In this study, the power transfer principle and resonant topology selection method of the wireless power transfer system are explained.The magnetizing inductance and mutual inductance models, which are representative modeling methods for representing the magnetic coupling between the transmitting and receiving coils in wireless power transfer systems, are analyzed; additionally, the differences between the two modeling methods are explained. Furthermore, the active and reactive power of the wireless power transfer coils are analyzed in detail using the mutual inductance model; based on this, the differences between the transformer and the wireless power transfer coil system are described. In addition, through active and reactive power analysis, the phase difference between the transmitting and receiving sides required for maximizing power transmission is derived. Finally, the representative topologies of the wireless power transfer system are analyzed, and the required selection method for the optimal wireless power transfer resonant topology is described by considering the power transfer principle and the input and output characteristics of the wireless power transfer system."
Power Quality Improvement in Distribution System Based on Dynamic Voltage Restorer Using Rational Energy Transformative Optimization Algorithm,2022,"['Dynamic Voltage Restorer (DVR)', 'Distributed Energy Resources', 'Harmonic Compensation', 'Rational Energy Transformative Optimization (RETO) Algorithm']",국문 초록 정보 없음,"In the power generation, transmission, and distribution system, power quality has always been essential to maintain stable power fl ow because of developing electronic loads applications. Various functionally operated electrical loads require an effi cient power quality to perform their operation. Power quality is aff ected by several aspects, namely voltage sags, swells, harmonics, etc. These issues cause signifi cant damage or produce failure in the load, which are the existing power quality challenges. The proposed work aims to reduce the previous eff ects and improve power quality along with Dynamic Voltage Restorer (DVR), which is implemented in this work to eliminate voltage sags, swells, and harmonics. A DVR is proposed based on inverter drop compensation to adjust the voltage to tolerance and is sensitive to meet the critical needs of power quality. In this system, the DVR-synchronized three-phase voltage in series is connected with the power supply step-up transformer with the appropriate amplitude and duration of discharge recovery power quality. To optimize the grid system, voltage amplitude and phase angle should be in an appropriate ratio. The DVR is a series controlled circuit, and the voltage is varied depending upon the load variation, which is analyzed by the proposed Rational Energy Transformative Optimization (RETO) Algorithm. This algorithm provides the suitable Pulse Width Modulation (PWM) to the Voltage Source Inverter (VSI), which independently generates and absorbs reactive power or actual power. DVR loads are sensitive to separate injection voltage recovery line voltages. DVR is a highly tuned output voltage waveform with high power quality with less harmonic compensation and mitigates voltage transients in the distribution system. The Performance of DVR is simulated under dynamic conditions with various parameters like steady-state error, Effi ciency, Total Harmonics Distortion (THD) in the load. All the above-determined parameters are evaluated in MATLAB SIMULINK 2017b software. The prototype DVR module is also developed for the proposed model; in this system, the PIC16F877A microcontroller will be implemented to monitor and control the entire system."
심층신경망을 이용한 딸기 생육상태 추정,2022,"['deep neural network', 'protected horticulture', 'strawberry', 'growth status']",국문 초록 정보 없음,"For high-quality, high-yield cultivation, especially work in protected horticulture such as the control of an appropriate environment and physical actions are essential. Recently, automating cultivation technique for agricultural work using long cultivation experience and artificial intelligence has been proposed. And automating work should be based on crop statuses such as age and condition. In particular, since fruits and vegetables have both vegetative and reproductive growth stages, it is important to understand more precisely the status of crops. The purpose of this study is to estimate the growth status of strawberry plant after transplanting from RGB images using a deep neural network. The images were acquired using a GoPro camera (GoPro HERO 10, GoPro Inc., USA) of the strawberry plant ‘Seolhyang’, which was transplanted in a venro-type glass greenhouse. Videos were filmed for 8 weeks at the interval of 1 week from July 6th. The images were augmented using ImageDataGenerator in keras module and inputted to a deep neural network model of various structures. The models used for training are MLP, CNN, and vision transformer. The model showed a performance of more than 95% accuracy. In the future, for the deep neural network structure that showed the highest performance, We are going to proceed ablation test for each RGB channel and compare performance for the different numbers of layers. The application of the deep neural network in this study shows that it could be the basis for the design of work for each growth status in the construction of an automated system for high quality and high yield in strawberry cultivation."
심층신경망을 이용한 딸기 생육상태 추정,2022,"['deep neural network', 'protected horticulture', 'strawberry', 'growth status']",국문 초록 정보 없음,"For high-quality, high-yield cultivation, especially work in protected horticulture such as the control of an appropriate environment and physical actions are essential. Recently, automating cultivation technique for agricultural work using long cultivation experience and artificial intelligence has been proposed. And automating work should be based on crop statuses such as age and condition. In particular, since fruits and vegetables have both vegetative and reproductive growth stages, it is important to understand more precisely the status of crops. The purpose of this study is to estimate the growth status of strawberry plant after transplanting from RGB images using a deep neural network. The images were acquired using a GoPro camera (GoPro HERO 10, GoPro Inc., USA) of the strawberry plant ‘Seolhyang’, which was transplanted in a venro-type glass greenhouse. Videos were filmed for 8 weeks at the interval of 1 week from July 6th. The images were augmented using ImageDataGenerator in keras module and inputted to a deep neural network model of various structures. The models used for training are MLP, CNN, and vision transformer. The model showed a performance of more than 95% accuracy. In the future, for the deep neural network structure that showed the highest performance, We are going to proceed ablation test for each RGB channel and compare performance for the different numbers of layers. The application of the deep neural network in this study shows that it could be the basis for the design of work for each growth status in the construction of an automated system for high quality and high yield in strawberry cultivation."
What Can BERT Do for English Linguistics?: A Case Study of Examining It-cleft Constructions,2022,"['deep-learning', 'language model', 'it-cleft', 'pronominal case', 'platform']",국문 초록 정보 없음,"This study used the machine learning technique known as Bidirectional Encoder Representations from Transformers (BERT) to understand the pronominal distribution in it-cleft constructions. While language models (LMs) have successfully been used to examine and verify linguistic representations, most studies have focused on the general phenomenon of subject-verb agreement. As such, this study focused on the particular construction known as it-cleft, in order to consolidate the LM’s role as a linguistic platform. In the experiment using BERT, the surprisal of each case (i.e., nominative and accusative) is compared. The investigation successfully replicated the findings of previous studies, specifically by capturing the use of the accusative form, inhibition of the first-person pronominal, and collocate feature of the first-person pronominal with the complementizer who.The results from the LM also deliver further contribution to the study of it-cleft, involving the between number and the relationship between the pronominal and syntactic role. Thus, the usage of BERT both revealed new research questions while verifying evidence from previous studies on the it-cleft pronominal. This demonstrates that BERT can correctly represent particular constructions and broadens the possibilities of LMs as a linguistic platform."
Deep learning-based scalable and robust channel estimator for wireless cellular networks,2022,"['channel estimation', 'deep learning', 'wireless cellular networks']",국문 초록 정보 없음,"In this paper, we present a two-stage scalable channel estimator (TSCE), a deep learning (DL)-based scalable, and robust channel estimator for wireless cellular networks, which is made up of two DL networks to efficiently support different resource allocation sizes and reference signal configurations. Both networks use the transformer, one of cutting-edge neural network architecture, as a backbone for accurate estimation. For computation-efficient global feature extractions, we propose using window and window averaging-based self-attentions. Our results show that TSCE learns wireless propagation channels correctly and outperforms both traditional estimators and baseline DL-based estimators. Additionally, scalability and robustness evaluations are performed, revealing that TSCE is more robust in various environments than the baseline DL-based estimators."
Neural Network Language Models as Psycholinguistic Subjects: Focusing on Reflexive Dependency,2022,"['reflexive dependency', 'filler-gap dependency', 'gender mismatch effect', 'neural network language model', 'surprisal']",국문 초록 정보 없음,"The purpose of this study is to investigate the reflexive-antecedent dependency resolution accompanying the wh-filler-gap dependency resolution in neural network language models (LMs)’ sentence processing, comparing the processing result of LMs to the one of humans. To do so, we adopt the psycholinguistic methodology that Fraizer et al. (2015) used for humans. The neural-network language models employed in this study are four LMs: the Long Short-Term Memory (LSTM) trained on large datasets, the Generative Pre-trained Transformer-2 (GPT-2) trained on large datasets, an LSTM trained on small datasets (L2 datasets), and the GPT-2 trained on small datasets (L2 datasets). We found that only the LMs trained on large datasets were sensitive to the dependency between a reflexive and its antecedent matching in gender, but all of the four neural LMs failed to learn reflexive-antecedent dependency accompanying wh-filler-gap dependency. Furthermore, we also found that the neural LMs have a learning bias in gender mismatch."
수어 동작 키포인트 중심의 시공간적 정보를 강화한 Sign2Gloss2Text 기반의 수어 번역,2022,"['Deep Learning', 'Sign Language Translation', 'Human Pose Estimation', 'Keypoints', 'Sign2Gloss2Text', 'Transformer', 'Spatial-temporal Structure Information']",국문 초록 정보 없음,"Sign language has completely different meaning depending on the direction of the hand or the change of facial expression even with the same gesture. In this respect, it is crucial to capture the spatial-temporal structure information of each movement. However, sign language translation studies based on Sign2Gloss2Text only convey comprehensive spatial-temporal information about the entire sign language movement. Consequently, detailed information (facial expression, gestures, and etc.) of each movement that is important for sign language translation is not emphasized. Accordingly, in this paper, we propose Spatial-temporal Keypoints Centered Sign2Gloss2Text Translation, named STKC-Sign2 Gloss2Text, to supplement the sequential and semantic information of keypoints which are the core of recognizing and translating sign language. STKC-Sign2Gloss2Text consists of two steps, Spatial Keypoints Embedding, which extracts 121 major keypoints from each image, and Temporal Keypoints Embedding, which emphasizes sequential information using Bi-GRU for extracted keypoints of sign language. The proposed model outperformed all Bilingual Evaluation Understudy(BLEU) scores in Development(DEV) and Testing(TEST) than Sign2Gloss2Text as the baseline, and in particular, it proved the effectiveness of the proposed methodology by achieving 23.19, an improvement of 1.87 based on TEST BLEU-4."
