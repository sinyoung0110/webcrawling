title,date,keywords,abstract,multilingual_abstract
순환신경망을 이용한 한글 필기체 생성,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
순환신경망을 이용한 한글 필기체 인식,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
장단기 기억 신경망과 공간적 순환 신경망을 이용한 배경차분,2016,[],"본 논문에서는 순환 신경망을 이용하여 동영상에서의 배경과 전경을 구분하는 알고리즘을 제안한다. 순환신경망은 일련의 순차적인 입력에 대해서 내부의 루프(loop)를 통해 이전 입력에 의한 정보를 지속할 수 있도록 구성되는 신경망을 말한다. 순환 신경망의 여러 구조들 가운데, 우리는 장기적인 관계에도 반응할 수 있도록 장단기 기억 신경망(Long short-term memory networks, LSTM)을 사용했다. 그리고 동영상에서의 시간적인 연결 뿐 아니라 공간적인 연관성도 배경과 전경을 판단하는 것에 영향을 미치기 때문에, 공간적 순환 신경망을 적용하여 내부 신경망(hidden layer)들의 정보가 공간적으로 전달될 수 있도록 신경망을 구성하였다. 제안하는 알고리즘은 기본적인 배경차분 동영상에 대해 기존 알고리즘들과 비교할만한 결과를 보인다.",다국어 초록 정보 없음
순환 신경망(LSTM)을 이용한 영화 평점 예측,2016,[],"소비자의 선호도 및 여론을 정량적인 방법으로 분석하기 위해 비정형 데이터의 분석은 필수적인 요소가 되고 있다. 하지만 비정형 데이터는 언어의 구조 및 모호성 등으로 인해 분석하기 어려운 형태이다. 따라서 본 연구는 최근 각광받고 있는 인공신경망, 특히 그 중에서도 순환 신경망의 한 모델인 Deep LSTM 을 이용하여 비정형 데이터를 분석하고 이를 활용하여 어순 및 어감 등의 언어의 구조적 문제에도 효과적인 정략적 모델을 설계하여 학습하고 이를 기존의 인공신경망 모델과 비교 분석하고자 한다.",다국어 초록 정보 없음
순환 신경망을 이용한 상담 대화 만족도 초기 탐지 기법,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
순환 신경망을 이용한 반도체 자동화 물류시스템 내 정체 예측,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
순환형 신경망을 이용한 가젯티어 자동 구축 방법,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
순환형 신경망 인코더-디코더를 이용한 논문 요약,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
순환 신경망을 이용한 궤적 기반의 단체 행동 인식,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
GRU 기반 순환 신경망에서의 배치정규화 효과 연구,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
LSTM 구조의 순환 신경망을 이용한 음향 환경 분류,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
CBOW 기반 양방향 순환 신경망을 이용한 감성 분석,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
CBOW 기반 양방향 순환 신경망을 이용한 감성 분석,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
시공간 정보 집중 모델 및 순환 신경망을 사용한 동영상 자동 묘사 시스템,2016,[],국문 초록 정보 없음,"Automatic video description is a challenging problem using computer vision and natural language processing. Previously, researches on automatic video description only engaged either spatial or temporal information, so mutual information of spatio-temporal domain is neglected. In this paper, in order to regard spatial and temporal information simultaneously, a spatio-temporal attention based automatic video description is proposed."
순환 신경망 기반 대용량 텍스트 데이터 분류 기술,2016,[],국문 초록 정보 없음,다국어 초록 정보 없음
이미지 캡션 생성을 위한 심층 신경망 모델 학습과 전이,2016,[],"본 논문에서는 이미지 캡션 생성과 모델 전이에 효과적인 심층 신경망 모델을 제시한다. 본 모델은 멀티 모달 순환 신경망 모델의 하나로서, 이미지로부터 시각 정보를 추출하는 컨볼루션 신경망 층, 각 단어를 저차원의 특징으로 변환하는 임베딩 층, 캡션 문장 구조를 학습하는 순환 신경망 층, 시각 정보와 언어 정보를 결합하는 멀티 모달 층 등 총 5 개의 계층들로 구성된다. 특히 본 모델에서는 시퀀스 패턴 학습과 모델 전이에 우수한 LSTM 유닛을 이용하여 순환 신경망 층을 구성하고, 컨볼루션 신경망 층의 출력을 임베딩 층뿐만 아니라 멀티 모달 층에도 연결함으로써, 캡션 문장 생성을 위한 매 단계마다 이미지의 시각 정보를 이용할 수 있는 연결 구조를 가진다. Flickr8k, Flickr30k, MSCOCO 등의 공개 데이터 집합들을 이용한 다양한 비교 실험을 통해, 캡션의 정확도와 모델 전이의 효과 면에서 본 논문에서 제시한 멀티 모달 순환 신경망 모델의 우수성을 입증하였다.",다국어 초록 정보 없음
딥러닝의 모형과 응용사례,2016,"['딥러닝', '합성곱신경망', '순환신경망', '오류역전파 알고리즘', 'Deep learning', 'Convolutional neural networks', 'Recurrent neural networks', 'Error backpropagation algorithm']",국문 초록 정보 없음,"Deep learning model is a kind of neural networks that allows multiple hidden layers. There are various deep learning architectures such as convolutional neural networks, deep belief networks and recurrent neural networks. Those have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Among those architectures, convolutional neural networks and recurrent neural networks are classified as the supervised learning model. And in recent years, those supervised learning models have gained more popularity than unsupervised learning models such as deep belief networks, because supervised learning models have shown fashionable applications in such fields mentioned above.  Deep learning models can be trained with backpropagation algorithm. Backpropagation is an abbreviation for “backward propagation of errors” and a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of an error function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the error function.  Convolutional neural networks use a special architecture which is particularly well-adapted to classify images. Using this architecture makes convolutional networks fast to train. This, in turn, helps us train deep, muti-layer networks, which are very good at classifying images. These days, deep convolutional networks are used in most neural networks for image recognition. Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. By local receptive fields, we mean that each neuron in the first(or any) hidden layer will be connected to a small region of the input(or previous layer’s) neurons. Shared weights mean that we’re going to use the same weights and bias for each of the local receptive field. This means that all the neurons in the hidden layer detect exactly the same feature, just at different locations in the input image. In addition to the convolutional layers just described, convolutional neural networks also contain pooling layers. Pooling layers are usually used immediately after convolutional layers. What the pooling layers do is to simplify the information in the output from the convolutional layer. Recent convolutional network architectures have 10 to 20 hidden layers and billions of connections between units. Training deep learning networks has taken weeks several years ago, but thanks to progress in GPU and algorithm enhancement, training time has reduced to several hours.  Neural networks with time-varying behavior are known as recurrent neural networks or RNNs. A recurrent neural network is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. Early RNN models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem such as vanishing gradient and exploding gradient. The gradient can get smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time. If the network runs for a long time, that can make the gradient extremely unstable and hard to learn from. It has been possible to incorporate an idea known as long short-term memory units (LSTMs) into RNNs. LSTMs make it much easier to get good results when traini"
