title,date,keywords,abstract,multilingual_abstract
러시아 해외정보기관의 역할과 안보 인식의 변화: 해외정보국(SVR)과 정보총국(GRU)의 정보활동을 중심으로,2018,"['러시아 해외정보기관의 역할', '안보인식의 변화', '해외정보국', '정보총국', '해외 첩보 활동', 'Role of Russia’s Overseas Intelligence Agencies', 'Change of Awareness of Security', 'SVR', 'GRU', 'Overseas Intelligence Activities']","소련 해체와 함께, KGB가 연방보안국(FSB)과 해외정보국(SVR)으로 분리되었다. 정보총국(GRU)은 큰 폭의 변화 없이 그 역할을 계속해 오고 있다. 이들 중에서 해외정보를 수집하고 있는 정보기관의 역할을 조사하게 되면, 러시아의 안보인식이 변화되고 있음을 알 수 있다. 러시아의 SVR과 GRU의 역할을 중심으로 본다면, 옐친시기와 푸틴시기의 안보 인식에서 차이를 보인다. 옐친 시기 SVR과GRU의 해외정보 활동은 미온적인 수준의 정치 및 군사안보에 초점이 맞추어진경향이 강했고, 푸틴이 집권하는 2000년 이후부터는 보다 적극적인 정치 및 군사안보와 경제안보에 보다 많은 관심을 갖고 해외정보수집에 나서고 있음을 알수 있다. 이러한 현실은 당시의 안보 인식을 엿볼 수 있도록 한다.국가안보 문제가 다양한 분야에서 제기되고 있기 때문에, SVR이 담당하는 정보수집 활동은 정치 및 군사영역 중심에서 경제 및 산업기술, 안보영역 등 다양한 분야로 확장되었다. SVR과 GRU 국장의 과거 경력에서도 러시아의 당시 해외 첩보 활동 및 안보 상황을 엿볼 수 있도록 한다. SVR 국장의 경력은 임명권자인 대통령의 정책 방향을 이해하는 데 도움을 주고, SVR의 핵심 역할을 짐작할 수 있도록 한다. 정치 및 군사안보 영역에서 경제안보에 보다 많은 관심을기울이면서 해외정보 수집이 이루어지고 있음을 짐작하도록 한다. 그리고 GRU 의 해외 첩보 활동을 보면, 러시아가 당면한 안보 수준을 짐작할 수 있도록 한다.","Along with the collapse of Soviet Union, KGB was divided into Federal Security Service(FSB) and Foreign Intelligence Service(SVR).Main Intelligence Directorate(GRU) has continued the role without any huge change. A close look at the role of intelligence agencies gathering overseas information shows that there has been change in awareness of national security in Russia. When looking at the role of SVR and GRU in Russia, we can see the difference in awareness of national security at a time of Yeltsin Administration and Putin Administration. We can see that SVR and GRU’s overseas intelligence activities of Yeltsin Administration was likely to focus on political and military security in no active way but Putin Administration has been more interest in political and military security and economic security in gathering overseas information. This reality makes us see awareness of security of that time.As national security issues have been raised in various areas, intelligence collection activities have been expanded to various areas including economic, industrial technology and security area from focus on political and military area. Past career of directors of SVR and GRU shows then overseas intelligence activities and security situation of Russia. A look at the career of SVR director will be helpful in understanding policy direction of President who appoints them and make us guess core role of SVR. It makes us guess of gathering overseas information while taking more interest in economic security than political and military security area. And GRU’s overseas intelligence activities make us guess the security level that Russia is facing."
LSTM 기반의 sequence-to-sequence 모델을 이용한 한글 자동 띄어쓰기,2018,"['인코딩-디코딩', 'sequence-to-sequence', 'LSTM', '순차정보 신경망', '자동 띄어쓰기', '드롭아웃', '계층 정 규화', 'encoding-decoding', 'LSTM', 'sequence-to-sequence neural network', 'auto spacing', 'drop-out', 'layer normalization']","자동 띄어쓰기 특성을 효과적으로 처리할 수 있는 LSTM(Long Short-Term Memory Neural Networks) 기반의 RNN 모 델을 제시하고 적용한 결과를 분석하였다. 문장이 길거나 일부 노이즈가 포함된 경우에 신경망 학습이 쉽지 않은 문제를 해결하기 위하여 입력 데이터 형식과 디코딩 데이터 형식을 정의하고, 신경망 학습에서 드롭아웃, 양방향 다층 LSTM 셀, 계층 정규화기법, 주목 기법(attention mechanism)을 적용하여 성능을 향상시키는 방법을 제안하였다. 학습 데이터로는 세종 말뭉치 자료를 사용하였으며, 학습 데이터가 부분적으로 불완전한 띄어쓰기가 포함되어 있었음에도 불구하고, 대량의 학습 데이터를 통해 한글 띄어쓰기에 대한 패턴이 의미 있게 학습되었다. 이것은 신경망에서 드롭아웃 기법을 통해 학습 모델의 오버피팅이 되지않도록 함으로써 노이즈에 강한 모델을 만들었기 때문이다. 실험결과로 LSTM sequence-to-sequence 모델이 재현율과 정확도를 함께 고려한 평가 점수인 F1 값이 0.94로 규칙 기반 방식과 딥러닝 GRU-CRF보다 더 높은 성능을 보였다.","We proposed a LSTM-based RNN model that can effectively perform the automatic spacing characteristics. For those long or noisy sentences which are known to be difficult to handle within Neural Network Learning, we defined a proper input data format and decoding data format, and added dropout, bidirectional multi-layer LSTM, layer normalization, and attention mechanism to improve the performance. Despite of the fact that Sejong corpus contains some spacing errors, a noise-robust learning model developed in this study with no overfitting through a dropout method helped training and returned meaningful results of Korean word spacing and its patterns. The experimental results showed that the performance of LSTM sequence-to-sequence model is 0.94 in F1-measure, which is better than the rule-based deep-learning method of GRU-CRF."
딥러닝 기반 기사단위 및 문단 단위별 분류,2018,"['Genre Classification', 'Deep Learning', 'Word2Vec', 'Long Short-Term Memory (LSTM)', 'Gated Recurrent Unit (GRU)', 'Convolutional Neural Networks (CNN)', 'Word embedding']",,"Text classification has been studied for a long time in the Natural Language Processing field. In this paper, we propose an article- and paragraph-level genre classification system using Word2Vec-based LSTM, GRU, and CNN models for large-scale English corpora. Both article- and paragraph-level classification performed best in accuracy with LSTM, which was followed by GRU and CNN in accuracy performance. Thus, it is to be confirmed that in evaluating the classification performance of LSTM, GRU, and CNN, the word sequential information for articles is better than the word feature extraction for paragraphs when the pre-trained Word2Vec-based word embeddings are used in both deep learning-based article- and paragraph-level classification tasks."
단어의 의미와 문맥을 고려한 순환신경망 기반의 문서 분류,2018,"['문서 분류', 'Doc2vec 순환신경망']","본 논문에서는 단어의 순서와 문맥을 고려하는 특징을 추출하여 순환신경망(Recurrent Neural Network)으로 문서를 분류하는 방법을 제안한다. 단어의 의미를 고려한 word2vec 방법으로 문서내의 단어를 벡터로 표현하고, 문맥을 고려하기 위해 doc2vec으로 입력하여 문서의 특징을 추출한다. 문서분류 방법으로 이전 노드의 출력을 다음 노드의 입력으로 포함하는 RNN 분류기를 사용한다. RNN 분류기는 신경망 분류기 중에서도 시퀀스 데이터에 적합하기 때문에 문서 분류에 좋은 성능을 보인다. RNN에서도 그라디언트가 소실되는 문제를 해결해주고 계산속도가 빠른 GRU(Gated Recurrent Unit) 모델을 사용한다. 실험 데이터로 한글 문서 집합 1개와 영어 문서 집합 2개를 사용하였고 실험 결과 GRU 기반 문서 분류기가 CNN 기반 문서 분류기 대비 약 3.5%의 성능 향상을 보였다.","In this paper, we propose a method to classify a document using a Recurrent Neural Network by extracting features considering word sense and contexts. Word2vec method is adopted to include the order and meaning of the words expressing the word in the document as a vector. Doc2vec is applied for considering the context to extract the feature of the document. RNN classifier, which includes the output of the previous node as the input of the next node, is used as the document classification method. RNN classifier presents good performance for document classification because it is suitable for sequence data among neural network classifiers. We applied GRU (Gated Recurrent Unit) model which solves the vanishing gradient problem of RNN. It also reduces computation speed. We used one Hangul document set and two English document sets for the experiments and GRU based document classifier improves performance by about 3.5% compared to CNN based document classifier."
한글 음소 단위 딥러닝 모형을 이용한 감성분석,2018,"['Sentiment Analysis', 'Deep Learning', 'Sequential Model', 'Phoneme Unit', 'LSTM', 'GRU']",,"Sentiment analysis is a technique of text mining that extracts feelings of the person who wrote the sentence like movie review. The preliminary researches of sentiment analysis identify sentiments by using the dictionary which contains negative and positive words collected in advance. As researches on deep learning are actively carried out, sentiment analysis using deep learning model with morpheme or word unit has been done. However, this model has disadvantages in that the word dictionary varies according to the domain and the number of morphemes or words gets relatively larger than that of phonemes. Therefore, the size of the dictionary becomes large and the complexity of the model increases accordingly.We construct a sentiment analysis model using recurrent neural network by dividing input data into phoneme-level which is smaller than morpheme-level. To verify the performance, we use 30,000 movie reviews from the Korean biggest portal, Naver. Morpheme-level sentiment analysis model is also implemented and compared. As a result, the phoneme- level sentiment analysis model is superior to that of the morpheme-level, and in particular, the phoneme-level model using LSTM performs better than that of using GRU model. It is expected that Korean text processing based on a phoneme-level model can be applied to various text mining and language models."
FastText와 셀프 매칭 어텐션 기반 포인터 네트워크를 이용한 한국어 상호참조해결,2018,"['상호참조해결', '셀프 매칭 어텐션', '포인터 네트워크', 'FastText', 'unknown word', 'coreference resolution', 'self-matching attention', 'pointer networks word']","셀프 매칭 어텐션 메커니즘은 자기 자신에 대한 얼라인먼트 점수를 계산하는 방법이며, 주어진 시퀀스에 대하여 서로 유사한 단어 간의 얼라인먼트 점수가 더 높게 계산되어 상호참조해결에 도움이 될 수 있다. FastText는 입력 단어를 음절 단위 n-gram으로 나누어 학습하는 방법으로, 단어의 변형이 심하거나 단어 사전에 없는 unknown 단어를 처리하는데 적합하다. 본 논문에서는 셀프 매칭 어텐션 매커니즘을 기반으로 한 포인터 네트워크를 상호참조해결에 적용하고, 상호참조해결에서 발생하는 unknown 단어문제를 해결하기 위하여 FastText로 사전 학습한 단어 표현을 사용할 것을 제안한다. 실험 결과, 본 논문에서 제안한 모델 중에서 Self_att_ffnn 모델이 CoNLL F1 (test) 73.55%, Self_att_gru4 모델이 CoNLL F1 (test) 73.60%로 일반 포인터 네트워크보다 각각 2.72%, 1.52%의 성능 향상을 보였다.","Self-matching attention mechanism is a method of calculating an alignment score for oneself, and two sequences applying an attention mechanism are the same sequence. Applying the self-matching attention mechanism to a given sequence, can facilitate solving coreference resolution by calculating higher alignment scores between similar words. FastText is a method to learn an input word by dividing the input word into a character n-gram. The FastText is suitable for dealing with unknown word not in a vocabulary or in a word variant. In this paper, we propose to apply pointer networks based on the self-matching attention mechanism to coreference resolution and to use word embedding pre-trained with FastText to solve an unknown word problem in coreference resolution. As a result, self_att_ffnn model showed 73.55% of CoNLL F1 (test) and self_att_gru4 model showed 73.60% of CoNLL F1 (test) among the proposed models, and models showed a performance improvement of 2.72% and 1.52%, respectively."
내부정보유출 위험등급 판단을 위한 딥 러닝 모델 성능비교에 관한 연구,2018,"['보안', '빅데이터', '보안로그', '이벤트', '내부정보 유출 방지', '위험관리', 'Insider Data Leakage', 'Risk classification', 'Deep Learning', 'Performance Comparison']",,"Recently, there has been an increase in the leakage of internal information in the companies. Most of the internal information leakage is caused by insiders. Companies are applying several security solutions to prevent this. However, there is still a lack of research on solutions applying deep learning to detection of data leakage with limitations in preventing leakage in advance. In this paper, a method to judge internal data leakage risk classification is proposed by applying various deep learning models. Security events are generated for each user by mapping the security log and user information obtained through the security device to determine the risk of internal information leakage. Training data based on the mapped data is generated, and applied to deep learning models such as MLP, LSTM, and GRU to compare the performance between models. In addition, the number of hidden nodes and the activation function are changed and their effects are examined. Experimental results showed that the model with softmax activation function applied to the GRU model without dropout showed the highest performance and 93% accuracy."
RNN을 활용한 도시철도 역사 부하 패턴 추정,2018,"['Urban railway station', 'Load pattern', 'Deep learning', 'Recursive neural networks', 'Gated recurrent unit']",,"For effective electricity consumption in urban railway station such as peak load shaving, it is important to know each electrical load pattern by various usage. The total electricity consumption in the urban railway substation is already measured in Korea, but the electricity consumption for each usage is not measured. The author proposed the deep learning method to estimate the electrical load pattern for each usage in the urban railway substation with public data such as weather data. GRU (gated recurrent unit), a variation on the LSTM (long short-term memory), was used, which aims to solve the vanishing gradient problem of standard a RNN (recursive neural networks). The optimal model was found and the estimation results with that were assessed."
