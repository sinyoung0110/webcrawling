title,date,keywords,abstract,multilingual_abstract
ResNet 기반의 전이학습을 이용한 갈라짐 및 멍든 감귤(Citrus unshiu) 선별,2018,"['딥러닝', '전이학습', '감귤 분류', 'ResNet']","전이학습(transfer learning)은 사전 학습이 완료된 딥네트워크 모델(pretrained neural network model)에서 이미 학습된 가중치값(weights)들을 새로운 과제를 위한 네트워크에 전이(transfer)하여 원하는 모델에 맞게 학습을 시키는 딥러닝 기술로써 가용한 데이터셋이 충분하지 않거나 딥네트워크 학습에 사용될 고사양 컴퓨터의 활용이 어려울 경우에 널리 사용된다. 최근 다양한 연구에서 딥러닝 전이학습을 활용한 사례들이 보고되고 있는데, 농업부문에서도 전이학습 활용 가능성이 높게 평가되고 있다. 이에 본 연구에서는 전이학습을 활용하여 감귤 선별기에서 갈라짐(cracked) 혹은 멍든(bruised) 감귤을 분류하는 영상처리 알고리즘을 개발하고자 하였다. 본 연구에서 기반으로 사용한 사전 학습이 완료된 딥네트워크 모델은 ResNet-50 과 ResNet-100 이었고, 이 두 모델의 마지막 10개의 네트워크 층(network layer)들을 제거하여 감귤 선별에 적합한 층으로 새로이 구성하여 재학습을 수행하였다. 재학습을 위하여서는 400개의 감귤 이미지가 사용되었고, 재학습 후 네트워크 모델의 검증을 위하여서는 학습에 사용되지 않은 100개의 감귤 이미지가 사용되었다. 학습과 검증에는 다음 3종류의 감귤 이미지가 사용이 되었다: 1) 건강한 감귤, 2) 껍질에 갈라짐이 포함된 감귤 그리고 3) 껍질이 멍든 감귤. ResNet-50 과 ResNet-100 기반의 전이학습을 활용한 결과, 두 네트워크 모델에서 각각 96.5% 및 96.8% 의 분류정확도가 획득되었다. ResNet 기반의 전이학습은 감귤 분류에 매우 유망한 결과를 보여주었고 추후 네트워크 모델의 세밀 조정(fine tuning)을 통해 정확도 향상을 추구할 수 있을 것으로 판단된다. 하지만 본 연구에서는 제한된 수의 감귤 이미지가 데이터셋으로 사용되었기에 추후 더 많은 감귤 이미지 확보를 통해 모델의 정확도를 검증해야 할 필요가 있다.",다국어 초록 정보 없음
슈퍼픽셀 이미지 분할을 이용한 ResNet 기반 백혈구 감별 알고리즘 개발,2018,"['Super-pixel', 'Residual Network', 'Segmentation', 'Classification', 'White Blood Cell']",국문 초록 정보 없음,"In this paper, we propose an efficient WBC 14-Diff classification which performs using the WBC-ResNet-152, a type of CNN model. The main point of view is to use Super-pixel for the segmentation of the image of WBC, and to use ResNet for the classification of WBC.  A total of 136,164 blood image samples (224x224) were grouped for image segmentation, training, training verification, and final test performance analysis.  Image segmentation using super-pixels have different number of images for each classes, so weighted average was applied and therefore image segmentation error was low at 7.23%.  Using the training data-set for training 50 times, and using soft-max classifier, TPR average of 80.3% for the training set of 8,827 images was achieved. Based on this, using verification data-set of 21,437 images, 14-Diff classification TPR average of normal WBCs were at 93.4% and TPR average of abnormal WBCs were at 83.3%. The result and methodology of this research demonstrates the usefulness of artificial intelligence technology in the blood cell image classification field.  WBC-ResNet-152 based morphology approach is shown to be meaningful and worthwhile method. And based on stored medical data, in-depth diagnosis and early detection of curable diseases is expected to improve the quality of treatment."
ResNet-50 합성곱 신경망을 위한 고정 소수점 표현 방법,2018,['ASIC'],국문 초록 정보 없음,"Recently, the convolutional neural network shows high performance in many computer vision tasks. However, convolutional neural networks require enormous amount of operation, so it is difficult to adopt them in the embedded environments. To solve this problem, many studies are performed on the ASIC or FPGA implementation, where an efficient representation method is required. The fixed-point representation is adequate for the ASIC or FPGA implementation but causes a performance degradation. This paper proposes a separate optimization of representations for the convolutional layers and the batch normalization layers. With the proposed method, the required bit width for the convolutional layers is reduced from 16 bits to 10 bits for the ResNet-50 neural network. Since the computation amount of the convolutional layers occupies the most of the entire computation, the bit width reduction in the convolutional layers enables the efficient implementation of the convolutional neural networks."
Resnet을 이용한 감정 인식 기반 콘텐츠 반응 분석 시스템,2018,[],국문 초록 정보 없음,다국어 초록 정보 없음
ResNet 을 이용한 손그림기반 캐릭터 검색 웹서비스,2018,[],국문 초록 정보 없음,다국어 초록 정보 없음
합성곱 신경망을 활용한 위내시경 이미지 분류에서 전이학습의 효용성 평가,2018,"['Gastroscope', 'Convolutional Neual Network', 'Transfer learning', 'Resnet', 'Inception', 'VGGnet']",국문 초록 정보 없음,"Stomach cancer is the most diagnosed cancer in Korea. When gastric cancer is detected early, the 5-year survival rate is as high as 90%. Gastroscopy is a very useful method for early diagnosis. But the false negative rate of gastric cancer in the gastroscopy was 4.6~25.8% due to the subjective judgment of the physician. Recently, the image classification performance of the image recognition field has been advanced by the convolutional neural network. Convolutional neural networks perform well when diverse and sufficient amounts of data are supported. However, medical data is not easy to access and it is difficult to gather enough high-quality data that includes expert annotations. So This paper evaluates the efficacy of transfer learning in gastroscopy classification and diagnosis. We obtained 787 endoscopic images of gastric endoscopy at Gil Medical Center, Gachon University. The number of normal images was 200, and the number of abnormal images was 587. The image size was reconstructed and normalized. In the case of the ResNet50 structure, the classification accuracy before and after applying the transfer learning was improved from 0.9 to 0.947, and the AUC was also improved from 0.94 to 0.98. In the case of the InceptionV3 structure, the classification accuracy before and after applying the transfer learning was improved from 0.862 to 0.924, and the AUC was also improved from 0.89 to 0.97. In the case of the VGG16 structure, the classification accuracy before and after applying the transfer learning was improved from 0.87 to 0.938, and the AUC was also improved from 0.89 to 0.98. The difference in the performance of the CNN model before and after transfer learning was statistically significant when confirmed by T-test (p < 0.05). As a result, transfer learning is judged to be an effective method of medical data that is difficult to collect good quality data."
전이학습에 방법에 따른 컨벌루션 신경망의영상 분류 성능 비교,2018,"['Deep Learning', 'Computer Vision', 'Convolutional Neural Network', 'Transfer Learnin']",국문 초록 정보 없음,"Core algorithm of deep learning Convolutional Neural Network(CNN) shows better performance than other machine learning algorithms. However, if there is not sufficient data, CNN can not achieve satisfactory performance even if the classifier is excellent. In this situation, it has been proven that the use of transfer learning can have a great effect. In this paper, we apply two transition learning methods(freezing, retraining) to three CNN models(ResNet-50, Inception-V3, DenseNet-121) and compare and analyze how the classification performance of CNN changes according to the methods. As a result of statistical significance test using various evaluation indicators, ResNet-50, Inception-V3, and DenseNet-121 differed by 1.18 times, 1.09 times, and 1.17 times, respectively. Based on this, we concluded that the retraining method may be more effective than the freezing method in case of transition learning in image classification problem."
보안 감시를 위한 심층학습 기반 다채널 영상 분석,2018,"['Video Surveillance', 'Object detection', 'Multi-object tracking', 'Deep learning', 'Probabilistic data association filter', '영상보안감시', '객체 검출', '다중 객체 추적', '심층학습', '확률적 데이터 연관 필터']","본 논문에서는 영상 보안 감시를 위한 심층학습 객체 검출과 다중 객체 추적을 위한 확률적 데이터연관 필터를 연계한 영상분석 기법을 제안하고, GPU를 이용하여 구현하는 방안을 제시한다. 제안하는 영상분석 기법은 객체 검출과 추적으로 순차적으로 수행한다. 객체 검출을 위한 심층학습은 ResNet을 이용하고, 다중 객체 추적을 위하여 확률적 데이터 연관 필터를 적용한다. 제안하는 영상분석 기법은 임의의 영역으로 불법으로 침입하는 사람을 검출하거나 특정 공간에 출입하는 사람을 계수하는데 응용할 수 있다. 시뮬레이션을 통하여 약 27fps의 속도로 48채널의 영상을 분석할 수 있음을 보이고, RTSP 프로토콜을 통하여 실시간 영상분석이 가능함을 보인다.","In this paper, a video analysis is proposed to implement video surveillance system with deep learning object detection and probabilistic data association filter for tracking multiple objects and suggests its implementation using GPU. The proposed video analysis technique involves object detection and object tracking sequentially. The deep learning network architecture uses ResNet for object detection and applies probabilistic data association filter for multiple objects tracking. The proposed video analysis technique can be used to detect intruders illegally trespassing any restricted area or to count the number of people entering a specified area. As results of simulations and experiments, 48 channels of videos can be analyzed at a speed of about 27 fps and real-time video analysis is possible through RTSP protocol."
이진화된 컨벌루션 신경망의 효율적인 SIMD 구현,2018,"['인공 신경망', '컨벌루션 신경망', '이진화', '이미지 분류', '임베디드 시스템']","본 논문에서는 이진화된 컨벌루션 신경망 (Convolutional Neural Network; CNN)의 효율적인 구현을 제시한다. 이진화된 CNN은 기존 CNN에 이진화 과정을 추가하여 각각의 파라미터와 컨벌루션의 입력이 단일 비트로 표현될 수 있도록 변형한 것이다. 제안하는 구현에서는, 다수의 이진화된 파라미터들과 컨벌루션의 입력들을 하나의 워드로 묶어서 저장하고, CNN에서 연산 량 대부분을 차지하는 기존 컨벌루션을 이진화된 컨벌루션으로 대체하여, Bitwise XNOR-Bitcount으로 구현하였다. 이러한 SIMD 처리 방식의 구현은 CNN의 전체적인 메모리 요구량과 연산 량을 크게 감소시킬 수 있다. 실제로 LeNet-5와 ResNet-18을 대상으로 제안하는 구현은 분석 성능에서 기존의 결과와 비교하여 대등한 수준을 유지하면서도, 수행 시간을 기존 구현의 결과 대비 최대 89% 단축하고, 메모리 요구량은 기존 구현의 결과 대비 최대 95% 축소한다.","This paper presents the efficient implementation of the binarized convolutional neural network (CNN). The binarized CNN is designed by modifying the conventional CNN so as to include the binarization processes for the parameters and the activation outputs. In the proposed implementation, multiple binarized parameters and multiple activation outputs are packed into a single word, and the inner-products are calculated by performing simple bitwise XNOR followed by bit-counting operations. Owing to such SIMD optimization, both the overall number of the computations and the memory footprint are reduced significantly. LeNet-5 and ResNet-18 are implemented based on the proposed SIMD optimization. When compared to the straightforward implementation of the non-binarized CNN model, the proposed implementation shows the significant reduction in terms of the inference time and the memory footprint, while maintaining the analysis performance."
Breast cancer histology images classification: Training from scratch or transfer learning?,2018,"['Breast cancer', 'Histopathological images', 'Convolutional neural network', 'Full training', 'Transfer learning']",국문 초록 정보 없음,"We demonstrated the ability of transfer learning in comparison with the fully-trained network on the histopathological imaging modality by considering three pre-trained networks: VGG16, VGG19, and ResNet50 and analyzed their behavior for magnification independent breast cancer classification. Concurrently, we examined the effect of training–testing data size on the performance of considered networks. A fine-tuned pre-trained VGG16 with logistic regression classifier yielded the best performance with 92.60% accuracy, 95.65% area under ROC curve (AUC), and 95.95% accuracy precision score (APS) for 90%–10% training–testing data splitting. Layer-wise fine-tuning and different weight initialization schemes can be a future aspect of this study."
심층신경망 기반 총채벌레 탐색에 관한 연구,2018,"['객체 탐색', '볼록총채벌레', '빠른 지역기반 객체 탐색', '심층신경망', '합성곱 신경망', 'Convolutinal network', 'deep learning', 'Faster R-CNN', 'object detection', 'Scirtothrips dorsalis Hood']","최근 감귤농업에서 주요해층으로 분류되는 미소 객체 (tiny object)인 볼록총채벌레 (Scirtothrips dorsalis Hood)의 탐색은 관심이 많고 어려운 작업으로 알려져 있다. 본 논문에서는 심층신경망을 이용하여 볼록총채벌레를 탐색 (detection)하고자 한다. 분석자료는 황색끈끈이트랩 이미지자료 (250×150mm, 5472×3648픽셀)이며 합성곱 신경망 (convolutional neural network, CNN)인 ResNet을 기반으로 하는 Faster R-CNN (faster regions with CNN) 탐색모형을 사용하였다. 이미지넷(ImageNet)을 사전 학습한 가중치를 사용하고 초모수 (hyperparameter)를 격자탐색법(grid search)으로 선택한 모형을 제안한다. 제안된 모형의 AUC (area under curve)는 0.91로 아주 좋은 결과를 보이는데, 제안된 모형으로 볼록총채벌레의 생태를 파악하여 보다 더 정밀한 방제가 이뤄질 수 있을 것으로 기대한다.","In this paper, we study on a detection of Scirtothrips dorsalis Hood, which is classified as a major insect in citrus farming. The detection is based on the deep neural networks, specifically the Faster R-CNN (faster regions with CNN) model based on CNN (convolutional neural network), with the yellow sticky trap image data (250×150mm, 5472×3648pixels). It was found that the model performance becomes unstable when the object is too small and rare. In order to solve this problem, we use pretrained weights to set the initial value of the model, as well as we select hyperparameters by grid search. Result shows that our proposed model has an high AUC (area under curve) value 0.91. We expect that it would be possible to know more precisely the lifespan of the Scirtothrips dorsalis Hood and to control them more precisely through our proposed model."
CNN 소실점 검출을 이용한 차선 검출,2018,"['lane detection', 'deep learning', 'convolutional neural networks', 'transfer learning']",국문 초록 정보 없음,"Lane detection is essential in autonomous navigation. Conventional algorithms use hand crafted features which produce difficulties because of diverse image variations from illumination variations, occlusions and shadows. Recently, deep learning based approaches have provided more robust results. In this paper, we present an algorithm for the robust detection of lanes by finding vanishing points with convolutional neural networks. We use two modified CNN architectures, where the final output layer consists of four elements. The epipole and the angles of the current driving lane each have two elements. Experiments are performed by using two modified structures of the NVIDIA end-to-end model[9] and the ResNet-50 model[10]."
딥 러닝 기반의 악성흑색종 분류를 위한 컴퓨터 보조진단 알고리즘,2018,"['Deep Learning', 'Machine Learning', 'Malignant Melanoma', 'Convolutional Neural Network', 'Computer Aided Diagnosis']",국문 초록 정보 없음,"The malignant melanoma accounts for about 1 to 3% of the total malignant tumor in the West, especially in the US, it is a disease that causes more than 9,000 deaths each year. Generally, skin lesions are difficult to detect the features through photography. In this paper, we propose a computer-aided diagnosis algorithm based on deep learning for classification of malignant melanoma and benign skin tumor in RGB channel skin images. The proposed deep learning model configures the tumor lesion segmentation model and a classification model of malignant melanoma. First, U-Net was used to segment a skin lesion area in the dermoscopic image. We could implement algorithms to classify malignant melanoma and benign tumor using skin lesion image and results of expert’s labeling in ResNet. The U-Net model obtained a dice similarity coefficient of 83.45% compared with results of expert’s labeling. The classification accuracy of malignant melanoma obtained the 83.06%. As the result, it is expected that the proposed artificial intelligence algorithm will utilize as a computer-aided diagnosis algorithm and help to detect malignant melanoma at an early stage."
Partitioning Compute Units in CNN Acceleration for Statistical Memory Traffic Shaping,2018,[],국문 초록 정보 없음,"<P>Convolutional Neural Networks (CNNs) have become the default choice for processing visual information, and the design complexity of CNNs has been steadily increasing to improve accuracy. To cope with the massive amount of computation needed for such complex CNNs, the latest solutions utilize blocking of an image over the available dimensions (e.g., horizontal, vertical, channel, and kernel) and batching of multiple input images to improve data reuse in the memory hierarchy. While there has been a large collection of works on maximizing data reuse, only a few studies have focused on the memory bottleneck problem caused by limited bandwidth. Bandwidth bottleneck can easily occur in CNN acceleration as CNN layers have different sizes with varying computation needs and as batching is typically performed over each layer of CNN for an ideal data reuse. In this case, the data transfer demand for a layer can be relatively low or high compared to the computation requirement of the layer, and therefore temporal fluctuations in memory access can be induced eventually causing bandwidth problems. In this paper, we first show that there exists a high degree of fluctuation in memory access to computation ratio depending on CNN layers and functions in the layer being processed by the compute units (cores), where the compute units are tightly synchronized to maximize data reuse. Then we propose a strategy of partitioning the compute units where the cores within each partition process a batch of input data in a synchronous manner to maximize data reuse but different partitions run asynchronously. Because the partitions stay asynchronous and typically process different CNN layers at any given moment, the memory access traffic sizes of the partitions become statistically shuffled. Thus, the partitioning of compute units and asynchronous use of them make the total memory access traffic size be smoothened over time, and the degree of partitioning determines a tradeoff between data reuse efficiency and memory bandwidth utilization efficiency. We call this smoothing statistical memory traffic shaping, and we show that it can lead to 8.0 percent of performance gain on a commercial 64-core processor when running ResNet-50.</P>"
임베디드 보드에서 실시간 의미론적 분할을 위한 심층 신경망 구조,2018,"['딥 러닝', '심층 신경망', '의미론적 분할', '자율주행', '임베디드 보드', 'deep learning', 'neural network', 'semantic segmentation', 'autonomous driving', 'embedded board']","본 논문은 자율주행을 위한 실시간 의미론적 분할 방법으로 최적화된 심층 신경망 구조인Wide Inception ResNet (WIR Net)을 제안한다. 신경망 구조는 Residual connection과 Inception module 을 적용하여 특징을 추출하는 인코더와 Transposed convolution과 낮은 층의 특징 맵을 사용하여 해상도를 높이는 디코더로 구성하였고 ELU 활성화 함수를 적용함으로써 성능을 올렸다. 또한 신경망의 전체 층수를 줄이고 필터 수를 늘리는 방법을 통해 성능을 최적화하였다. 성능평가는 NVIDIA Geforce gtx 1080 과 TX1 보드를 사용하여 주행환경의 Cityscapes 데이터에 대해 클래스와 카테고리별 IoU를 평가하였다.실험 결과를 통해 클래스 IoU 53.4, 카테고리 IoU 81.8의 정확도와 TX1 보드에서 640×360, 720×480 해상도 영상처리에 17.8fps, 13.0fps의 실행속도를 보여주는 것을 확인하였다.","We propose Wide Inception ResNet (WIR Net) an optimized neural network architecture as a real-time semantic segmentation method for autonomous driving. The neural network architecture consists of an encoder that extracts features by applying a residual connection and inception module, and a decoder that increases the resolution by using transposed convolution and a low layer feature map. We also improved the performance by applying an ELU activation function and optimized the neural network by reducing the number of layers and increasing the number of filters. The performance evaluations used an NVIDIA Geforce GTX 1080 and TX1 boards to assess the class and category IoU for cityscapes data in the driving environment. The experimental results show that the accuracy of class IoU 53.4, category IoU 81.8 and the execution speed of 640x360, 720x480 resolution image processing 17.8fps and 13.0fps on TX1 board."
DenseNet 기반의 이미지 압축,2018,[],"본 논문에서는 기존 신경망 기반의 이미지 압축에 많이 사용되었던 신경망인 ResNet 을 대신하여 더 적은 개수의 파라미터를 사용하여 좋은 성능을 낼 수 있는 신경망 구조인 DenseNet 을 이미지 압축에 사용한다. 이미지 압축을 위해 사용되는 신경망 구조는 일반적으로 오토 인코더 구조인데, 병목 층에서 정보 손실이 상당히 많이 발생한다. 따라서 이미지 압축에서 신경망 내에서의 정보 전달은 상당히 중요하다. 기존의 논문에서는 이를 위해 이전의 정보를 그대로 뒤로 전달해주는 구조인 ResNet 을 사용하여 깊은 층에 대해서도 수렴이 잘 되는 결과를 보여주었다. 그러나 많은 수의 파라미터를 사용하는 단점을 해결하기 위해 본 논문에서는 DenseNet 을 이미지 압축에 사용하였고, 병목 층에서의 정보 손실로 인해 이미지의 고주파수 성분이 사라지는 현상을 해결하기 위해 원래 이미지와 JPEG2000 으로 압축한 이미지와의 차이를 추가 입력으로 넣어주어서 주관적인 화질을 개선하였다.",다국어 초록 정보 없음
소표본 의료 영상의 전이 학습을 위한 Feature Extractor 기법의 성능 비교 및 분석,2018,[],"본 논문은 소표본 의료용 영상 분석의 정확도 향상을 위해 전이학습 모델을 feature extractor로 구축하여 학습시키는 방법을 연구하였으며 성능 평가를 위해 선학습모델로 AlexNet, ResNet, DenseNet을 사용하여 fine tuning 기법을 적용하였을 때와의 성능을 비교 분석하였다. 그 결과 실험에 사용된 3개의 모델에서 fine tuning 기법보다 향상된 정확도를 보임을 확인하였고, 또한 ImageNet으로 학습된 AlexNet, ResNet, DenseNet이 소표본 의료용 X-Ray 영상에 적용될 수 있음을 보였다.",다국어 초록 정보 없음
메모리 추가 신경망을 이용한 희소 악성코드 분류,2018,"['Malware Classification', 'Visualization', 'Memory Augmented Neural Network']","악성코드의 수가 가파르게 증가하면서 기업 및 공공기관, 금융기관, 병·의원 등을 타깃으로 한 사이버 공격 피해사례가 늘어나고 있다. 이러한 흐름에 따라 학계와 보안 업계에서는 악성코드 탐지를 위한 다양한 연구를 진행하고 있다. 최근 들어서는 딥러닝을 비롯해 머신러닝 기법을 적용하는 형태의 연구가 많이 진행되는 추세다. 이 중 합성곱 신경망(CNN: Convolutional Neural Network), ResNet 등을 이용한 악성코드 분류 연구의 경우에는 기존의 분류 방법에 비해 정확도가 크게 향상된 것을 확인할 수 있다. 그러나 타깃 공격의 특징 중 하나는 사용된 악성코드가 불특정 다수를 상대로 광범위하게 퍼뜨리는 형태가 아닌, 특정 대상을 타깃으로 한 맞춤형 악성코드라는 점이다. 이러한 유형의 악성코드는 그 수가 많지 않기 때문에 기존에 연구되어온 머신러닝이나 딥러닝 기법을 적용하기에 한계가 있다. 본 논문은 타깃형 악성코드와 같이 샘플의 양이 부족한 상황에서 악성코드를 분류하는 방법에 대해 다루고 있다. 메모리가 추가된 신경망(MANN: Memory Augmented Neural Networks) 모델을 이용하였고 각 그룹별 20개의 소량 데이터로 구성되어 있는 악성코드 데이터셋에 대해 최대 97%까지 정확도로 분류할 수있음을 확인하였다.","As the number of malicious code increases steeply, cyber attack victims targeting corporations, public institutions, financial institutions, hospitals are also increasing. Accordingly, academia and security industry are conducting various researches on malicious code detection. In recent years, there have been a lot of researches using machine learning techniques including deep learning. In the case of research using Convolutional Neural Network, ResNet, etc. for classification of malicious code, it can be confirmed that the performance improvement is higher than the existing classification method. However, one of the characteristics of the target attack is that it is custom malicious code that makes it operate only for a specific company, so it is not a form spreading widely to a large number of users. Since there are not many malicious codes of this kind, it is difficult to apply the previously studied machine learning or deep learning techniques. In this paper, we propose a method to classify malicious codes when the amount of samples is insufficient such as targeting type malicious code. As a result of the study, we confirmed that the accuracy of 97% can be achieved even with a small amount of data by applying the Memory Augmented Neural Networks model."
자동차 주행환경에서 보행자 분류를 위한 딥러닝 모델의 전이학습 및 성능비교,2018,"['pedestrian classification', 'deep-learning', 'automobile driving environment', 'transer learning', 'INRIA database']",국문 초록 정보 없음,"In this paper, a performance comparison of deep-learning models for pedestrian classification under automobile driving environment is performed. Most automobiles nowadays are equipped with black boxes, and driver assistance systems are also applied to camera based image processing technologies. Pedestrian classification plays an important role in determining the final decision whether a candidate region is a person or not. We perform the transfer learning based on AlexNet, GoogLeNet, and ResNet that are well known as deep-learning models. For comparison experiments of the deep learning models, we used INRIA database and Chosun University (CU) database constructed under automobile driving environment. The INRIA training data set is used for transfer learning and performance validation is used with INRIA testing data set and CU database. The experimental results showed that the performance of ResNet based on transfer learning outperformed AlexNet and GoogLeNet."
Convolutional Neural Network Architecture 에 따른 Cone beam artifact 제거 성능 비교,2018,[],국문 초록 정보 없음,"CBCT(Cone beam CT) is widely used in diagnosis and treatment planning of implant dentistry, orthopedics, and interventional radiology. However, the reconstructed images by CBCT geometry generate cone beam artifacts, which would disturb lesion detectability and degrade diagnostic accuracy. In this work, we present convolutional neural network(CNN) based cone beam artifacts correction method, which is computationally efficient and achieve better performance in artifact correction. We compared the performance of the cone beam artifacts reduction via U-Net and ResNet models, which are trained with simulated CBCT images. Our result showed that U-Net performs better than ResNet in cone beam artifact reduction."
Residual Convolutional Neural Network Revisited with Active Weighted Mapping,2018,"['Deep Learning', 'Resnet', 'Weighted mapping']",국문 초록 정보 없음,다국어 초록 정보 없음
Video Captioning with Visual and Semantic Features,2018,"['Attention-Based Caption Generation', 'Deep Neural Networks', 'Semantic Feature', 'Video Captioning']",국문 초록 정보 없음,"Video captioning refers to the process of extracting features from a video and generating video captions using the extracted features. This paper introduces a deep neural network model and its learning method for effective video captioning. In this study, visual features as well as semantic features, which effectively express the video, are also used. The visual features of the video are extracted using convolutional neural networks, such as C3D and ResNet, while the semantic features are extracted using a semantic feature extraction network proposed in this paper. Further, an attention-based caption generation network is proposed for effective generation of video captions using the extracted features. The performance and effectiveness of the proposed model is verified through various experiments using two large-scale video benchmarks such as the Microsoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT)."
Video Captioning with Visual and Semantic Features,2018,"['Attention-Based Caption Generation', 'Deep Neural Networks', 'Semantic Feature', 'Video Captioning']",국문 초록 정보 없음,"Video captioning refers to the process of extracting features from a video and generating video captions usingthe extracted features. This paper introduces a deep neural network model and its learning method foreffective video captioning. In this study, visual features as well as semantic features, which effectively expressthe video, are also used. The visual features of the video are extracted using convolutional neural networks,such as C3D and ResNet, while the semantic features are extracted using a semantic feature extractionnetwork proposed in this paper. Further, an attention-based caption generation network is proposed foreffective generation of video captions using the extracted features. The performance and effectiveness of theproposed model is verified through various experiments using two large-scale video benchmarks such as theMicrosoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT)."
Video Captioning with Visual and Semantic Features,2018,"['Attention-Based Caption Generation', 'Deep Neural Networks', 'Semantic Feature', 'Video Captioning']",국문 초록 정보 없음,"Video captioning refers to the process of extracting features from a video and generating video captions using the extracted features. This paper introduces a deep neural network model and its learning method for effective video captioning. In this study, visual features as well as semantic features, which effectively express the video, are also used. The visual features of the video are extracted using convolutional neural networks, such as C3D and ResNet, while the semantic features are extracted using a semantic feature extraction network proposed in this paper. Further, an attention-based caption generation network is proposed for effective generation of video captions using the extracted features. The performance and effectiveness of the proposed model is verified through various experiments using two large-scale video benchmarks such as the Microsoft Video Description (MSVD) and the Microsoft Research Video-To-Text (MSR-VTT)."
Multimodal Feature Learning for Video Captioning,2018,[],국문 초록 정보 없음,"<P>Video captioning refers to the task of generating a natural language sentence that explains the content of the input video clips. This study proposes a deep neural network model for effective video captioning. Apart from visual features, the proposed model learns additionally semantic features that describe the video content effectively. In our model, visual features of the input video are extracted using convolutional neural networks such as C3D and ResNet, while semantic features are obtained using recurrent neural networks such as LSTM. In addition, our model includes an attention-based caption generation network to generate the correct natural language captions based on the multimodal video feature sequences. Various experiments, conducted with the two large benchmark datasets, Microsoft Video Description (MSVD) and Microsoft Research Video-to-Text (MSR-VTT), demonstrate the performance of the proposed model.</P>"
DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos,2018,"['Activity Detection', 'Bi-directional LSTM', 'Deep Neural Networks', 'Untrimmed Video']",국문 초록 정보 없음,"We propose a novel deep neural network model for detecting human activities in untrimmed videos. The process of human activity detection in a video involves two steps: a step to extract features that are effective in recognizing human activities in a long untrimmed video, followed by a step to detect human activities from those extracted features. To extract the rich features from video segments that could express unique patterns for each activity, we employ two different convolutional neural network models, C3D and I-ResNet. For detecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directional recurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmark dataset, we show the high performance of the proposed DeepAct model."
DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos,2018,"['Activity Detection', 'Bi-directional LSTM', 'Deep Neural Networks', 'Untrimmed Video']",국문 초록 정보 없음,"We propose a novel deep neural network model for detecting human activities in untrimmed videos. The process of human activity detection in a video involves two steps: a step to extract features that are effective in recognizing human activities in a long untrimmed video, followed by a step to detect human activities from those extracted features. To extract the rich features from video segments that could express unique patterns for each activity, we employ two different convolutional neural network models, C3D and I-ResNet. For detecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directional recurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmark dataset, we show the high performance of the proposed DeepAct model."
단백질 이차 구조 예측을 위한 합성곱 신경망의 구조,2018,[],국문 초록 정보 없음,"Deep learning has been actively studied for predicting protein secondary structure based only on the sequence information of the amino acids constituting the protein. In this paper, we compared the performances of the convolutional neural networks of various structures to predict the protein secondary structure. To investigate the optimal depth of the layer of neural network for the prediction of protein secondary structure, the performance according to the number of layers was investigated. We also applied the structure of GoogLeNet and ResNet which constitute building blocks of many image classification methods. These methods extract various features from input data, and smooth the gradient transmission in the learning process even using the deep layer. These architectures of convolutional neural networks were modified to suit the characteristics of protein data to improve performance."
DeepAct: A Deep Neural Network Model for Activity Detection in Untrimmed Videos,2018,"['Activity Detection', 'Bi-directional LSTM', 'Deep Neural Networks', 'Untrimmed Video']",국문 초록 정보 없음,"We propose a novel deep neural network model for detecting human activities in untrimmed videos. Theprocess of human activity detection in a video involves two steps: a step to extract features that are effective inrecognizing human activities in a long untrimmed video, followed by a step to detect human activities fromthose extracted features. To extract the rich features from video segments that could express unique patternsfor each activity, we employ two different convolutional neural network models, C3D and I-ResNet. Fordetecting human activities from the sequence of extracted feature vectors, we use BLSTM, a bi-directionalrecurrent neural network model. By conducting experiments with ActivityNet 200, a large-scale benchmarkdataset, we show the high performance of the proposed DeepAct model."
다양한 합성곱 신경망 접근법을 이용한 잡초 이미지 분류,2018,[],국문 초록 정보 없음,"In this paper, we present a multimodal approach for weeds classification. We apply the transfer learning to classify on Convolutional Neural Networks(CNN) VGG16, Inception-Resnet, and Mobilenet separately. Then, we combine probabilities returned from each model, and start voting by scoring classes. We choose a class that has the highest score to conclude the final classification. We experiment on own weeds dataset and achieve 95.927% accuracy after voting on fusion classification."
