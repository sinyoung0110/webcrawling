title,date,keywords,abstract,multilingual_abstract
딥러닝 기반 6D 객체 포즈 개선을 위한 3D 윤곽 부정합 손실,2023,,,
Application of Improved Variational Recurrent Auto-Encoder for Korean Sentence Generation,2018,"['딥러닝', '생성모델', '변분순환오토인코더', '한국어 문장 생성', '보간법', 'deep learning', 'generative models', 'variational recurrent auto-encoder', 'Korean sentence generation', 'interpolation']",,"Due to the revolutionary advances in deep learning, performance of pattern recognition has increased significantly in many applications like speech recognition and image recognition, and some systems outperform human-level intelligence in specific domains. Unlike pattern recognition, in this paper, we focus on generating Korean sentences based on a few Korean sentences. We apply variational recurrent auto-encoder (VRAE) and modify the model considering some characteristics of Korean sentences. To reduce the number of words in the model, we apply a word spacing model. Also, there are many Korean sentences which have the same meaning but different word order, even without subjects or objects; therefore we change the unidirectional encoder of VRAE into a bidirectional encoder. In addition, we apply an interpolation method on the encoded vectors from the given sentences, so that we can generate new sentences which are similar to the given sentences. In experiments, we confirm that our proposed method generates better sentences which are semantically more similar to the given sentences."
RNN Auto-Encoder의 시계열 임베딩을 이용한 자동작곡,2018,"['RNN', 'Automatic Composition', 'Auto-Encoder']",,
주파수 영역 수동소나 표적신호 분리 및 잡음 제거 알고리즘,2018,"['Passive sonar', 'Frequency domain', 'DEMON', 'JADE', 'SDAE']",,
다차원 데이터에 대한 심층 군집 네트워크의 성능향상 방법,2018,"['Deep Learning', 'Clustering', 'Auto Encoder', 'K-means', 'Dimension Reduction', 'Deep Clustering Networks']",,
Deep Hashing for Semi-supervised Content Based Image Retrieval,2018,"['Semi-supervised deep hashing', 'binary hashing', 'convolutional auto-encoders', 'deep learning', 'image semantics']",,"Content-based image retrieval is an approach used to query images based on their semantics. Semantic based retrieval has its application in all fields including medicine, space, computing etc. Semantically generated binary hash codes can improve content-based image retrieval. These semantic labels / binary hash codes can be generated from unlabeled data using convolutional autoencoders. Proposed approach uses semi-supervised deep hashing with semantic learning and binary code generation by minimizing the objective function. Convolutional autoencoders are basis to extract semantic features due to its property of image generation from low level semantic representations. These representations of images are more effective than simple feature extraction and can preserve better semantic information. Proposed activation and loss functions helped to minimize classification error and produce better hash codes. Most widely used datasets have been used for verification of this approach that outperforms the existing methods."
LSTM 기반의 sequence-to-sequence 모델을 이용한 한글 자동 띄어쓰기,2018,"['인코딩-디코딩', 'sequence-to-sequence', 'LSTM', '순차정보 신경망', '자동 띄어쓰기', '드롭아웃', '계층 정 규화', 'encoding-decoding', 'LSTM', 'sequence-to-sequence neural network', 'auto spacing', 'drop-out', 'layer normalization']","자동 띄어쓰기 특성을 효과적으로 처리할 수 있는 LSTM(Long Short-Term Memory Neural Networks) 기반의 RNN 모 델을 제시하고 적용한 결과를 분석하였다. 문장이 길거나 일부 노이즈가 포함된 경우에 신경망 학습이 쉽지 않은 문제를 해결하기 위하여 입력 데이터 형식과 디코딩 데이터 형식을 정의하고, 신경망 학습에서 드롭아웃, 양방향 다층 LSTM 셀, 계층 정규화기법, 주목 기법(attention mechanism)을 적용하여 성능을 향상시키는 방법을 제안하였다. 학습 데이터로는 세종 말뭉치 자료를 사용하였으며, 학습 데이터가 부분적으로 불완전한 띄어쓰기가 포함되어 있었음에도 불구하고, 대량의 학습 데이터를 통해 한글 띄어쓰기에 대한 패턴이 의미 있게 학습되었다. 이것은 신경망에서 드롭아웃 기법을 통해 학습 모델의 오버피팅이 되지않도록 함으로써 노이즈에 강한 모델을 만들었기 때문이다. 실험결과로 LSTM sequence-to-sequence 모델이 재현율과 정확도를 함께 고려한 평가 점수인 F1 값이 0.94로 규칙 기반 방식과 딥러닝 GRU-CRF보다 더 높은 성능을 보였다.","We proposed a LSTM-based RNN model that can effectively perform the automatic spacing characteristics. For those long or noisy sentences which are known to be difficult to handle within Neural Network Learning, we defined a proper input data format and decoding data format, and added dropout, bidirectional multi-layer LSTM, layer normalization, and attention mechanism to improve the performance. Despite of the fact that Sejong corpus contains some spacing errors, a noise-robust learning model developed in this study with no overfitting through a dropout method helped training and returned meaningful results of Korean word spacing and its patterns. The experimental results showed that the performance of LSTM sequence-to-sequence model is 0.94 in F1-measure, which is better than the rule-based deep-learning method of GRU-CRF."
