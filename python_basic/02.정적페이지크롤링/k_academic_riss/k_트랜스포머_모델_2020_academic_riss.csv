keyword,count
BERT,8
트랜스포머,6
Transformer,4
transformer,3
attention mechanism,2
Stray load loss,2
LLC resonant converter,2
Multi-winding,2
Cross-regulation,2
Voltage transfer ratio for multi-output structure,2
Leakage inductanc,2
language model,2
Machine Translation,2
Economic Analysis,2
Conservation Voltage Reduction,2
딥러닝,2
Distribution System,2
Direct Method,2
Core loss,1
Clamp,1
Tank,1
high manganese steel,1
Shell type transformer,1
문서요약,1
Distribution transformer,1
Optimum design,1
ICP,1
E-mode,1
H-mode,1
frequency,1
transformer model,1
power transfer efficiency,1
기계 독해 질의 응답,1
deep-learning,1
형태소-문장 변환기,1
generative summarization,1
OOV 랜덤 마스킹,1
Fine-Tuning,1
Question Answering,1
Masked Language Model,1
트랜스포머 언어 모델,1
Language Model,1
-19,1
LVRT/HVRT test device,1
Grid code,1
Grid connected wind turbines,1
High Voltage Ride Through(HVRT),1
Low Voltage Ride Through(LVRT),1
생성요약,1
미등록 단어 인식,1
질의 분석,1
심층신경망,1
정답유형 분석,1
machine reading question answering,1
Inverter,1
Load current sensorless,1
Observer,1
Three-phase UPS,1
DC bias · Epstein frame · Grain-oriented electrical steel · Magnetostriction,1
Line-to-earth fault,1
Line-to-earth voltage,1
Symmetrical coordinates method,1
Unearthed system,1
Zero sequence voltage,1
Fault diagnosis,1
Fault detection,1
Ground-fault resistance (GFR),1
Grounding resistance (GR),1
Rotor ground fault,1
Position measurement,1
Sample and hold,1
Signal conditioner,1
Renewable energy sources,1
query analysis,1
transformer language models,1
answer type,1
Fault analysis,1
text summarization,1
Hybrid power grid model,1
PSCAD,1
LVDT,1
Short circuit,1
DC distribution,1
Decoupling power control,1
DAB(Dual-Active-Bridge) converter,1
Multi-port converter,1
Multiple transformers,1
recognition of unknown word,1
Automatic Post Editing,1
morpheme-to-sentence converter,1
random masked OOV,1
copying mechanism,1
시퀀스-투-시퀀스,1
한국어 형태소 분석기,1
주의 메커니즘,1
복사 메커니즘,1
Natural Language Processing,1
자연어 처리,1
Pre-trained Model,1
사전훈련 된 모델,1
RNN,1
순환신경망,1
Attention,1
어텐션,1
Deep Neural network,1
Denoising Autoencoder,1
Unsupervised Learning,1
Transfer learning,1
Korean morphological analyzer,1
sequence-to-sequence,1
Sentence-piece tokenizer,1
machine translation,1
의도분석,1
의도파악,1
의미유사도,1
deep neural network,1
natural language understanding,1
intention analysis,1
semantic similarity,1
automatic post-editing,1
Transformer for language model,1
multi-encoder architecture,1
기계 번역,1
번역문 자동 사후 교정,1
다중 인코더 구조,1
주의 집중 기법,1
Language model,1
Language model based on DNN,1
speech recognition,1
end-to-end model,1
small-data speech recognition,1
neural machine translation,1
Bidirectional Encoder Representations from Transformers,1
semantic role labeling,1
machine learning,1
bidirectional encoder representations from transformers,1
신경망기계번역,1
언어모델,1
비정상적 변동성,1
translation quality,1
기계학습,1
unreasonable volatility,1
기계번역,1
기계번역 사후교정,1
인공신경망 기계번역,1
자연어이해,1
Deep Learning,1
Neural Machine Translation,1
언어 모델,1
의미역 결정,1
음성 인식,1
신호 대 신호 변환,1
종단간 음성 인식,1
적은 데이터 음성 인식,1
voice conversion,1
transformer network,1
signal-to-signal conversion,1
음성 변환,1
트랜스포머 네트워크,1
BCT model · Migration–migration model · Nanocomposites · Surface discharge,1
CIPlanning,1
Four distribution ports,1
Hydraulic transformer,1
Efficiency characteristics,1
Theoretical and Experimental,1
Energy saving,1
PDDL,1
LTL,1
Synchronous condensers,1
