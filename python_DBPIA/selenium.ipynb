{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출할 논문의 수 :  1562\n",
      "추출한 논문의 수 :  726\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 726 entries, 0 to 725\n",
      "Data columns (total 1 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   keywords  398 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#CNN, #DL Model, #Training and Validation Data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#바퀴식 주행로봇, #경로 찾기, #딥강화학습, #딥러닝, #자율주행로봇, #Whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#분류, #감성어휘, #병원, #실내이미지, #딥러닝, #Classification...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Instance segmentation, #Deep learning, #Slope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#PM\\n10\\nDeep Learning, #Satellite Image, #Sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords\n",
       "0  #CNN, #DL Model, #Training and Validation Data...\n",
       "1  #바퀴식 주행로봇, #경로 찾기, #딥강화학습, #딥러닝, #자율주행로봇, #Whe...\n",
       "2  #분류, #감성어휘, #병원, #실내이미지, #딥러닝, #Classification...\n",
       "3  #Instance segmentation, #Deep learning, #Slope...\n",
       "4  #PM\\n10\\nDeep Learning, #Satellite Image, #Sen..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selenium의 webdriver를 사용하기 위한 import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 페이지 로딩을 기다리는데에 사용할 time 모듈 import\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 데이터프레임을 다루기 위한 pandas import\n",
    "import pandas as pd\n",
    "\n",
    "# 크롷링하는 함수\n",
    "def croll():\n",
    "    for i in range(1, 22):\n",
    "        if i == 6:\n",
    "            continue\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, f'/html/body/main/article[2]/section/section[2]/section[2]/article[{i}]/section[1]/section[2]/article/a/h2').click()\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//*[@id=\"thesisTitle\"]').text\n",
    "        except:\n",
    "            title = ''\n",
    "\n",
    "        try:\n",
    "            date = driver.find_element(By.CSS_SELECTOR, '#dpMain > section > section.thesisDetail__info > section.thesisDetail__journal > ul > li:nth-child(4) > span:nth-child(1)').text\n",
    "        except:\n",
    "            try:\n",
    "                date = driver.find_element(By.CSS_SELECTOR, '#dpMain > section > section.thesisDetail__info > section.thesisDetail__journal.projectDetail__journal > ul > li:nth-child(2) > p.projectDetail__advisoir__desc').text\n",
    "            except:\n",
    "                date = ''\n",
    "        \n",
    "        try:\n",
    "            abstract = driver.find_element(By.CSS_SELECTOR, '#dpMain > section > section.thesisDetail__abstract > div.abstractTxt').text\n",
    "        except:\n",
    "            abstract = ''\n",
    "\n",
    "        try:\n",
    "            # 모든 키워드 <a> 태그 찾기\n",
    "            keyword_elements = driver.find_elements(By.CSS_SELECTOR, 'div.keywordWrap a.keywordWrap__keyword')\n",
    "            \n",
    "            # 각 키워드의 텍스트를 가져와 문자열로 결합\n",
    "            keyword_text = \", \".join([element.text for element in keyword_elements])\n",
    "        except:\n",
    "            keyword_text = ''\n",
    "        \n",
    "        titles.append(title)\n",
    "        dates.append(date)\n",
    "        abstracts.append(abstract)\n",
    "        keywords.append(keyword_text)\n",
    "        \n",
    "        time.sleep(1.5)\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(1.5)\n",
    "        if i % 3 == 0:\n",
    "            time.sleep(random.randint(3,7))\n",
    "\n",
    "\n",
    "# 논문 검색 설정\n",
    "search = input('검색할 키워드 : ').strip()\n",
    "min_year = input('추출할 최소 년도를 입력 : ').strip()\n",
    "max_year = input('추출할 최대 년도를 입력 : ')\n",
    "time.sleep(1)\n",
    "\n",
    "# 크롬드라이버 및 메인페이지 실행\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.dbpia.co.kr/')\n",
    "\n",
    "# 페이지가 완전히 로딩되도록 5초동안 기다림\n",
    "time.sleep(5)\n",
    "\n",
    "# 설정한 키워드로 검색\n",
    "driver.find_element(By.XPATH, '//*[@id=\"searchInput\"]').click()\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"searchInput\"]').send_keys(search)\n",
    "time.sleep(0.2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"submitSearchInput\"]').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# 페이지를 100개씩 보기로 설정\n",
    "# element = driver.find_element(By.XPATH, '//*[@id=\"selectWrapper\"]')\n",
    "# driver.execute_script(\"arguments[0].click();\", element)\n",
    "# time.sleep(3)\n",
    "# driver.find_element(By.XPATH, '//*[@id=\"get100PerPage\"]')\n",
    "# driver.execute_script(\"arguments[0].click();\", element)\n",
    "# time.sleep(3)\n",
    "\n",
    "# 페이지를 불러올 년도 설정\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMin\"]').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMin\"]').send_keys(min_year)\n",
    "time.sleep(0.3)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMax\"]').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMax\"]').send_keys(max_year)\n",
    "time.sleep(0.3)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeSearch\"]').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# 페이지 수 게산\n",
    "article_num = driver.find_element(By.XPATH, '//*[@id=\"totalCount\"]').text\n",
    "article_num = int(article_num[:-1].replace(',',''))\n",
    "page_num = article_num // 20 + 1\n",
    "print('추출할 논문의 수 : ', article_num)\n",
    "\n",
    "# 정보를 저장해둘 리스트 선언\n",
    "titles = []\n",
    "dates = []\n",
    "abstracts = []\n",
    "keywords = []\n",
    "\n",
    "# 첫 페이지 크롤링\n",
    "croll()\n",
    "\n",
    "# 나머지 페이지 크롤링\n",
    "for x in range(2,page_num+1):\n",
    "    try:\n",
    "        if x % 10 == 1:\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"goNextPage\"]').click()\n",
    "        else:\n",
    "            if x % 10 ==0:\n",
    "                driver.find_element(By.XPATH, '//*[@id=\"pageList\"]/a[10]').click()\n",
    "            else:\n",
    "                driver.find_element(By.XPATH, f'//*[@id=\"pageList\"]/a[{x%10}]').click()\n",
    "        time.sleep(random.randint(5,7))\n",
    "        if x % 5 == 0:\n",
    "            time.sleep(random.randint(10, 15))\n",
    "        croll()\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# 데이터 프레임 생성\n",
    "data = pd.DataFrame({\n",
    "    'title' : titles,\n",
    "    'date' : dates,\n",
    "    'abstract' : abstracts,\n",
    "    'keyword' : keywords\n",
    "})\n",
    "\n",
    "# 키워드 데이터를 키워드용 데이터프레임으로 변환\n",
    "keywords_data = pd.DataFrame({\n",
    "    'keywords': keywords  # 각 논문별 키워드 리스트\n",
    "})\n",
    "\n",
    "# 키워드 리스트를 explode로 펼치기\n",
    "keywords_data = keywords_data.explode('keywords').reset_index(drop=True)\n",
    "    \n",
    "\n",
    "# 데이터 프레임을 csv 형태로 저장\n",
    "csv_file_path = f\"{search}_{min_year}_dbpia.csv\"\n",
    "data.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# 데이터 프레임을 csv 형태로 저장\n",
    "csv_file_path_keywords = csv_file_path = f\"{search}_{min_year}_dbpia_keywords.csv\"\n",
    "keywords_data.to_csv(csv_file_path_keywords, index=False)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "print('추출한 논문의 수 : ', len(titles))\n",
    "\n",
    "#데이터 확인\n",
    "dff = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "dff.info()\n",
    "\n",
    "dff.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium의 webdriver를 사용하기 위한 import\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 페이지 로딩을 기다리는데에 사용할 time 모듈 import\n",
    "import time\n",
    "import random\n",
    "\n",
    "# 데이터프레임을 다루기 위한 pandas import\n",
    "import pandas as pd\n",
    "\n",
    "# 크롷링하는 함수\n",
    "def croll():\n",
    "    for i in range(1, 22):\n",
    "        if i == 6:\n",
    "            continue\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, f'/html/body/main/article[2]/section/section[2]/section[2]/article[{i}]/section[1]/section[2]/article/a/h2').click()\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, '//*[@id=\"thesisTitle\"]').text\n",
    "        except:\n",
    "            title = ''\n",
    "\n",
    "        try:\n",
    "            date = driver.find_element(By.CSS_SELECTOR, '#dpMain > section > section.thesisDetail__info > section.thesisDetail__journal > ul > li:nth-child(4) > span:nth-child(1)').text\n",
    "        except:\n",
    "            try:\n",
    "                date = driver.find_element(By.CSS_SELECTOR, '#dpMain > section > section.thesisDetail__info > section.thesisDetail__journal.projectDetail__journal > ul > li:nth-child(2) > p.projectDetail__advisoir__desc').text\n",
    "            except:\n",
    "                date = ''\n",
    "        \n",
    "        try:\n",
    "            abstract = driver.find_element(By.CSS_SELECTOR, '#dpMain > section > section.thesisDetail__abstract > div.abstractTxt').text\n",
    "        except:\n",
    "            abstract = ''\n",
    "\n",
    "        try:\n",
    "            # 모든 키워드 <a> 태그 찾기\n",
    "            keyword_elements = driver.find_elements(By.CSS_SELECTOR, 'div.keywordWrap a.keywordWrap__keyword')\n",
    "            \n",
    "            # 각 키워드의 텍스트를 가져와 문자열로 결합\n",
    "            keyword_text = \", \".join([element.text for element in keyword_elements])\n",
    "        except:\n",
    "            keyword_text = ''\n",
    "        \n",
    "        titles.append(title)\n",
    "        dates.append(date)\n",
    "        abstracts.append(abstract)\n",
    "        keywords.append(keyword_text)\n",
    "        \n",
    "        time.sleep(1.5)\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(1.5)\n",
    "        if i % 3 == 0:\n",
    "            time.sleep(random.randint(3,7))\n",
    "\n",
    "\n",
    "# 논문 검색 설정\n",
    "search = input('검색할 키워드 : ').strip()\n",
    "min_year = input('추출할 최소 년도를 입력 : ').strip()\n",
    "max_year = input('추출할 최대 년도를 입력 : ')\n",
    "time.sleep(1)\n",
    "\n",
    "# 크롬드라이버 및 메인페이지 실행\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.dbpia.co.kr/')\n",
    "\n",
    "# 페이지가 완전히 로딩되도록 5초동안 기다림\n",
    "time.sleep(5)\n",
    "\n",
    "# 설정한 키워드로 검색\n",
    "driver.find_element(By.XPATH, '//*[@id=\"searchInput\"]').click()\n",
    "time.sleep(0.5)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"searchInput\"]').send_keys(search)\n",
    "time.sleep(0.2)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"submitSearchInput\"]').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# 페이지를 100개씩 보기로 설정\n",
    "# element = driver.find_element(By.XPATH, '//*[@id=\"selectWrapper\"]')\n",
    "# driver.execute_script(\"arguments[0].click();\", element)\n",
    "# time.sleep(3)\n",
    "# driver.find_element(By.XPATH, '//*[@id=\"get100PerPage\"]')\n",
    "# driver.execute_script(\"arguments[0].click();\", element)\n",
    "# time.sleep(3)\n",
    "\n",
    "# 페이지를 불러올 년도 설정\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMin\"]').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMin\"]').send_keys(min_year)\n",
    "time.sleep(0.3)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMax\"]').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeMax\"]').send_keys(max_year)\n",
    "time.sleep(0.3)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"yearRangeSearch\"]').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# 페이지 수 게산\n",
    "article_num = driver.find_element(By.XPATH, '//*[@id=\"totalCount\"]').text\n",
    "article_num = int(article_num[:-1].replace(',',''))\n",
    "page_num = article_num // 20 + 1\n",
    "print('추출할 논문의 수 : ', article_num)\n",
    "\n",
    "# 정보를 저장해둘 리스트 선언\n",
    "titles = []\n",
    "dates = []\n",
    "abstracts = []\n",
    "keywords = []\n",
    "\n",
    "# 첫 페이지 크롤링\n",
    "croll()\n",
    "\n",
    "# 나머지 페이지 크롤링\n",
    "for x in range(2,page_num+1):\n",
    "    try:\n",
    "        if x % 10 == 1:\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"goNextPage\"]').click()\n",
    "        else:\n",
    "            if x % 10 ==0:\n",
    "                driver.find_element(By.XPATH, '//*[@id=\"pageList\"]/a[10]').click()\n",
    "            else:\n",
    "                driver.find_element(By.XPATH, f'//*[@id=\"pageList\"]/a[{x%10}]').click()\n",
    "        time.sleep(random.randint(5,7))\n",
    "        if x % 5 == 0:\n",
    "            time.sleep(random.randint(10, 15))\n",
    "        croll()\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# 데이터 프레임 생성\n",
    "data = pd.DataFrame({\n",
    "    'title' : titles,\n",
    "    'date' : dates,\n",
    "    'abstract' : abstracts,\n",
    "    'keyword' : keywords\n",
    "})\n",
    "\n",
    "# 키워드 데이터를 키워드용 데이터프레임으로 변환\n",
    "keywords_data = pd.DataFrame({\n",
    "    'keywords': keywords  # 각 논문별 키워드 리스트\n",
    "})\n",
    "\n",
    "# 키워드 리스트를 explode로 펼치기\n",
    "keywords_data = keywords_data.explode('keywords').reset_index(drop=True)\n",
    "    \n",
    "\n",
    "# 데이터 프레임을 csv 형태로 저장\n",
    "csv_file_path = f\"{search}_{min_year}_dbpia.csv\"\n",
    "data.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# 데이터 프레임을 csv 형태로 저장\n",
    "csv_file_path_keywords = csv_file_path = f\"{search}_{min_year}_dbpia_keywords.csv\"\n",
    "keywords_data.to_csv(csv_file_path_keywords, index=False)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "print('추출한 논문의 수 : ', len(titles))\n",
    "\n",
    "#데이터 확인\n",
    "dff = pd.read_csv(csv_file_path, encoding='utf-8')\n",
    "dff.info()\n",
    "\n",
    "dff.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
