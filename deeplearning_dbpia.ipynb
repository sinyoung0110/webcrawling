{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dbpia.co.kr/search/topSearch?searchOption=all&query=%EB%94%A5%EB%9F%AC%EB%8B%9D'\n",
    "\n",
    "res = []\n",
    "\n",
    "driver = webdriver.Chrome() # 크롬 드라이버 생성\n",
    "driver.get(url) # 사이트 접속하기\n",
    "\n",
    "checkbox = driver.find_element(By.ID, 'domesticRecord_064001') # KCI 등재 체크박스\n",
    "\n",
    "if not checkbox.is_selected(): # 만약 KCI 등재 체크박스가 체크되어 있지 않다면 체크\n",
    "    checkbox.click()\n",
    "\n",
    "driver.implicitly_wait(10) # 페이지 로딩이 완료될 때까지 대기\n",
    "\n",
    "cnt = 0 \n",
    "page = []\n",
    "\n",
    "for n in range(1,10): # KCI에 등재된 논문은 총 1943건(한 페이지당 20건씩 있음)\n",
    "    driver.execute_script(\"onclick=setPageNum({})\".format(n))\n",
    "    # 자바스크립트로 페이지가 이동되기 때문에 해당 코드를 실행\n",
    "    time.sleep(10) # 페이지가 완전히 로드될 때까지 10초 대기\n",
    "    html = driver.page_source \n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml') # 페이지 소스를 lxml로 변환\n",
    "    # lxml : String형식의 HTML문서 -> 살아있는 HTML문서\n",
    "\n",
    "    for_thesis = soup.find('section',attrs={\"id\":\"searchResultList\"}) # 논문 목록\n",
    "\n",
    "    for i in for_thesis.find_all('article',attrs={'class':'thesis__summary'}):\n",
    "        title = i.find('h2').get_text() # 제목 \n",
    "        date = i.find('span',attrs={'class':'thesis__item'}).get_text() # 날짜\n",
    "        abstract = i.find('span',attrs={'class':'thesis__abstract'}) # abstract \n",
    "        abstract2 = [abstract.get_text() if abstract != None else ''][0] \n",
    "        # 만약 abstract가 존재하지 않는다면 ''으로 대신함\n",
    "        if date[:4] in [str(j) for j in range(2014,2024)]: # 2014~2023년에 해당되는 논문만(1430건)\n",
    "            res += [[title, date, abstract2]] # [제목, 날짜, abstract 내용]을 리스트에 저장함\n",
    "            cnt += 1\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 1430건의 논문 수집 성공\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35 entries, 0 to 34\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   제목        35 non-null     object\n",
      " 1   날짜        35 non-null     object\n",
      " 2   abstract  35 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 972.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(res, columns=['제목','날짜','abstract'])\n",
    "df.info()\n",
    "\n",
    "df.to_csv('dbpia2014_2023_미술_치료.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35 entries, 0 to 34\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  35 non-null     int64  \n",
      " 1   제목          35 non-null     object \n",
      " 2   날짜          35 non-null     float64\n",
      " 3   abstract    23 non-null     object \n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "dff = pd.read_csv('dbpia2014_2023_미술_치료.csv',encoding='utf-8')\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>제목</th>\n",
       "      <th>날짜</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentinel-2 위성과 국토위성에서의 딥러닝 기반 초해상화 기법 비교 연구</td>\n",
       "      <td>2023.12</td>\n",
       "      <td>딥러닝 기반 초해상화 모델은 저해상도 영상을 효과적으로 고해상도로 변환하여 보다 선...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>시뮬레이션 데이터로 학습된 딥러닝 모델을 사용한 항공 라이다 데이터의 건물 모델링</td>\n",
       "      <td>2023.12</td>\n",
       "      <td>이를 위해 딥러닝 기술을 사용하여 건물 모델링의 중요한 과정인 지붕면 분할을 수행하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>위상잠금 적외선열화상의 가우시안 푸리에 변환 및 딥러닝을 활용한 해상도 향상 연구</td>\n",
       "      <td>2023.12</td>\n",
       "      <td>또한, 노이즈가 존재하는 이미지의 해상도를 향상하기 위해 딥러닝 기법 중 초고해상도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>딥러닝 기반 초해상화 모델을 이용한 드론 사진의 해상도 향상 연구</td>\n",
       "      <td>2023.12</td>\n",
       "      <td>고해상 위성 영상 및 항공 영상을 이용할 수 있으나, 경제적 부담으로 인해 지속적인...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>공간 질의응답 시스템에서의 개체링킹과 딥러닝 모델을 활용한 멘션 탐지</td>\n",
       "      <td>2023.12</td>\n",
       "      <td>이를 위해 딥러닝 모델인 BERT, RoBERTa 그리고 ChatGPT를 사용하여 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             제목       날짜  \\\n",
       "0           0    Sentinel-2 위성과 국토위성에서의 딥러닝 기반 초해상화 기법 비교 연구  2023.12   \n",
       "1           1  시뮬레이션 데이터로 학습된 딥러닝 모델을 사용한 항공 라이다 데이터의 건물 모델링  2023.12   \n",
       "2           2  위상잠금 적외선열화상의 가우시안 푸리에 변환 및 딥러닝을 활용한 해상도 향상 연구  2023.12   \n",
       "3           3           딥러닝 기반 초해상화 모델을 이용한 드론 사진의 해상도 향상 연구  2023.12   \n",
       "4           4         공간 질의응답 시스템에서의 개체링킹과 딥러닝 모델을 활용한 멘션 탐지  2023.12   \n",
       "\n",
       "                                            abstract  \n",
       "0  딥러닝 기반 초해상화 모델은 저해상도 영상을 효과적으로 고해상도로 변환하여 보다 선...  \n",
       "1  이를 위해 딥러닝 기술을 사용하여 건물 모델링의 중요한 과정인 지붕면 분할을 수행하...  \n",
       "2  또한, 노이즈가 존재하는 이미지의 해상도를 향상하기 위해 딥러닝 기법 중 초고해상도...  \n",
       "3  고해상 위성 영상 및 항공 영상을 이용할 수 있으나, 경제적 부담으로 인해 지속적인...  \n",
       "4  이를 위해 딥러닝 모델인 BERT, RoBERTa 그리고 ChatGPT를 사용하여 ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "searchQ = \"deeplearning\"\n",
    "startYear = 2014\n",
    "endYear = 2024\n",
    "\n",
    "print(\"start crawling..\")\n",
    "\n",
    "path = 'chromedriver'\n",
    "driver = webdriver.Chrome(path)\n",
    "xpath = driver.find_element_by_xpath\n",
    "\n",
    "#스타트업 검색\n",
    "driver.get('https://www.dbpia.co.kr/')\n",
    "keyword = driver.find_element_by_id('keyword')\n",
    "keyword.clear()\n",
    "keyword.send_keys(searchQ)\n",
    "serach_click_btn = xpath(\"//*[@id='header']/div[4]/div[2]/div[1]/div[1]/a\")\n",
    "driver.execute_script(\"arguments[0].click();\",serach_click_btn)\n",
    "\n",
    "#날짜 지정 및 체크박스 설정\n",
    "xpath(\"//*[@id='dev_sartYY']\").send_keys(startYear)\n",
    "xpath(\"//*[@id='dev_endYY']\").send_keys(endYear)\n",
    "click_btn = xpath(\"//*[@id='sidebar']/form/div[2]/div/div[1]/ul/li[4]/p/button\")\n",
    "driver.execute_script(\"arguments[0].click();\",click_btn)\n",
    "\n",
    "for i in range(1):\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='pub_check_sort3_0']\")))\n",
    "        driver.execute_script(\"arguments[0].click()\",btn)\n",
    "        btn2 = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='pub_check_sort3_1']\")))\n",
    "        driver.execute_script(\"arguments[0].click()\",btn2)\n",
    "    except:\n",
    "        print('retry')\n",
    "        time.sleep(1)\n",
    "\n",
    "# base_url = 'https://www.dbpia.co.kr/search/topSearch#none'\n",
    "# test_list = []\n",
    "# driver.get(base_url)\n",
    "\n",
    "# 더보기\n",
    "wCount = 0\n",
    "while(True):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        more = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='contents']/div[2]/div[3]/div[3]/div[3]/div/a\")))\n",
    "        driver.execute_script(\"arguments[0].click()\",more)\n",
    "    except:\n",
    "        print('retry')\n",
    "        break\n",
    "    wCount += 1\n",
    "    print(\" + page [{}]\".format(wCount))\n",
    "\n",
    "\n",
    "items_source = driver.page_source\n",
    "soup = BeautifulSoup(items_source, 'html.parser')\n",
    "\n",
    "\n",
    "items = soup.find('div','searchListArea').find('div','listBody').find('ul').find_all('li', 'item')\n",
    "\n",
    "# 논문제목, 저자, 퍼블리셔, 저널명,볼륨,날짜,초록\n",
    "titleL = []\n",
    "authorL = []\n",
    "# authorsL = []\n",
    "publisherL = []\n",
    "journalL = []\n",
    "volumeL = []\n",
    "dateL = []\n",
    "abstractL = []\n",
    "tLen = len(items)\n",
    "print(\"start parsing\")\n",
    "\n",
    "iCount = 0\n",
    "for item in items :\n",
    "    iCount += 1\n",
    "    if iCount % 20 == 0:\n",
    "        print(\" parsing.. [{}/{}]\".format(iCount, tLen))\n",
    "\n",
    "    title = ''\n",
    "    try : title = item.find('div','titWrap').find('a').text\n",
    "    except : title = ''\n",
    "\n",
    "    author = ''\n",
    "    try : author = item.find('li','author').text\n",
    "    except : author = ''\n",
    "\n",
    "    authors = ''\n",
    "    try : authors = item.find('li','author').find('input')['value']\n",
    "    except : authors = ''\n",
    "\n",
    "    publisher = ''\n",
    "    try : publisher = item.find('li','publisher').text\n",
    "    except : publisher = ''\n",
    "\n",
    "    journal = ''\n",
    "    try : journal = item.find('li','journal').text\n",
    "    except : journal = ''\n",
    "\n",
    "    volume = ''\n",
    "    try : volume = item.find('li','volume').text\n",
    "    except : volume = ''\n",
    "\n",
    "    date = ''\n",
    "    try : date = item.find('li','date').text\n",
    "    except : date = ''\n",
    "\n",
    "    abstract = ''\n",
    "    baseDetailUrl = \"https://www.dbpia.co.kr\"\n",
    "    pUrl = ''\n",
    "    try : pUrl = item.find('div','titWrap').find('a')['href']\n",
    "    except : pUrl = ''\n",
    "    if (pUrl != ''):\n",
    "        pUrl = baseDetailUrl + pUrl\n",
    "        driver.get(pUrl)\n",
    "        try : driver.find_element_by_xpath('//*[@id=\"#pub_modalOrganPop\"]').click()\n",
    "        except : pass\n",
    "        time.sleep(0.1)\n",
    "        try : driver.find_element_by_xpath('//*[@id=\"#pub_modalLoginPop\"]').click()\n",
    "        except : pass\n",
    "\n",
    "        try :\n",
    "            driver.find_element_by_xpath('//*[@id=\"pub_abstract\"]/div[2]/div/div[1]/div[2]/a').click()\n",
    "            eachPage = driver.page_source\n",
    "            ePsoup = BeautifulSoup(eachPage, 'html.parser')\n",
    "            abstract = ePsoup.find('div','abstFull').find('p','article').text\n",
    "        except : abstract = ''\n",
    "\n",
    "    titleL.append(title)\n",
    "    authorL.append(author)\n",
    "    # authorsL.append(authors)\n",
    "    publisherL.append(publisher)\n",
    "    journalL.append(journal)\n",
    "    volumeL.append(volume)\n",
    "    dateL.append(date)\n",
    "    abstractL.append(abstract)\n",
    "\n",
    "print(\"date to .csv file\")\n",
    "\n",
    "resultDict = dict(title = titleL,\n",
    "              author = authorL,\n",
    "              publisher = publisherL,\n",
    "              journal = journalL,\n",
    "              volume = volumeL,\n",
    "              date = dateL,\n",
    "              abstract = abstractL)\n",
    "\n",
    "fName = \"dbpia_{}_{}_{}.csv\".format(searchQ, startYear, endYear)\n",
    "\n",
    "DB = pd.DataFrame(resultDict)\n",
    "DB.to_csv(fName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
