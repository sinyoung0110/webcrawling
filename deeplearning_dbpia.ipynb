{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.dbpia.co.kr/search/topSearch?searchOption=all&query=%EB%94%A5%EB%9F%AC%EB%8B%9D#a'\n",
    "\n",
    "res = []\n",
    "\n",
    "driver = webdriver.Chrome() # 크롬 드라이버 생성\n",
    "driver.get(url) # 사이트 접속하기\n",
    "\n",
    "checkbox = driver.find_element(By.ID, 'domesticRecord_064001') # KCI 등재 체크박스\n",
    "\n",
    "if not checkbox.is_selected(): # 만약 KCI 등재 체크박스가 체크되어 있지 않다면 체크\n",
    "    checkbox.click()\n",
    "\n",
    "driver.implicitly_wait(100) # 페이지 로딩이 완료될 때까지 대기\n",
    "\n",
    "cnt = 0 \n",
    "page = []\n",
    "\n",
    "for n in range(1,187): # KCI에 등재된 논문은 총 3724건(한 페이지당 20건씩 있음)\n",
    "    driver.execute_script(\"onclick=setPageNum({})\".format(n))\n",
    "    # 자바스크립트로 페이지가 이동되기 때문에 해당 코드를 실행\n",
    "    time.sleep(5) # 페이지가 완전히 로드될 때까지 10초 대기\n",
    "    html = driver.page_source \n",
    "\n",
    "    soup = BeautifulSoup(html, 'lxml') # 페이지 소스를 lxml로 변환\n",
    "    # lxml : String형식의 HTML문서 -> 살아있는 HTML문서\n",
    "\n",
    "    for_thesis = soup.find('section',attrs={\"id\":\"searchResultList\"}) # 논문 목록\n",
    "\n",
    "    for i in for_thesis.find_all('article',attrs={'class':'thesis__summary'}):\n",
    "        title = i.find('h2').get_text() # 제목 \n",
    "        date = i.find('span',attrs={'class':'thesis__item'}).get_text() # 날짜\n",
    "        abstract = i.find('span',attrs={'class':'thesis__abstract'}) # abstract \n",
    "        abstract2 = [abstract.get_text() if abstract != None else ''][0] \n",
    "        # 만약 abstract가 존재하지 않는다면 ''으로 대신함\n",
    "        if date[:4] in [str(j) for j in range(2014,2024)]: # 2014~2023년에 해당되는 논문만(1430건)\n",
    "            res += [[title, date, abstract2]] # [제목, 날짜, abstract 내용]을 리스트에 저장함\n",
    "            cnt += 1\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 1430건의 논문 수집 성공\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=['제목','날짜','abstract'])\n",
    "df.info()\n",
    "\n",
    "df.to_csv('dbpia2014_2023_미술_치료.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv('dbpia2014_2023_미술_치료.csv',encoding='utf-8')\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "searchQ = \"deeplearning\"\n",
    "startYear = 2014\n",
    "endYear = 2024\n",
    "\n",
    "print(\"start crawling..\")\n",
    "\n",
    "path = 'chromedriver'\n",
    "driver = webdriver.Chrome(path)\n",
    "xpath = driver.find_element_by_xpath\n",
    "\n",
    "#스타트업 검색\n",
    "driver.get('https://www.dbpia.co.kr/')\n",
    "keyword = driver.find_element_by_id('keyword')\n",
    "keyword.clear()\n",
    "keyword.send_keys(searchQ)\n",
    "serach_click_btn = xpath(\"//*[@id='header']/div[4]/div[2]/div[1]/div[1]/a\")\n",
    "driver.execute_script(\"arguments[0].click();\",serach_click_btn)\n",
    "\n",
    "#날짜 지정 및 체크박스 설정\n",
    "xpath(\"//*[@id='dev_sartYY']\").send_keys(startYear)\n",
    "xpath(\"//*[@id='dev_endYY']\").send_keys(endYear)\n",
    "click_btn = xpath(\"//*[@id='sidebar']/form/div[2]/div/div[1]/ul/li[4]/p/button\")\n",
    "driver.execute_script(\"arguments[0].click();\",click_btn)\n",
    "\n",
    "for i in range(1):\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        btn = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='pub_check_sort3_0']\")))\n",
    "        driver.execute_script(\"arguments[0].click()\",btn)\n",
    "        btn2 = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='pub_check_sort3_1']\")))\n",
    "        driver.execute_script(\"arguments[0].click()\",btn2)\n",
    "    except:\n",
    "        print('retry')\n",
    "        time.sleep(1)\n",
    "\n",
    "# base_url = 'https://www.dbpia.co.kr/search/topSearch#none'\n",
    "# test_list = []\n",
    "# driver.get(base_url)\n",
    "\n",
    "# 더보기\n",
    "wCount = 0\n",
    "while(True):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        more = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='contents']/div[2]/div[3]/div[3]/div[3]/div/a\")))\n",
    "        driver.execute_script(\"arguments[0].click()\",more)\n",
    "    except:\n",
    "        print('retry')\n",
    "        break\n",
    "    wCount += 1\n",
    "    print(\" + page [{}]\".format(wCount))\n",
    "\n",
    "\n",
    "items_source = driver.page_source\n",
    "soup = BeautifulSoup(items_source, 'html.parser')\n",
    "\n",
    "\n",
    "items = soup.find('div','searchListArea').find('div','listBody').find('ul').find_all('li', 'item')\n",
    "\n",
    "# 논문제목, 저자, 퍼블리셔, 저널명,볼륨,날짜,초록\n",
    "titleL = []\n",
    "authorL = []\n",
    "# authorsL = []\n",
    "publisherL = []\n",
    "journalL = []\n",
    "volumeL = []\n",
    "dateL = []\n",
    "abstractL = []\n",
    "tLen = len(items)\n",
    "print(\"start parsing\")\n",
    "\n",
    "iCount = 0\n",
    "for item in items :\n",
    "    iCount += 1\n",
    "    if iCount % 20 == 0:\n",
    "        print(\" parsing.. [{}/{}]\".format(iCount, tLen))\n",
    "\n",
    "    title = ''\n",
    "    try : title = item.find('div','titWrap').find('a').text\n",
    "    except : title = ''\n",
    "\n",
    "    author = ''\n",
    "    try : author = item.find('li','author').text\n",
    "    except : author = ''\n",
    "\n",
    "    authors = ''\n",
    "    try : authors = item.find('li','author').find('input')['value']\n",
    "    except : authors = ''\n",
    "\n",
    "    publisher = ''\n",
    "    try : publisher = item.find('li','publisher').text\n",
    "    except : publisher = ''\n",
    "\n",
    "    journal = ''\n",
    "    try : journal = item.find('li','journal').text\n",
    "    except : journal = ''\n",
    "\n",
    "    volume = ''\n",
    "    try : volume = item.find('li','volume').text\n",
    "    except : volume = ''\n",
    "\n",
    "    date = ''\n",
    "    try : date = item.find('li','date').text\n",
    "    except : date = ''\n",
    "\n",
    "    abstract = ''\n",
    "    baseDetailUrl = \"https://www.dbpia.co.kr\"\n",
    "    pUrl = ''\n",
    "    try : pUrl = item.find('div','titWrap').find('a')['href']\n",
    "    except : pUrl = ''\n",
    "    if (pUrl != ''):\n",
    "        pUrl = baseDetailUrl + pUrl\n",
    "        driver.get(pUrl)\n",
    "        try : driver.find_element_by_xpath('//*[@id=\"#pub_modalOrganPop\"]').click()\n",
    "        except : pass\n",
    "        time.sleep(0.1)\n",
    "        try : driver.find_element_by_xpath('//*[@id=\"#pub_modalLoginPop\"]').click()\n",
    "        except : pass\n",
    "\n",
    "        try :\n",
    "            driver.find_element_by_xpath('//*[@id=\"pub_abstract\"]/div[2]/div/div[1]/div[2]/a').click()\n",
    "            eachPage = driver.page_source\n",
    "            ePsoup = BeautifulSoup(eachPage, 'html.parser')\n",
    "            abstract = ePsoup.find('div','abstFull').find('p','article').text\n",
    "        except : abstract = ''\n",
    "\n",
    "    titleL.append(title)\n",
    "    authorL.append(author)\n",
    "    # authorsL.append(authors)\n",
    "    publisherL.append(publisher)\n",
    "    journalL.append(journal)\n",
    "    volumeL.append(volume)\n",
    "    dateL.append(date)\n",
    "    abstractL.append(abstract)\n",
    "\n",
    "print(\"date to .csv file\")\n",
    "\n",
    "resultDict = dict(title = titleL,\n",
    "              author = authorL,\n",
    "              publisher = publisherL,\n",
    "              journal = journalL,\n",
    "              volume = volumeL,\n",
    "              date = dateL,\n",
    "              abstract = abstractL)\n",
    "\n",
    "fName = \"dbpia_{}_{}_{}.csv\".format(searchQ, startYear, endYear)\n",
    "\n",
    "DB = pd.DataFrame(resultDict)\n",
    "DB.to_csv(fName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
